ID,title,content,date
1aieu3j,Facts,N/A,2024-02-04 03:59:33
14442pi,"""We have great datasets""",N/A,2023-06-08 09:08:22
12t22p4,i just want sleep,N/A,2023-04-20 14:52:52
16mnj2y,I've finally built the perfect data pipeline!,N/A,2023-09-19 10:40:10
12zhvtk,PSA: Learn Vendor Agnostic Technologies!,N/A,2023-04-26 13:39:02
1arwf4u,"Had an onsite interview with one of FAANG, all 6 interviewers were Indian ","7 if I count the person who did phone screen. Had a positive experience with majority of the interviewers but hiring manager and another interviewer appeared very uninterested and seems didn’t even read my resume. Almost 0 coding and majority was behavioral questions despite the fact that this is mid level data eng position. 
With this much skewed perceived diversity, I can’t help thinking they’re looking for another person from their own culture. 


Edit:
Seems like many other also witness this trend: https://www.reddit.com/r/cscareerquestions/s/pnt5Zidl1X 

",2024-02-16 01:35:03
15ae6kp,The data engineer came to me... tears in his eyes,"Turns out databases are ""relational"" or something",2023-07-26 18:48:33
12m8ml7,Exporting to excel is always a people pleaser...,N/A,2023-04-14 18:52:24
1bd5wv8,It’s happening guys,N/A,2024-03-12 19:20:07
s054b4,2022 Mood,N/A,2022-01-09 23:43:39
xis5vv,Data driven organisations,N/A,2022-09-19 23:22:08
12l9mzx,Who owns data quality?,N/A,2023-04-13 22:40:57
1brqa92,Is this chart accurate?,N/A,2024-03-30 19:37:33
1aujhqw,How true is this!,Source: twitter,2024-02-19 09:39:22
oyju56,DataEngineering 2021 in one pic,N/A,2021-08-05 14:46:15
o210i3,I wrote a children's book / illustrated guide to Apache Kafka,"hi fellow Data Engineers, just wanted to share a beginner-friendly resource that I've been working on for Apache Kafka. I spent 3 months illustrating this digital book, which I'm calling Gently Down the Stream: [http://www.gentlydownthe.stream/](http://www.gentlydownthe.stream/)

I know that normal tech books that span hundreds of pages are often intimidating for beginners (I wrote one of those too, btw: [http://kafka-streams-book.com/](http://kafka-streams-book.com/)). So I just wanted to create something a little simpler, more colorful, playful, and fun. Hope you enjoy it!

&#x200B;

edit: wow, thanks for the kind words everyone!! I'll be creating more books like this in the future, and will be announcing them on my new Twitter account if you're interested in following: [https://twitter.com/\_round\_robin/](https://twitter.com/_round_robin/)",2021-06-17 16:15:50
1420fjz,I’ve had the definition wrong this entire time…,N/A,2023-06-06 02:19:44
udboyq,I've been a big data engineer since 2015. I've worked at FAANG for 6 years and grew from L3 to L6. AMA,"See title.

Follow me on YouTube here. I talk a lot about data engineering in much more depth and detail!  [https://www.youtube.com/c/datawithzach](https://www.youtube.com/c/datawithzach)  


Follow me on Twitter here [https://www.twitter.com/EcZachly](https://www.twitter.com/EcZachly)  


Follow me on LinkedIn here [https://www.linkedin.com/in/eczachly](https://www.linkedin.com/in/eczachly)

&#x200B;",2022-04-27 19:29:47
151xsis,"Data Scientists -- Ok, now I get it.",[DELETED] ` this message was mass deleted/edited with redact.dev `,2023-07-17 10:11:47
y0qipu,Your Snowflake credits at work.,N/A,2022-10-10 21:24:03
1bc0bkv,"ELI5: what is ""Self-service Analytics"" (comic)",N/A,2024-03-11 10:47:22
y3rxoe,It's amazing how many organizations workflows still revolve around Excel. I've seen CFOs and COOs folders filled with 20 different versions of the same Excel file.,N/A,2022-10-14 11:56:44
p8crow,The struggle is real.,N/A,2021-08-20 20:01:39
10mk6bc,The current data landscape,N/A,2023-01-27 12:57:49
18ffzmx,I Was Happier Being a Bartender Compared to Being a 6 Figure DE,"Used to be a resort bartender during college for 4 years. Been a DE for 3 years. Went to college for Information Science, graduated, got a job as a data engineer making $85K. Since then I’ve been working on my MSDS part time, and 3 shitty data jobs later I am making $110K. 

Data is interesting at its core but the industry ruins it. Layoffs everywhere, shitty tech stacks, bad stakeholders, boomer bosses, bait and switch job opportunities, remote roles being replaced with 100% on site. 

Overall this career path has been the worst decision of my life, despite looking good on paper. To say my life has declined in every way since going from bartender to DE is an understatement - I am absolutely miserable. Used to have friends, go on dates, socialize all day, have tons of free time. All of that is gone in favor of long hours, boring shit jobs, leetcode, living with parents after being thrown around the country for different toxic roles. 

Anyone else go into data and realize the grass wasn’t greener? I am dropping out of my MSDS and leaving this industry in January. This whole journey was a waste of ~6 years of work. ",2023-12-10 23:02:27
1945s14,Guess the data type ಠ_ಠ,N/A,2024-01-11 16:31:27
198kif0,"My company just put out 3 data engineering jobs last year, guess who we got?","As per title, my company put out 3 entry level data engineer jobs last year. The pay range was terrible, 60 - 80k. 

We ended up hiring a data engineer with 3 yoe at a Fortune 100, a data engineer with 1 yoe and a masters in machine learning, and a self taught engineer who has built applications that literally make my applications look like children's books. 

They've jumped on projects with some of our previous entry level hires from 2019-2022 and made them look like chumps. 

All of them were looking for jobs for at least 4-6 months. 

Just wanted to share a data point on the state of the market last year in 2023. 

Funny thing is that I don't expect any of them to stay when the job market picks up, and we may have a mass exodus on our hands. ",2024-01-17 01:44:50
p88ujn,"{""null""}",N/A,2021-08-20 16:37:01
18ix6hd,How Netflix does Data Engineering,"A collection of videos shared by Netflix from their [Data Engineering Summit](https://netflixtechblog.com/our-first-netflix-data-engineering-summit-f326b0589102)

* [The Netflix Data Engineering Stack](https://youtu.be/QxaOlmv79ls)
* [Data Processing Patterns](https://www.youtube.com/watch?v=vuyjK2TFZNk&list=PLSECvWLlUYeF06QK5FOOELvgKdap3cQf0&index=3)
* [Streaming SQL on Data Mesh using Apache Flink](https://www.youtube.com/watch?v=TwcWvwU7B64&list=PLSECvWLlUYeF06QK5FOOELvgKdap3cQf0&index=4)
* [Building Reliable Data Pipelines](https://www.youtube.com/watch?v=uWmJxbhI304&list=PLSECvWLlUYeF06QK5FOOELvgKdap3cQf0&index=5)
* [Knowledge Management — Leveraging Institutional Data](https://www.youtube.com/watch?v=F4N8AmScZ-w&list=PLSECvWLlUYeF06QK5FOOELvgKdap3cQf0&index=6)
* [Psyberg, An Incremental ETL Framework Using Iceberg](https://www.youtube.com/watch?v=jRckeOedtx0&list=PLSECvWLlUYeF06QK5FOOELvgKdap3cQf0&index=8)
* [Start/Stop/Continue for optimizing complex ETL jobs](https://www.youtube.com/watch?v=Dr8LMn-nJGc&list=PLSECvWLlUYeF06QK5FOOELvgKdap3cQf0&index=9)
* [Media Data for ML Studio Creative Production](https://youtu.be/1gGi3NBZk7M)",2023-12-15 10:32:06
109fhh8,Happy (or not so happy) Wednesday! What part of your technical work do you dread the most? What are you doing about it?,N/A,2023-01-11 20:47:16
p31jni,Was the data clean??,N/A,2021-08-12 14:58:18
szrg3i,Yep,N/A,2022-02-23 20:43:40
tuobs4,"Completed my first Data Engineering project with Kafka, Spark, GCP, Airflow, dbt, Terraform, Docker and more!","[Dashboard](https://github.com/ankurchavda/streamify/blob/main/images/dashboard.png?raw=true)

First of all, I'd like to start with thanking the instructors at the [DataTalks.Club](https://DataTalks.Club) for setting up a completely free [course](https://github.com/DataTalksClub/data-engineering-zoomcamp#data-engineering-zoomcamp). This was the best course that I took and the project I did was all because of what I learnt there :D.

TL;DR below.

# Git Repo:

[Streamify](https://github.com/ankurchavda/streamify)

# About The Project:

The project streams events generated from a fake music streaming service (like Spotify) and creates a data pipeline that consumes real-time data. The data coming in would is similar to an event of a user listening to a song, navigating on the website, authenticating. The data is then processed in real-time and stored to the data lake periodically (every two minutes). The hourly batch job then consumes this data, applies transformations, and creates the desired tables for our dashboard to generate analytics. We try to analyze metrics like popular songs, active users, user demographics etc.

# The Dataset:

[Eventsim](https://github.com/Interana/eventsim) is a program that generates event data to replicate page requests for a fake music web site. The results look like real use data, but are totally fake. The docker image is borrowed from [viirya's fork](https://github.com/viirya/eventsim) of it, as the original project has gone without maintenance for a few years now.

Eventsim uses song data from [Million Songs Dataset](http://millionsongdataset.com/) to generate events. I have used a [subset](http://millionsongdataset.com/pages/getting-dataset/#subset) of 10000 songs.

# Tools & Technologies

* Cloud - [**Google Cloud Platform**](https://cloud.google.com/)
* Infrastructure as Code software - [**Terraform**](https://www.terraform.io/)
* Containerization - [**Docker**](https://www.docker.com/), [**Docker Compose**](https://docs.docker.com/compose/)
* Stream Processing - [**Kafka**](https://kafka.apache.org/), [**Spark Streaming**](https://spark.apache.org/docs/latest/streaming-programming-guide.html)
* Orchestration - [**Airflow**](https://airflow.apache.org/)
* Transformation - [**dbt**](https://www.getdbt.com/)
* Data Lake - [**Google Cloud Storage**](https://cloud.google.com/storage)
* Data Warehouse - [**BigQuery**](https://cloud.google.com/bigquery)
* Data Visualization - [**Data Studio**](https://datastudio.google.com/overview)
* Language - [**Python**](https://www.python.org/)

# Architecture

[Streamify Architecture](https://preview.redd.it/lnpruyl305r81.jpg?width=1222&format=pjpg&auto=webp&s=48f5993d4a8e142b7c32846535ad1ae501e2a332)

# Final Dashboard

[Streamify Dashboard](https://preview.redd.it/0r3yfde005r81.png?width=1436&format=png&auto=webp&s=4d21c6f47ebbfe6f9201afdd4284ca83643e4e3b)

You can check the actual dashboard [here](https://datastudio.google.com/s/uVy77npmwvg). I stopped it a couple of days back so the data might not be recent.

# Feedback:

There are lot of experienced folks here and I would love to hear some constructive criticism on what things could be done in a better way. Please share your comments.

# Reproduce:

I have tried to document the project thoroughly, and be really elaborate about the setup process. If you chose to learn from this project and face any issues, feel free to drop me a message.

&#x200B;

**TL;DR:** Built a project that consumes real-time data and then ran hourly batch jobs to transform the data into a dimensional model for the data to be consumed by the dashboard.",2022-04-02 17:30:38
ygieh8,"Data engineering projects with template: Airflow, dbt, Docker, Terraform (IAC), Github actions (CI/CD) & more","Hello everyone,

Some of my posts about DE projects (for portfolio) were well received in this subreddit. (e.g. [this](https://www.reddit.com/r/dataengineering/comments/nto0nd/data_engineering_project_for_beginners_v2/) and [this](https://www.reddit.com/r/dataengineering/comments/om3wl5/data_engineering_project_with_a_live_dashboard/))

   But many readers reached out with difficulties in setting up the infrastructure, CI/CD, automated testing, and database changes. With that in mind, I wrote this article [https://www.startdataengineering.com/post/data-engineering-projects-with-free-template/](https://www.startdataengineering.com/post/data-engineering-projects-with-free-template/) which sets up an Airflow + Postgres + Metabase stack and can also set up AWS infra to run them, with the following tools

1. **`local development`**: [Docker](https://www.docker.com/) & [Docker compose](https://docs.docker.com/compose/)
2. **`DB Migrations`**: [yoyo-migrations](https://ollycope.com/software/yoyo/latest/)
3. **`IAC`**: [Terraform](https://www.terraform.io/)
4. **`CI/CD`**: [Github Actions](https://github.com/features/actions)
5. **`Testing`**: [Pytest](https://docs.pytest.org/en/7.1.x/)
6. **`Formatting`**: [isort](https://pycqa.github.io/isort/) & [black](https://github.com/psf/black)
7. **`Lint check`**: [flake8](https://github.com/pycqa/flake8)
8. **`Type check`**: [mypy](http://mypy-lang.org/)

I also updated the below projects from my website to use these tools for easier setup.

1. [DE Project Batch edition](https://www.startdataengineering.com/post/data-engineering-project-for-beginners-batch-edition/) Airflow, Redshift, EMR, S3, Metabase
2. [DE Project to impress Hiring Manager](https://www.startdataengineering.com/post/data-engineering-project-to-impress-hiring-managers/) Cron, Postgres, Metabase
3. [End-to-end DE project](https://www.startdataengineering.com/post/data-engineering-project-e2e/) Dagster, dbt, Postgres, Metabase

An easy-to-use template helps people start building data engineering projects (for portfolio) & providing a good understanding of commonly used development practices. Any feedback is appreciated. I hope this helps someone :) 

Tl; DR: Data infra is complex; use this template for your portfolio data projects 

Blog: https://www.startdataengineering.com/post/data-engineering-projects-with-free-template/
Code: https://github.com/josephmachado/data_engineering_project_template",2022-10-29 12:29:46
1553rxb,"Barbenheimer, Data Engineering edition",N/A,2023-07-20 21:24:56
obyzb9,When my prof asks me to “find information on every person whose been pardoned ever for the past 4 presidencies”,N/A,2021-07-02 00:28:43
1b1f95l,Expectation from junior engineer ,N/A,2024-02-27 15:56:43
zaqmay,If data engineering did Spotify Wrapped,N/A,2022-12-02 16:33:11
124d6qi,State of Data Engineering 2022,N/A,2023-03-28 04:49:55
10leclf,Follow up on that Google Drive question...,N/A,2023-01-26 00:19:57
uojh8n,Data Scientist: building a fabulous AI out of garbage,N/A,2022-05-13 04:16:53
whz7pw,Anyone read this book? It came out in 2022 so it's very modern and up to date.,N/A,2022-08-06 21:50:03
lio4nh,"How I feel in the coding interview when I get asked about BSTs and I damn well know all I’m going to do is call apis, parse json, and copy to Redshift.",N/A,2021-02-12 23:48:50
y2bl65,"What’s your process for deploying a data pipeline from a notebook, running it, and managing it in production?",N/A,2022-10-12 18:32:20
sexic6,How I feel today,N/A,2022-01-28 18:39:55
qr5z5v,"Ladies and gentlemen, I have good news and I wouldn't have been able to do it without this wholesome and helpful community",N/A,2021-11-10 22:28:13
otwwfe,This nice illustration or visualization of the Data Pipeline by semantix (https://semantix.com.br/data-platform/). Hope it may some insights for new DE friends.,N/A,2021-07-29 12:53:02
v4hhz7,Just getting into Apache Airflow...this is the first thing that came to mind,N/A,2022-06-04 04:52:52
1767l40,Just do a quick 30min to 1hr take home test. 🤡 🤡,This is UMortgage interview assessment. Reads to me like free work more than skills assessment.,2023-10-12 13:55:58
10kl6lg,Finally got a job,"I did it! After 8 months of working as a budtender for minimum wage post-graduation, more than 400 job applications, and 12 interviews with different companies I finally landed a role as a data engineer. I still couldn't believe it till my first day, which was yesterday. Just got my laptop, fob, and ID card, still feels so unreal. Learned a lot from this sub and I'm forever grateful for you guys.",2023-01-25 00:31:50
rr7paf,I'm Leaving FAANG After Only 4 Months,"I apologize for the clickbaity title, but I wanted to make a post that hopefully provides some insight for anyone looking to become a DE in a FAANG-like company. I know for many people that's the dream, and for good reason. Meta was a fantastic company to work for; it just wasn't for me. I've attempted to explain why below.

## It's Just Metrics
I'm a person that really enjoys working with data early in its lifecycle, closer to the collection, processing, and storage phases. However, DEs at Meta (and from what I've heard all FAANG-like companies) are involved much later in that lifecycle, in the analysis and visualization stages. In my opinion, DEs at FAANG are actually Analytics Engineers, and a lot of the work you'll do will involve building dashboards, tweaking metrics, and maintaining pipelines that have already been built. Because the company's data infra is so mature, there's not a lot of pioneering work to be done, so if you're looking to _build_ something, you might have better luck at a smaller company.

## It's All Tables
A lot of the data at Meta is generated in-house, by the products that they've developed. This means that any data generated or collected is made available through the logs, which are then parsed and stored in tables. There are no APIs to connect to, CSVs to ingest, or tools that need to be connected so they can share data. It's just tables. The pipelines that parse the logs have, for the most part, already been built, and thus your job as a DE is to work with the tables that are created every night. I found this incredibly boring because I get more joy/satisfaction out of working with really dirty, raw data. That's where I feel I can add value. But data at Meta is already pretty clean just due to the nature of how it's generated and collected. If your joy/satisfaction comes from helping Data Scientists make the most of the data that's available, then FAANG is definitely for you. But if you get your satisfaction from making unusable data usable, then this likely isn't what you're looking for.

## It's the Wrong Kind of Scale
I think one of the appeals to working as a DE in FAANG is that there is just so much data! The idea of working with petabytes of data brings thoughts of how to work at such a large scale, and it all sounds really exciting. That was certainly the case for me. The problem, though, is that this has all pretty much been solved in FAANG, and it's being solved by SWEs, not DEs. Distributed computing, hyper-efficient query engines, load balancing, etc are all implemented by SWEs, and so ""working at scale"" means implementing basic common sense in your SQL queries so that you're not going over the 5GB memory limit on any given node. I much prefer ""breadth"" over ""depth"" when it comes to scale. I'd much rather work with a large variety of data types, solving a large variety of problems. FAANG doesn't provide this. At least not in my experience.

## I Can't Feel the Impact
A lot of the work you do as a Data Engineer is related to metrics and dashboards with the goal of helping the Data Scientists use the data more effectively. For me, this resulted in all of my impact being along the lines of ""I put a number on a dashboard to facilitate tracking of the metric"". This doesn't resonate with me. It doesn't motivate me. I can certainly understand how some people would enjoy that, and it's definitely important work. It's just not what gets me out of bed in the morning, and as a result I was struggling to stay focused or get tasks done. 

In the end, Meta (and I imagine all of FAANG) was a great company to work at, with a lot of really important and interesting work being done. But for me, as a Data Engineer, it just wasn't my thing. I wanted to put this all out there for those who might be considering pursuing a role in FAANG so that they can make a more informed decision. I think it's also helpful to provide some contrast to all of the hype around FAANG and acknowledge that it's not for everyone and that's okay. 

## tl;dr
I thought being a DE in FAANG would be the ultimate data experience, but it was far too analytical for my taste, and I wasn't able to feel the impact I was making. So I left.",2021-12-29 13:02:08
1aggfae,"Got a flight this weekend, which do I read first?",I’m an Analytics Engineer who is experienced doing SQL ETL’s. Looking to grow my skillset. I plan to read both but is there a better one to start with?,2024-02-01 17:28:14
yyh6l9,What are your favourite GitHub repos that shows how data engineering should be done?,"Looking to level up my skills and want to know what repos out there follow good data engineering practice.

What accounts/repos stood out to you?

Which repos do you find yourself peeking at from time to time?

Which ones taught you something that you didn't already know?",2022-11-18 10:50:18
owkdly,53+ years of experience in data engineering…,N/A,2021-08-02 18:16:35
1ajyazo,Is there a DE equivalent to this?,Thought about posting in r/DataAnalysis but figured it fit here more as this is the exact reason I am trying so hard to leave my DA role and get into DE.,2024-02-06 01:56:18
zdj8y1,Data engineering with ChatGPT,N/A,2022-12-05 20:51:31
yfuknq,It's not always Old Man Jenkins...,N/A,2022-10-28 17:15:39
1b7ojk4,An actual post in my company Slack today ,Mentally preparing myself for the eventual request to untangle this mess ,2024-03-06 02:44:28
19bg4jf,I’m releasing a free data engineering boot camp in March,"Meeting 2 days per week for an hour each. 

Right now I’m thinking: 

- one week of SQL
- one week of Python (focusing on REST APIs too) 
- one week of Snowflake 
- one week of orchestration with Airflow
- one week of data quality 
- one week of communication and soft skills 

What other topics should be covered and/or removed? I want to keep it time boxed to 6 weeks. 

What other things should I consider when launching this? 

If you make a free account at dataexpert.io/signup you can get access once the boot camp launches. 

Thanks for your feedback in advance!",2024-01-20 16:55:10
14zh0hc,"It's not a glamorous life, but we all know who really drives the bus",N/A,2023-07-14 13:50:59
prlh97,"Big Data Pipelines on AWS, Azure & GCP (associated blog in comments)",N/A,2021-09-20 02:06:44
104xlft,Free Data engineering bootcamp - Data Engineering Zoomcamp - starts in 10 days," Do you want to learn Data Engineering? In 2023, we start another iteration of Data Engineering Zoomcamp. It's a free, practical, 10-week long course about the main concepts in Data Engineering.   


Join us to learn about:

* Docker, Terraform and GCP
* Orchestration with Prefect
* Creating a data warehouse with BigQuery
* Analytics engineering and dbt
* Batch processing and Spark
* Stream processing and Kafka

It starts on the 16th of January, 2023.  

Sign up here: [https://github.com/DataTalksClub/data-engineering-zoomcamp](https://github.com/DataTalksClub/data-engineering-zoomcamp)",2023-01-06 15:54:24
y9xa2k,"It is a recession after all, isn't it?",N/A,2022-10-21 15:56:57
149urpv,"The ""Big Three's"" Data Storage Offerings",N/A,2023-06-15 06:24:23
y7dxl4,How are you exporting your prod DB tables to your data warehouse?,N/A,2022-10-18 17:54:49
18ak69g,What opinion about data engineering would you defend like this?,N/A,2023-12-04 13:27:11
14midyu,Now in Snowflake: GROUP BY ALL,N/A,2023-06-29 22:20:32
nh53m5,Is there any interest in a collection of DE interview practice questions (Data Modeling/Architecture/SQL),"I’ve done A LOT of Senior DE interviews at FAANG etc recently. I noticed that there isn’t a lot of real world interview questions online, you sort of have to rely on previous work experience. 

So now I’m building a InterviewCake style website for DE questions. It’s gonna be more subjective than algorithms obviously but I think there’s a lot of value in having questions to mull over.

Would people be interested in something like this?",2021-05-20 16:44:50
1au9s4s,New DE advice from a Principal,"So I see a lot of folks here asking how to break into Data Engineering, and I wanted to offer some advice beyond the fundamentals of learning tool X. I've hired and trained dozens of people in this field, and at this point I've got a pretty solid sense of what makes someone successful in it. This is what I'd personally recommend.


1. Focus on SWE fundamentals. The algorithms and algebra you learned in school can feel a little impractical for day-to-day work, but they're the core of the powerful distributed processing engines you work with in DE. Moving data around efficiently requires a strong understanding of hardware behavior and memory management. Orchestration tools like Airflow are just regular applications with servers and API's like anything else. Realistically, you're not going to walk into your first DE job with experience with DE tools, but you can reason through solutions based on what you know about software in general. The rest will come with time and training.


2. Learn battle-tested modeling and architecture patterns and where to apply them. Again, the fundamentals will serve you very well here. Data teams are often tasked with handling data from all over the company, across many contexts and business domains. Trying to keep all of that straight and building bespoke solutions for each one will not only drive you insane, but will end up wasting a ton of time and money reinventing the wheel and reverse-engineering long-forgotten one-offs. Using durable, repeatable patterns is one way to avoid that. Get some books on the subject and start reading.


3. Have a clear Definition of Done for your projects that includes quality controls and ongoing monitoring. Data pipelines are uniquely vulnerable to changes entirely outside of your control, since it's highly unlikely that you are the producer of the input data. Think carefully about how eventual changes in upstream data would affect your workload - where are the fragile points, and how you can build resiliency into them. You don't have to (and realistically can't) account for every scenario upfront, but you can take simple steps to catch issues before they reach the CEO's dashboard.


4. This is a team sport. Empathy for stakeholders and teammates, in particular assuming good intentions and that previous decisions were made for a good reason, is the #1 thing I look for in a candidate outside of reasoning skills. I have disqualified candidates for off-handed comments about colleagues ""not knowing what they're talking about"", or dragging previous work when talking about refactoring a pipeline. Your job as a steward for the data platform is to understand your stakeholders and build something that allows them to safely and effectively interact with it. It's a unique and complex system which they likely don't, and shouldn't have to, have as deep an understanding of as you do. Behave accordingly. 


5. Understand what responsible data stewardship looks like. Data is often one of, if not the most, expensive line item for a company. As a DE you are being trusted with the thing that can make or break a company's success both from a cost and legal liability perspective. In my role I regularly make architecture decisions that will cost or pay someone's salary - while it will probably take you a long time to get to that point, being conscientious of the financial impact/risk of your projects makes the jobs of people who do have to make those decisions (the ones who hire and promote you) much easier. 


6. Beware hype trains and silver bullets. Again, I have disqualified candidates of all levels for falling into this trap. Every tool, language, and framework was built (at least initially) to solve a specific problem, and when you choose to use it you should understand what that problem is. You're absolutely allowed to have a preferred toolbox, but over-indexing on one solution is an indicator that you don't really understand the problem space or the pitfalls of that thing. I've noticed a significant uptick in this problem with the recent popularity of AI; if you're going to use/advocate for it, you'd better be prepared to also speak to the implications and drawbacks.


Honorable mention: this may be controversial but I strongly caution against inflating your work experience in this field. Trust me, they'll know. It's okay and expected that you don't have big data experience when you're starting out - it would be ridiculous for me to expect you to know how to scale a Spark pipeline without access to an enterprise system. Just show enthusiasm for learning and use what you've got to your advantage.


I believe in you! You got this.

Edit: starter book recommendations in this thread https://www.reddit.com/r/dataengineering/s/sDLpyObrAx",2024-02-19 00:30:46
mwr0ji,Happened in the making of this meme as well,N/A,2021-04-23 08:55:29
yx2qsb,How are you monitoring your data pipelines and what are you using to debug production issues?,N/A,2022-11-16 19:03:13
15f9uab,Fancy dashboards with volatile data pipelines!,N/A,2023-08-01 11:29:09
13f4sbf,I didn’t know you guys were paid THIS well,N/A,2023-05-12 00:09:48
xkyzt8,I like caravans more.,N/A,2022-09-22 11:39:26
1abmrzv,"yes, I really said it",N/A,2024-01-26 16:45:56
uh5juq,worried about the transition from data engineering to big data engineer,"I've been building ETL pipelines, warehousing solutions, doing schema design, and setting up data-streaming for several years now at companies like Amazon and more recently at smaller pre-ipo unicorn companies. My concern is that I'm only 5'11 (5'10.5"" really but whose counting) and my concern is that I may not be qualified to call myself a big data engineer as a result. Does anyone else have this problem? Thank you.",2022-05-03 02:00:11
s59bpv,Free data engineering course starts tomorrow!,"&#x200B;

https://preview.redd.it/l9vrn2r741c81.jpg?width=800&format=pjpg&auto=webp&s=46c01ba2b0792f43dbd9d11fc0a952ae61b6d567

We at [DataTalks.Club](https://DataTalks.Club) are running a free data engineering course.

&#x200B;

We'll cover:

* GCP, Terraform, Docker, SQL
* Data pipelines orchestration (Airflow)
* Data warehousing (Big Query)
* Analytics engineering (DBT)
* Batch processing (Spark)
* Streaming (Kafka)

More details here: [https://github.com/DataTalksClub/data-engineering-zoomcamp](https://github.com/DataTalksClub/data-engineering-zoomcamp)

&#x200B;

See you tomorrow!",2022-01-16 10:44:04
14663ur,r/dataengineering will be joining the blackout from June 12-14 to protest the proposed API changes which will end 3rd party apps.,"[See here for the original r/dataengineering thread on this issue.](https://www.reddit.com/r/dataengineering/comments/140xtxu/does_the_de_community_want_to_join_the_reddit/)

# What's going on?

A recent Reddit policy change threatens to kill many beloved third-party mobile apps, making a great many quality-of-life features not seen in the official mobile app **permanently inaccessible** to users.

On May 31, 2023, Reddit announced they were raising the price to make calls to their API from being free to a level that will kill every third party app on Reddit, from [Apollo](https://www.reddit.com/r/apolloapp/comments/13ws4w3/had_a_call_with_reddit_to_discuss_pricing_bad/) to [Reddit is Fun](https://www.reddit.com/r/redditisfun/comments/13wxepd/rif_dev_here_reddits_api_changes_will_likely_kill/) to [Narwhal](https://www.reddit.com/r/getnarwhal/comments/13wv038/reddit_have_quoted_the_apollo_devs_a_ridiculous/jmdqtyt/) to [BaconReader](https://www.reddit.com/r/baconreader/comments/13wveb2/reddit_api_changes_and_baconreader/).

Even if you're not a mobile user and don't use any of those apps, this is a step toward killing other ways of customizing Reddit, such as Reddit Enhancement Suite or the use of the old.reddit.com desktop interface.

This isn't only a problem on the user level: many subreddit moderators depend on tools only available outside the official app to keep their communities on-topic and spam-free.

# What's the plan?

On June 12th, [many subreddits](https://old.reddit.com/r/ModCoord/comments/1401qw5/incomplete_and_growing_list_of_participating/) will be going dark to protest this policy. Some will return after 48 hours: others will go away *permanently* unless the issue is adequately addressed, since many moderators aren't able to put in the work they do with the poor tools available through the official app. This isn't something any of us do lightly: we do what we do because *we love Reddit*, and we truly believe this change will make it impossible to keep doing what we love.

The two-day blackout isn't the *goal*, and it isn't the end. Should things reach the 14th with no sign of Reddit choosing to fix what they've broken, we'll use the community and buzz we've built between then and now as a tool for further action.

What can *you* do?

1. **Complain.** Message the mods of [r/reddit](https://www.reddit.com/r/reddit/).com, who are the admins of the site: message [/u/reddit](https://www.reddit.com/u/reddit/): submit a [support request](https://support.reddithelp.com/hc/en-us/requests/new): comment in relevant threads on [r/reddit](https://www.reddit.com/r/reddit/), such as [this one](https://www.reddit.com/r/reddit/comments/12qwagm/an_update_regarding_reddits_api/), leave a negative review on their official iOS or Android app- and sign your username in support to this post.
2. **Spread the word.** Rabble-rouse on related subreddits. Meme it up, make it spicy. Bitch about it to your cat. Suggest anyone you know who moderates a subreddit join us at our sister sub at [r/ModCoord](https://www.reddit.com/r/ModCoord/) \- but please don't pester mods you *don't* know by simply spamming their modmail.
3. **Boycott** ***and*** **spread the word...to Reddit's competition!** Stay off Reddit entirely on June 12th through the 13th- instead, take to your favorite *non*\-Reddit platform of choice and make some noise in support!
4. **Don't be a jerk.** As upsetting this may be, threats, profanity and vandalism will be worse than useless in getting people on our side. Please make every effort to be as restrained, polite, reasonable and law-abiding as possible.

&#x200B;

*Any communication during the blackout will be made via* [*our official mailing list*](https://dataengineeringcommunity.substack.com/)*. Please sign up if you wish to receive updates.*",2023-06-10 17:34:04
zr2klf,ETL using pandas,N/A,2022-12-20 23:01:14
10fg07o,just got laid off (FAANG),"hi all, its been a pretty awful day. Two months ago my boss transferred out to a new team, my team was integrated in with another. Sure enough this morning i woke up to an email letting me know im no longer a valued member of the team.   


at this point im feeling in the pits. over the last few years ive gone above and beyond for this company, spending untold unpaid evening and weekend hours trying to push our projects forward.   


not sure about the next steps, but i felt like i needed to vent. if anyone out there knows of any DE opportunities, please DM me.",2023-01-18 19:18:49
vkfs57,"I created a pipeline extracting Reddit data using Airflow, Docker, Terraform, S3, dbt, Redshift, and Google Data Studio","Dashboard \~ [Link](https://datastudio.google.com/reporting/e927fef6-b605-421c-ae29-89a66e11ea18)

Github Project - [Link](https://github.com/ABZ-Aaron/Reddit-API-Pipeline)

# Overview

Built this a while ago, but refactored recently.

I put it together after going through the DataTalksClub [Zoomcamp](https://github.com/DataTalksClub/data-engineering-zoomcamp). The aim was develop basic skills in a number of tools and to visualise r/DataEngineering data over time.

I'm currently learning DE, so project is FAR from perfect, and tools used are very much overkill, but it was a good learning experience. 

I've written out the README in a way that others can follow along, set it up themselves without too much trouble, and hopefully learn a thing or two.

# Pipeline

1. Extract r/dataengineering data using the Reddit API.
2. Load file into AWS S3.
3. Copy file data to AWS Redshift.
4. Orchestrate the above with Airflow & Docker on a schedule.
5. Run some VERY basic transforms with dbt (not necessary)
6. Visualise with Google Data Studio
7. Setup (and destroy) AWS infra with Terraform

# Notes

Redshift only had a 2 month free trial, so I've destroyed my cluster. The source for my dashboard is now a CSV with some data I downloaded from Redshift before shutting down. I may create an alternate pipeline with more basic & free tools.",2022-06-25 14:05:54
14vw6y3,Typical interview with Airflow enjoyer,N/A,2023-07-10 14:50:32
13l9ur0,DBT lays off 15% of their staff,"DBT will be reducing their headcount by 15% of their global team. This reduction will impact every function of the business. 

My team had to migrate away from DBT after their price hike, so this is not surprising.


https://www.getdbt.com/blog/dbt-labs-update-a-message-from-ceo-tristan-handy/",2023-05-18 20:26:44
uyc87m,"""You wouldn't understand, it is not SQL!!"" - An advice for upcoming data engineers.","TLDR; Advocate for yourself as an engineer, don't let other people tell you what you know. Show them.

A bit about me before the actual story: I've worked for a mid-size tech company in the HR industry for the past 2 years as a data engineer. Before this, I've held back-end roles and other data engineer positions in companies ranging from seed-stage startups to banks. I've maintained codebases in Java, Python, Scala, and Go alongside some of the smartest people in the data industry. Saying all this not to brag, but just to show you guys I am not just a random guy with no credentials. I am an engineer.

On to the story,

4 months back the CTO hired an Engineer Manager (let's call him Scott) to help improve our workflows and mature the company from an organizational standpoint. The first red flag came when the data engineering team (myself and three other people) were not invited to the biweekly engineering meetings created by Scott. No biggie, could have been just a mistake, nothing to get mad about. When one of my co-workers nicely asked Scott to share the zoom link with us, he said ""Umm. We are not talking about anything related to analytics in those meetings but I'll add you if you insist"". Oh boy.

Two weeks later, I tried to get Scott to meet with us to talk about our data architecture and the different projects we had going on at the moment (Projects that had several SENIOR software engineers in it btw). His response?

""I appreciate the offer but I really should take my first few weeks at the company to get up to speed with my engineering team about THEIR projects. We can discuss data analytics at a later time.""

The audacity of this man!

I quickly responded with an email with the CTO cc'd saying how urgent some of our work was and how we would appreciate a meeting with the leadership team (Scott included) to talk about it. The CTO immediately send google an invite to a meeting with all of us and thanked the data team for driving software talks with the new manager. But apparently, that wasn't enough for Scott to understand he was being dismissive of our highly technical team just because we had the word ""data"" in front of our titles.

Here is the worst part and the reason for the post:

The front-end team has different Github repos for code written in React and Python. There was a problem in one of their visualizations where the data showed to clients was completely different from what we have in our database. Because of this, I created a ticket to try to pair with one of the front-end folks to debug the code behind the reports and for them to share the GitHub repo with me. Come to find out, the ticket was removed from the front-end inbox a few days later!

What was left was a comment from Scott on why he removed the ticket, saying:

""Removed from the team's backlog and moved to a later date. There is no reason to share GitHub credentials with the data team as the dashboard is written in Python, not in Raw SQL""

The audacity, the gall, the gumption, the nerve of this man!

This is where I want to give a bit of advice to future data engineers/SWEs, don't take this kind of talk lightly, advocate your abilities and your expertise at every step of your career. When someone is deliberately dismissing your abilities as an engineer or even your years of experience, stand up and correct them. Otherwise, your role within your organization will become diminished as people won't trust you can get the job done. This happens more often than you think, especially when you encounter managers/senior engineers who are know-it-alls who think their shit doesn't stink.

Now, I was irate. As I said before, I'm an engineer who is passionate about data. Not a data analytics guy who only understands excel and SQL. So I immediately screenshotted Scott's comment, send it to our team and the CTO in a slack channel, and wrote how Scott's actions were excluding us from the engineering team, making our work much harder, and somewhat denigrating our abilities. I ended the message with something like:

""I know he is new to the team, but I don't think we must advocate our resumes to him. Our titles should be sufficient.""

A couple of days later, Scott met with our team for the first time to apologize to us. a little too late if you ask me.",2022-05-26 17:01:52
12ekdv2,Data engineers processing data access requests,N/A,2023-04-07 12:54:01
tfm2hx,This job at Chewy looks very interesting.,N/A,2022-03-16 16:31:32
v8k8gs,Me when the DAG run fails,N/A,2022-06-09 16:08:00
13umeek,"So I watched a few videos about Fabric, and started to cry a little...",[DELETED] ` this message was mass deleted/edited with redact.dev `,2023-05-29 05:58:16
wriowg,"If you know, you know",N/A,2022-08-18 12:46:42
nto0nd,Data Engineering project for beginners V2,"Hello everyone,

A while ago, I wrote an [article](https://www.reddit.com/r/dataengineering/comments/gq2bmf/data_engineering_project_for_beginners/) designed to help people who are new to data engineering, build an end-to-end data pipeline and learn some of the best practices in data engineering.

Although this article was well-received, it was hard to set up, follow, and used Airflow 1.10. Hence, I  made setup easy, made code more understandable, and upgraded to Airflow 2.

Blog: [https://www.startdataengineering.com/post/data-engineering-project-for-beginners-batch-edition](https://www.startdataengineering.com/post/data-engineering-project-for-beginners-batch-edition)

Repo: [https://github.com/josephmachado/beginner\_de\_project](https://github.com/josephmachado/beginner_de_project)

Appreciate any questions, feedback, comments. Hope this helps someone.",2021-06-06 15:18:03
164ybot,Pathway from Data Analyst to Data Engineer: Tips & Takeaways,"Long time lurker here looking for some feedback!

I'm delivering an internal talk to other consultants at my company (primarily Data Analytics Consultants) about my personal journey to become a Data Engineer ( Photographer --> DA --> DE ).

I've been compiling a list of tips and takeaways to punctuate my talk and I'm hoping to get some input from the r/dataengineering brain trust.

**Here's what I've got (in no particular order):**

1. **Data Engineering is fundamentally just moving data around**, and reshaping it.
2. **Be curious.** Learn how things work. Try stuff out. Experiment.
3. Become **intimately familiar with data types**, sources, and structures.
4. **Learn a General Purpose Programming Language**. It doesn't really matter which one, it's the fundamentals that are important—everything else is just syntax.
   1. If you don't know which one to pick, start with Python
5. **Get good at SQL**. It's nearly 50 years old and you're probably going to retire before it does.
   1. No matter what systems and tools you use there's a good chance that it probably uses SQL or something pretty similar.
   2. Even more modern data stores, like data lakes, are still queried using SQL
6. **Learn to use the command line (PowerShell, CMD or Bash).** There are so many problems that can be solved much faster in the terminal.
7. **Learn how computers work, at least a little bit.** How do they communicate? How do they process and store information? What does a server do?
8. **Do as many personal projects as you can.** Sign up for a GitHub account and publish them there.
9. **Get really comfortable using APIs and parsing JSON data.** Outside of databases this is probably how you're going to interact with most of your data.
10. **Get really good at your tools, and then get better.** But, also be at least familiar with what else is out there.
11. **Understand the differences between a Database, Data Lake, and Data Warehouse.** What's the difference between OLTP and OLAP?
12. **Learn to use a Cloud Platform.** AWS has a pretty good Free Tier, try it out and learn what the different services do.
13. **Strong business knowledge is extremely valuable** in both Data Engineering and Data Analytics.
14. Understand different business metrics and how they're calculated.
15. **Learn to find the grain (level of detail) of data.** How is it structured? What is the smallest unit? What exactly is a ""row"" in this table?
16. **When it comes to data, everything (almost) is either a JSON, XML, CSV/\*SV, SQLite, or Database.**
17. Even proprietary files with different extensions are probably one of these. Tableau and Alteryx files are just XML files, and many applications store data in .db files (SQLite).
18. Sometimes a file is just a zipped folder of files. Excel for example is just a zipped folder of XML files.

That's what I've got so far, but the talk is next week so I've got some time to make changes.

What did I miss? What should I remove?

Thanks Team! 😘

Edit: fixed some indenting issues. 14 -> 13.1; 15 -> 14; 16 -> 15; 17 -> 15.1; 18 -> 15.2.

Edit 2: nvm, I'm not allowed to have nice things.",2023-08-29 23:36:15
129w7vp,I got the job!,"I felt the need to let everyone on this subreddit know I got my dream job offer.

You gave me a bollocking for calling OLAP cubes outdated. I'm sorry I pissed all of you off. 

You pointed out I'm applying for the wrong jobs, and the platform engineering roles are sometimes hidden in devops and software engineering adverts.

You advised that an in-person second stage interview is likely to be a whiteboarding session when I didn't know what to expect.

I made it!

Thank You!",2023-04-02 19:58:51
p3kpq9,1 year of must-read articles,"Last week I've featured 1 year of must-read content about data in one post. If you want to understand better trendy concepts: Modern Data Stack, Data Mesh, Analytics Engineering you can start by reading those articles.

I add directly in Reddit the reading list, but if you want to read my opinion on the matter or support this kind of content do not hesitate to subscribe to the weekly newsletter [there](https://www.blef.fr/data-news-must-read-articles/).

## Modern Data Stack

* [The Modern Data Stack: Past, Present, and Future](https://blog.getdbt.com/future-of-the-modern-data-stack/)
* [Why the Future of ETL Is Not ELT, But EL(T)](https://airbyte.io/blog/why-the-future-of-etl-is-not-elt-but-el) \+ Reddit thread about [Is it just me or ELT seems over hyped?](https://www.reddit.com/r/dataengineering/comments/o5jjig/is_it_just_me_or_elt_seems_over_hyped/)
* [Data & Data Engineering — the past, present, and future](https://medium.com/@eczachly/data-data-engineering-the-past-present-and-future-ac3ad5795ddf)
* [Building The Modern Data Team](https://pedram.substack.com/p/modern-data-team)

## Data Mesh

* [What the Heck is a Data Mesh?!](https://cnr.sh/essays/what-the-heck-data-mesh)
* [Building a data mesh to support an ecosystem of data products at Adevinta](https://medium.com/adevinta-tech-blog/building-a-data-mesh-to-support-an-ecosystem-of-data-products-at-adevinta-4c057d06824d)
* [Orginal writeup: Data monolith to mesh](https://martinfowler.com/articles/data-monolith-to-mesh.html)

## Data Engineering

* [Data Engineering Roadmap](https://github.com/datastacktv/data-engineer-roadmap) — DataStack drawn an awesome roadmap to discover all data engineering concepts. A must seen.
* [How Data Engineering Works](https://www.youtube.com/watch?v=qWru-b6m030) — A YouTube video describing and illustrating in 14 minutes how Data Engineering works.
* [Data Engineering Manifesto](https://medium.com/connecting-dots-blog/the-data-engineering-manifesto-37626aaa208f) — I love it. A poster with 9 principles regarding data engineering.
* [Data Engineering in 5400-words](https://twitter.com/chipro/status/1357329955131191298?ref_src=twsrc%5Etfw%7Ctwcamp%5Etweetembed%7Ctwterm%5E1357329955131191298%7Ctwgr%5E%7Ctwcon%5Es1_&ref_url=https%3A%2F%2Fwww.redditmedia.com%2Fmediaembed%2Flclm3s%3Fresponsive%3Dtrueis_nightmode%3Dfalse) — Chip Huyen wrote a huge Google Doc with her lecture note on the basics of data engineering.
* [One Skill Every Data Engineer Needs](https://medium.com/geekculture/the-most-important-skill-for-data-engineers-46fc1faff6ae)
* [We Don't Need Data Scientists, We Need Data Engineers](https://www.mihaileric.com/posts/we-need-data-engineers-not-data-scientists/)
* [Introduction to Databases](https://github.com/oleg-agapov/data-engineering-book/blob/master/book/2-beginner-path/2-1-databases/databases.md)

## Data analytics (and teams)

* [How should our company structure our data team?](https://medium.com/snaptravel/how-should-our-company-structure-our-data-team-e71f6846024d)
* [Analytics is at a crossroads](https://benn.substack.com/p/analytics-is-at-a-crossroads) \+  [Against SQL](https://scattered-thoughts.net/writing/against-sql) \+ [For SQL](https://pedram.substack.com/p/for-sql)
* [What makes a data analyst excellent (part 2)](https://towardsdatascience.com/what-makes-a-data-analyst-excellent-17ee4651c6db)

## Data Lineage, cataloging, observability, etc.

This part is present in the original post but more an opening for later waiting for consolidation in that space.

&#x200B;

That's all for me. I hope you'll like this kind of curation content (I know a lot of this have been already published here but sporadically) and if you think something should be added here I open the discussion 👇",2021-08-13 10:28:20
1b44v59,Why are there so many ETL tools when we have SQL and Python?,"I've been wondering why there are so many ETL tools out there when we already have Python and SQL. What do these tools offer that Python and SQL don't? Would love to hear your thoughts and experiences on this.

And yes, as a junior I’m completely open to the idea I’m wrong about this😂",2024-03-01 20:39:32
xvr7sm,The only insightful venn diagram I've ever made,N/A,2022-10-04 21:26:19
10lsa27,Don't Fall for the Hype: A Data Professional's Perspective on Familiar Concepts Rebranded as Innovations,"As a data professional with 20 years of experience, I've seen repeated terms in tech over and over again. Today, I discovered ""**Personalized API**"", yet another new term for something that already existed. It's similar to an **Analytics API**, **Semantic Layer**, and other existing technologies.

It's important to remember that just because a term is new, doesn't mean the technology is. Data modeling with **Kimball/Inmon**, Reverse ETL, Data Mesh, and Data Lakes are just a few examples of familiar concepts that are being rebranded as new innovations.

**One Big Table (OBT)**, mainly a big denormalized table, is the same as core and data marts **Dimensional Modeling** by Kimball/Inmon, published initially in February 1996, using **Materialized Views** to persist them.

**Reverse ETL** is just an additional step to the existing data pipeline, or you might call it **Master Data Management (MDM)**, where Business people in typically ""stewardship"" add business data back to the DWH. **Semantic Layers** have been here since the beginning of Business Intelligence tools (called BO Universe); one could say it's also a fancy term for **OLAP Cubes**. Highly praised, although they have existed since the beginning of BI with SSAS (MS), OBIEE (Oracle), and SAP BI/BW (SAP).

**Data Mesh** is another hyped term and another name for microservices, which we fought a couple of years back, breaking out of monolithic **Data Warehouses** or B**usiness Intelligence** applications. And if you want to add another buzzword here, **software-defined assets**, try a similar thing with defining each **Data Product** as an asset. **Data Contracts**, haven't we validated schemas and data types all our life? 😉 Encoding data tests in our applications, sometimes through creating an abstraction in-between with an API, sometimes within the ETL tool as part of **Data Governance**.

**Data Lake** is another term for a **Data Warehouse** on top of distributed files. A **Lakehouse** is a data warehouse based on open standards.

Let's not get caught up in buzzwords and hype. Let's focus on understanding the technology and its capabilities, rather than the name it's given. Have you encountered similar situations in your career, or what terms reminded you of something you learned long ago?",2023-01-26 13:52:34
10e5fus,Job search for Data Engineering in Stockholm (2yoe),N/A,2023-01-17 06:49:14
12j7r5a,can't wait for an end to end python stack with no JVM,N/A,2023-04-12 03:04:04
v9hwf8,My Job Search as a Mid-Level Data Engineer since March 2022,N/A,2022-06-10 21:20:12
12meohj,One day we’ll get the respect we deserve 🥲,N/A,2023-04-14 21:45:47
150qcx2,"Is this fear-mongering, or is this actually truthful?",N/A,2023-07-15 23:24:51
18xj97r,Why does nothing ever get used?,"Dashboards, views, tables, pipelines, entire data marts. Why does 90% of the work I do never get used?   

I used to be one of the best BA's in my entire company so I am very good at requirements gathering and understanding what the business is trying to accomplish. Most of the work that I get comes from the CEO/VP level (global corporation not startup so real CEO and real VP 's) so a lot of people seem like they are very invested in solving these problems and my work always gets rave reviews.....but once things go into prod they basically never get touched.  

Six months ago I just.... stopped doing QA.. I have been relying on the ""scream test"", I mark tickets resolved and immediately move to prod and only do QA if someone screams that something is wrong. I have yet to hear back on anything.",2024-01-03 13:20:50
16vhp70,Worst Data Engineering Mistake youve seen?,"I started work at a company that just got databricks and did not understand how it worked.

So, they set everything to run on their private clusters with all purpose compute(3x's the price) with auto terminate turned off because they were ok with things running over the weekend.  Finance made them stop using databricks after two months lol.

Im sure people have fucked up worse.  What is the worst youve experienced?",2023-09-29 17:26:27
wvor7o,The problem with data industry is hiring roles instead of people,"Data Engineer, Database Architecht, Data Scientist, Solution Architecht, Data Specialist...

Each one of these categories contains a wide variety of skillsets with a lot of overlap. Some companies call anyone who knows SQL a Data Engineer, and some companies call anyone who knows XGBoost a Data Scientist.  On the flip side, I've seen companies that have one Data Engineer and are running around with their heads cut off because the CTO decided they needed a new platform. I've seen companies that have one Data Scientist and when hired, the CTO says ""Ok, you're a scientist. Now do some data!""

I started out as an actuary in 2013, then moved to Data Science in 2018, and now lean heavier on the Data Engineering and Solution Architecting side of things because that's where the demand and money is at.

I've done tons of staff aug for companies and I have noticed a similar pattern: they can't find talent that has a holistic view on data. The data engineers only know and care about data engineering, data scientists only care about their algorithms, etc. There's no collaboration, communication or understanding of the other sides of the shop and no one there to form the bridge. 

I think the problem stems from there being TOO much out there to understand and be competent at. So younger folk go off to the youtubes and watch surface level videos on a technology so they can put ""proficient"" on their resume. Then when they're thrown onto their first project, they have to either figure it out quickly, or embarrass themselves. The ""thrown to the wolves"" strategy is very common in corporate culture. 

My advice to the young folk: take time to understand the theoretical knowledge of why you're doing something rather than just because your boss told you to. Think about what you would have done differently if you were in a leadership position and what technologies you would rely on. If you rely on a specific tool or technology, what are the pros and cons of using it over another technology? 

If I had to suggest a book, it'd be ""Designing Data-Intensive Applications"" by Martin Kleppmann. It's a very dense book, but contains a lot of valuable information. It's important to remember that technologies are just a tool and what's popular and in demand right now, might not be the case in the future. Otherwise, I'd still be in Excel formulating solutions in  VBA instead of realizing I should have created a Python pipeline all together. 

In terms of core technologies to know:

* SQL: Not just SELECT \*, but DDL, DML, CTE, windowing functions etc.
   * Rule of thumb: If you can do it in SQL, do it in SQL
* Python: It's quick enough for most cases and has huge community support (easier to find job placement)
* Spark/Distributed Computing: Distributed computing is going nowhere, but the query engine used will vary. I say Spark because it's the easiest to learn platform for now. PySpark is really intuitive, but the underlying concepts around drivers/excututors/tuning clusters is where the real value comes in. Spark is open source, has a lot of community support, and is in demand right now. The skillsets learned from Spark/distributed computing are transferrable to other platforms like Snowflake, AWS Athena, Dremio, Presto/Trino, Ignite, Impala etc.
* Streaming technology: Also important to distinguish the difference between batch processing and streaming. Some companies will need insane latency requirements and you have to think about the physical devices the data interacts with in order to meet those requirements. Apache Flink, Cassandra/Kafka are useful starting points. Kafka reins supreme right now, but it's a hugely competitive area right now.",2022-08-23 13:24:19
pkdprn,2nd time this week and it's Wednesday,N/A,2021-09-08 16:17:51
14yfh6p,"Python library for automating data normalisation, schema creation and loading to db","

Hey Data Engineers!,

For the past 2 years I've been working on a library to automate the most tedious part of my own work - data loading, normalisation, typing, schema creation, retries, ddl generation, self deployment, schema evolution... basically, as you build better and better pipelines you will want more and more.

The value proposition is to automate the tedious work you do, so you can focus on better things.

So dlt is a library where in the easiest form, you shoot response.json() json at a function and it auto manages the typing normalisation and loading.

In its most complex form, you can do almost anything you can want, from memory management, multithreading, extraction DAGs, etc.

The library is in use with early adopters, and we are now working on expanding our feature set to accommodate the larger community.

Feedback is very welcome and so are requests for features or destinations.

The library is open source and will forever be open source. We will not gate any features for the sake of monetisation - instead we will take a more kafka/confluent approach where the eventual paid offering would be supportive not competing. 

Here are our [product principles](https://dlthub.com/product/) and docs page and our [pypi page](https://pypi.org/project/dlt/).



I know lots of you are jaded and fed up with toy technologies - this is not a toy tech, it's purpose made for productivity and sanity.



Edit: Well this blew up! Join our growing slack community on dlthub.com ",2023-07-13 08:57:27
14abng6,Is data at every company still an absolute mess?,"So I switched from mechanical engineering to IoT data engineering about a year ago. At first I was pretty oblivious to a lot of stuff, but as I've learned I look around in horror.

There's so much duplicate information, bad source data, free-for-all solo project DBs.

Everything is a mess and I can't help but think most other companies are like this. Both companies I've worked for didn't start hiring a serious amount of IT infrastructure until a few years ago. The data is clearly getting better but has a loooong way to go.

And now with ML, Industry 4.0, and cloud being pushed I feel companies will all start running before they walk and everything will be a massive mess.

I thought data jobs were peaking now but in reality I think they're just now going to start growing, thoughts?",2023-06-15 19:56:19
130rfc2,What's your favorite data quality horror story?,"My personal favorite... A man's health insurance bill went up astronomically after moving from the EU to the USA because his height was listed at 1.8ft instead of meters. Needless to say, the insurance company decided someone shaped like a 180lb pancake is a high-risk individual to insure.",2023-04-27 15:26:04
114vyvz,Snowflake pushing snowpark really hard,N/A,2023-02-17 19:57:07
qsgd02,The ecosystem be like that sometimes,N/A,2021-11-12 17:34:58
nvj4m2,"Now that Snowflake can store and analyze unstructured data, Padme is in for a great surprise",N/A,2021-06-09 00:41:22
16frhic,The state of data content on LinkedIn: you can reduce costs by just doing less! Game changing,N/A,2023-09-11 10:11:02
11kth8p,Getting tired of “How do I break into DE posts”,"I would like to see more substantial content in this Reddit.  Lately the depth and quality of recent posts have not really added anything to the community.

Are there any thoughts on how to lift and improve it?",2023-03-07 08:21:34
19f9dba,"Well guys, this is the end",🥹,2024-01-25 13:30:21
1bp335j,Airflow homies be like...,N/A,2024-03-27 14:22:09
yez0ad,It's cron all the way down,N/A,2022-10-27 18:10:00
12asp78,MLOps is 98% Data Engineering,"After a few years and with the hype gone, it has become apparent that MLOps overlap more with Data Engineering than most people believed.

I wrote my thoughts on the matter and the awesome people of the MLOps community were kind enough to host them on their blog as a guest post. You can find the post here:

[https://mlops.community/mlops-is-mostly-data-engineering/](https://mlops.community/mlops-is-mostly-data-engineering/)",2023-04-03 18:13:24
1491swe,A must-read data engineering collection,"I just finished writing up a welcome gift for my newsletter, but I wanted to share at least the list of links here. 

For comments on all the books & articles, don't hesitate to subscribe to [https://www.finishslime.com/](https://www.finishslime.com/).  

*FWIW: I have read all of these, and I did consider all of them very helpful for my data engineering skills! This is not a bogus collection of what others have shared.* 

# Books 

* [Designing Data-Intensive Applications: The Big Ideas Behind Reliable, Scalable, and Maintainable Systems](https://www.amazon.de/Designing-Data-Intensive-Applications-Reliable-Maintainable/dp/1449373321) \- Martin Kleppmann
* [Fundamentals of Data Engineering](https://www.amazon.de/-/en/Joe-Reis/dp/1098108302) \- Reis & Housley
* [Data Science for Business](https://www.amazon.de/-/en/Foster-Provost/dp/1449361323/) \- Provost & Fawcett
* [Big Data](https://www.amazon.de/-/en/Nathan-Marz/dp/1617290343): Principles and best practices of scalable realtime data systems - Nathan Marz
* [Database Reliability Engineering](https://www.amazon.com/Database-Reliability-Engineering-Designing-Operating/dp/1491925949/): Designing and Operating Resilient Database Systems - Campbell Majors
* [Storytelling with data](https://www.amazon.com/Storytelling-Data-Visualization-Business-Professionals/dp/1119002257) \- Nussbaumer Knaflic
* [Data Mesh](https://www.amazon.com/Data-Mesh-Delivering-Data-Driven-Value/dp/1492092398/) \- Zhamak Dehghani

# Articles from last year

* [Stop aggregating away the signal in your data](https://stackoverflow.blog/2022/03/03/stop-aggregating-away-the-signal-in-your-data/%20) — Zan Armstrong 
* [Data Mesh in practice](https://www.starburst.io/info/data-mesh-in-practice-ebook/%20) — Max Schultze & Arif Wider
* [The future of the modern data stack](https://www.montecarlodata.com/the-future-of-the-modern-data-stack/) — Barr Moses
* [Reshaping data engineering](https://preset.io/blog/reshaping-data-engineering/) — Maxime Beauchemin
* [Emerging Architectures for modern data infrastructure](https://a16z.com/2020/10/15/emerging-architectures-for-modern-data-infrastructure/) — Matt Bornstein, Jennifer Li, Martin Casado
* [Dodging the data bottleneck, data mesh at starship](https://www.starship.xyz/medium_blog_posts/dodging-the-data-bottleneckdata-mesh-at-starship/) — Taavi Pungas
* [3 Level data lakes](https://youtu.be/4zLCUPNIV3M) — Paul Singman
* [Miro's journey to data monitoring](https://medium.com/miro-engineering/our-journey-to-data-engineering-monitoring-c14d6ff20351) — Goncalo Costa, Ricardo Souza
* [Photobox data platform](https://medium.com/photobox-technology-product-and-design/photobox-new-data-platform-da5d70296ba0) — Stefan Solimito
* [Talk on Functional Data Engineering](https://www.youtube.com/watch?v=4Spo2QRTz1k&feature=youtu.be&themeRefresh=1) — Maxime Beauchemin

# Overall great articles 

* [The Rise of the Data Engineer](https://medium.com/free-code-camp/the-rise-of-the-data-engineer-91be18f1e603)
* [The Modern Stack of ML Infrastructure](https://outerbounds.com/blog/the-modern-stack-of-ml-infrastructure/)
* [The Downfall of the Data Engineer](https://maximebeauchemin.medium.com/the-downfall-of-the-data-engineer-5bfb701e5d6b)
* [How to Move Beyond a Monolithic Data Lake to a Distributed Data Mesh](https://martinfowler.com/articles/data-monolith-to-mesh.html)
* [Functional Data Engineering — a modern paradigm for batch data processing](https://maximebeauchemin.medium.com/functional-data-engineering-a-modern-paradigm-for-batch-data-processing-2327ec32c42a)
* [Data Mesh Principles and Logical Architecture](https://martinfowler.com/articles/data-mesh-principles.html)
* [The Future Of Business Intelligence Is Open Source](https://preset.io/blog/future-of-business-intelligence/)
* [Tristan Handy on the changing face of the data stack](https://mixpanel.com/blog/tristan-handy-changing-data-stack/)
* [The Future of the Data Engineer](https://preset.io/blog/the-future-of-the-data-engineer/)
* [The Modern Data Stack: Past, Present, and Future](https://www.getdbt.com/blog/future-of-the-modern-data-stack/)
* [The Case for Dataset-Centric Visualization](https://preset.io/blog/dataset-centric-visualization/)
* [Building The Modern Data Team](https://databased.pedramnavid.com/p/modern-data-team)
* [Introducing Entity-Centric Data Modeling for Analytics](https://preset.io/blog/introducing-entity-centric-data-modeling-for-analytics/)
* [We Don't Need Data Scientists, We Need Data Engineers](https://www.mihaileric.com/posts/we-need-data-engineers-not-data-scientists/)
* [How should our company structure our data team?](https://medium.com/super/how-should-our-company-structure-our-data-team-e71f6846024d)
* [What makes a data analyst excellent?](https://towardsdatascience.com/what-makes-a-data-analyst-excellent-17ee4651c6db)
* [Data Strategy: Good Data vs. Bad Data](https://towardsdatascience.com/data-strategy-good-data-vs-bad-data-d40f85d7ba4e)
* [What Companies REALLY Want in an Analytics Engineer](https://medium.com/geekculture/what-companies-really-want-in-an-analytics-engineer-1ac03ff4494a)
* [Stop using so many CTEs](https://hex.tech/blog/stop-using-so-many-ctes/)
* [7 Antifragile Principles for a Successful Data Warehouse](https://blog.picnic.nl/7-antifragile-principles-for-a-successful-data-warehouse-574b655f0bc6)

What about you? Got anything to add? I bet!",2023-06-14 07:52:01
13wqhby,Databricks and Snowflake: Stop fighting on social,"I've had to unfollow Databricks CEO as it gets old seeing all these Snowflake bashing posts. Bordeline click bait. Snowflake leaders seem to do better, but are a few employees I see getting into it as well. As a data engineer who loves the space and is a fan of both for their own merits (my company uses both Databricks and Snowflake) just calling out this bashing on social is a bad look. Do others agree? Are you getting tired of all this back and forth?",2023-05-31 16:11:34
105o50o,Data pipeline design patterns,"Hello everyone,

I've been building data pipelines for a while now. In contrast to the availability of examples of code design patterns, & data modeling techniques, there are few to none on data flow design patterns. In my experience building pipelines, using the appropriate data flow patterns increases feature delivery speed, decreases toil during pipeline failures, and builds trust with stakeholders.

With that in mind, I wrote an article that goes over the most commonly used data flow design patterns, what they do, when to use them, and, more importantly, when not to use them. It's aimed to give an overview of the typical data flow patterns and guidelines for choosing the appropriate one for your use case.

[https://www.startdataengineering.com/post/design-patterns/](https://www.startdataengineering.com/post/design-patterns/)

I'd love to hear about any patterns that I have missed. Any feedback is appreciated. I hope this helps someone :)",2023-01-07 12:40:02
17mi1iz,Be aware of the AI snake salesman,"I got a demo of a new product a team is gonna implement in my company that is going to ‘use AI to help users find data and reports’. 

It had nothing to do with AI. You had to manually tag every report with answers that report could answer. Then if someone’s question they typed in matched a tag it would put it at the top of the search results. 

I asked what was the AI component of this? They said because the bot responds to the question. 

It’s a good reminder to try and educate business users about AI and how it isn’t currently going to solve all their issues.",2023-11-03 00:15:36
15wunwt,Me around Customer Success,N/A,2023-08-21 02:32:01
1bix81r,F1 team Williams used Excel as their database to track the car components (hundreds of thousands of different components),N/A,2024-03-19 22:21:09
1bkebv5,We (Dagster) are throwing a party,"Hi /r/dataengineering,

I'm Pete, the CEO at Dagster Labs. We're launching our new product, Dagster+, in a few weeks, and are having launch parties in SF and NYC to preview the release with the community and enjoy some food, drinks, and good company.

This subreddit has been very supportive of us over the past few years, and we'd love to see you there! They're happening the evening of April 12th, and you can sign up [here](https://share.hsforms.com/10i2u9WzCRyK77-D0MCn89gq55vz).

Spots are limited, so we’ll reach out to confirm attendance as available closer to the date of the parties. Thanks in advance for expressing your interest!",2024-03-21 18:57:57
160nyxw,Just got certified! - Databricks Certified Data Engineer Professional,"Hi all!

Just successfully completed the Databricks Data Engineering Professional certification. Admittedly, the Professional certification was pretty difficult, primarily due to the lack of resources available online and the extensive range of concepts (including Apache Spark™, Delta Lake, MLflow, Databricks CLI, and more) you are expected to know.

Here are the resources I used:

1. Databricks Advanced Data Engineering Course (Free): Used my customer account. It's open for anyone to sign up, and they even offer a 2-week trial. You can download the .dbc files, upload them to the community edition workspace, and get hands-on experience.
2. Udemy: [Databricks Certified Data Engineer Professional Course](https://www.udemy.com/course/databricks-certified-data-engineer-professional) \- Currently, this is the only course available. While some topics like MLFlow and certain CLI concepts are missing, making it slightly outdated, it's an excellent for a decent foundation.
3. Practice Tests:  
\- [Practice Exams for Databricks Data Engineer Professional](https://www.udemy.com/course/practice-exams-databricks-data-engineer-professional-k/) \- This is a decent resource to pinpoint weak areas. However, it lacks questions on MLFlow and CLI, and it's from the same author as the above course.  
- [Databricks Data Engineer Professional Practice Exams](https://www.udemy.com/course/databricks-data-engineer-professional-practice-exams-i) \- An invaluable resource, that I relied on heavily throughout prep. The questions are in-depth and cover all topics. Ensure you go through both the questions and answers meticulously.

4. YouTube Resources:  
\- [Advanced Analytics](https://www.youtube.com/@AdvancingAnalytics): A fantastic channel for deep-diving into a plethora of concepts.

- [Stephanie Rivera](https://www.youtube.com/@stephanieamrivera): My go-to for Databricks training videos.

 

I had a lot of trouble gathering resources to use. Hope this helps!

&#x200B;",2023-08-25 03:44:08
xyxpku,"Built and automated a complete end-to-end ELT pipeline using AWS, Airflow, dbt, Terraform, Metabase and more as a beginner project!","GitHub repository: [https://github.com/ris-tlp/audiophile-e2e-pipeline](https://github.com/ris-tlp/audiophile-e2e-pipeline)

Pipeline that extracts data from [Crinacle's](https://crinacle.com/) Headphone and InEarMonitor rankings and prepares data for a Metabase Dashboard. While the dataset isn't incredibly complex or large, the project's main motivation was to get used to the different tools and processes that a DE might use.

## Architecture

https://preview.redd.it/4nl5gasv4ms91.jpg?width=1858&format=pjpg&auto=webp&s=f0766ad2d58e19689474acc5a51ef35c9388284d

Infrastructure provisioning through [Terraform](https://www.terraform.io/), containerized through [Docker](https://www.docker.com/) and orchestrated through [Airflow](https://airflow.apache.org/). Created dashboard through [Metabase](https://www.metabase.com/).

DAG Tasks:

1. Scrape data from [Crinacle's](https://crinacle.com/) website to generate bronze data.
2. Load bronze data to [AWS S3](https://aws.amazon.com/s3/).
3. Initial data parsing and validation through [Pydantic](https://github.com/pydantic/pydantic) to generate silver data.
4. Load silver data to [AWS S3](https://aws.amazon.com/s3/).
5. Load silver data to [AWS Redshift](https://aws.amazon.com/redshift/).
6. Load silver data to [AWS RDS](https://aws.amazon.com/rds/) for future projects.
7. and 8. Transform and test data through [dbt](https://docs.getdbt.com/) in the warehouse.

## Dashboard

The dashboard was created on a local Metabase docker container, I haven't hosted it anywhere so I only have a screenshot to share, sorry!

&#x200B;

https://preview.redd.it/9a5zv15y4ms91.jpg?width=2839&format=pjpg&auto=webp&s=8df8ba42d8ab602fb72dc9ffcc2102e6a517e0c5

## Takeaways and improvements

1. I realize how little I know about advance SQL and execution plans. I'll definitely be diving deeper into the topic and taking on some courses to strengthen my foundations there.
2. Instead of running the scraper and validation tasks locally, they could be deployed as a Lambda function so as to not overload the airflow server itself.

Any and all feedback is absolutely welcome! I'm fresh out of university and trying to hone my skills for the DE profession as I'd like to integrate it with my passion of astronomy and hopefully enter the data-driven astronomy in space telescopes area as a data engineer! Please feel free to provide any feedback!",2022-10-08 16:58:28
sm5bd0,Seems like dbt's the solution to everything,N/A,2022-02-06 19:34:33
nntbv1,STAY FAR AWAY FROM UDACITY's DATA ENGINEERING COURSE,"I picked up this course because it was ""on sale""... this is legit the worst course of all time and a complete waste of time and money.. The material is decent, but there are plenty of gaps in instructions, missing data sets and outdated code... For the price, this is just unacceptable and I can't believe I wasted my money. Please stay far far away!",2021-05-29 18:25:24
17vxmth,Microsoft data products - merry-go-round of mediocrity,"Hey r/dataengineering,

For anyone that says this is my fault for specializing in Microsoft stack - you're absolutely, 100% correct. I blame only myself. 

The incessant cycle of ""progress"". I'm reaching my wit's end with how we're handling tech debt. It seems like every other year, there's a new 'bright new day' in the Microsoft analytics stack, and it's driving me nuts.

First off, let's address the myth of avoiding tech debt. Spoiler alert: it's a fairy tale. Every couple of years, MS flips the script, and suddenly, what was cutting-edge is now old news. The execs, bless their hearts, eat up all the marketing spiel and suddenly, last year's innovation is this year's digital paperweight.

It's a merry-go-round of mediocrity So, what do we do? We slap a new 'notebook' GUI over Spark clusters and pat ourselves on the back for 'innovation.' It's a cycle as predictable as it is frustrating. Microsoft partners? Under constant pressure to sell whatever's been rebranded this week, with awards handed out for sales volume, not product quality. 

We've all heard the mantras: ""ADF is the way,"" ""Databricks is the way,"" ""Synapse is the way,"" ""Fabric is the way."" It's just a parade of platforms, each hailed as the messiah of data engineering, but they're not, they're very naughty boys, only to be replaced by the next shiny thing in a year or two.

I (and anyone working with Azure/MS tech) need to get some self-respect and leave the execs, wordcels and 'platnum's to it.",2023-11-15 16:40:11
wklueu,So today I learned Data Swamps exist. Anyone ever have to deal with one in a production environment?,N/A,2022-08-10 02:33:50
1bh6jx2,Has anyone made the jump from a $100-150k to +$200k position?,"Background: Currently a self taught data engineer with about 3 years of DE experience and 3 years of data analyst experience. Graduated with a non-CS quant major from a decent university and make around $130k a year at a remote MCOL job. 

Has anyone made it from a mid-tier paying job to a high paying job without moving to a HCOL area?

I'm wondering if the jobs themselves that are higher paying are actually more difficult in complexity? My current company has a pretty modern tech stack with good engineering practices, at least I think.

Is the best way to get those higher paying jobs to get a referral and grind leetcode/learn fundamentals?",2024-03-17 19:42:56
153o48v,Fact,N/A,2023-07-19 07:37:16
1549emd,Is it normal for data engineers to be lacking basic technical skills?,"I've been at my new company for about 4 months.  I have 2 years of CRUD backend experience and I was hired to replace a senior DE (but not as a senior myself) on a data warehouse team.  This engineer managed a few python applications and Spark + API ingestion processes for the DE team.  

I am hired and first tasked to put these codebases in github, setup CI/CD processes, and help upskill the team in development of this side of our data stack.  It turns out the previous dev just did all of his development on production directly with no testing processes or documentation.  Okay, no big deal.  I'm able to get the code into our remote repos, build CI/CD pipeline with Jenkins (with the help of an adjacent devops team), and overall get the codebase updated to a more mature standing.  I've also worked with the devops team to build out docker images for each of the applications we manage so that we can have proper development environments. Now we have visibility, proper practices in place, and it's starting to look like actual engineering.

Now comes the part where everything starts crashing down.  Since we have a more organized development practices, our new manager starts assigning tasks within these platforms to other engineers.  I come to find out that the senior engineer I replaced was the only data engineer who had touched these processes within the last year.  I also learn that none of the other DE's (including 4 senior DE's) have any experience with programming outside of SQL.  

Here's a list of some of the issues I've run into:  
Engineer wants me to give him prod access so he can do his development there instead of locally.

Senior engineers don't know how to navigate a CLI.

Engineers have no idea how to use git, and I am there personal git encyclopedia.

Engineers breaking stuff with a git GUI, requiring me to fix it.

Engineers pushing back on git usage entirely.

Senior engineer with 12 years at the company does not know what a for-loop is.

Complaints about me requiring unit testing and some form of documentation that the code works before pushing to production.

Some engineers simply cannot comprehend how Docker works, and want my help to configure their windows laptop into a development environment (I am not helping you stand up a Postgres instance directly on your Windows OS).

I am at my wits end.  I've essentially been designated as a mentor for the side of the DE house that I work in.  That's fine, but I was not hired as a senior, and it is really demotivating mentoring the people who I thought should be mentoring me.  I really do want to see the team succeed, but there has been so much pushback on following best-practices and learning new skills.  Is this common in the DE field?

&#x200B;",2023-07-19 22:42:45
z6s0pe,Airflow DAG with 150 tasks dynamically generated from a single module file,N/A,2022-11-28 09:24:26
lclm3s,Lecture Notes on Data Engineering Basics - Stanford CS 329S,N/A,2021-02-04 17:58:14
1b6ghh6,"Accepted an offer, 2 weeks later got dream offer from another company","So I accepted an offer with a decent comp at a bank. Role is remote I started and got my work laptop mailed and have been going through on boarding. 

Now I've just gotten an offer from another company which I thought ghosted me and I'm in a bit of a dilemma. The offer is 60% more than my current comp. I'm not even questioning it tbh I am definitely going to accept, I know my current company can't match and of course they won't I literally just started. 

Whats my best course of action? Just tell them about the job? Bullshit something else (like medical issue) and say I can't work anymore?

Edit: while the job is remote they did fly me out for my first week so I can meet the core team so that does add another insult when I leave. ",2024-03-04 17:19:17
12v9d3v,Is it normal to not remember Pandas commands and need to constantly Google them?,"I use Pandas pretty much daily and except from the usual head(), keys(), dtypes etc, I always have to Google things like groupby to remember the syntax. I know how to use them all but does this syndrome disappear as you get more experienced or does everyone Google these things too? SQL commands I remember a lot as it's plain English but Pandas, no.",2023-04-22 15:28:41
1amizw6,Data lovers!,N/A,2024-02-09 07:49:43
t5tp9p,Every Freaking Time,N/A,2022-03-03 15:10:47
12u2542,Step-by-step tutorial: Building a Kimball dimensional model with dbt,"Hey everyone! I am thrilled to announce that as part of the [dbt technical writing mentorship program](https://www.getdbt.com/blog/technical-writing-mentorship-program/), I have just published a brand new developer blog article for all my fellow data enthusiasts out there! In this tutorial, I provide a step-by-step guide on how to build a Kimball dimensional model with dbt. 

* Blog article: [https://docs.getdbt.com/blog/kimball-dimensional-model](https://docs.getdbt.com/blog/kimball-dimensional-model) 
* Repository: [https://github.com/Data-Engineer-Camp/dbt-dimensional-modelling](https://github.com/Data-Engineer-Camp/dbt-dimensional-modelling) 

I had trouble finding clear explanations on this topic myself, which is why I decided to write one and share my knowledge with the community. Check out my latest article and let me know what you think!",2023-04-21 13:41:59
11nqc61,Tencent Data Engineer: Why We Went from ClickHouse to Apache Doris?,"This article is co-written by me and my colleague Kai Dai. We are both data platform engineers at Tencent Music (NYSE: TME), a music streaming service provider with a whopping 800 million monthly active users. To drop the number here is not to brag but to give a hint of the sea of data that my poor coworkers and I have to deal with everyday.

# What We Use ClickHouse For?

The music library of Tencent Music contains data of all forms and types: recorded music, live music, audios, videos, etc. As data platform engineers, our job is to distill information from the data, based on which our teammates can make better decisions to support our users and musical partners.

Specifically, we do all-round analysis of the songs, lyrics, melodies, albums, and artists, turn all this information into data assets, and pass them to our internal data users for inventory counting, user profiling, metrics analysis, and group targeting.

https://preview.redd.it/y36uy7do4xma1.png?width=1280&format=png&auto=webp&s=5690569dac32b9206e14f187d31d9de0fc4ccdf0

We stored and processed most of our data in Tencent Data Warehouse (TDW), an offline data platform where we put the data into various tag and metric systems and then created flat tables centering each object (songs, artists, etc.).

Then we imported the flat tables into ClickHouse for analysis and Elasticsearch for data searching and group targeting.

After that, our data analysts used the data under the tags and metrics they needed to form datasets for different usage scenarios, during which they could create their own tags and metrics.

The data processing pipeline looked like this:

https://preview.redd.it/18em4jjr4xma1.png?width=1280&format=png&auto=webp&s=00bcc93010957518038f892c185d7bd7803b7d94

# Why ClickHouse is Not a Good Fit

When working with the above pipeline, we encountered a few difficulties:

1. **Partial Update**: Partial update of columns was not supported. Therefore, any latency from any one of the data sources could delay the creation of flat tables, and thus undermine data timeliness.
2. **High storage cost**: Data under different tags and metrics was updated at different frequencies. As much as ClickHouse excelled in dealing with flat tables, it was a huge waste of storage resources to just pour all data into a flat table and partition it by day, not to mention the maintenance cost coming with it.
3. **High maintenance cost**: Architecturally speaking, ClickHouse was characterized by the strong coupling of storage nodes and compute nodes. Its components were heavily interdependent, adding to the risks of cluster instability. Plus, for federated queries across ClickHouse and Elasticsearch, we had to take care of a huge amount of connection issues. That was just tedious.

# Transition to Apache Doris

[Apache Doris](https://github.com/apache/doris), a real-time analytical database, boasts a few features that are exactly what we needed in solving our problems:

1. **Partial update**: Doris supports a wide variety of data models, among which the Aggregate Model supports real-time partial update of columns. Building on this, we can directly ingest raw data into Doris and create flat tables there. The ingestion goes like this: Firstly, we use Spark to load data into Kafka; then, any incremental data will be updated to Doris and Elasticsearch via Flink. Meanwhile, Flink will pre-aggregate the data so as to release burden on Doris and Elasticsearch.
2. **Storage cost**: Doris supports multi-table join queries and federated queries across Hive, Iceberg, Hudi, MySQL, and Elasticsearch. This allows us to split the large flat tables into smaller ones and partition them by update frequency. The benefits of doing so include a relief of storage burden and an increase of query throughput.
3. **Maintenance cost**: Doris is of simple architecture and is compatible with MySQL protocol. Deploying Doris only involves two processes (FE and BE) with no dependency on other systems, making it easy to operate and maintain. Also, Doris supports querying external ES data tables. It can easily interface with the metadata in ES and automatically map the table schema from ES so we can conduct queries on Elasticsearch data via Doris without grappling with complex connections.

What’s more, Doris supports multiple data ingestion methods, including batch import from remote storage such as HDFS and S3, data reads from MySQL binlog and Kafka, and real-time data synchronization or batch import from MySQL, Oracle, and PostgreSQL. It ensures service availability and data reliability through a consistency protocol and is capable of auto debugging. This is great news for our operators and maintainers.

Statistically speaking, these features have cut our storage cost by 42% and development cost by 40%.

During our usage of Doris, we have received lots of support from the open source Apache Doris community and timely help from the SelectDB team, which is now running a commercial version of Apache Doris.

https://preview.redd.it/4epkzulg5xma1.png?width=1280&format=png&auto=webp&s=2247711ec449b0af555a1132033a3a7528f9519c

# Further Improvement to Serve Our NeedsIntroduce a Semantic Layer

Speaking of the datasets, on the bright side, our data analysts are given the liberty of redefining and combining the tags and metrics at their convenience. But on the dark side, high heterogeneity of the tag and metric systems leads to more difficulty in their usage and management.

Our solution is to introduce a semantic layer in our data processing pipeline. The semantic layer is where all the technical terms are translated into more comprehensible concepts for our internal data users. In other words, we are turning the tags and metrics into first-class citizens for data definement and management.

https://preview.redd.it/7yxzag2k5xma1.png?width=1280&format=png&auto=webp&s=f345a13e729a35cafcb0e7d432e87b8b163f82df

**Why would this help?**

For data analysts, all tags and metrics will be created and shared at the semantic layer so there will be less confusion and higher efficiency.

For data users, they no longer need to create their own datasets or figure out which one is applicable for each scenario but can simply conduct queries on their specified tagset and metricset.

# Upgrade the Semantic Layer

Explicitly defining the tags and metrics at the semantic layer was not enough. In order to build a standardized data processing system, our next goal was to ensure consistent definition of tags and metrics throughout the whole data processing pipeline.

For this sake, we made the semantic layer the heart of our data management system:

https://preview.redd.it/yitj349p5xma1.png?width=1280&format=png&auto=webp&s=f8077b99a71f0718becb2dd7497aa719e5cc90a3

**How does it work?**

All computing logics in TDW will be defined at the semantic layer in the form of a single tag or metric.

The semantic layer receives logic queries from the application side, selects an engine accordingly, and generates SQL. Then it sends the SQL command to TDW for execution. Meanwhile, it might also send configuration and data ingestion tasks to Doris and decide which metrics and tags should be accelerated.

In this way, we have made the tags and metrics more manageable. A fly in the ointment is that since each tag and metric is individually defined, we are struggling with automating the generation of a valid SQL statement for the queries. If you have any idea about this, you are more than welcome to talk to us.

# Give Full Play to Apache Doris

As you can see, Apache Doris has played a pivotal role in our solution. Optimizing the usage of Doris can largely improve our overall data processing efficiency. So in this part, we are going to share with you what we do with Doris to accelerate data ingestion and queries and reduce costs.

**What We Want?**

https://preview.redd.it/1s4n2nls5xma1.png?width=1280&format=png&auto=webp&s=0be5c21e347b2fec7eabb475bdab86444c2efb8d

Currently, we have 800+ tags and 1300+ metrics derived from the 80+ source tables in TDW.

When importing data from TDW to Doris, we hope to achieve:

* **Real-time availability:** In addition to the traditional T+1 offline data ingestion, we require real-time tagging.
* **Partial update**: Each source table generates data through its own ETL task at various paces and involves only part of the tags and metrics, so we require the support for partial update of columns.
* **High performance**: We need a response time of only a few seconds in group targeting, analysis and reporting scenarios.
* **Low costs**: We hope to reduce costs as much as possible.

**What We Do?**

1. **Generate Flat Tables in Flink Instead of TDW**

https://preview.redd.it/of8zcyyu5xma1.png?width=1280&format=png&auto=webp&s=1bcc31f236b2373385a2e2954445cd13568987d3

Generating flat tables in TDW has a few downsides:

* **High storage cost**: TDW has to maintain an extra flat table apart from the discrete 80+ source tables. That’s huge redundancy.
* **Low real-timeliness**: Any delay in the source tables will be augmented and retard the whole data link.
* **High development cost**: To achieve real-timeliness would require extra development efforts and resources.

On the contrary, generating flat tables in Doris is much easier and less expensive. The process is as follows:

* Use Spark to import new data into Kafka in an offline manner.
* Use Flink to consume Kafka data.
* Create a flat table via the primary key ID.
* Import the flat table into Doris.

As is shown below, Flink has aggregated the five lines of data, of which “ID”=1, into one line in Doris, reducing the data writing pressure on Doris.

https://preview.redd.it/qk6mr24x5xma1.png?width=1280&format=png&auto=webp&s=56fd72f14962af9bae20d74e06e947d84efd0658

This can largely reduce storage costs since TDW no long has to maintain two copies of data and KafKa only needs to store the new data pending for ingestion. What’s more, we can add whatever ETL logic we want into Flink and reuse lots of development logic for offline and real-time data ingestion.

**2. Name the Columns Smartly**

As we mentioned, the Aggregate Model of Doris allows partial update of columns. Here we provide a simple introduction to other data models in Doris for your reference:

**Unique Model**: This is applicable for scenarios requiring primary key uniqueness. It only keeps the latest data of the same primary key ID. (As far as we know, the Apache Doris community is planning to include partial update of columns in the Unique Model, too.)

**Duplicate Model**: This model stores all original data exactly as it is without any pre-aggregation or deduplication.

After determining the data model, we had to think about how to name the columns. Using the tags or metrics as column names was not a choice because:

I. Our internal data users might need to rename the metrics or tags, but Doris 1.1.3 does not support modification of column names.

II. Tags might be taken online and offline frequently. If that involves the adding and dropping of columns, it will be not only time-consuming but also detrimental to query performance.

Instead, we do the following:

* **For flexible renaming of tags and metrics**, we use MySQL tables to store the metadata (name, globally unique ID, status, etc.). Any change to the names will only happen in the metadata but will not affect the table schema in Doris. For example, if a `song_name` is given an ID of 4, it will be stored with the column name of a4 in Doris. Then if the `song_name` is involved in a query, it will be converted to a4 in SQL.
* **For the onlining and offlining of tags**, we sort out the tags based on how frequently they are being used. The least used ones will be given an offline mark in their metadata. No new data will be put under the offline tags but the existing data under those tags will still be available.
* **For real-time availability of newly added tags and metrics**, we prebuild a few ID columns in Doris tables based on the mapping of name IDs. These reserved ID columns will be allocated to the newly added tags and metrics. Thus, we can avoid table schema change and the consequent overheads. Our experience shows that only 10 minutes after the tags and metrics are added, the data under them can be available.

Noteworthily, the recently released Doris 1.2.0 supports Light Schema Change, which means that to add or remove columns, you only need to modify the metadata in FE. Also, you can rename the columns in data tables as long as you have enabled Light Schema Change for the tables. This is a big trouble saver for us.

**3. Optimize Date Writing**

Here are a few practices that have reduced our daily offline data ingestion time by 75% and our CUMU compaction score from 600+ to 100.

* Flink pre-aggregation: as is mentioned above.
* Auto-sizing of writing batch: To reduce Flink resource usage, we enable the data in one Kafka Topic to be written into various Doris tables and realize the automatic alteration of batch size based on the data amount.
* Optimization of Doris data writing: fine-tune the the sizes of tablets and buckets as well as the compaction parameters for each scenario:

`max_XXXX_compaction_thread`  
`max_cumulative_compaction_num_singleton_deltas`

* Optimization of the BE commit logic: conduct regular caching of BE lists, commit them to the BE nodes batch by batch, and use finer load balancing granularity.

https://preview.redd.it/q5dqk7b76xma1.png?width=1280&format=png&auto=webp&s=f8e491d403cb52cb92a0a99802ecccb5633def23

**4. Use Dori-on-ES in Queries**

About 60% of our data queries involve group targeting. Group targeting is to find our target data by using a set of tags as filters. It poses a few requirements for our data processing architecture:

* Group targeting related to APP users can involve very complicated logic. That means the system must support hundreds of tags as filters simultaneously.
* Most group targeting scenarios only require the latest tag data. However, metric queries need to support historical data.
* Data users might need to perform further aggregated analysis of metric data after group targeting.
* Data users might also need to perform detailed queries on tags and metrics after group targeting.

After consideration, we decided to adopt Doris-on-ES. Doris is where we store the metric data for each scenario as a partition table, while Elasticsearch stores all tag data. The Doris-on-ES solution combines the distributed query planning capability of Doris and the full-text search capability of Elasticsearch. The query pattern is as follows:

`SELECT tag, agg(metric)`   
 `FROM Doris`   
 `WHERE id in (select id from Es where tagFilter)`  
 `GROUP BY tag`

As is shown, the ID data located in Elasticsearch will be used in the sub-query in Doris for metric analysis.

In practice, we find that the query response time is related to the size of the target group. If the target group contains over one million objects, the query will take up to 60 seconds. If it is even larger, a timeout error might occur.

After investigation, we identified our two biggest time wasters:

I. When Doris BE pulls data from Elasticsearch (1024 lines at a time by default), for a target group of over one million objects, the network I/O overhead can be huge.

II. After the data pulling, Doris BE needs to conduct Join operations with local metric tables via SHUFFLE/BROADCAST, which can cost a lot.

https://preview.redd.it/yhfjj9uh6xma1.png?width=1280&format=png&auto=webp&s=10d6f5fadd5c6818600fb3a39191bbd75292981d

Thus, we make the following optimizations:

* Add a query session variable `es_optimize` that specifies whether to enable optimization.
* In data writing into ES, add a BK column to store the bucket number after the primary key ID is hashed. The algorithm is the same as the bucketing algorithm in Doris (CRC32).
* Use Doris BE to generate a Bucket Join execution plan, dispatch the bucket number to BE ScanNode and push it down to ES.
* Use ES to compress the queried data; turn multiple data fetch into one and reduce network I/O overhead.
* Make sure that Doris BE only pulls the data of buckets related to the local metric tables and conducts local Join operations directly to avoid data shuffling between Doris BEs.

https://preview.redd.it/or4nmhsk6xma1.png?width=1280&format=png&auto=webp&s=d615401e35e42dd29feff840483a825865dc2f02

As a result, we reduce the query response time for large group targeting from 60 seconds to a surprising 3.7 seconds.

Community information shows that Doris is going to support inverted indexing since version 2.0.0, which is soon to be released. With this new version, we will be able to conduct full-text search on text types, equivalence or range filtering of texts, numbers, and datetime, and conveniently combine AND, OR, NOT logic in filtering since the inverted indexing supports array types. This new feature of Doris is expected to deliver 3\~5 times better performance than Elasticsearch on the same task.

**5. Refine the Management of Data**

Doris’ capability of cold and hot data separation provides the foundation of our cost reduction strategies in data processing.

* Based on the TTL mechanism of Doris, we only store data of the current year in Doris and put the historical data before that in TDW for lower storage cost.
* We vary the numbers of copies for different data partitions. For example, we set three copies for data of the recent three months, which is used frequently, one copy for data older than six months, and two copies for data in between.
* Doris supports turning hot data into cold data so we only store data of the past seven days in SSD and transfer data older than that to HDD for less expensive storage.

# Conclusion

Thank you for scrolling all the way down here and finishing this long read. We’ve shared our cheers and tears, lessons learned, and a few practices that might be of some value to you during our transition from ClickHouse to Doris. We really appreciate the help from the Apache Doris community and the SelectDB team, but we might still be chasing them around for a while since we attempt to realize auto-identification of cold and hot data, pre-computation of frequently used tags/metrics, simplification of code logic using Materialized Views, and so on and so forth.",2023-03-10 14:14:15
xzw46d,How I got a Data Engineering position in less than two months of applying,"I am seeing quite a bit of post on here with people asking for advice on their resumes or expressing their difficulties in landing a Data Engineering position or even getting an interview at all. 

I want to share some tips that helped me as someone who is also fairly new to the Data Engineering world. My previous work experience ranged from doing basic SQL/Excel stuff for the first two years of my career and then advancing into more of a Database Developer position for another two years. Once of the things that was discouraging me to most is that I had a pretty bad 3 year gap in employment for anything tech related, I was laid off in 2019 and decided to finish a degree I was working on in Data Science which seemed to be the hottest job title back in 2018-2019. Not saying it isn't hot anymore but it seems like Data Engineering is an even hotter and in demand title at the moment. I got caught up with collecting unemployment , working odd jobs off the books, door dashing etc for a few years but this past summer I decided it was time to get back in the tech world before it was too late.

So in July I made it my mission to get a job by the end of the summer so I could start when my two kids went back to school.

First off the only platform I used to apply for jobs was LinkedIN. It seemed like the most professional and provided the most information about the types of jobs I was applying for. I never used LinkedIn Premium before but because there was a 1 month free trial I went and enabled that too which then started to give you better insights on which jobs you would be a top candidate for.

Here are the main tips I have for navigating the application process on LinkedIN :

1) Get your easy apply set up so you can one click apply or click your way through the simple questions they have on the job postings 

2) Skill Assessments : This is a big one. Anytime I applied for a job on LinkedIN it would offer for me to take a skill assessment test where you would receive a badge if you scored over a certain percentage. Many of them also came with videos and mini courses to take to help you pass them. Any skill assessment I felt I had a shot at passing I would take, and even if you fail the first time it lets you retry it twice I believe and you get an idea of how the questions are. So I took every single one I could (SQL, Excel, Python , Azure, PowerBI, R, a few more I can't remember off the top of my head. The only ones I didn't take were the ones I had no chance of passing because I had no prior experience with such as Java Development, JavaScript libraries I've never used, C++. But because all the jobs I was applying for were data related I didn't encounter those to much.

I know for a fact this helped considerably because when I would receive responses from the recruiters it would show a copy of what my application looked like on their end and it would say so and so has 3/3 of the required skills. So they know you aren't just making up stuff on your resume you are showing them that you know at least the basics.

3) Become familiar with a cloud platform. This was the biggest change I have seen since 2019 is the massive shift towards cloud based platforms, software as a service and all that good stuff. I went with Azure just because my previous experience was mostly in Microsoft SQL Server. Azure offers a 1 month free trial and with it you get $200 credit. I signed up for this and began using a bunch of different tools related to data engineering, mostly Azure Data Factory but I also spinned up an Azure SQL Server Instance, an Azure Cosmos Database which is free for a year and a Linux VM. Made some basic pipelines in Data Factory to ETL data from one source to another and learned quite a bit about using the command line interfaces PowerShell and Bash. When people asked me about this stuff in interviews I was able to answer basic questions about it and it showed them I was interested in learning new things on my own.

4) Check the number of applicants for the LinkedIN Jobs. Alot of people only spam the easy apply button, which of course I did too. But for many of the jobs that don't offer this option and require a full application on their HR platform I noticed the number of applicants were much lower. The job I currently have now was one of the ones I couldn't easy apply for and I saw it only had a 20 or so applicants as opposed to the hundreds you would see for the easy apply ones. Don't skip over these jobs just because the application process will take a few more minutes especially if it's one you really want.

5) Apply for any job you feel you can do. The job says 5 years of experience required but you only have 3 ? Apply anyway. Shoot as high as you can. Those are the ideal candidate requirements they are looking for but they know they aren't always going to find someone with that much experience. Don't get discouraged if you don't meet the minimum requirements. This is a numbers game and you want to apply for as many jobs as you possibly can especially since things have gone remote for a large part. 

6) During the interviews if you don't know something just be honest with them. Don't try to BS your way around it because this will only make you look worse if they find out you are being dishonest. Showing that you are willing to learn new things looks alot better than getting caught in a lie. 

7) Ask for criticism on your resume. This sub reddit seems to be a great source for that as well as the SQL one, Data Science, or just programming subs in general. Ask someone to do a mock interview too if you can.

8) Most important one - Don't give up! There are so many jobs available out there. Don't get discouraged. Keep learning new things. Learn from your previous mistakes. You will get one eventually. 

I started applying for jobs in mid July and I got so many offers for interviews that I had to start rejecting them. Out of the 10 interviews I did , 8 of them made it to the second round, 5 to the third. The one I ended up getting involved 5 separate interviews actually. And the day I started that job I got an offer for one of the other ones I had applied for that actually paid slightly more but it was a contract/hourly job for 9 months and it didn't seem as interesting as the one I already started so I had to turn it down. A few of the other ones I made it to the third round either went with a different candidate or just ghosted me which seems very common these days. Don't take it to you personally if this happens to you some people just don't have the common courtesy to get back to you.

Anyway I hope this is helpful or encouraging to anyone who reads it and if you have any questions for me please feel free to ask.",2022-10-09 20:57:21
t3mxlh,Apache Airflow for Beginners Tutorial Series,"Hey there,

I have been using Airflow for a couple of years in my work. I think it is a great tool for data pipeline or ETL management. Therefore, I have created this tutorial series to help folks like you want to learn Apache Airflow. So far, there are 12 episodes uploaded, and more will come.

If you are interested, you can watch the whole playlist on [YouTube](https://www.youtube.com/watch?v=z7xyNOF8tak&list=PLwFJcsJ61oujAqYpMp1kdUBcPG0sE0QMT). If you think it is helpful, consider subscribing to my [youtube channel](https://www.youtube.com/c/coder2j) and star my [GitHub repository](https://github.com/coder2j/airflow-docker). Comment what topics you want to see or discuss about Airflow in the next episode.

Latest episode: [Airflow Hooks S3 PostgreSQL](https://www.youtube.com/watch?v=rcG4WNwi900&list=PLwFJcsJ61oujAqYpMp1kdUBcPG0sE0QMT&index=13)

Updated Tutorial Episode 16.05.2022

1. [Introduction and Local Installation](https://youtu.be/z7xyNOF8tak)
2. [Get Airflow running in Docker](https://youtu.be/J6azvFhndLg)
3. [Airflow Core Concepts in 5 mins](https://youtu.be/mtJHMdoi_Gg)
4. [Airflow Task Lifecycle and Basic Architecture](https://youtu.be/UFsCvWjQT4w)
5. [Airflow DAG with BashOperator](https://youtu.be/CLkzXrjrFKg)
6. [Airflow DAG with PythonOperator and XComs](https://youtu.be/IumQX-mm20Y)
7. [Airflow TaskFlow API](https://youtu.be/9y0mqWsok_4)
8. [Airflow Catchup and Backfill](https://youtu.be/OXOiUeHOQ-0)
9. [Schedule Airflow DAG with Cron Expression](https://youtu.be/tpuovQFUByk)
10. [Airflow Connection and PostgresOperator](https://youtu.be/S1eapG6gjLU)
11. [Add Python Dependencies via Airflow Docker Image Extending and Customizing](https://youtu.be/0UepvC9X4HY)
12. [AWS S3 Key Sensor Operator](https://youtu.be/vuxrhipJMCk)
13. [Airflow Hooks S3 PostgreSQL](https://www.youtube.com/watch?v=rcG4WNwi900&list=PLwFJcsJ61oujAqYpMp1kdUBcPG0sE0QMT&index=13)",2022-02-28 18:57:27
ow7od0,Any interest in DE interview questions & experience material ?,"In the past 2 months I have given around 10 DE interviews(With startups, small and large corporates) , which gave me a fair idea of what one can expect in a DE interview.

I have saved most of the questions, which I can share in a blog, if of course there is a demand for the same.

Let me know if it would help the members on this subreddit.

Suggestion : use !remindme 7 days to be reminded in a week. I'll compile the data by then and will share it here.

Here you go :
 https://www.linkedin.com/posts/niteshx2_bigdata-dataengineer-interview-activity-6834361837778198528-AxF2",2021-08-02 04:47:44
18ja9hq,How I interview data engineers,"Hi everybody,

This is a bit of a self-promotion, and I don't usually do that (I have never done it here), but I figured many of you may find it helpful.

For context, I am a Head of data (& analytics) engineering at a Fintech company and have interviewed hundreds of candidates.

What I have outlined in my blog post would, obviously, not apply to every interview you may have, but I believe there are many things people don't usually discuss.

Please go wild with any questions you may have.

https://open.substack.com/pub/datagibberish/p/how-i-interview-data-engineers?r=odlo3&utm_campaign=post&utm_medium=web&showWelcome=true",2023-12-15 21:05:34
sexcgm,"I've had 7 interviews this week alone and more due next week, here is what I learned","Hi all,

Often see 'how do I get into the field' posts, and whilst they're no doubt useful to some, I seldom see interview advice, learnings etc. Perhaps they just don't appear in my feed, but thought it might be useful to talk about my experiences in broad terms. 

Worth mentioning that I'm a senior DE, GCP certified among a few other certs useful in this space and I'm going for other senior roles which use a broader tech stack and can help me develop. 


Learning 1: 

no one knows what they are looking for.

Why do I say this? Well, it seems as though each company has its own definition of what a data engineer does. It could be that in some companies a DE role involves only analytical engineering, whilst in others its pipeline management only and in others its a hybrid dev ops, pipeline and analytics engineer. 

I consider myself to have most of the relevant skills in this space but  the conversations I've had with hiring managers (often SEM/ HOD level) have been so widely varied, that it's worth familiarising yourself with the concepts of dev ops/ infra management/ analytics. 

One company stated in their job spec that they were happy for someone to have an understanding of Kafka and would be trained on the job, whilst in fact wanted a streaming expert. So whilst I had already recognised this an area to develop for myself, I would say that you should be more than familiar with streaming concepts (types of windows, exactly once vs at most once etc etc) if streaming is in the job spec. 

Learning 2: 

Have some code ready to discuss with your interviewer. 

My recruiter got in touch with a position last week and followed it up a few times. I've had other recruiters do this amazing thing called prep and have had them run through a list of things we'd be doing in the interview. I asked this particular recruiter about this and they replied but just the day before, and told me that they wanted me to go through some code with the hiring manager. Lucky I had something I could share, but I would suggest you have a personal project ready just in case. 

Learning 3: 

People are using AWS more than any other cloud. 

Not a problem, just an observation. 

Learning 4: 

Some hiring managers are just there to feel good about their 25 years experience and shit all over you. 

It's worth being ballsy with these people and start asking them technical questions in return. They may have a solid understanding of architecture but they won't know it all. Just because you don't know the answer to their question doesn't invalidate you. I had to ask a hiring manager WHY the problem they were asking was even designed that way. Of course you're there to evidence your skills but make sure you challenge, even if its just for you. 


Learning 5: 

Most of the directors I spoke to are fucking clueless. 

Learning 6: 

Make sure you brush up on your basics. 

I've not interviewed for a while and my mind went blank when I was asked about functional programming. It's one of those things one might read over in a document or whatever but commit that shit to memory. Other basic questions I identified as the 'basics' was 

How do you define structured / unstructured data?

What makes a database relational? 

What is OOP? 

What is setverless? 

What is distributed processing? - this came in various forms. 

What is insertfiletype? When would you use this file type? 

How do you describe denormalised data? 


These are questions that at first, I found profoundly tricky answering because I didn't have any nice quick answer for. But after a quick Google for some consolidation and revision , I was able to better summarise. I could certainly tell you about parquet, but is it useful to know that it was created by Apache? Probably not. 


Learning 6: 

ALWAYS make sure the company you're going to will support your personal development beyond the scope of your role. 

If they aren't prepared to do this, they are too corporate and bureaucratic and often will never flex for you. As a data engineer, you're in demand and can call the shots. 

And finally, out of the 7 I've been on I think 5 went well. I've been invited back for 2 second rounds already. 4 of these interviews were today alone andi think all went really well, so we shall see. I'm not in any rush to leave the company im at, I just wanted to see if I could fly.




Edit: thanks all for the engagement. Just wanted to mention I'm in the UK. I'm sure much of my experience can be transferred to any location but just thought I'd mention as the processes might vary.",2022-01-28 18:32:42
12s61tg,Forreal though,N/A,2023-04-19 18:45:38
1041369,Seems like astronomer quietly laid off 20%,N/A,2023-01-05 15:03:32
u9o08x,For the love of god please use consistent formatting and descriptive aliases in your SQL,"People need to be able to read your queries and understand them. This includes your future self, a.k.a. the poor sap who will be stuck trying to decipher your caffeine addled hieroglyphics at 2am when a critical pipeline breaks.

This isn't beat poetry. Use a standardized naming scheme and formatting style, and be descriptive. I can't believe how many people I've encountered in this field who don't follow this advice.",2022-04-22 20:32:35
zqqsqx,2022 data buzzwords translated to their actual meaning,"ELT: “shift your cost center to your warehouse”  


Modern Data Stack - “shift your cost center to your warehouse”  


Zero ETL:  “shift your cost center to your warehouse \*now with more lock in!\*”  


Credits:  “shift your costs to….variable”  


No code: “shift to needing two tools for the same job”  


Low code: “shift to coding normally”  


Batch:  “Business model for NYSE:SNOW”  


Real-time: “somewhere between nano seconds and hours”  


Data quality: “the thing we keep talking about and would like to get to someday”  


Streaming SQL: “Vendor-specific mashups of various strategies for bolting notions of time variance into a language not designed for it”  


Schemaless: “there is a schema, but we don’t know what it is”  


Bonus alternative ELT definition: ""we changed our schema and broke the data pipeline, but we can make the analysts deal with it""  


What others are we missing?  


Great thread of comments on this prompt as well: [https://www.linkedin.com/feed/update/urn:li:activity:7009593010644557825/](https://www.linkedin.com/feed/update/urn:li:activity:7009593010644557825/)",2022-12-20 15:06:55
ilr6er,Modern Data Engineer Roadmap 2020,"Hey everyone — In the last couple of weeks I've put a lot of effort into creating a high quality, comprehensive roadmap for data engineers. Hope you'll find it useful.

Here is the Github repo with the roadmap: [https://github.com/datastacktv/data-engineer-roadmap](https://github.com/datastacktv/data-engineer-roadmap)

Let me know what you think!",2020-09-03 10:52:50
u1gkua,Building a Data Engineering Project in 20 Minutes,"I created a [fully open-source project](https://www.sspaeti.com/blog/data-engineering-project-in-twenty-minutes/) with tons of tools where you'd learn web-scraping with real-estates, uploading them to S3, Spark and Delta Lake, adding Data Science with Jupyter, and ingesting into Druid, visualising with Superset and managing everything with Dagster.

I want to build another one for my personal finance with tools such as Airbyte, dbt, and DuckDB. Is there any other recommendation you'd include in such a project? Or just any open-source tools you'd want to include? I was thinking of adding a metrics layer with [MetricFlow](https://transform.co/metricflow/) as well. Any recommendations or favourites are most welcome.",2022-04-11 20:22:31
13hebz5,DE's when a new job uses a different cloud platform,N/A,2023-05-14 14:55:49
y2mdxi,Thanks to everyone here that contributes,"I would say I am still in the early years of being a data engineer and there is so much to learn. But lurking in this subreddit has helped me learn new tools, techniques, best practices, and so much more. So thanks to everyone that contributes in this subreddit. To those that give helpful advice to someone that may have asked the same question 100x. This subreddit has been like a free mentoring program at times!",2022-10-13 02:02:27
199fg7g,Did I get bamboozled into a data engineering job?,"I'm coming up on 1.5 YoE at my job where my title is ""data analyst"". This is my first real job and I got it out of college. Up until today, I assumed that I was a data analyst doing data analysty things and building a career in data analytics. However, since finding out that data engineering is a separate thing, I've started to suspect that I may actually be working in an entry-level data engineering role.

The job description asked for mastery of Tableau and proficiency in Python. Since starting, I've used Python for scripting a fair amount, but have used Tableau EDA a grand total of zero times. They trained me up in Alteryx, an ETL tool, and now my work mainly consists of Alteryx, SQL, and Python.

90% of my work is building automated data pipelines for other teams; they come to us with some process that they're doing manually in Excel and we make it automatic for them. We follow an Agile framework, gather requirements, build and test, deploy and support. Our typical end product is an app that another team uses, not a dashboard.

Am I actually a trainee data engineer? ",2024-01-18 02:43:48
svxsep,5 Beginner Data Engineering Exercises - all ready to go!,N/A,2022-02-19 01:17:38
mfrpw9,How Data Engineering Works,N/A,2021-03-29 15:13:29
14ws2ht,PARTITION BY whatever,N/A,2023-07-11 13:53:51
zcwydz,New to Data engineering and wanted to learn more about it and hoping this is the right way to go about it. Done with steps 1 and 2 for this and just wanted to know is GoogleBigQuery or Snowflake free ? and could anyone just explain step 5. Didnt understand what to do there. Thanks,N/A,2022-12-05 04:34:12
om3wl5,"Data engineering project, with a live dashboard","Hello fellow Redditors, 

  I've been interviewing engineers for a while. When someone has a side project listed on their resume I think it's pretty cool and try to read through it. But reading through the repo is not always easy and is time-consuming. This is especially true for data pipeline projects, which are not always visual (like a website). 

  With this issue in mind, I wrote an article that shows how to host a dashboard that gets populated with near real-time data. This also covers the basics of project structure, automated formatting, testing, and having a README file to make your code professional.

  The dashboard can be linked to your resume and LinkedIn profile.  I believe this approach can help showcase your expertise to a hiring manager. 

  [https://www.startdataengineering.com/post/data-engineering-project-to-impress-hiring-managers/](https://www.startdataengineering.com/post/data-engineering-project-to-impress-hiring-managers/)

  Hope this helps someone. Any feedback is appreciated.",2021-07-17 13:06:45
xsb7rm,Data Engineering Zoomcamp - free data engineering course comes back!,"We're launching another iteration of Data Engineering Zoomcamp in January 2023

&#x200B;

Join us too and learn about:

* Docker
* Orchestration
* Data lakes
* Data warehousing
* Analytics engineering
* Batch processing 
* Streaming

&#x200B;

More information here: [https://github.com/DataTalksClub/data-engineering-zoomcamp](https://github.com/DataTalksClub/data-engineering-zoomcamp)",2022-09-30 20:10:13
nn3x58,Finding the perfect identifier for entity matching,N/A,2021-05-28 18:11:54
vbd44r,The API I created as an intern surprisingly made it to production!,"I really wanted to share this as I'm so happy with the work I did. 

&#x200B;

A little background, I was a summer intern a few years back at a large and well known company in the US. I was supposed to create an API that services downstream (internal) clients that were currently connecting to the Data Lake directly. We all knew the data lake was migrating to Redshift, but didn't know when it would be live, so they had me creating the API for the current PostgreSQL DB anyways despite my mild protest. In the middle of the internship, the actual migration to Redshift started so they let me start creating a middle layer with AWS. 

&#x200B;

I honestly had no idea what I was doing, but my manager forced me to fully research top to bottom which tools I should use (as they didn't know the answer themselves) and had me justify in writing the why and how behind each tool I selected before I started creating. I ended up using AWS Lambda and some other layers, including third party IDP, to service the 12 or so clients with highly varied requirements. I basically made it so they could interact with Redshift as if there was no middle layer, but incorporated other tools that allowed for easier security management and operational management from a data lake owner perspective. After all the research and back-and-forth to get proper permissions I was finally able to create and successfully test my API on Postman.

&#x200B;

I didn't think what I created was was actually going to be used as a contractor guy who was supposed to be my mentor sandbagged me towards the end and said he knew a better way to do it without so many layers, to which I was all ears on his better idea but he withheld the specifics. I felt like all my work was for naught and chalked it up as a learning experience and moved on. I just met my old boss in person the other day and he told me that they were actually using my architecture in production! Albeit, with some minor changes. It was so awesome to hear that I got it right in the end, it is a much needed confidence boost.",2022-06-13 14:11:10
18567aq,Me as an ETL engineer watching people build data tables with no regard to what goes in them.,N/A,2023-11-27 15:58:17
1531jz7,the devs chose mongo again smh,N/A,2023-07-18 15:28:44
1beltlg,What is the hardest you have ever seen someone work manually?,"I once worked with a team who was in charge of some sales dashboards. Their process to update them was to have someone individually open the PDF's of every new invoice for the week, enter the dollar figures into an excel sheet, and then update the workbook  datasource with the new static excel file.

I work for a global market leader, we are lapping the #2 company behind us 5 times over. I would estimate that 5-10% of our headcount is allocated to jobs like these.",2024-03-14 13:59:19
140xtxu,Does the DE community want to join the Reddit protest?,"Don't Let Reddit Kill 3rd Party Apps!

#What's going on?

A recent Reddit policy change threatens to kill many beloved third-party mobile apps, making a great many quality-of-life features not seen in the official mobile app **permanently inaccessible** to users.

On May 31, 2023, Reddit announced they were raising the price to make calls to their API from being free to a level that will kill every third party app on Reddit, from [Apollo](https://www.reddit.com/r/apolloapp/comments/13ws4w3/had_a_call_with_reddit_to_discuss_pricing_bad/) to [Reddit is Fun](https://www.reddit.com/r/redditisfun/comments/13wxepd/rif_dev_here_reddits_api_changes_will_likely_kill/) to [Narwhal](https://www.reddit.com/r/getnarwhal/comments/13wv038/reddit_have_quoted_the_apollo_devs_a_ridiculous/jmdqtyt/) to [BaconReader](https://www.reddit.com/r/baconreader/comments/13wveb2/reddit_api_changes_and_baconreader/).

Even if you're not a mobile user and don't use any of those apps, this is a step toward killing other ways of customizing Reddit, such as Reddit Enhancement Suite or the use of the old.reddit.com desktop interface [](/ajwtf ""and everything that goes with it!"").

This isn't only a problem on the user level: many subreddit moderators depend on tools only available outside the official app to keep their communities on-topic and spam-free.

#What's the plan? 

On June 12th, [many subreddits](https://old.reddit.com/r/ModCoord/comments/1401qw5/incomplete_and_growing_list_of_participating/) will be going dark to protest this policy. Some will return after 48 hours: others will go away *permanently* unless the issue is adequately addressed, since many moderators aren't able to put in the work they do with the poor tools available through the official app. This isn't something any of us do lightly: we do what we do because *we love Reddit*, and we truly believe this change will make it impossible to keep doing what we love.

The two-day blackout isn't the *goal*, and it isn't the end. Should things reach the 14th with no sign of Reddit choosing to fix what they've broken, we'll use the community and buzz we've built between then and now as a tool for further action.

What can *you* do?

1. **Complain.** Message the mods of /r/reddit.com, who are the admins of the site: message /u/reddit: submit a [support request](https://support.reddithelp.com/hc/en-us/requests/new): comment in relevant threads on /r/reddit, such as [this one](https://www.reddit.com/r/reddit/comments/12qwagm/an_update_regarding_reddits_api/.), leave a negative review on their official iOS or Android app- and sign your username in support to this post.

2. **Spread the word.** Rabble-rouse on related subreddits. Meme it up, make it spicy. Bitch about it to your cat. Suggest anyone you know who moderates a subreddit join us at our sister sub at /r/ModCoord.

3. **Boycott *and* spread the word...to Reddit's competition!** Stay off Reddit entirely on June 12th through the 13th- instead, take to your favorite *non*-Reddit platform of choice and make some noise in support!

4. **Don't be a jerk.** As upsetting this may be, threats, profanity and vandalism will be worse than useless in getting people on our side. Please make every effort to be as restrained, polite, reasonable and law-abiding as possible.

**Further reading**

https://www.reddit.com/r/Save3rdPartyApps/comments/13yh0jf/dont_let_reddit_kill_3rd_party_apps/

https://www.reddit.com/r/apolloapp/comments/13ws4w3/had_a_call_with_reddit_to_discuss_pricing_bad/

https://old.reddit.com/r/ModCoord/comments/1401qw5/incomplete_and_growing_list_of_participating/

https://www.reddit.com/r/SubredditDrama/comments/1404hwj/mods_of_rblind_reveal_that_removing_3rd_party/

https://www.reddit.com/r/redditdev/comments/13wsiks/api_update_enterprise_level_tier_for_large_scale/jmolrhn/?context=3

edit: [Open Letter regarding API pricing](https://www.reddit.com/r/ModCoord/comments/13xh1e7/an_open_letter_on_the_state_of_affairs_regarding/)

[View Poll](https://www.reddit.com/poll/140xtxu)",2023-06-05 01:33:53
11zudt1,If I have to run this data pipeline one more time I'm going to lose my mind,That is all. Thank you,2023-03-23 19:21:05
yfx3m1,How I landed a $287k offer for entry-level Data Engineer at FAANG+,"A lot of posts and advice seem to focus on getting into DE after college, or transitioning into DE from an outside field. I want to tell my story as an inspiration that; there are other related roles and industries that are easier to break into; as I did. Analyst, Business Intelligence Engineer, and Analytics Engineer are all noble roles that make for easy pivots into Data Engineering with a great deal of overlap in skill set with Data Engineering. 

Here’s my career progression, to show you how I made it from $40k to $287k in 7 years:

Salary: $40k

Title: Account Executive

Out of college, first job in software sales. The company I worked for and product I sold was in the Business Intelligence space, so I wanted to learn everything about it. I quickly became the most technical salesperson in my org, and tried to transition to a more technical solutions engineer role, but was blocked by management. So I left after 18 months. 

Salary: $80k

Title: Account Executive

I worked selling a competing product to the former product, again learning everything I could. I began to learn SQL. After 14 months, I was laid off as the whole division went under. 

Salary: $100k

Title: Sr. Analyst

I went to work in the healthcare space. Because of how technical I had become in my first sales job, I easily qualified for a tool-specific specialist at healthcare company. I was by far the most advanced on my team in this tool. My SQL was my biggest weakness, and I learned fast and studied countless hours outside of work. I left after 3 years due to lack of opportunity for advancement and political messes. 

Salary: $150k+$15k bonus 

Title: Technical Lead

I worked as a contractor for a FAANG company which is a great brand for my resume, but I was just a lowly contractor. I led complex projects writing SQL and a tiny bit of python. I studied python a lot. After 2 years, I tried to convert to FTE and it became obvious that it wouldn’t happen. For the last 5 months, I spent about 5 hours a day studying algorithms and data structures and/or interviewing. After 2.5 years, I resigned. 

Salary: $175k+$25k bonus+$87k stock+$40k sign on bonus

Title: Data Engineer

I received probably 5 offers between about 30 interviews. I got 1 FAANG offer ($225k, L5 BIE) 1 Top N offer ($287k DE) and 1 fortune 100 offer ($250k Sr DE). My advice here is that; I knew algorithms and data structures would be my weakness, so I focused there. With solid SQL, Algos, and behavioral, the rest I would need to figure out as I went. I began making lists of of topics that I was weak in as I went through more and more interviews, and I would get a few inches deep into each topic and/or make study sheets and flash cards as I went. A large element of this was luck. I focused on concepts, not tools. Normal forms, for example, are conceptual and tool-agnostic. There was also a massive luck element to this game. Some companies went for hard algos questions; I failed those interviews. Some interviewers wanted to talk about optimizing a pig job; I failed that interview. Was pig worth learning because one company asked about it? No. I stayed the course of conceptual topics. 

Despite my title as a Technical Lead as a contractor at FAANG, I interviewed for a Sr. role. Because the scope of some of the projects I worked on didn’t impact multiple technical teams, I was down-leveled to entry-level Data Engineer. I declined the offer, and planned to accept another offer. A week later, the recruiter contacted me again and told me things changed on their end and they really wanted me. While they weren’t able to up-level me, they were able to negotiate the salary to the upper-end of the range for entry-level Data Engineer.",2022-10-28 18:41:38
16uu03a,Tools that seemed cool at first but you've grown to loathe?,I've grown to hate Alteryx.  It might be fine as a self service / desktop tool but anything enterprise/at scale is a nightmare.  It is a pain to deploy.  It is a pain to orchestrate.  The macro system is a nightmare to use.  Most of the time it is slow as well.  Plus it is extremely expensive to top it all off.,2023-09-28 22:34:28
14rt3ur,Is cloud a big scam?,[DELETED] ` this message was mass deleted/edited with redact.dev `,2023-07-06 00:56:26
138cvct,Welcome to JOIN hell,N/A,2023-05-05 06:22:35
ydg0g9,What do you do when your data pipeline depends on someone else’s pipeline and that upstream pipeline fails?,N/A,2022-10-25 21:16:39
14ckwyq,Stack Overflow Will Charge AI Giants for Training Data,N/A,2023-06-18 13:47:00
wsimt4,TIL,N/A,2022-08-19 16:41:47
10ybytx,Thanking my plumber every time,"So after building a near real time pipeline with super complex parsing and data quality in record speed your dashboard gal/guy sends an email to C-level showing her/his awesome dashboard and just cc you in the email.



Nobody would know 

— time spent to find optimal parallelism


— time spent for solving weird dq issues


— time spent to make the pipeline dynamically handle schema change


— time spent for threading it all together in orchestrator


After all these years still hurts on how thankless this job is.

Only lesson : Thank your plumber next time you meet him.",2023-02-10 01:03:04
yntyev,Skills to Learn for $200k - $300k Position?,"I'm currently a data warehouse engineer making $140k/year.  What skills should I master next to increase my salary to $200k and eventually to $300k?  What are some positions with higher salaries that I could transition to?  Any advice would be greatly appreciated!

Below are a few skills that I have already mastered:

* SQL Server
* SSIS
* SSRS
* Tableau
* Excel
* VBA
* Visual Basic.Net
* C#

I'm interested in data engineering, GCP/Azure/AWS, AI/machine learning, automation, big data, etc. and have a strong programming background.  I love learning new technologies, and there's so much that I want to learn, but want to focus on the most useful skills first that a $200k - $300k position would most likely require.",2022-11-06 16:00:07
yd28wn,U.K. gov consider this a decent package for a Lead DE…,N/A,2022-10-25 11:15:59
vjwrdv,"I've been a DE at FAANG for over 4 years, gone from L4 to L6, AMA","Y'all enjoyed this when ezachly did it, so thought  I'd give it a go.

I'm a different kind of DE to him I think too, so it's good to have a bit of variety.

Edit: Thanks for the AMA, feel free to ask more questions, I'll reply async. Sorry I disappeared, time zone difference is a bit tricky. Good luck with all your data engineering endeavours.",2022-06-24 19:47:48
xe5x3t,Controversial and blunt guide for why you can't get a DE job.,"Alright lads, we need to have a chat.


**Introduction**

There's a serious amount of posts in here from people who can't get a DE job.  These posts are along the lines of ""I can't get a DE job no matter how hard I try but I want one how do I get one there must be a secret thanks"".  

So, as a grateful member of this community, I intend on giving back in the form of creating the thread to end all threads.  For some it will be the advice and kick up the arse they needed to hear in order to get their head together. For others, it will crush their already crushed dreams (obligatory Sun Tzu: ""Victorious warriors win first and then go to war, while defeated warriors go to war first and then seek to win""). 

**So...why are *you* giving *me* advice? Seriously, who are you?**

I was you once upon a time (probably/maybe).  I lost my job during the great pando of 2020 and had never written a line of code before or worked in tech or IT before.  I began teaching myself how to code and 6 months later I got my first DE job (My title was and is still now Data Engineer.  Surprisingly, I also do Data Engineer things).  Whilst we're probably not equal in terms of working experience (I had a completely different career before this), we would have definitely been equal in terms of our objectives and what we had - no experience and wanting to break into the DE field.  

On that note, of course this is all from my perspective of ""I can do it, so you can too"", so there's going to be some pretty harsh advice because of two reasons.  One, some of you need a good ol' talking to and two, I'm obviously shit at communicating.

So, let's get going.


**Your CV/Resume sucks.**

This has to be one of the most common problems on here.  You have people experienced and inexperienced all with the same problem - your CV/resume sucks.  It isn't that you suck or your skills are bad, it's just that you have no idea how to write a CV.  And that's alright, it just needs to be said.

Why does your CV suck? Because it's either all over the place, lacks consistency, you're blatantly overselling yourself and people are seeing it a mile off, or is just a plain boring read.  

A CV should tell a story about why you are doing what you are doing.  It should be easy to read, logical to follow (I never ever want to be looking at three different places in your CV.  Should be top down), allow the reader to make their own assumptions about your skill level based on your work, and the only questions left to ask should be covered with ""We can just ask them that in an interview"".  Skills should be what you are comfortable talking about and have done a little of but want to work in.  If you include every single skill you know it makes you look desperate to please to search algorithm and makes you look like you have no idea what you actually want to do.  

That's the general format covered.  Why else does it suck? If you're experienced, you haven't talked a single bit about what you did was important.  Your CV needs to read closer to a list of accomplishments and stuff you're proud of rather than a list of duties.  Not only is it more fun to write about, but it gives the person reading an idea of how you communicate, what you value, and how you think.  Value is the key word here.  Engineering and programming really is just a means to an end and the actual goal is being able to identify and create valuable things for somebody else.


**Not experienced? Applying with projects? Your projects suck.**

So, we've gone over why your CV is probably bad if you are experienced.  So, let's talk about if you have zero experience and are looking at breaking into the industry.

There is an extremely high probability your projects are absolute garbage.  It's incredibly easy to make garbage projects - you can copy them from the internet free of charge.  It's why they are garbage - you don't learn anything from it.  Being extra blunt - there have been a lot shite projects here because almost all of them are copypasta.  Generic binary classification model for some health problem, using the Spotify API to map your most played songs, making a weather app.  They are all boring and over done so the first thing people will think see that in your portfolio is ""These are boring and over done"".  Bonus punishment points if your portfolio is nothing but copypasta.

What are ""good"" projects? Honestly, any data pipeline (get data in, transform, store) is a great start for anybody starting out.  Storing in a database of sorts is even better.  With good logging and unit tests is really getting up there.  Including CI/CD on top of that and you are truly onto a winner.  If it's in the cloud, you're basically a DE at this point so local is fine.  But you'll 100% need to know at least what cloud services do and how they work i.e. the concept of serverless architecture.  The difficult part here is they have to be personal and you will have to have used your own ideas to make it.  If you get asked about your pipeline design decisions during the interview, it should be super easy because you made every single decision.  If it's somebody else's work, you are very likely to fail.

Asking anybody for ""good projects"" is my number one hated question if you are trying to get into somewhere from nothing because if you can't come up with your own ideas, then the job itself is going to be very very hard.  Coming up with ways to solve problems is literally the game here so if you can't invent your own problems with no constraints and unlimited freedom on how you solve it, solving actual problems with actual requirements is going to be a pain in the tits.


**Your mindset sucks.**

So, we've covered application problems.  

One of most annoying things to read is people saying they can't get jobs with no experience or that job ads ask for loads of skills they don't have.  I agree this is common, however, what I disagree with this mindset of giving up at the first hurdle.  If you think you can do the job, then just apply anyway.  The worst thing they can do is say no.  If people keep saying no, then your problem is likely to be your CV sucks.  Go fix it and try again.

Don't let not knowing every single technology on the list stop you.  If you know your lakes from your warehouses, if you have written a few of your own basic pipelines and have a solid Github with DE related projects, what you need is time, patience, and perserverance.  Rest assured if you are tenacious enough, you will become lucky enough.  Applying for jobs isn't by it's nature easy.  It's draining, it's hard work, and, like some things just do in life, needs energy put into it to be successful.  

Be brave.  Small barriers are exactly that - small.  Whatever you don't know, you can learn on the job and you 100% can if you trust yourself.  I know.  It sounds like bullshit but I openly invite you all to try a little bravery and a little bravado when applying.

**Conclusion.**

The real goal of this is to emphasise that job hunting as well as interviewing are themselves skills. You can be the greatest engineer in your area but if there's no introspection on what you can do better, then you will of course be doomed to being stuck in whatever role you have now.  You could be exactly what a company is looking for but you have to be brave and willing to take risks when opportunities arise.",2022-09-14 16:10:09
17iyl70,Great on-prem open source source modern data stack,Went to a dbt meetup a few days ago and this guy from the Barcelona Supercomputing Lab presented his fully open source on-prem modern data stack. Pretty cool imo. They were working with health data from hundreds of hospitals all across EU so they couldn’t use the cloud.,2023-10-29 08:36:02
11lvm8z,"I got a data engineering horror story, what is yours?","I don't know about you, but I have plenty of data engineering horror stories to share. I'd love to hear the one that still gives you shivers.

  
**Here's my highlight:**  


* I'm at a 500 people medium sized company, 300-400$m revenue. But we're growing strong, at 10-20% each year. Ingest data via python, dbt for transformations, storage in PostgreSQL and Tableau as BI tool on top of it.
* We've been pushing the adoption of Tableau, and the company is eating it up, they love it. They are all over the dashboards.
* What we're particular proud of is our **""north star metric**"" dashboard, showing our new key metric, based on a recent business pivot.
* In our most important customer segment, it looks like it's starting to grow **exponentially**! Everyone is excited!

Suddenly an important manager calls me up

""*hey, something is wrong with the north star. The dashboard looked completely different yesterday! Our exponential growth is gone! Surely there is something wrong, please fix it by this evening. Tomorrow is the board meeting and I'm presenting the exponential growth.""*

  
Took us some time to understand this one... Apparently, ALL data changed, the complete metric in this customer segment broke in, not just for today, but also for yesterday, the day before, and so on...

  
After some research, we realized a huge problem: The biggest customer in that segment left a few months ago, and filed a ""deletion request"". The upstream team responsible for this followed through, and basically ""detached the relevant data from the customer account"". 

  
So there we were. We didn't even know about this process, and had no chance to recover. The manager was left without his exponential growth. 

  
*Aftermath: So what we did from then on is to turn on snapshotting of important data sources (using dbt). After being really unhappy, the manager was still convinced of the underlying exponential growth which shouldn't be reliant on one big customer, but the situation felt terrible. And I'm quite happy that only data from basically one customer went down the drain.*

\----

  
How about you? Do you have a horror story to share?",2023-03-08 12:52:53
141vfwl,I feel like I won the job lottery and it has sent my imposter syndrome into the stratosphere,"A couple years ago I put my career on pause and moved back home to be closer to an ailing parent. I was a data analyst in my previous role, where I mostly worked in Excel, Power BI, a bit of SQL and Python. During this pause I beefed up my SQL, Python and stats knowledge and built some portfolio projects in Streamlit to showcase my interests and abilities. I also learned Git along the way as well.

About a year ago I started job hunting where I was mostly looking for analyst roles. As I progressed in the job hunt I became increasingly interested in analytics engineering but felt like my skills weren't quite there yet. At the time I was thinking the best path would be getting another DA role and using it to catapult into an engineering role down the line. Fast forward a year, countless job applications, numerous interviews later, I finally had the most incredible breakthrough.

Last week someone in my network connected me to the owner of a software development company that builds AI tools for customers. We exchanged a couple texts, hoped on a call, got talking about my journey in the world of data and how I ultimately want to get into analytics engineering. He then asked me if I was comfortable with window functions in SQL (I am) and if I know basic data structures (I do). At that point he asked me if I could send him a copy of my CV and if I could meet with him and some of his teammates for a dinner later in the week. I go to the dinner, meet the teams senior software engineers, we hit it off over some tacos, and they essentially create a role for me out of thin air! It was all very serendipitous and I still have no idea how this happened. They even told me that they weren't in the market for hiring but liked my story/journey/tenacity so much that they wanted me to come and work for them. The owner even told me as we were shaking hands and saying goodbye that he doesn't care about my background and that ""it is all about investing in the right people"".

Yes, they are a legitimate company and have several large clients that you have heard of. I received their job offer today and I nearly threw up after reading it. The title is Data Automation Developer, so most of my work is going to be in automation testing and they have also thrown in some ELT tasks as well. It's a fully remote role and one that I never imagined I would have in my career. While I don't feel like I am being setup for failure, and I do really like the team, I can't help but feel immense imposter syndrome when I look at the job posting that they created and I see all sorts of things I have no experience in. Is this even real life?? I understand the idea of hiring on someones potential but this is all incredibly daunting. Has anyone had a similar experience to this?",2023-06-05 23:08:45
uu9j14,Created my First Data Engineering Project a Surf Report,"# Surfline Dashboard

Inspired by this post: [https://www.reddit.com/r/dataengineering/comments/so6bpo/first\_data\_pipeline\_looking\_to\_gain\_insight\_on/](https://www.reddit.com/r/dataengineering/comments/so6bpo/first_data_pipeline_looking_to_gain_insight_on/)

&#x200B;

I just wanted to get practice with using AWS, Airflow and docker. I currently work as a data analyst at a fintech company but I don't get much exposure to data engineering and mostly live in sql, dbt and looker. I am an avid surfer and I often like to journal about my sessions. I usually try to write down the conditions (wind, swell etc...) but I sometimes forget to journal the day of and don't have access to the past data. Surfline obviously cares about forecasting waves and not providing historical information. In any case seemed to be a good enough reason for a project.

Repo Here:

[https://github.com/andrem8/surf\_dash](https://github.com/andrem8/surf_dash)

&#x200B;

# Architecture

# 

https://preview.redd.it/ckjp60xdhp091.png?width=9792&format=png&auto=webp&s=f7cae5b8fb1167bebef44753dadb8d73a4d5c2dc

# Overview

The pipeline collects data from the surfline API and exports a csv file to S3. Then the most recent file in S3 is downloaded to be ingested into the Postgres datawarehouse. A temp table is created and then the unique rows are inserted into the data tables. Airflow is used for orchestration and hosted locally with docker-compose and mysql. Postgres is also running locally in a docker container. The data dashboard is run locally with ploty.

# ETL

https://preview.redd.it/vgfsrk8ihp091.png?width=1895&format=png&auto=webp&s=841cf9002ac643a17536ebf92dde63714a1e3989

# Data Warehouse - Postgres

# 

https://preview.redd.it/dhy74eykhp091.png?width=1039&format=png&auto=webp&s=cbed0caf7e89198aa0d68a96a254fc846ba13212

# Data Dashboard

&#x200B;

https://preview.redd.it/896bizwnhp091.png?width=627&format=png&auto=webp&s=86cadc6e6ab8fa594e49e0a3fca56df2d9c72740

# Learning Resources

Airflow Basics:

&#x200B;

\[Airflow DAG: Coding your first DAG for Beginners\]([https://www.youtube.com/watch?v=IH1-0hwFZRQ](https://www.youtube.com/watch?v=IH1-0hwFZRQ))

&#x200B;

\[Running Airflow 2.0 with Docker in 5 mins\]([https://www.youtube.com/watch?v=aTaytcxy2Ck](https://www.youtube.com/watch?v=aTaytcxy2Ck))

&#x200B;

S3 Basics:

&#x200B;

\[Setting Up Airflow Tasks To Connect Postgres And S3\]([https://www.youtube.com/watch?v=30VDVVSNLcc](https://www.youtube.com/watch?v=30VDVVSNLcc))

&#x200B;

\[How to Upload files to AWS S3 using Python and Boto3\]([https://www.youtube.com/watch?v=G68oSgFotZA](https://www.youtube.com/watch?v=G68oSgFotZA))

&#x200B;

\[Download files from S3\]([https://www.stackvidhya.com/download-files-from-s3-using-boto3/](https://www.stackvidhya.com/download-files-from-s3-using-boto3/))

&#x200B;

Docker Basics:

&#x200B;

\[Docker Tutorial for Beginners\]([https://www.youtube.com/watch?v=3c-iBn73dDE](https://www.youtube.com/watch?v=3c-iBn73dDE))

&#x200B;

\[Docker and PostgreSQL\]([https://www.youtube.com/watch?v=aHbE3pTyG-Q](https://www.youtube.com/watch?v=aHbE3pTyG-Q))

&#x200B;

\[Build your first pipeline DAG | Apache airflow for beginners\]([https://www.youtube.com/watch?v=28UI\_Usxbqo](https://www.youtube.com/watch?v=28UI_Usxbqo))

&#x200B;

\[Run Airflow 2.0 via Docker | Minimal Setup | Apache airflow for beginners\]([https://www.youtube.com/watch?v=TkvX1L\_\_g3s&t=389s](https://www.youtube.com/watch?v=TkvX1L__g3s&t=389s))

&#x200B;

\[Docker Network Bridge\]([https://docs.docker.com/network/bridge/](https://docs.docker.com/network/bridge/))

&#x200B;

\[Docker Curriculum\]([https://docker-curriculum.com/](https://docker-curriculum.com/))

&#x200B;

\[Docker Compose - Airflow\]([https://medium.com/@rajat.mca.du.2015/airflow-and-mysql-with-docker-containers-80ed9c2bd340](https://medium.com/@rajat.mca.du.2015/airflow-and-mysql-with-docker-containers-80ed9c2bd340))

&#x200B;

Plotly:

&#x200B;

\[Introduction to Plotly\]([https://www.youtube.com/watch?v=hSPmj7mK6ng](https://www.youtube.com/watch?v=hSPmj7mK6ng))",2022-05-20 22:34:05
rdftxf,Hold the line fellow data engineers,"I’ve been interviewing around and so far every place I’ve interviewed with hasn’t been able to secure a data engineer because they require in office work after the pandemic.  

All I wanted to say is keep it up guys. If you’re looking for a new job that is remote work only, keep putting pressure on these companies!",2021-12-10 19:01:00
13qg3t1,Why can I not understand what DataBricks is? Can someone explain slowly?!,"I have experience as a BI Developer / Analytics Engineer using dbt/airflow/SQL/Snowflake/BQ/python etc... I think I have all the concepts to understand it, but nothing online is explaining to me exactly what it is, can someone try and explain it to me in a way which I will understand?",2023-05-24 08:56:40
vzlnh7,"I made a pipeline that integrates London bike journeys with weather data using Google Cloud, Airflow, Spark, BigQuery and Data Studio","Like [another recent post](https://www.reddit.com/r/dataengineering/comments/vkfs57/i_created_a_pipeline_extracting_reddit_data_using/), I developed this pipeline after going through the [DataTalksClub Data Engineering course](https://github.com/DataTalksClub/data-engineering-zoomcamp). I am working in a data-intensive STEM field currently, but was interested in learning more about cloud technologies and data engineering. 

The pipeline digests two separate datasets: one that records bike journeys that take place using London's public cycle hire scheme, and another that contains daily weather variables on a 1km x 1km grid across the entirety of the UK. The pipeline integrates these two datasets into a single BigQuery database. Using the pipeline, you can investigate the 10 million journeys that take place each year, including the time, location and weather for both the start and end of each journey.

The repository has a detailed [README](https://github.com/jackgisby/tfl-bikes-data-pipeline/blob/main/README.md) and additional documentation both within the Python scripts and in the [docs/](https://github.com/jackgisby/tfl-bikes-data-pipeline/tree/main/docs) directory.

The GitHub repository: [https://github.com/jackgisby/tfl-bikes-data-pipeline](https://github.com/jackgisby/tfl-bikes-data-pipeline)

&#x200B;

**Key pipeline stages**

1. Use Docker/Airflow to ingest weekly cycling data to Google Cloud Storage
2. Use Docker/Airflow to ingest monthly weather to Google Cloud Storage
3. Send a Spark job to a Google Cloud Dataproc cluster to transform the data and load it to a BigQuery database
4. Use Data Studio to create dashboards

[Overview of the technologies used and the main pipeline stages](https://preview.redd.it/x8b8gmwqfpb91.png?width=1497&format=png&auto=webp&s=9eeb04e7dfcd8d25a73b97c3fcf0a6e81a549e6f)

&#x200B;

**BigQuery Database**

I tried to design the BigQuery database like a star schema, although my journeys ""fact table"" doesn't actually have any key measures. The difficult part was creating the weather ""dimension"" table, which includes recordings each day in a 1km x 1km grid across the UK. I joined it to the journeys/locations tables by finding the closest grid point to each cycle hub.

[Schema for the final BigQuery database](https://preview.redd.it/rtnvexdqfpb91.png?width=689&format=png&auto=webp&s=8bac5877b9378b56bba588734bb48dbf2bfc611c)

&#x200B;

**Dashboards**

I made a couple of dashboards, the first visualises the main dataset (the cycle journey data), for instance in the example below.

[Dashboard filtered for the four most popular destinations from 2018-2021](https://preview.redd.it/ak8a0q2yfpb91.png?width=2845&format=png&auto=webp&s=a8dda8a1f4878a6689ed5f845719f371cd491e83)

And another to show how the cycle data can be integrated with the weather data.

[A dashboard comparing the number of journeys taking place to the daily temperature in 2018 and 2019. The data is for journeys starting at \\""Hop Exchange, The Borough\\"" in London](https://preview.redd.it/r0cborybgpb91.png?width=1834&format=png&auto=webp&s=3ca83e7ddb611fbdc99328ad0e437f142de88a52)

**Data sources**

* Transport for London Cycling Data: [https://cycling.data.tfl.gov.uk/](https://cycling.data.tfl.gov.uk/)
* Weather Data: [https://catalogue.ceda.ac.uk/uuid/4dc8450d889a491ebb20e724debe2dfb](https://catalogue.ceda.ac.uk/uuid/4dc8450d889a491ebb20e724debe2dfb)

&#x200B;

The pipeline has a number of **limitations**, including:

* The pipeline is probably too complex for the size of the data, but I was interested in learning Airflow/Spark and cloud concepts
* I do some data transformations before uploading the weather data to Google Cloud Storage. I believe it would be better to separate the Airflow process from this computation
* It might be worth using Google's Cloud Composer to host Airflow rather than running it locally or on a virtual machine
* The Spark script is overly complex, it would be better to split this up into multiple scripts
* There is a lack of automated testing, validation of input data and logging
* In reality, the weather aspect of the pipeline is probably a bit overkill. The weather at the start and end of each journey is unlikely to be too different. Instead of collecting weather variables for each cycle hub, I could have achieved a similar effect by including a single variable for London as a whole. 

I stopped developing the pipeline as I have other work to do and my Google Cloud trial is coming to an end. But, I'm interested in hearing in any advice/criticisms about the project.",2022-07-15 10:46:57
vhcn7n,The State of Data Engineering 2022,N/A,2022-06-21 12:52:08
13jgov7,Secret To Optimizing SQL Queries - Understand The SQL Execution Order,N/A,2023-05-16 20:45:01
t4kz8u,Wtf is a datalake?,"When I first heard the term used a few years ago I thought it was some advanced highly integrated composition of AI/ML techniques, data, and datastores.    Now, as I decided to learn a little bit about them a datalake seems to be nothing more than a giant bucket for you to toss your ""similiar"" data into.

By the latter, that would mean my  designated storage of 16TBs,  which consists of nothing but horse porn, is a data Lake.

So can someone please explain what a data Lake is, what it does, and how it's used?

EDIT:  Question answered.  Y'all rock, so glad I found this sub.",2022-03-01 23:01:10
o64f6n,"DataBricks is providing top notch learning material for Data Engineers and Data Scientists Worth 2000USD(1.45 Lakh Rs) for Free. Verified June 22, 2021.","Update 26-07-2021:

DataBricks is providing top notch learning material for Data Engineers and Data Scientists Worth 2000USD(1.45 Lakh Rs) for Free.

The courses and trainings are working again.

Please go to the below post for further instructions:

https://www.reddit.com/r/Stream2Learn/comments/ortlng/databricks_is_providing_topnotch_learning/?utm_source=share&utm_medium=web2x&context=3

Update 29-06-2021:
---------------------------------------------------------------------------------
The coupon has expired and i will keep you guys posted on this same thread.

Meanwhile as the coupon has expired , as a consolation i am planning to upload a set of Udemy courses with free certificates on a daily basis which are available for a limited time. So kindly make use of the opportunity ASAP.

The courses might vary from various tech stacks!

The post is below here:

https://www.youtube.com/watch?v=J8Oau6zn8f4

<b>The above link would be updated on a daily basis with around 10 to 15 courses per day which are fully free and have lifetime access in Udemy.</b>

---------------------------------------------------------------------------------

Databricks Code:

[https://www.youtube.com/watch?v=iVy9rGZmDoU](https://www.youtube.com/watch?v=iVy9rGZmDoU)

Check the above Video.

DataBricks is providing top notch learning material for Data Engineers and Data Scientists Worth 2000USD(1.45 Lakh Rs) for Free.  

https://preview.redd.it/xh0fvmxtnx671.png?width=2747&format=png&auto=webp&s=1a7c4cea1a82c5ba7e173fd4d47554822a7a9954



I am personally enrolled into data engineering path as I am into Bigdata domain.  


The learning material is top notch as it is prepared by the original creators of Apache Spark.  


Here are the steps to get enrolled for free:  
Check the Video.  

Apply Coupon ""DB_PE"" and click on checkout  

Create your account and get 2000USD course for free  
Enjoy Learning!!  


Share as much as you can so that others can also get a chance to learn too .  


I have also pinned the steps in comments too if anyone is not able to click on link can check the comment section  

Code is working again.

Please do subscribe to my channel . And a like and a comment there would be great as well.

 https://www.youtube.com/channel/UCjO8Jq2sdpuI134axhMp0Fg",2021-06-23 03:19:33
18wnsqj,Data Testing Cheat Sheet: 12 Essential Rules," 

1. **Source vs Target Data Reconciliation:** Ensure correct loading of customer data from source to target. Verify row count, data match, and correct filtering.
2. **ETL Transformation Test:** Validate the accuracy of data transformation in the ETL process. Examples include matching transaction quantities and amounts.
3. **Source Data Validation:** Validate the validity of data in the source file. Check for conditions like NULL names and correct date formats.
4. **Business Validation Rule:** Validate data against business rules independently of ETL processes. Example: Audit Net Amount - Gross Amount - (Commissions + taxes + fees).
5. **Business Reconciliation Rule:** Ensure consistency and reconciliation between two business areas. Example: Check for shipments without corresponding orders.
6. **Referential Integrity Reconciliation:** Audit the reconciliation between factual and reference data. Example: Monitor referential integrity within or between databases.
7. **Data Migration Reconciliation:** Reconcile data between old and new systems during migration. Verify twice: after initialization and post-triggering the same process.
8. **Physical Schema Reconciliation:** Ensure the physical schema consistency between systems. Useful during releases to sync QA & production environments.
9. **Cross Source Data Reconciliation:** Audit if data between different source systems is within accepted tolerance. Example: Check if ratings for the same product align within tolerance.
10. **BI Report Validation:** Validate correctness of data on BI dashboards based on rules. Example: Ensure sales amount is not zero on the sales BI report.
11. **BI Report Reconciliation:** Reconcile data between BI reports and databases or files. Example: Compare total products by category between report and source database.
12. **BI Report Cross-Environment Reconciliation:** Audit if BI reports in different environments match. Example: Compare BI reports in UAT and production environments.

[Data Testing Cheat Sheet](https://preview.redd.it/mknzrwbvn0ac1.png?width=1887&format=png&auto=webp&s=f9023dd75ddde8a5f4355eec6e0d0be08c836e48)",2024-01-02 11:59:27
13znm1j,What is the Leetcode equivalent for Data Engineering?,"Actively interviewing so I need some prep material for Data wrangling questions if there is a single source out there.

I'm looking for a source around questions like:

\- Given a source data (JSON, CSV), derive insights to answer questions

\- Clean up a given dataset to answer questions etc.

\- Python dictionary / Json API response manipulation.

&#x200B;

Thank you.",2023-06-03 19:56:52
19c2ftl,Some Data Scientists write bad Python code and are stubborn in code reviews,"My first job title in tech was Data Scientist, now I'm officially a Data Engineer, but working somewhere in Data Science/Engineering, MLOps and as a Python Dev.

I'm not claiming to be a good programmer with two and a half years of professional experience, but I think some of our Data Scientists write bad Python code.  


Here I explain why:

* Using generic execptions instead of thinking about what error they really want to catch
* They try to encapsulate all functions as static methods in classes, even though it's okay to use free standing functions sometimes
* They don't use enums (or don't know what enums are used for)
* Sometimes they use bad method names -> they think `da_file2tbl_file()` is better than `convert_data_asset_to_mltalble()` (What do you think is better?)
* Overengineering: Use of design patterns with 70 lines of code, although one simple free-standing function with 10 lines would have sufficed (-> but I respect the fact that an effort is made here to learn and try out new things)
* Use of global variables, although this could easily have been solved with an instance variable or a parameter extension in the method header
* Too many useless and redundant comments like:  
`# Creating dataframe`  
`df = pd.DataFrame(...)`
* Use of magic strings/numbers instead of constants
* etc ...

What are your experiences with Data Scientists or Data Engineers using Python?

I don't despise anyone who makes such mistakes, but what's bad is that some Data Scientists are stubborn and say in code reviews: ""But I want to encapsulate all functions as static methods in a class or ""I think my 70-line design pattern is better than your 10-code-line function"" or ""I'd rather use global variables. I don't want to rewrite the code now."" I find that very annoying. Some people have too big an ego. But code reviews aren't about being the smartest in the room, they're about learning from each other and making the product better.  


Last year I started learning more programming languages. Kotlin and Rust.  I'm working on a personal project in Kotlin to rebuild our machine learning infrastructure and I'm still at tutorial level with Rust.  Both languages are amazing so far and both have already helped me to be a better (Python) programmer. What is your experience? Do you also think that learning more (statically typed) languages makes you a better developer? ",2024-01-21 12:31:19
13oaw8m,Cloud Comparison by simonholdorf,N/A,2023-05-22 00:14:01
ye10az,"Name that Title.. I’ll go first, Analytics Project Manager","I’m joking, kinda, not really depending on the company lol",2022-10-26 15:33:32
u2uyty,PSA: Don't apply for Data Engineering positions at FAANG,"Of course , this depends entirely on what you want to do.

If you enjoy less of the software engineering side of Data Engineering, you enjoy building dashboards and using GUI tools.. then a DE position at FAANG is perfect.

However, if you want to do more software engineering, programming, computer science, etc. Then what you want to do is apply for positions such as **Software Engineer - Data**

Here's an example of a [job positing from Amazon](https://www.amazon.jobs/en/jobs/1823062/software-engineer-recommender-systems-big-data-distributed-computing-machine-learning)

Yes, this does mean you will have to know computer science, software engineering, data structures and algorithms. It also means you will have to know how to do leetcode medium / hards (both SQL and traditional ones based on heaps, graphs, stacks, queues, etc). It is also beneficial to have some fullstack / backend software engineering experience for these positions, because there is a huge carry over. You would also want good System Design knowledge / software architecture knowledge.

Just wanted to post this because I see a lot of people complaining that their new DE job isn't what they hoped for. Now you know what to look for!",2022-04-13 17:00:12
1aofpbr,What we learned after running Airflow on Kubernetes for 2 years,N/A,2024-02-11 19:07:49
146rj9m,Does anyone else hate Pandas?,"I’ve been in data for ~8 years - from DBA, Analyst, Business Intelligence, to Consultant. Through all this I finally found what I *actually* enjoy doing and it’s DE work.

With that said - I absolutely hate Pandas. It’s almost like the developers of Pandas said “Hey. You know how everyone knows SQL? Let’s make a program that uses completely different syntax. I’m sure users will love it”

Spark on the other hand did it right.

Curious for opinions from other experienced DEs - what do you think about Pandas?

*Thanks everyone who suggested Polars - definitely going to look into that",2023-06-11 11:26:53
p3b3xd,Just got hired as a DE in FAANG,"It's obviously nice to brag to my friends, but none of them are in this space, so it doesn't feel as good to share this with them. I thought you all would appreciate it more. This is the proudest I've been, and I can't wait to dive in! Give me any tips/advice for onboarding if you've got them!

ETA: I wrote a blog post about my path leading to here and the interview process itself, which can be found [here](https://tibblesnbits.com/posts/de-interview-faang).",2021-08-12 23:03:24
mqseze,"Educational project I built: ETL Pipeline with Airflow, Spark, s3 and MongoDB.","While I was learning about Data Engineering and tools like Airflow and Spark, I made this educational project to help me understand things better and to keep everything organized:

https://github.com/renatootescu/ETL-pipeline

Maybe it will help some of you who, like me, want to learn and eventually work in the DE domain.

What do you think could be some other things I could/should learn?",2021-04-14 15:02:30
1aqq9vc,What the Hell is a Data Lake?,"I’m writing this post after, admittedly, never having used a “data lake” (I think) but have thought I knew what it was… on a few different occasions.

I know it’s a place to store data, but what data structure does it use? Presumably, it is a directory structure and storage is nonmutable… block storage, like S3. is that it?

Then there’s MongoDB Atlas which says it’s a Data Lake and I’m like wtf… I’ve used Mongo before to record some JSON documents without a schema, but that’s not what I know a Data Lake to be. Unless “Atlas” is something else…

So now I’m wondering, is a Data Lake less about the underlying structure and properties, and more about what goes in (raw data) and what derived data comes out (effectively ELT). Is this general idea, being used at large scale, what constitutes a “Data Lake?”

Help me out here guy please.",2024-02-14 15:59:10
18j0ygk,"""We have so many challenging projects!"" ",N/A,2023-12-15 14:10:32
15xoran,I am a 10 YOE (SSIS/low-code) DE preparing to transition into tier 1 tech companies. Here's my study plan in case it helps someone else.,"Everything is listed in order of importance. I'm breaking my prep down into:

1. **DS & Algorithms**
   1. Python Data Structures (Dicts, Lists, Sets, Tuples)
   2. CS Data Structures (Hash, Strings, Trees, Graphs, ArrayLists, Linked Lists, Heaps)
   3. Algorithms (BFS, DFS, Binary Search, Sorting)
   4. Concepts (\*Big O\*, Recursion, DP, Memory)
   5. Book: Cracking the coding interview - use (a) Technical *Approach* and (b) Chapter Explanations ; avoid problem sets
   6. Sites: Leetcode (no more than medium python for each major concept) ; get premium and take advantage of ""Learn"" cards for Recursion and DP.
   7. Sites: Technical Handbook - tells you what you're being evaluated on --- its not just about getting the right answer!
2. **System Design**
   1. Analytics Platforms -
      1. Research the companies you are interested in and understand why they use the technologies they do. Biggest misconception about DE System Design is that it is like SWE System Design -- it is not.
      2. Focus is on: tapping into Operational Data Stores (ODS), using Extract Transform Load (ETL) for batch or streaming processes, storing data with proper partitioning and tools, using data for Reports/Dashboards or serving it up to ML models with APIs.
   2. The *Approach* \-
      1. [Youtube Video by Mikhail Smarshchok](https://www.youtube.com/watch?v=bUHFg8CZFws) By far the best video I have seen on approach. For content, see above.
      2. Book: Alex Xu System Design Interview
      3. Site: Grokking the System Design Interview
   3. SWE Fundamentals - Doesn't hurt to know foundational System Design concepts. They are all related and approach resources will cover what you need to know.
   4. API Design - Site: Grokking the API Design Interview (I haven't personally started yet)
3. **Product Sense (for meta this is # 2 priority)**
   1. What is product sense? To understand and troubleshoot your product means you need to measure the right metrics. Your daily active users (DAU) has tanked dramatically, how do you find out what's the issue? What metrics do you capture and look for? How do you use them to improve your product?
   2. Site: Youtube Channel - Emma Ding - *Approach* and concepts
   3. Resources: Meta Data Engineer Guide (by meta engineers)
4. **Data Modeling**
   1. Book: The data warehouse toolkit (this is the only book on the subject I have ever read, rest I've googled problems when I ran into them for work)
   2. SWE interview snippets - when people dive into ""design uber"" or ""design twitter"", they often set up the data model. SWE system design interviews are worth browsing for this concept
5. **ML Concepts**
   1. Supervised, Unsupervised, Deep Learning, Model Eval -- There's many resources out there, I paid $2000 for MIT Great Learning Course and they have a nice modular learning platform.
   2. Model Ops / Deployment: Book - Machine Learning Design Patterns
   3. Approach: Book - Machine Learning System Design Interview
6. **Cloud (AWS is the most commonly used)**
   1. Learn about common DE tools used for ETL
   2. Learn about common ML tools
   3. Get a cert if you want

&#x200B;

\*Approach resources will help you with developing a methodology for answering certain types of questions. You could understand a DS and probably coded it in college, but you may not be able to use it in an interview which is time-constrained and high-pressure without a good approach.

\*Books - z library

This study guide is my second attempt at trying after passing meta and roblox loops, but ultimately getting down-leveled with no offer. This guide is for senior DE positions; if you are entry-level, you may focus less on System Design and cover high-level ML and cloud concepts.

&#x200B;

Current TC: $240K (Cash, Bonus) No equity -- HCOL",2023-08-22 00:01:04
10h56jr,I started a new DE job in Dec and I suck at git.,"That's it. I was an sql dev and a data analyst before that. The closest thing I have to version control was renaming each new file with a date. I am feeling like a lamb in fire. My new coworkers are nice about but I do not want to keep asking them for help.

Edit: thank you all so much",2023-01-20 19:02:11
rdw3b3,Data Engineering Jargon,"I wrote a list of 50 - I'll share ten at a time.

1- 10 is below.

11-20 is [here](https://www.reddit.com/r/dataengineering/comments/rem26j/data_engineering_jargon_part_2/)

21-30 is [here](https://www.reddit.com/r/dataengineering/comments/rfbuu8/data_engineering_jargon_part_3/)

31-40 is [here](https://www.reddit.com/r/dataengineering/comments/rg5vr0/data_engineering_jargon_part_4/)

**1. Data Dump**

A file or a table containing a significant amount of data to be analysed or transferred.

*A table containing the ""data dump"" of all customer addresses.*

**2. Data Pipelines**

A data processing method akin to a pipeline, which starts with data ingestion then processing then completion.

*A pipeline where customer address data is ingested from source A and then aggregated according to their cities and this new information is loaded into destination B.*

**3. DBA**

Database Administrator is an admin role that understands the particular database technology and how to get the best out of it. This includes improving performance, backups and recovery.

*Performance tuning the database to respond better to particular complex data queries.*

**4. Data Warehouse**

A method of organising data to make it easy to analyse and report to make business decisions

*Oracle data warehouse. Organising customer data in a data warehouse to be able to report the number of newly acquired customers.*

**5. Data Mart**

A subset of a data warehouse, created for a very specific business use case.

*Finance data mart storing all the relevant financial information required by the Accounting team to process their month-end cycles.*

**6. ODS**

Operational data store generally stores limited and current information to help simple queries. Unable to handle historical or complex data queries.

*An ODS for daily stock fluctuations in a warehouse help the warehouse manager decide what to prioritise in the next order delivery.*

**7. EDW**

The same as a data warehouse except it includes all the data within an organisation. This means that the entire enterprise can rely on this warehouse for their business decisions.

*Organising sales, customer, marketing and finance data in an enterprise data warehouse to be able to create several key management reports.*

**8. RDBMS**

Relational database management system. All of the above examples are RDBMS, meaning they store data in a structured format using rows and columns.

*A Microsoft SQL server database.*

**9. In-memory DB**

Traditional databases have been used for complex calculations and queries. They store information on the actual disk in the computer. In-memory DB stores all the information on their memory (RAM), this allows for rapid calculations without read and write a function to a normal disk.

*A drill-down functionality of a live dashboard.*

**10. Data Lake**

A repository for all kinds of structured and unstructured data. Mainly based on Hadoop storage technology. Called a lake as it is flexible enough to store anything from raw data to unstructured email files.

*Hadoop Data Lake. Storing logs of all customers called into the inbound call centre including call duration.*

11-20 is [here](https://www.reddit.com/r/dataengineering/comments/rem26j/data_engineering_jargon_part_2/)

21-30 is [here](https://www.reddit.com/r/dataengineering/comments/rfbuu8/data_engineering_jargon_part_3/)

31-40 is [here](https://www.reddit.com/r/dataengineering/comments/rg5vr0/data_engineering_jargon_part_4/)",2021-12-11 09:19:43
kt5329,Microsoft has a ton of free courses for data engineering. Here is one with 10 hours of content for DataBricks,N/A,2021-01-08 15:49:14
ituxbu,Data Engineering from the Ground Up - Part 1 - Baby's First Data Pipeline,N/A,2020-09-16 12:57:24
197t8fz,"Apache Iceberg: SQL and ACID semantics in the front, scalable object storage in the back",N/A,2024-01-16 03:52:52
18hlsqb,Can someone explain the job of data engineer like I'm a baboon?,"At my previous job, I was a data analyst, but I've come across a lot of people who tell me that I should go into data engineering. Problem is, every time I ask someone who is in that field, they honestly cannot tell me what it is that they do. Like, I have never met a single data engineer in any company I've worked for who has given me a simple and reasonable explanation for what they do.



At my previous job, I designed ETL queries using SQL and Python, wrote APIs for interfacing with different database softwares for example I created an API in Python to automatically connect to Google BigQuery, retrieve data for the last 30 days, and then move it into other data sources 


I also performed audits on data, so where there were gaps in data or areas where they said that data was incorrect, I would go hunting and find gaps in the data that didn't make sense, for example, why is there missing data between these two linked tables? Is there a specific date that there's missing data?


Finally, I created new links between data across different sources, for example from snowflake to BigQuery, even a little bit of access",2023-12-13 17:16:40
13ster7,Reddit Sentiment Analysis Real-Time* Data Pipeline,"Hello everyone!

I wanted to share with you a side project that I started working on recently just in my free time taking inspiration from other similar projects. I am almost finished with the basic objectives I planned but there is always room for improvement. I am somewhat new to both Kubernetes and Terraform, hence looking for some feedback on what I can further work on. The project is developed entirely on a local Minikube cluster and I have included the system specifications and local setup in the README.

  
Github link: [https://github.com/nama1arpit/reddit-streaming-pipeline](https://github.com/nama1arpit/reddit-streaming-pipeline)

&#x200B;

The Reddit Sentiment Analysis Data Pipeline is designed to collect live comments from Reddit using the Reddit API, pass them through Kafka message broker, process them using Apache Spark, store the processed data in Cassandra, and visualize/compare sentiment scores of various subreddits in Grafana. The pipeline leverages containerization and utilizes a Kubernetes cluster for deployment, with infrastructure management handled by Terraform.

Here's the brief workflow:

* A containerized Python application to collect real-time reddit comments from certain subreddits and ingest them into the Kafka broker
* Zookeeper and Kafka pods act as a message broker for providing the comments to other applications.
* A Spark container running job to consume raw comments data from the kafka topic, process it and pour it into the data sink, i.e. Cassandra tables.
* A Cassandra database is used to store and persist the data generated by the Spark job.
* Grafana establishes a connection with the Cassandra database. It queries the aggregated data from Cassandra and presents it visually to users  through a dashboard. Grafana dashboard sample link: [https://raw.githubusercontent.com/nama1arpit/reddit-streaming-pipeline/main/images/grafana\_dashboard.png](https://raw.githubusercontent.com/nama1arpit/reddit-streaming-pipeline/main/images/grafana_dashboard.png)

I am relatively new to almost all the technologies used here, especially Kafka, Kubernetes and Terraform, and I've gained a lot of knowledge while working on this side project. I have noted some important improvements that I would like to make in the README. Please feel free to point out if there are any cool visualisations I can do with such data. I'm eager to hear any feedback you may have regarding the project!

PS: I'm also looking for more interesting projects and opportunities to work on. Feel free to DM me

Edit: I added this post right before my 18 hour flight. After landing, I was surprised by the attention it got. Thank you for all the kind words and stars.",2023-05-27 00:38:15
vj10xz,How do you guys ace your SQL skills?,"I am asking about mastering them. Like queries with varying levels of complexity. Some of the Technical Analysts I've worked with have written most mind-blowing Scripts with ease. I encounter the databases daily and want to acquire that levels of proficiency. I am familiar with SQL but I want to take it to the next level. Would you guys suggest me the best places to start exploring and also the strategies that worked for you to enhance your SQL skillsets. 

Thanks in advance!!!",2022-06-23 16:34:08
so6bpo,First Data Pipeline - Looking to gain insight on Rust Cheaters,"Hello Everyone,

I posted to this subreddit about a roadmap I created to learn data engineering topics. The community was great at giving advice. [Original Roadmap Post](https://www.reddit.com/r/dataengineering/comments/qpua91/data_engineering_road_map_for_a_computer_science/)

I have now completed my first data pipeline, data warehouse, and dashboard. The purpose of this project is to collect data about Rust cheaters. Ultimately, leading to insights about cheaters. I found some interesting insights. Read below!

&#x200B;

# Architecture

&#x200B;

https://preview.redd.it/vy557o1ttqg81.jpg?width=4096&format=pjpg&auto=webp&s=444b5f5e8653a585192731daf3e7d0b6848efd27

&#x200B;

# Overview

The pipeline collects tweets from a Twitter account(rusthackreport) that posts banned Rust player Steam profiles in real-time. The profile URLs are then extracted from the tweet data and stored in a temp s3 bucket. Ongoing, the steam profile URLs are used to extract the steam profile data via the Steam Web API. Lastly, the data is transformed and staged to be inserted into the fact and dim tables.

&#x200B;

# ETL Flow - Hourly

&#x200B;

https://preview.redd.it/ccmvqm9yuqg81.png?width=2493&format=png&auto=webp&s=faafbee1f525a1ae99198148883681b43c07c883

# Data Warehouse - Postgres

&#x200B;

https://preview.redd.it/ip4xj553vqg81.png?width=1796&format=png&auto=webp&s=1dc4f2d7aaaf8123f2576d12560c89cb08555834

# Data Dashboard

&#x200B;

[Dashboard Data Studio\(Updates Hourly\): https:\/\/datastudio.google.com\/u\/0\/reporting\/85aa118b-9def-48e4-8c88-b3db1e34e3ff\/page\/Ic8kC](https://i.redd.it/1fvsprz4vqg81.gif)

&#x200B;

# Data Insights

* The US has the most accounts banned for cheating with Russia trailing behind.
* Most cheaters have a level 1 steam account.
* The top 3 cheater names

1. 123
2. NeOn
3. xd

* The most common profile picture is the default steam profile picture.
* The majority of cheaters get banned between 0 and 10 hours.
* The top 3 games that cheaters own

1. **Counter-Strike: Global Offensive**
2. **PUBG: BATTLEGROUNDS**
3. **Apex Legends.**

* Top 3 Steam Groups

1. [Rustoria](https://steamcommunity.com/groups/rustoria)
2. [Andysolam](https://steamcommunity.com/groups/andysolam)
3. [Payday](https://steamcommunity.com/games/218620/memberslist/)

* Cheaters use [Archi's SC Farm](https://steamcommunity.com/groups/archiasf) to boost their accounts. It's a cheater's attempt to make their account look more legitimate to normal players.
* Profile Visibility - A lot of people believe if a profile is private it's a cheater. More cheaters have public profiles than private profiles.

1. Friends of Friends - 2,565
2. Private - 824
3. Friends Only - 133

You can look further at the [data studio link](https://datastudio.google.com/reporting/85aa118b-9def-48e4-8c88-b3db1e34e3ff).

&#x200B;

# Project Github

[https://github.com/jacob1421/RustCheatersDataPipeline](https://github.com/jacob1421/RustCheatersDataPipeline)

&#x200B;

&#x200B;

# Acknowledgment

# I want to thank Emily(mod#1073). She is a mod in the discord server for this subreddit! She was very helpful and went above and beyond when helping me with my data warehouse architecture. Thank you, Emily!

&#x200B;

Lastly, I would appreciate any constructive criticism. What technologies should I target next? Now that I have a project under my belt I will start applying.

[Help me by reviewing my resume?](https://www.reddit.com/r/dataengineering/comments/sotfp3/review_my_resume_please)",2022-02-09 06:04:40
12da1uw,I messed up today…,"Found out a query that I wrote was causing an issue with duplication. The duplication compounded therefore causing one of the tables to grow exponentially larger and larger each time. It’s also on a scheduled run every hour. Problem ended up costing almost $30k….

Anyone got any stories of when they fucked up?",2023-04-06 05:19:54
wl2xqx,My attempt to explain the Cloud to my daughters,"I hope it's okay to share a passion project here.

I have been thinking a lot about Kafka and The Cloud. There's something magical about a place of unlimited resources, shared environments, and boundless cloud rivers, and I thought it was the perfect recipe for a fun children's book. So, I decided to create a Winnie-the-pooh inspired book on the subject: [https://a.walktothe.cloud/](https://a.walktothe.cloud/)

I thought it might be interesting to this community so wanted to share. I'm also curious if there have been any subjects you've wanted to translate for beginners or non-tech folks? I think it's an interesting and fun space, and wonder if others enjoy doing this, as well.",2022-08-10 17:11:45
zebb3o,The most shared resources from r/dataengineering,"Hi, fellow data engineers!

I've built a tool to find the best resources shared on r/dataengineering as well as other subreddits.Here is the link: [https://www.gembase.ai/search?q=data+engineering](https://www.gembase.ai/search?q=data+engineering)

**Architecture**

I gathered all the archive data from Reddit.

* Then extracted URLs with Go on a big EC2 machine.
* The screenshots and titles were scraped using Python + Playwright hosted on \~1000 ECS tasks.
* The recommendations were offline computed with R + Tidyverse.

I hope you will enjoy it. Feedback and questions are really appreciated.",2022-12-06 16:38:45
qw6htq,"After exactly one month and three days of starting my Azure Data Engineer preparation, I have good news!",N/A,2021-11-17 19:08:20
165hs7c,Influencers in data are doing no justice to the industry,"A few of them aside, most are writing stuff just to fill the gaps. Nothing meaningful, just piece after piece of barely important content.

The reason they somewhat ""succeed"":

1. Most of the data world isn't in the large companies or bleeding edge startups. They find these insightful because their world moves at a much slower pace. When you peel apart the content, anyone with slight experience will tell you there isn't much in there, but without that insight, you get sucked into this.
2. Somewhat related to 1. but most of the newbies in Data Engineering don't have great role models or people to follow. It's bound to be this way because the industry is only recently become popular and some people have taken advantage of that to position themselves as leaders.

Data Twitter, on the other hand, is much more cliquey. Guarding and almost gatekeeping their world. They don't even like the LinkedIn data influencers and sometimes even hate on people in other parts of Data Twitter too.

All of this just hurts the industry more. If you have stuff to share, just write, don't do this nonsense, and collectively pull down everyone else.  I hope the people putting in the real work to share content and not fluff get more of the limelight than these people.

Most of this isn’t new. A previous post highlighted this as well. https://www.reddit.com/r/dataengineering/comments/161zmp3/follow_up_on_my_previous_post_who_are_some_of_the/",2023-08-30 15:26:14
ymjubx,Cool ML Engineering diagram.,N/A,2022-11-05 04:15:21
1b67xnz,Giving up data engineering,"Hi,

I've been a data engineer for a few years now and I just dont think I have what it takes anymore.

The discipline requires immense concentration, and the amount that needs to be learned constantly has left me burned out. There's no end to it.

I understand that every job has an element of constant learning, but I think it's the combination of the lack of acknowledgement of my work (a classic occurrence in data engineering I know), and the fact that despite the amount I've worked and learned, I still only earn slightly more than average (London wages/life are a scam). I have a lot of friends who work classic jobs (think estate agent, operations assistant, administration manager who earn just as much as I do, but the work and the skill involved is much less)

To cut a long story short, I'm looking for some encouragement or reasons to stay in the field if you could offer some. I was thinking of transitioning into a business analyst role or to become some kind of project manager, because my mental health is taking a big hit.

Thank you for reading.",2024-03-04 10:42:03
kf3kr1,Apache Airflow 2.0 Released,N/A,2020-12-17 18:53:22
jhfg6p,"Long time Opensource users? Start contributing to Open Source projects, if you know Python (knowing Airflow is a plus), I can help you contribute your first commit to Airflow :) -- I am Apache Airflow Committer, PMC Member and Release Manager.",N/A,2020-10-24 19:52:49
1131jqq,"Finnhub streaming data pipeline using Spark, Kafka, Kubernetes and more - Github repo & more info in the comments",N/A,2023-02-15 16:10:25
wq4ims,New open-source notebook,"We're currently working on a new open-source notebook to shape the future of building data pipelines.

We would love for you to test out our current version in a collaborative effort to create better workflows for data scientists (and other data and machine learning professionals).

Repo: [https://github.com/mage-ai/mage-ai](https://github.com/mage-ai/mage-ai)  
More about Mage: [https://mage.ai](https://mage.ai/)  
Join our slack community: [https://mage.ai/chat](https://mage.ai/chat)",2022-08-16 20:21:34
ojutua,"MySQL Deadlocks, Basically",N/A,2021-07-14 02:26:20
mdpz25,"I wrote a tutorial on PySpark basics, how to use it in Google Colab, and some fine-tuning tips","Hello!

A few months back, I wrote a [PySpark tutorial](https://jacobceles.github.io/knowledge_repo/colab_and_pyspark/) hoping it would be beneficial for folks looking for a quick ramp-up to using it. I got some positive feedback and so thought it would be a good idea to share it here so that more people can refer to it. I use it as a cheat sheet when I forget something, but the main objective of the tutorial is to:

* Gain a proper understanding of the most common PySpark functions available.
* A short introduction to Google Colab.
* Get some insight into tuning PySpark jobs.
* Answer a few common questions beginners usually have.

I used Google Colab as it is free, easy to set up, share, and convenient to use with other Google services like Google Drive. Please let me know if you have any comments about it or find it nifty! You can find the tutorial here:

[https://jacobceles.github.io/knowledge\_repo/colab\_and\_pyspark/](https://jacobceles.github.io/knowledge_repo/colab_and_pyspark/)",2021-03-26 14:33:33
t5dfo0,Anyone noticing a trend where Data Engineer jobs are advertised but the role is really BI or Analyst work?,"I’ve seen a few jobs now that either in the ad or on the phone call are very obviously not about building pipelines or infrastructure but straight up building reports in tableau? I’m talking 90% of your time is spent in tableau.

I’m assuming these places were struggling to get people for these roles which is why they’ve jazzed it up a bit. But wondering if anyone else has experienced this?",2022-03-02 23:36:06
1bjcybi,Can We Stop Using Marketing Terms to Define Data Warehouses?,"This comes up a lot in random posts. Snowflake is a data warehouse. BigQuery is a data warehouse. PostgreSQL, MySQL, and SQL Server are not. We have let companies like Snowflake, Oracle, etc. redefine data warehouse from it's data-centric meaning, to a platform-centric one. 

A data warehouse is a collection of disparate sources modeled to provide efficient querying. Just about any DB system can be part a data warehouse solution, but the platform itself is not the data warehouse. Snowflake is a great solution for larger use cases where it saves significant engineering resources. For some tiny DW with rows in the low millions, it is probably going to be very expensive compared to other platforms. 

I know this sounds pedantic, but as data engineers, we should be precise with our terms. Doing anything else leads to confusion and misunderstandings. In the end, we should perform analysis and choose the best tool for the job. It very well might be one of the advertised ""data warehouses"". It may be Postgres. It may be something else. It's our job to find the right solution with hard data, not marketing hype.",2024-03-20 13:05:02
uhifqj,Why does everyone want to work at FAANG?,"Can anyone tell me why this is such a big thing?   
Why is this the ultimate goal?   
Is it just the money or the prestige of working at Facebook, Amazon, Apple, Netflix or Google? 

I worked at a 350.000+ employees German company for over 10 years. What I found is that in large companies you are actually bound by a lot of processes and structures. You are a cog in the big machine.   
For instance I heard from engineers at Facebook that the tasks are very narrow and many are just glorified ETL developers.

Wouldn't it be a lot better to aim for a job with great opportunities, where you and your small team can actually move something? A job where you are responsible end to end and learn and use many skills?

I don't get it.",2022-05-03 15:19:31
uxhmh0,"If you could only recommend one book to enhance your knowledge of data engineering, what would it be?","As the title states. I’ve heard about “designing data intensive applications” quite a bit, but are there any better comprehensive books?",2022-05-25 13:40:01
16zm47c,What data engineering tools are popular right now?,"Hi All,

Just wondering what data engineering tool(ETL, warehouse, what have you) is most widely used these days. Seems every week i get distracted and try to learn some new tool, and i really want to narrow it down so i can be more focused. 

Seems that SQL is the only constant, but i know there's more to that. tia 

&#x200B;

&#x200B;",2023-10-04 12:37:26
1432zk2,How to become a good Data Engineer?,"I'm currently in my first job with 2 years of experience. I feel lost and I'm not as confident as I probably should be in data engineering.

What things should I be doing over the next few years to become more experienced and valuable as a Data Engineer?

- What is data engineering really about? Which parts of data engineering are the most important?
- Should I get experience with as many tools as possible, or focus on the most popular tools?
- Are side/personal projects important or helpful? What projects could I do for data engineering?

Any info would be great. There are so many things to learn that I feel paralyzed when I try to pick one.",2023-06-07 04:15:24
jcaflo,Awesome data engineering learning path,N/A,2020-10-16 14:07:14
13gln9a,My top 14 tips for Snowflake Data Engineers. What would you add?,"If you're working on **Snowflake** as a Data Engineer, this article might be interesting:

[Snowflake Top Tips for Data Engineers - Loading and Transforming Data](https://www.analytics.today/blog/top-14-snowflake-data-engineering-best-practices)

**Quote:**  

>*If the only tool you have is a hammer - you tend to see every problem as a nail.*  Abraham Maslow.

**In Summary**

1. **Follow the standard ingestion pattern:**  This involves the multi-stage process of landing the data files in cloud storage and loading them to a landing table before transforming the data.  Breaking the overall process into predefined steps makes it easier to orchestrate and test.  
2. **Retain history of raw data:**  Unless your data is sourced from a raw data lake, it makes sense to keep the raw data history which should ideally be stored using the [VARIANT](https://docs.snowflake.com/en/sql-reference/data-types-semistructured.html#variant) data type to benefit from automatic schema evolution.  This means you can truncate and re-process data if bugs are found in the transformation pipeline and provide an excellent raw data source for Data Scientists.  While you may not yet have any machine learning requirements yet, it's almost certain you will, if not now, then in the coming years.  Remember that Snowflake data storage is remarkably cheap, unlike on-premises solutions. 
3. **Use multiple data models:**   On-premises data storage was so expensive it was not feasible to store multiple copies of data, each using a different data model to match the need.  However, using Snowflake, it makes sense to store raw data history in either structured or variant format, cleaned and conformed data in [3rd Normal Form](https://dwbi1.wordpress.com/2011/03/28/storing-history-on-3rd-normal-form/) or using a [Data Vault](https://www.analytics.today/blog/when-should-i-use-data-vault) model. Finally, data is ready for consumption in a [Kimball Dimensional Data model](https://www.kimballgroup.com/data-warehouse-business-intelligence-resources/kimball-techniques/dimensional-modeling-techniques/).  Each data model has unique benefits, and storing the results of intermediate steps has huge architectural benefits, not least the ability to reload and reprocess the data in case of mistakes.
4. **Use the right tool:**  As the quote above implies, if you only know one tool, you'll use it inappropriately some times.  The decision about which tool to use should be based upon a range of factors, including, the existing skill set in the team, whether you need rapid near real-time delivery, and whether you're doing a once-off data load or a regular repeating process.  Be aware that Snowflake can natively handle a range of file formats, including Avro, Parquet, ORC, JSON and CSV. There is extensive guidance on [loading data into Snowflake](https://docs.snowflake.com/en/user-guide-data-load.html#loading-data-into-snowflake) on the online documentation.
5. **Use COPY or SNOWPIPE to load data:**  Around 80% of data loaded into a data warehouse is either ingested using a regular batch process or, increasingly, immediately after the data files arrive.  By far, the fastest, most cost-efficient way to load data is using COPY and SNOWPIPE, so avoid the temptation to use other methods (for example, queries against external tables) for regular data loads.  Effectively, this is another example of *using the right tool*.  
6. **Avoid JDBC or ODBC for regular large data loads:**  Another *right tool* recommendation.  While a JDBC or ODBC interface may be fine to load a few megabytes of data, these interfaces will not scale to the massive throughput of COPY and SNOWPIPE.  Use them by all means, but not for large regular data loads.
7. **Avoid Scanning Files:** Using the COPY command to ingest data, use [partitioned staged data](https://docs.snowflake.com/en/user-guide/data-load-considerations-manage.html#partitioning-staged-data-files) files.  This reduces the effort of scanning large numbers of data files in cloud storage.
8. **Choose a suitable Virtual Warehouse size:**  Don’t assume an X6-LARGE warehouse will load huge data files any faster than an X-SMALL.  Each physical file is loaded sequentially on a single CPU, and it is more sensible to load most loads on an X-SMALL warehouse.  Consider splitting massive data files into 100-250MB chunks and loading them on a larger (perhaps MEDIUM size) warehouse.
9. **Ensure 3rd party tools push down:**  ETL tools like Ab Initio, Talend and Informatica were originally designed to extract data from source systems into an ETL server, transform the data and write them to the warehouse.  As Snowflake can draw upon massive on-demand compute resources and automatically scale out, it makes no sense to use and have data copied to an external server.  Instead, use the ELT (Extract, Load and Transform) method, and ensure the tools generate and execute SQL statements on Snowflake to maximise throughput and reduce costs.  Excellent examples include [DBT](https://www.getdbt.com/) and [Matillion](https://www.matillion.com/).
10. **Transform data in Steps:**  A common mistake by inexperienced data engineers is to write huge SQL statements that join, summarise and process lots of tables in the mistaken belief this is an efficient way of working.  In reality, the code becomes over-complex, difficult to maintain, and, worst still, often performs poorly.  Instead, break the transformation pipeline into multiple steps and write results to intermediate tables.  This makes it easier to test intermediate results, simplifies the code and often produces simple SQL code that runs faster.  
11. **Use Transient tables for intermediate results:**  During a complex ELT pipeline, write intermediate results to a [transient table](https://docs.snowflake.com/en/user-guide/tables-temp-transient.html#transient-tables) which may be truncated before the next load.  This reduces the time-travel storage to just one day and avoids an additional seven days of fail-safe storage.  By all means, use [temporary tables](https://docs.snowflake.com/en/user-guide/tables-temp-transient.html#temporary-tables) if sensible, but the option to check the results of intermediate steps in a complex ELT pipeline is often helpful.
12. **Avoid row-by-row processing:**  As described in the article on [Snowflake Query Tuning](https://www.analytics.today/blog/top-3-snowflake-performance-tuning-tactics), Snowflake is designed to ingest, process and analyse billions of rows at amazing speed.  This is often referred to as *set-at-a-time processing.*  However, people tend to think about *row-by-row processing*, which sometimes leads to programming loops that fetch and update rows one at a time.  Be aware that row-by-row processing is the biggest way to kill query performance.  Use SQL statements to process all table entries simultaneously and avoid row-by-row processing at all costs.
13. **Use Query Tags:**  When you start any multi-step transformation task, set the [session query tag using](https://docs.snowflake.com/en/sql-reference/sql/alter-session.html#alter-session):  **ALTER SESSION SET QUERY\_TAG = 'XXXXXX'** and **ALTER SESSION UNSET QUERY\_TAG**.  This stamps every SQL statement until reset with an identifier and is invaluable to System Administrators.  As every SQL statement (and QUERY\_TAG) is recorded in the [QUERY\_HISTORY](https://docs.snowflake.com/en/sql-reference/account-usage/query_history.html#query-history-view) view, you can track the job performance over time.  This can be used to quickly identify when a task change has resulted in poor performance, identify inefficient transformation jobs or indicate when a job would be better executed on a larger or smaller warehouse.
14. **Keep it Simple:**  Probably the best indicator of an experienced data engineer is the value they place on ***simplicity***.  You can always make a job 10% faster, generic, or more elegant, and it *may* be beneficial, but it's *always* beneficial to simplify a solution.  Simple solutions are easier to understand, easier to diagnose problems and are therefore easier to maintain.  Around 50% of the performance challenges I face are difficult to resolve because the solution is a single, monolithic complex block of code.  The first thing I do is break down the solution into steps and only then identify the root cause.  

Would you agree with the above?  What would you add?",2023-05-13 16:15:07
13fmd1h,How do you handle junior people who are better than you in terms of technical ability?,"I try to be super open and receptive to feedback if someone has a better  more efficient way of doing things. I take notes obsessively or ask plenty questions.

When we do a code review someone who more junior than me corrects my code or says it can be improved somewhere. 

It isn’t overly pedantic corrections that junior people get into. However I do feel that little bit of insecurity that you aren’t good enough technically or that you lose respect if don’t shine in your logic.  

You can’t be a chef and not know how to cook a meal.

One thing I find, is it lights a bit of fire in your belly. If you’re expected to be a team lead you cannot be bad technically. It pushed self development.

How do you manage junior correcting you without coming across as overly submissive and still have their respect?",2023-05-12 14:12:23
1aw7368,Open source DBT core alternative written in Rust (30x faster),"Hey everyone,

**TLDR:** Louis here from Quary - we have spent the past few months re-engineering DBT core (python) into rust to create a fast data transformation (SQL inference & modelling) package in Rust.

With DBT Core (Data Build Tool) you would need to spin up a server to run Python to make the package work. Thanks to Rust WASM we are able to make the transformation engine portable so that it runs entirely in the browser.

We wanted to give back to this community so we have decided to Open Source the entire project under an MIT license to give back to this community.

Looking forward to hearing your thoughts in the comments! (A GitHub star is always appreciated 😃)

***EDIT: adding clarification***

**Quary will be easier to build on top of**

Because of our Rust core, we can expose JS, Python, and other bindings, making it easier to build additional tooling on top of Quary. For example, we've built our ""cloud"" offering on a WASM compilation.

**Column-Level Lineage**

The Quary core contains column-level lineage directly and this enables us to offer unique capabilities. For instance:

**Automated Inference and Documentation**

Our system can intelligently infer tests and generate documentation, significantly reducing your manual workload.

**Testing Efficiency**: By avoiding inferrable tests, we can skip tests that Quary knows to be true. In our template for example, we can skip around 1/3 of tests.

**Quary is better for handling sensitive data like PII.**

Because we can compile to WASM, our ""cloud offering"" interacts with your data warehouse from the client. Data doesn't flow through any of our servers, protecting your information.

**Speed-up for developer experience**

While the impact on database performance is minimal, one of the true benefits of Quary's fast core is developer experience. The fast core allows us to build experiences where we can provide faster feedback.

https://preview.redd.it/3kisasoefwjc1.jpg?width=3612&format=pjpg&auto=webp&s=38666d5d348282797edbc19aa69d0df040fc3ed5",2024-02-21 08:25:56
10sxj6r,Why do all my BI initiatives end up like this? 😩,N/A,2023-02-03 22:36:33
xl4sag,All-in-one tool for data pipelines!,"Our team at [Mage](https://mage.ai) have been working diligently on this new open-source tool for building, running, and managing your data pipelines at scale.

https://preview.redd.it/tn5w1wur4gp91.png?width=4336&format=png&auto=webp&s=d59e720ce9a68bc416896ef6b14c357a0b452abd

Drop us a comment with your thoughts, questions, or feedback!

Check it out: [https://github.com/mage-ai/mage-ai](https://github.com/mage-ai/mage-ai)  
Try the live demo (explore without installing): [http://demo.mage.ai](http://demo.mage.ai)  
Slack: [https://mage.ai/chat](https://mage.ai/chat)

Cheers!",2022-09-22 15:38:53
17fr8d5,To my data engineers: why do you like working as a data engineer?,What made you get into data engineering and what is keeping you as one? I recently started self learning to become one but i’m sure learning about data engineering is much different than actually being an engineer. Thanks,2023-10-24 23:51:59
15gzgne,Polars gets seed round of $4 million to build a compute platform,N/A,2023-08-03 09:40:24
zq4eg6,explaining what the data modeler on the team does,N/A,2022-12-19 21:39:30
ufl9tx,Apache Airflow 2.3.0 is out !,"&#x200B;

https://preview.redd.it/dzqcw8txnqw81.png?width=914&format=png&auto=webp&s=e528d30f4107b723a2ace301375a03f348df340b

Apache Airflow 2.3.0 is out! Soo many things to talk about 👇👇👇

➡️ This is the biggest **Apache Airflow** release since 2.0.0

➡️ 700+ commits since 2.2 including 50 new features, 99 improvements, 85 bug fixes 

The following are the biggest & noteworthy changes👇👇👇:

👉 Dynamic Task Mapping: [https://airflow.apache.org/docs/apache-airflow/2.3.0/concepts/dynamic-task-mapping.html](https://airflow.apache.org/docs/apache-airflow/2.3.0/concepts/dynamic-task-mapping.html)

👉 Grid View replaces Tree View

👉 The new \`airflow db clean\` CLI command for purging old records

👉 First class support for DB downgrade - \`airflow db downgrade\` command - [https://airflow.apache.org/docs/apache-airflow/2.3.0/usage-cli.html#downgrading-airflow](https://airflow.apache.org/docs/apache-airflow/2.3.0/usage-cli.html#downgrading-airflow)

👉 New Executor: LocalKubernetesExecutor

👉 Create Connection in native JSON format - no need to figure out the URI format

👉 And a new ""SmoothOperator"" -- This is a surprise ! And a very powerful feature, try it out and let me know what you think about it 😃

&#x200B;

📦 PyPI: [https://pypi.org/project/apache-airflow/2.3.0/](https://pypi.org/project/apache-airflow/2.3.0/)

📚 Docs: [https://airflow.apache.org/docs/apache-airflow/2.3.0](https://airflow.apache.org/docs/apache-airflow/2.3.0)

🛠️ Changelog: [https://airflow.apache.org/docs/apache-airflow/2.3.0/release\_notes.html](https://airflow.apache.org/docs/apache-airflow/2.3.0/release_notes.html)

🚢 Docker Image: ""docker pull apache/airflow:2.3.0""

🚏 Constraints: [https://github.com/apache/airflow/tree/constraints-2.3.0](https://github.com/apache/airflow/tree/constraints-2.3.0)

&#x200B;

\------

Details around the features

&#x200B;

**👉 Dynamic Task Mapping: No longer hacking around dynamic tasks !!**

Allows a way for a workflow to create a number of tasks at runtime based upon current data, rather than the DAG author having to know in advance how many tasks would be needed.

[https://airflow.apache.org/docs/apache-airflow/2.3.0/concepts/dynamic-task-mapping.html](https://airflow.apache.org/docs/apache-airflow/2.3.0/concepts/dynamic-task-mapping.html)

&#x200B;

https://preview.redd.it/sgn12pn6oqw81.png?width=914&format=png&auto=webp&s=283e0076f6de0b960bd36f7c481a9b8b54a8c918

👉 **Grid View replaces Tree View!!**

Show runs and tasks but leave dependency lines to the graph view and handles Task Groups better!

Paves way for DAG Versioning - to easily show versions, which was impossible to handle in Tree View ! yay!

PR: [https://github.com/apache/airflow/pull/18675](https://github.com/apache/airflow/pull/18675)

&#x200B;

https://preview.redd.it/36dn2tgboqw81.png?width=2384&format=png&auto=webp&s=39e279496d955a63ed6750a89ca7b96cb5d77b0e

https://preview.redd.it/2s5pqsgboqw81.png?width=1576&format=png&auto=webp&s=cda96e655fb4c715c0078be21599e523e2d3100d

&#x200B;

👉 **Create Connection in native JSON format - no need to figure out the URI format**

&#x200B;

https://preview.redd.it/5gjjcg0eoqw81.png?width=956&format=png&auto=webp&s=572745d6464e8439bb84f93587738b8ac8dca862

**👉 First class support for DB downgrade - \`airflow db downgrade\` command -** 

You can downgrade to a particular Airflow version or a to a specific Alembic revision id.

Includes a ""--show-sql-only"" to output all the SQL so that you can run it yourself!

[https://airflow.apache.org/docs/apache-airflow/2.3.0/usage-cli.html#downgrading-airflow](https://airflow.apache.org/docs/apache-airflow/2.3.0/usage-cli.html#downgrading-airflow)

&#x200B;

https://preview.redd.it/l4cibc2joqw81.png?width=2048&format=png&auto=webp&s=dceae470c316c61b67935c4ec6d632369b6504e7

**👉 The new \`airflow db clean\` CLI command for purging old records.** 

This will help reduce time when running DB Migrations (when updating Airflow version)

No need to use Maintenance DAGs anymore!

&#x200B;

https://preview.redd.it/x3f8jz4moqw81.png?width=2048&format=png&auto=webp&s=8eaca8a761232ce1fd7db94f66d7af7ce5766f13

**👉 New Executor: LocalKubernetesExecutor**

It provides the capability of running tasks with either LocalExecutor, which runs tasks within the scheduler service, or with KubernetesExecutor, which runs each task

in its own pod on a kubernetes cluster based on the task's queue

&#x200B;

**👉 DagProcessorManager can be run as standalone process now.** 

As it runs user code, separating it from the scheduler process and running it as an independent process in a different host is a good idea.

Run it with ""airflow dag-processor"" CLI coomand

📚 [https://airflow.apache.org/docs/apache-airflow/2.3.0/configurations-ref.html#standalone\_dag\_processor](https://airflow.apache.org/docs/apache-airflow/2.3.0/configurations-ref.html#standalone_dag_processor)

&#x200B;

👉 A single page to check release notes instead of UPDATING.md on GitHub  & Changelog on Airflow website: https://airflow.apache.org/docs/apache-airflow/2.3.0/release\_notes.html 

👉 And a new ""**SmoothOperator**"" - ""from airflow.operators.smooth import SmoothOperator""

This is a surprise! And a very powerful feature, try it out and let me know what you think about it 😃",2022-04-30 22:07:54
ueiklm,I did it!,"I've landed a data engineering job in my current company! I'm currently working as a marketing data analyst at a bank. After deciding on becoming a data engineer about a year ago, and lurking on this sub for even longer, I have finally succeeded.

How I did it?

* A little over a year ago **I expressed my desire to become a data engineer** to my manager and was very transparent about what I like most about my job (transforming data, building dashboards, writing SQL, etc.) and what I didn't like (generating insights, marketing fluff). As my manager is all about doing what you love to do as much as possible, my transparency set me up to be able to focus on projects that would play into my skills and ambitions.
* **I looked out for projects that have to do with data engineering**. My team is currently migrating to Azure stack for all data analytics. My team was in need of someone who was able to have a technical conversation with IT/Data Engineers about my team's requirements, so I've put myself forward. I've learned so much from just talking to other data engineers. More important, this is basically where I got my foot in the door.
* **Where did I get the knowledge to have any sort of conversation about data engineering? Read, read, read.** This sub (thank you all!), books (e.g., Designing Data-Intensive Applications), and blogs. But also by just trying stuff out myself. Tinkering with PostgreSQL, docker, Airflow, etc. I know everyone is very keen on building big projects for your portfolio, but my attention span in my spare time is too limited to focus on 1 big project for too long. So I did little things like running a Postgres database locally, extracting data from an api, trigger some python scripts with Airflow.
* **Certificates!** This was an important one, and was recommend by someone from this sub. As my company is on Azure, I got the Azure Fundamentals, Azure Data Fundamentals and Azure Data Engineer Associate certificates. They not only thought me a lot about Azure, but also about the cloud in general and other important data engineering concepts.
* **Start reaching out and get interviews.** This was the easiest part. One major selling point is that I was an analyst first, and therefore know how data and platform will be used from a user/analyst perspective. Any technical shortcomings are easy to overcome. Also, being able to speak both engineering/it language and analyst/end-user language was appreciated. Apparently, engineers at my company only speak engineering. Lucky me!

I hope this is of use to anyone! Either way, big thanks to this sub as it guided me every step of the way.",2022-04-29 10:05:36
sit0ep,"Data engineers who've cracked FAANG and other top tech companies (Adobe, Microsoft etc)..how did you do it?","1. What are the data engineering specific skills you've developed and what resources have you used? 

2. What do you put down on your resume? Are Cloud certs important?

3. What are the technologies you are working on right? How accurate was the job description when compared to what actually constitutes your daily tasks?",2022-02-02 16:34:10
gq2bmf,Data Engineering project for beginners,"Hi all,

Recently I saw a post on this sub reddit asking for beginner DE projects using common cloud services and DE tools. I have been asked this same question by my friends and colleagues who are trying to move into the data engineering field. So I decided to write a blog post explaining how to setup and build a simple batch based data processing pipeline using Airflow and AWS.

Initially I wanted to do it with both batch and streaming pipelines, but it soon got out of hand so decided to only do batch based first and depending on interest will do stream processing.

Blog: [https://www.startdataengineering.com/post/data-engineering-project-for-beginners-batch-edition](https://www.startdataengineering.com/post/data-engineering-project-for-beginners-batch-edition)

Repo: [https://github.com/josephmachado/beginner\_de\_project](https://github.com/josephmachado/beginner_de_project)

Appreciate any questions, feedback, comments. Hope this helps someone.",2020-05-25 02:05:55
18cj54r,Keep in mind the following when reading about anything tech online lol,N/A,2023-12-07 00:47:01
y8o1sy,Why use Spark at all?,"It  has been 5 years working in data space, I always found something better to solve a problem at hands than Spark. If I do intensive data application, probably the data would pipe through a distributed message queue like Kafka and then I will have a cluster to ingestion sequentially the data into the application -> no need for spark. If I wanna do real time analytics I would use something like Flink.  


If I want to do transformation, it's sql and dbt. I even go as far as using Trino or Presto to ingest csv directly (given that we need to have the schema anyways)  


I found no need for Spark, and I have no issues, sometimes I wonder why everyone is using spark, what is the point of it all.  


Ye that's all.",2022-10-20 04:17:12
166ah28,"Instacart, Databricks and Snowflake drama","Was reading an interesting blog post about how instacart migrated to databricks that mysteriously disappeared: https://www.instacart.com/company/how-its-made/how-instacart-ads-modularized-data-pipelines-with-lakehouse-architecture-and-spark/
I went looking for info and found some twitter threads and it
turns out instacart had saved about 70% on snowflake costs in 2023 by migrating to databricks. Databricks even advertised the [case study](https://pbs.twimg.com/media/F4s0qIcXYAAuvYD?format=jpg&name=medium) . One problem though, snowflakes CEO sits on instacarts board, which means a normally transparent blog had to delete its findings.

[Quote:](https://twitter.com/GergelyOrosz/status/1697192807801184561)
>Instacart cut Snowflake spend by 70% in 2023, while starting to migrate ETL loads to Databricks - then deletes blog post detailing migration. I email Instacart press team with questions but Snowlake press team comes back with a comment on behalf of Instacart 🤯. Snowflake’s CEO is on the board of directors for Instacart. The thing that blew my mind is how my email addressed only to Instacart’s press team ended up at Snowflake (who I never contacted) and why Snowflake makes/can make definite statements on behalf of Instacart. Emailed Instacart, and then Snowflake press team landed in my inbox referencing things that I only sent to Instacart, saying they hear I am writing an article and they want to give me facts. Never contacted them. Feels like Instacart pinged them.


So now databricks removed the [case study](https://pbs.twimg.com/media/F4s0qIYXAAArUkz?format=jpg&name=large) and snowflake even posted a [response](https://www.snowflake.com/blog/snowflake-and-instacart-the-facts/) which says its countering 'social media' misinformation but most of the details came from putting two and two together with instacarts own blog post.

Stumbled upon some drama just reading a tech blog, I have a feeling Instacarts tech blog team is getting a serious talking to and now will have to pass anything they post by the board. It was a really good post though, detailed and well thought out, I was looking to share the info with my team today.

Threads here: [1](https://twitter.com/GergelyOrosz/status/1696435748071772333) [2](https://twitter.com/modestproposal1/status/1695177654822191184) [3](https://twitter.com/GergelyOrosz/status/1697192807801184561)",2023-08-31 12:58:14
13gckaq,Snowflake SELECT * EXCLUDE,"So, i feel like a caveman discovering fire.  Apologies if this is something which is generally know, but just in case this helps anyone else with a typical SQL limitation.

I've just found out that on Snowflake you can do a ""SELECT *"" query and exclude specific column. for example:

 SELECT * EXCLUDE (field1, field2...) from tableName;

 I feel like ive been wanting this for YEARS in SQL but didn't know Snowflake had it! 

For me personally, It works as a shorthand way of making checksums for very wide tables when I'm having to implement SCD2/CDC without a usable date check field. Such as:

SELECT id, HASH(*) AS checksum FROM (SELECT * EXCLUDE (field1, fields2...) FROM tableName );",2023-05-13 09:16:28
yqohhm,Anyone here being let go by Meta?,"11,000 is a big number. I just wonder if any DE's are being let go. If so, whats your plan moving forward? Looks like the severance package is good though.",2022-11-09 16:53:46
qfiz51,I deleted data from production,"Over the last 7 years I've done some mistakes, as a therapeutic post I'm trying to write my experience about it to maybe help others but also to help me. Below an overlook of the stories.



# 1 — First I removed /usr 👤

It was late 2014. I was setuping an Hadoop cluster with Ambari. Everything was ok until I removed the `/usr` folder because the Namenode main partition was full and I was copy-pasting solutions from the internet in order to solve by myself rather than asking IT help.



What happened next? When you remove /usr you lost all the binaries and even if you can get back the sudo command is not usable anymore because it needs to belong to uid:0. So it's done you are locked outside of the server if you don't have root user access. 




# 2 — Then I removed /data in hdfs 📉

Around 2017. I had a working Hadoop cluster with Hive on Tez that was working like a charm. One Sunday I wake up and do a Slack check over the alerts channel and see that everything failed over the weekend.



After a small deep-dive I discover that the `/data` folder is missing in hdfs. What a weird issue. Why the `/data` folder is missing? I'm then asking to the team if they changed something to the cluster configuration. But as I go deeper I discover that I was responsible of the `hdfs dfs -rm /data` command run. Outch. 



Even today I don't know what happened this day and it took me around 3 days to get 60% of the data back. The rest was lost for ever. I learnt a lot from this situation.




# 3 — A colleague ran terraform destroy 💣

Someone from the team I was managing ran terraform destroy and we lost a GKE cluster, all our GCE instances and some SQL instances. We noticed the issue because someone asked in Slack: ""Is Metabase down?"" and then I looked over my shoulder and saw my colleague screen fully red with the terraform destroy command at the top.


By chance we did not lose any data because our buckets and BigQuery were not managed by terraform, it took us around 4 hours to get everything back up again.


# Conclusion

This post is a adapted version of a [longer version in my blog](https://www.blef.fr/data-deleted-from-production/) where I put some takeaways. I also want to start a discussion to say that this is normal to make mistakes. What is important is how you deal with it and how you learn from it.


Much love ❤️.",2021-10-25 15:16:43
11eezyq,"Brace yourselves... ""professional"" Data Mesh developer job ads incoming!",N/A,2023-02-28 17:50:41
o9w1mx,today I started in my new role as an ETL Developer after 5 years as a data analyst,I’m just very happy and just wanted to share!,2021-06-28 23:26:11
1b34q4i,I bombed the interviuw and feel like the dumbest person in the world,"I (M20) just had a second round of 1 on 1 session for data engineer trainee in a company. 

I was asked to reverse a string in python and I forgot the syntax of while loop. And this one mistake just put me in a downward spiral for the entire hour of the session. So much so that once he asked me if two null values will be equal and I said no, and he asked why but I could not bring myself to be confident enough to say anything about memory addresses even after knowing about it, he asked me about indexing in database and I could only answer it in very simple terms.

I feel really low right now, what can I do to improve and get better at interviewing.",2024-02-29 16:39:33
17p20y6,Why don't a lot of data engineers consider themselves software engineers?,"During my time in data engineering, I've noticed a lot of data engineers discount their own experience compared to software engineers who do not work in data.  Do a lot of data engineers not consider themselves a type of software engineer?



I find that strange, because during my career I was able to do a lot of work in python, java, SQL, and Terraform.  I also have a lot of experience setting up CI/CD pipelines and building cloud infrastructure.  In many cases, I feel like our field overlaps a lot with backend engineering.",2023-11-06 12:51:04
15y7v97,What do Data Engineers do after the data platform is completely setup and automated?,"I lead a small DE team at a medium sized retailer. Over the years we’ve setup the data platform on GCP, lambda architecture feeding data into BQ with DBT transforms. We’re reaching a point where we’ve pretty much ingested all the data sources used by the business, completed the data models, reverse ETL and automation CI/CD. I’m starting to draw a blank on what business focussed initiatives we should start taking up at this mature stage and would appreciate any thoughts from this community. 

Some things I’m considering are improving data quality checks, improving the data catalog and adding more metrics to the semantic layer. 

We’ve already done several rounds of optimisation and cloud cost control so I don’t see much more opportunity there. Most of our data usage in the business is batch focused including ML model training so we don’t really see the need to move to a kappa architecture either.",2023-08-22 14:43:23
ykbtnb,I'm sorry but is that low end of the salary range a joke? Especially for a big company like CVS .I made more at my very first analyst job which was all Excel/Access,N/A,2022-11-02 17:46:34
rw929w,5 Data Engineering Projects To Add To Your Resume - Seattle Data Guy,N/A,2022-01-05 00:23:33
16nlvln,A senior engineer's experience in the current job market,"A lot of posts I see  are around how bad the market is / how tough it is right now / putting out 100s of resumes / leet-coding / not getting any responses, so I'd like to submit another data point. 

For context: I've been affected by layoffs twice over the past 2 years, so I've been in ""this market"" twice. I have non-faang bay area experience on my resume, 5-10 years of experience, and typically apply for senior / staff type roles. 

Both times I've entered the market were pretty much the same: find 2-4 companies to apply to (I didn't rely on connections -- but that's typically the best way --, these were cold applications). Get 2 phone screens. Go through the process (usually a mix of behavioral and technical. Usually at least 1-2 live coding sessions) and end up with 2 offers at the end of it to decide between / bounce off of each other.

I am not a rockstar coder who can code any ds/a out there. I have a wide breadth of experience in big data technologies, but wouldn't consider myself an expert in any of them. I think I'm just a fairly smart problem solver who can talk to people and happens to have some company name-recognition on my resume. 

This is more aimed at senior engineers / people with 3+ years experience. I think the market is very similar looking at a macro scale to what it's always looked like (outside of prime covid). Entry level jobs don't exist for DE, mid-level jobs are also a bit rare and tough to get, and senior level talent is still needed by most companies. 

To those of you who are looking for your first role and putting out tons of applications and getting no responses back -- I'd recommend looking at smaller companies where you can wear a lot of hats, even if they're posted as analyst roles. As long as you get a database connection and can use sql, that's where most of us start. Alternatively, if you can learn just a little frontend, you might be able to get interviews for a jr. dev. The market is just super saturated with juniors and people making career changes post-covid, so it's really tough for new entrants. 

Anywho, enough rambling. Happy to answer any questions and am curious to see what other senior folks' experiences have been either getting hired or hiring.",2023-09-20 13:53:08
127aca3,Has anyone else moved into data engineering just to discover they hate it?,"A couple years ago I needed a job and took one on a hybrid data/full stack team, thought days engineering sounded cool. We had a reorg and I was moved from there to a data platform team. At first I kinda dug it since we had a good manager, but he got laid off, we share one manager with two other teams now so it's chaotic. 

On top of that it just doesn't feel like I actually build anything, it's just writing pipelines, dealing with different warehousing tools that someone else built, making sure all of the tables are organized for other people to use, and hooking up integrations for various tools. I feel like my engineering skills have gotten rusty, and I've applied around for other jobs but since I was too chicken to make a move during last years wild market I feel pigeonholed into data now that employers are pickier, and it's just going to get worse the longer I'm here. Doesn't help that I'm starting to realize that data engineers seem to make less for some reason.

I'm kinda just venting but I'm really tired of feeling like a back room data monkey, this job has me burnt out on coding overall and wanting to try to switch to product, or just move out of tech to do I don't even know what.",2023-03-31 04:41:22
xpoxj1,« What is an ETL? » and other hard questions.,"Hello fellow data engineers!

A junior is supposed to join my team and work directly with me. On the menu?
- databricks with PySpark
- AWS S3, glue, lambda etc.
- Data pipelines to monitor, with some scheduling
- Features for our data scientists etc.

Anyway, our recruitment is aimed at hiring somebody capable yet junior.

The expected experience is 1-2 year, knowledge of Python and SQL is required, we welcome AWS experience but it’s not necessary.

Of course we have a technical interview where we try to check who is best fit for joining us. And well. To be frank. It’s not great.

Almost every candidates stop at the question “what is an ETL”. The one that do know what it is look at us with a blank face when we ask “what would you do if the ETL you work on fails and the senior DE isn’t there to help you?”. We are talking about situational “technical” questions. And yet everyone stumbles.

SQL window functions? Ever heard of it? “Nope.”
Somebody dropped our prod DB, what do you do? “Well, if it’s being dropped, we get a pop up window telling us not to do it”

We also send a small piece of Python code, 30 lines or so, with instructions, that they can check but don’t have to complete before the interview:
1. A request to a public API endpoint via a try/catch (to the iris dataset)
2. Then a couple of comments that they should filter out the petal width and the species
3. And write as CSV.

Gosh. Like the amount of people that were just like “yeah here there is an if, and here else, I saw that before”, or that simply tell us “you didn’t give me an API”…

An AI PhD student (?) told me that he is learning programming languages like html, css and flask because he doesn’t need JavaScript for web dev (???) and couldn’t read Python code (?????).

Anyway, this is like, all our candidates. I have to work later with one of these people if we recruit them. Yet, the person that helps me interview them, questions if what we ask is too hard? I told them that no. I don’t care if they haven’t scaled thousands of pipeline, deployed a ML model to power a social network, how to optimise PySpark processing or architect a real time DB: I ask them what is an ETL.

I can’t train somebody from scratched when they can’t even read Python code. It’s like hiring a sous chef that doesn’t know what is the difference between boiling and frying ingredients! I just want to scrap the recruitment process and wait to start it later because this is depressing. I don’t know, am I unrealistic in the expectations for a junior? What is the lowest bar you set when recruiting juniors?

TL:DR; got poor DE candidates from my perspective (no knowledge of ETL). Fellow recruiter thinks the questions are too hard. How do you hire your juniors?

Edit: located in Europe, so maybe a different market than US based?",2022-09-27 18:40:44
plmukq,Some of these job postings out there are absolutely hilarious. WTF is this jumbled mess of shit?,N/A,2021-09-10 15:04:56
zzvb1o,"Free ""dbt for beginners"" course",N/A,2022-12-31 14:30:46
15la888,What is the most unproductive task you have to do as a data engineer?,I am new to data engineering and learning basic stuff but I am curious to know what's the most unproductive task you have to do as a data engineer.,2023-08-08 06:53:02
13dlgrr,Are SQL Query optimization skills important and demanded for data scientists/data engineers?,I don't know if SQL Query optimizations skills are demanded or relevant for data scientists/data engineers and data science/data engineering businesses. But I wonder if one with SQL Query optimization skills can stand out from the crowd of data scientists and data engineers and earn higher paychecks?,2023-05-10 09:35:03
wcw0nt,What is in your Data Stack? - Thread,"It would be really useful to get a sense of what data tools companies use to get an idea of what are the best options.

There are a relatively standard set of things most data teams do, but many product options to serve these

Got the idea from the quarterly salary thread, which is a really good resource.

To contribute, post info in the following format:

1. **ETL**
2. **Data Warehouse**
3. **Data Transformation**
4. **BI**
5. **Exploratory Data Analysis**
6. **Company Size** (approx # employees) *\[optional\]*
7. **Company Industry** *\[optional\]*
8. **Company HQ** (city, country) *\[optional\]*

\[Disclaimer - work at a data company\]",2022-07-31 18:56:32
tcdycl,Data Engineering Handbook,"Hi,I recently came across the GitLab data engineering handbook ([https://about.gitlab.com/handbook/business-technology/data-team/organization/engineering/](https://about.gitlab.com/handbook/business-technology/data-team/organization/engineering/))   
[https://handbook.mattermost.com/operations/research-and-development/engineering/data-engineering](https://handbook.mattermost.com/operations/research-and-development/engineering/data-engineering) and I really enjoyed it. Is there any similar recommendation/links from other companies?

Thank you

Edit: Added a few more links.
Edit1: updated name to gitlab",2022-03-12 10:50:21
rfpsnt,Data Engineering Zoomcamp - free Data Engineering course starting in January,"At [DataTalks.Club](https://DataTalks.Club) we're running a free data engineer course next month

We'll cover:

* Data warehousing (BigQuery)
* Batch processing (Airflow, Spark)
* Analytics engineering (DBT)
* Stream processing (Kafka)

And other things! 

&#x200B;

Learn more here: [https://github.com/DataTalksClub/data-engineering-zoomcamp](https://github.com/DataTalksClub/data-engineering-zoomcamp)

And sign up here: [https://airtable.com/shr6oVXeQvSI5HuWD](https://airtable.com/shr6oVXeQvSI5HuWD)

&#x200B;

See you soon on the course!",2021-12-13 20:53:05
oe687q,Spark learning resource that I have been maintaining with care. Use for learning new concepts/preparing for interviews. (Recommendations are welcome),N/A,2021-07-05 12:19:32
12han02,Which tools helps you make such animated gif for data pipelines?,N/A,2023-04-10 07:20:38
1bleg24,Should I learn data engineering? Got shamed in a team meeting.,"I am a data analyst by profession and majority of the time I spend time in building power bi reports. One of the SQL database we get data from is getting deprecated and the client team moved the data to Azure data lake. The client just asked our team (IT services) to figure how do we setup the data pipelines (they suggested  synapse)

Being the individual contributor in project I sought help from my company  management for a data engineer to pitch in to set this up or at least guide, instead I got shamed that I should have figured everything by now and I shouldn't have accepted to synapse approach in first place. They kept on asking questions about the data lake storage which I don't have experience working on.

Am I supposed to know data engineering as well, is it a bad move that I sought help as I don't have experience in data engineering. My management literally bullied me for saying I don't know data engineering. Am I wrong for not figuring it out, I know the data roles overlap but this was completely out of my expertise. Felt so bad and demotivated.

Edited(added more details) - I have been highlighting this to the management for almost a month, They arranged a data engineer from another project to give a 30 minutes lecture on synapse and its possibilities and vanished from the scene. I needed more help which my company didnt want to accommodate as it didnt involve extra billing. Customer was not ready to give extra money citing  SOW. I took over the project 4 months back with the roles and responsibilities aligned to descriptive stats and dashboards.

  
**Latest Update: The customer insists on a synapse setup, So my manager tried to sweet talk me to accept to do the work within a very short deadline, while masking the fact from the customer that I dont have any experience in this. I explicitly told the customer that I dont have any hands on in Synapse, they were shocked. I gave an ultimatum to my manager that I will build a PoC to try this out and will implement the whole setup within 4 weeks, while a data engineer will be guiding me for an hour/day.  If they want to get this done within the given deadline ( 6 days) they have to bring in a Data engineer, I am not management and I dont care whether they get billing or not. I told my manager that if If they dont accept to my proposal, they can release me from the project.** ",2024-03-23 00:09:01
1al3d2f,"Are data engineers really just ""software engineers""?","Ok, to preface, I'm venting a bit here but it's also somewhat of a genuine question.   
Story - I recently applied to a senior DE position for a well known consulting company. For the record, I've worked in Senior DE/BI roles over the past few years and I have a number of former colleagues and friends who work at this specific company so I know their tech stack and business fairly well. Also, for the record I am not a software engineer. I can hack my way through python or an OOP/functional language but SQL is my native dialect. Anyways, I applied for this role and the only glaring omission on my resume was Python experience. Given that I qualified in every other way the recruiter had me move forward to the technical assessment. The assessment was conducted in codility and there were three parts, a python coding portion, a sql coding portion and AWS questions. Coming out of the assessment I felt pretty good but I knew full well that my python solution was pretty rudimentary (admittedly), however it was functional and passed the test cases correctly. Anyways, I find out a few days later from the internal recruiter that my test results didn't fare so well. Although my sql solution was excellent and most of the AWS questions I answered correctly, my python solution wasn't efficient enough and failed on too many edge cases. As such the technical team couldn't recommend I move forward with the interview process (much to my dismay). Now, again... I never said I was a competent Python programmer, in fact I fully admitted that I had very little hands on experience in a business setting coding with python but I'm very familiar with OOP concepts and can pick up any language if/when needed. Either way it seemed like in this case my solution needed to impress the team more than it did.   
So, this brings me back to something the recruiter told me initially... her exact words were ""our data engineers are really software engineers at heart"". I'm wondering if this is becoming more and more the case as time goes on. When I got into BI and DE years ago SQL was the language of most importance (at least in my past roles)... now it seems that that isn't quite the case anymore. Thoughts?",2024-02-07 13:50:55
zgpcsx,r/dataengineering wrapped,N/A,2022-12-09 06:40:48
q63r5q,Google Cloud Data Engineer Certification available for free for a month on Coursera (till Nov 6).,Essentially all the google cloud courses are available for a month for free. Here's a link to the page on coursera - https://www.coursera.org/promo/google-cloud-free-courses-2021?utm_source=googlecloud&utm_medium=institutions&utm_campaign=NoCostTraining_Oct21_newsletter,2021-10-11 19:32:59
1ao16gb,Who uses DuckDB for real?,I need to know. I like the tool but I still didn’t find where it could fit my stack. I’m wondering if it’s still hype or if there is an actual real world use case for it. Wdyt?,2024-02-11 06:09:40
14qzt8y,Just got certified! - Databricks certified associate developer for apache spark 3.0 in Python,"Just got certified! I am a new data analyst who wants to hopefully move into the data engineering field.

I have done a few projects just finding it hard in the current market to find a job. Decided to keep working at my current job and in the meantime finish off a few certs to hopefully attract a few recruiters. Gonna go for the data engineer associate and professional next

For anyone wanting to get it, I highly recommend getting it, it stays forever(no expiry), fairly simple took me 2 weeks assuming you have general python syntax knowledge, plus access to documentation in the exam.

The resources I used were:

1. [https://www.udemy.com/course/databricks-certified-developer-for-apache-spark-30-practice-exams/](https://www.udemy.com/course/databricks-certified-developer-for-apache-spark-30-practice-exams/) \- used this to test my knowledge and basically research what topics are more likely to appear, has a nice breadth of important topics
2. [https://chowdhury-joyjit.medium.com/field-notes-for-the-databricks-certified-spark-developer-exam-ca0b6eb452fc](https://chowdhury-joyjit.medium.com/field-notes-for-the-databricks-certified-spark-developer-exam-ca0b6eb452fc) \- a good reference guide
3. [https://www.udemy.com/course/databricks-certified-developer-apache-spark-30-python/?referralCode=D20A15144B8AF7D2C5CF](https://www.udemy.com/course/databricks-certified-developer-apache-spark-30-python/?referralCode=D20A15144B8AF7D2C5CF) \- essentially 90% of the actual exam questions pretty decent explanations  


P.S Sorry couldn't forget to include [Spark Internals Explanation](https://www.youtube.com/watch?v=7ooZ4S7Ay6Y)! A phenomenal resource to dive deep on how Spark works under the hood",2023-07-05 04:13:09
qo0mun,"Learning SQL, beyond the basics","Hello, fellow Redditors,

Learning SQL beyond the basics can be difficult without a real project. With this in mind, I wrote an article that covers some concepts and techniques that can help you get better at SQL for data warehouses. I hope that these techniques + deliberate practice can help level up your sql skills.

[https://www.startdataengineering.com/post/improve-sql-skills-de/](https://www.startdataengineering.com/post/improve-sql-skills-de/)

Hope this helps someone :). Any feedback is appreciated.",2021-11-06 13:49:12
16qscvf,Everyone on this subreddit should be aware of market salaries,"I saw this post in cscareerquestionsEU. So far there is not so much data for DE job in EU. Please share and fill your data to make it transparent. I am just sharing the post and am not affiliated to anyone -

&#x200B;

Here is the original post -

Here are some sources, none of them are self-promotion nor intended as promotion. I am not being compensated for this, I just strongly believe in the value that salary transaprency brings:

1. [The Trimodal nature of salaries](https://blog.pragmaticengineer.com/software-engineering-salaries-in-the-netherlands-and-europe/)
2. [https://levels.fyi](https://levels.fyi/) \- make sure to filter for your location
3. [https://techpays.eu](https://techpays.eu/) \- basically a levels clone focused on EU with a subset of companies

tl;dr: Switzerland or HFT pays about US rates but has low # of companies and jobs, Netherlands can pay US rates but you have to know which companies, then you get 2nd tier US rates (regions like Austin, or companies like blue chip companies) in Munich, Berlin, London. Most other cities and countries don't have US competitive salaries. Poland is a favorite pick for US companies to have US teams so they pay around 50k (junior) to 150k (staff+) and COL is very low so savings rates can be quite high. Because they work in English, limited need to know Polish AFAIK.",2023-09-24 08:26:56
14nywsi,"Created my first Data Engineering Project which integrates F1 data using Prefect, Terraform, dbt, BigQuery and Looker Studio","## Overview

The pipeline collects data from the Ergast F1 API and downloads it as CSV files. Then the files are uploaded to Google Cloud Storage which acts as a data lake. From those files, the tables are created into BigQuery, then dbt kicks in and creates the required models which are used to calculate the metrics for every driver and constructor, which at the end are visualised in the dashboard.

[Github](https://github.com/InosRahul/f1-data-pipeline)

Architecture

&#x200B;

https://preview.redd.it/xvffk1orod9b1.png?width=3624&format=png&auto=webp&s=3cd2d18a4939f8b1000f1d572dffe8647fe51133

## Dashboard Demo

  


https://i.redd.it/ukolugwppd9b1.gif

[Dashboard](https://lookerstudio.google.com/reporting/9fd225dd-a9b8-45d9-87dc-7d7dbae0c841)

&#x200B;

## Improvements

* Schedule the pipeline a day after every race, currently it's run manually
* Use prefect deployment for scheduling it.
* Add tests.

[Data Source](https://ergast.com/mrd/)",2023-07-01 16:13:08
z1p5ny,Apache Spark™ for Dummies,"I wrote a high-level introduction about Apache Spark. Feel free to leave any feedback!

*When choosing a compute engine, there is no way around Spark. But where does Spark come from and why is it so popular?....*

[More on Medium <:](https://medium.com/microsoft-data-platform-community-hamburg/apache-spark-for-dummies-b77384e33c91)

&#x200B;",2022-11-22 09:20:01
y7ynko,"The data architecture ""pyramid of doom"" according to Dremio. This has lit up quite a discussion in my team's chat 😆 What do you think?",N/A,2022-10-19 10:06:40
16e4d6v,"Data engineering project: Apache Spark, Delta Lake, & Great Expectations running on Docker; Explaining some best practices!","Hello everyone,

There are many DE project posts out there. But they don't explain exactly why a specific approach was chosen. If you are trying to improve your data engineering skills or are the sole data person in your company, it can be hard to know how your technical skills are developing.

With this in mind, I wrote an article that explains high-level best practices (with links to specific approaches and a project), such as:

1. Using established data processing patterns

2. Data quality checks & code testing

3. Approach to make pipelines Idempotent

4. Metadata for debugging and tracking

I hope the posts explain the underlying concepts behind best practices and when to use them.

Blog: [https://www.startdataengineering.com/post/de\_best\_practices/](https://www.startdataengineering.com/post/de_best_practices/)

GitHub Code: [https://github.com/josephmachado/data\_engineering\_best\_practices](https://github.com/josephmachado/data_engineering_best_practices)

I appreciate any questions, feedback, or comments. I hope this helps someone.

&#x200B;",2023-09-09 12:20:27
zvfcjh,Discussion: Star schema and dimensional modeling is still the foundation of data engineering,"I keep myself abreast of trends in the industry as well as new technologies. It seems like most of the innovation in data engineering, aside from the release of Airflow and similar open source pipeline tools (to provide an alternative to SSIS and such) is related to handling huge and massive data volumes. Everything related to hadoop, spark, and even the parallel columnar databases like Redshift.

&#x200B;

Fundamentally, though, I still feel quite ""old school"" in my approach. I still find myself thinking in the Kimball mindset of conformed dimensions and an enterprise bus. Additivity, granularity, etc.

&#x200B;

Am I hopelessly behind the curve in some way? Is there some movement or phenomenon in data engineering that potentially makes classic concepts like this irrelevant?

&#x200B;

I saw that AWS was blasting info at the latest re:invent about how they are moving toward a ""no-ETL future"" by increasing interoperability between pieces in their ecosystem. I'm very skeptical of that because, IMO, most of the value of ETL is actually dimensional conformance and CDC/SCDs more than simply moving data from place to place.",2022-12-26 04:50:39
nk7j14,What articles are must reads for data engineers?,It can be about anything in the field :-),2021-05-24 20:29:12
13rrzx2,What are some good publicly available real-time data sources?,"I am attempting to source via the wisdom of the crowd here. I often find it hard to find good real-time data sources for learning about streaming, prototyping, or building hobby projects. I started researching and then created an ""Awesome List"" in a GitHub repo - [https://github.com/bytewax/awesome-public-real-time-datasets](https://github.com/bytewax/awesome-public-real-time-datasets). 

Does anyone have a good source I should add to this list?",2023-05-25 20:12:55
yve7sf,Master's thesis finished - Thank you,"Hi everyone! A few months ago I defended my **Master Thesis on Big Data** and got the maximum grade of 10.0 with honors. I want to thank this subreddit for the help and advice received in one of my previous posts. Also, if you want to build something similar and you think the project can be usefull for you, feel free to ask me for the Github page (I cannot attach it here since it contains my name and I think it is against the PII data community rules).

As a summary, I built an **ETL process** to get information about the latest music listened to by **Twitter** users (by searching for the hashtag #NowPlaying) and then queried **Spotify** to get the song and artist data involved. I used **Spark** to run the ETL process, **Cassandra** to store the data, a custom web application for the final visualization (**Flask** \+ table with DataTables + graph with Graph.js) and **Airflow** to orchestrate the data flow.

In the end I could not include the Cloud part, except for a deployment in a virtual machine (using GCP's Compute Engine) to make it accessible to the evaluation board and which is currently deactivated. However, now that I have finished it I plan to make small extensions in GCP, such as implementing the Data Warehouse or making some visualizations in Big Query, but without focusing so much on the documentation work.

Any feedback on your final impression of this project would be appreciated, as my idea is to try to use it to get a junior DE position in Europe! And enjoy my skills creating gifs with PowerPoint 🤣

https://i.redd.it/trlt7kqunzz91.gif

P.S. Sorry for the delay in the responses, but I have been banned from Reddit for 3 days for sharing so many times the same link via chat 🥲 To avoid another (presumably longer) ban, if you type ""**Masters Thesis on Big Data GitHub Twitter Spotify**"" in Google, the project should be the first result in the list 🙂",2022-11-14 22:07:29
15ni579,Got my first Data Engineer job,"Hi everyone, I wanted to say a big thanks to this sub-Reddit, as I just got my first job as a Data Engineer. 

After losing my job I decided to make a career change into Data Engineering from Data Science. From reading posts here for the past year or so my interest has grown in the area, leading up to this, so thanks everyone for all of the interesting and useful posts!

Any advice for topic area to read up on before I start? Or maybe courses/YouTube series to help me be ready? 

Thanks!",2023-08-10 17:07:22
124mi0z,"SMBC-comics.com ""now squeeze your points together to make your results look big""",N/A,2023-03-28 11:59:23
sjjaao,Discuss a data pipeline that you've worked on,"The vision of this subreddit is to learn by sharing our experiences. So, let's discuss in short about at least one data pipeline that we have worked on.

I'll put my 2 cents on the table.

I work in a medium sized bank in the engineering department that builds solutions for various teams present here. 

A data pipeline out of these will be one in which we provide our investment team with an insight about companies they are interested in (for investment). This insight is in the form of providing a status of outstanding charges (loans) that those companies have taken from other banks. 

In the first step of the pipeline data is scraped by crawlers from a publicly available government website and stored on a mongo cluster with timestamps of the time when the data was extracted.
In the second step this data is written on a kafka raw topic
Some transformations/cleaning are carried out and the fresh data is now written to a kafka rich topic
Next we write a Cassandra Loader which reads data from the rich topic and with the help of a mapping file loads data on to a cassandra namespace to it's corresponding column families. Cassandra is used as it helps to retain different versions of the same data based on a clustering key of yearmonth.
For faster insights (search queries) one version (the latest one) of this data is written on elasticsearch so that various indices can be created based on interested columns as per business requirements.

This is an example of an ETL pipeline (leaving aside the elasticsearch part). It can also be seen as an EtLT pipeline as transformations are also carried on top of the data in Cassandra by writing adhoc Spark jobs.

P.S : if you are taking away something from this post, please add some cents on the table so we all can become rich 😛",2022-02-03 13:32:33
scmpdl,Gitlab's Data Team Platform (in depth look at their stack),N/A,2022-01-25 20:11:25
oolpqh,"Saw this on Twitter. Is the specialization of data teams a bad thing, and will that change going forward?",N/A,2021-07-21 08:35:10
l0qg2l,Introduction to Databases for Data Engineers,N/A,2021-01-19 18:54:53
16o883v,"Python Pareto Principle - what is the 20% (algos, functions, libraries) that lets you develop 80% of code related to Data Engineering?",.,2023-09-21 06:14:35
13eo3b3,Is it worth learning Apache Spark in 2023?,According to stack overflow survey 2022 Apache Spark is one of the highest paying technologies. But I am not sure if I can trust this survey. I am really afraid I will waste my time . So people with more experience could you please let me know if Apache Spark is high demanded and high paying skill? Will learning internals of it worth my time?,2023-05-11 13:54:23
nuvhli,Starting A Data Engineering Project Series,"&#x200B;

 I am starting to put together a series on developing a data engineering project for your resume. 

I think I have seen a lot of people on this subreddit ask for it, so I am hoping it will really help you out!

That being said, I would love to hear about what you would like to see in a data engineering project series. Even if I don't use it now, I am sure I will use it later.

I did just put out the [first video in the series](https://www.youtube.com/watch?v=LJkVvNWlO0g). 

**But the TL;DR is here are 5 sources I reference in the video that you can get from online that you can practice setting up some form of data pipeline on. Even if the data is only updated once a quarter, its still good practice.** 

[San Fransisco Has A Lot Of Great Options In terms](https://datasf.org/opendata/) \- I don't like the US government's main data site as much as I like SFs. It's just easier to use

[Real estate APIs](https://gist.github.com/patpohler/36c731113fd113418c0806f62cbb9e30) \- These do often cost a little, but the cheapest one on the list was around $40 a month, which isn't terrible (as long as you finish your project in a month or two)

[Census Data](https://www.census.gov/data/datasets.html) 

[Consumer Finance Complaints](https://www.consumerfinance.gov/data-research/consumer-complaints/) 

[News RSS Feeds](https://github.com/damklis/DataEngineeringProject)

**Why did I start a youtube channel?** 

Honestly, I really want to start getting data engineering a little more lime light. 

I don't think we will ever have the same shine as data science. However, I think we play a very important role that I personally really enjoy and I want to save some people who think they want to be data scientists, who really want to be data engineers.",2021-06-08 03:42:15
kx7tul,"We don't need data scientists, we need data engineers",N/A,2021-01-14 15:30:16
1bo4nne,"Finding a new job, ridiculous ","Hello guys after finishing a contract in a company I’m searching for another opportunity in Europe based remotely and what I see in the job descriptions in LinkedIn are 27 technologies needed for the position and you have to be an expert, even not a senior position (I have 3.5 years of experience), what is happening here?

You need to know: python, pyspark, scala , JavaScript, java, azure, aws, gcp (and all the the technologies), databricks, airflow, Kafka, sql, no sql, data lakes, dwh, oracle, ETL’s, terraform, Jenkins, kubernetes… and more 

Ofc all of this fluent and proficient, lol



And not even senior positions… what would you recommend, guys?
I’ve been working with azure data factory/synapse/Databricks with python/pyspark and sql, doing etl/elt pipelines from on-premise ddbb or simple excels or cloud ddbb, or api’s.

",2024-03-26 10:35:24
1913k8k,Who are the GOATS of DE?,"This can a subjective question, DE is still niche and there is no such thing as a ranking but wanted to know if you guys have a high role model in the area. 

For example in programming there are well respected names on the likes of Linus Torvald or Guido Van Rossum.

This can be any inspiring youtuber, book writer, DE influencer or whatever.",2024-01-07 21:46:35
162rlqg,How are you lowering your data platform costs?,"Going by the recent S-1 filling from Instacart there is a tangible example of cost cutting.
Their Snowflake bill was $13M, $28M, $51M in 2020/21/22. 2023 bill will be $15M.
What they did behind the scenes hasn’t been revealed but I am pretty sure this isn’t the first of such changes. If you know more about this, do share.

How is this community thinking about costs in these economic times? If you aren’t focused on costs and already run pretty well, do share that too.",2023-08-27 14:18:01
wjhw2w,What exactly is a data brick?,"I am looking to get into data engineering and had an interview recently where data bricks were mentioned. Can someone explain to me exactly what a data brick is, and how it relates to a SQL table or Data Mart. From how it was explained to me, it was a self contained database that has its own flavor of SQL used to query it. 

I know I can simply Google this question but I want to hear from other data engineers about it.",2022-08-08 19:33:01
ruz2v1,Dont understand this job post no pyspark but extensive experience of spark jobs with python?,N/A,2022-01-03 10:53:50
11x4u47,What is the hottest tech stack in Data Engineering world now?,I know we all work in different tech stacks. But what is the hottest tech stack at the moment (also have good future perspective) for any data engineers to pursue? Thank you.,2023-03-21 02:56:17
10ijsqy,I built an LLM-powered tool that can understand the structure of any website and extract the desired data in the format you want.,N/A,2023-01-22 13:34:45
vi2iv0,(Almost) OpenSource data stack for a personal DE project. Before jumping on the project I would have liked to have some advice on things to fix or improve in this structure! do you think that this stack could work?,N/A,2022-06-22 10:59:35
vemxch,Does anyone think the cost of Snowflake is a problem?,"Recently, I talked with a data team leader of a healthcare startup. The data team leader is hesitant to export data from MySQL to Snowflake because the cost of Snowflake is a bit high for his company. Does anyone also concern about the cost of Snowflake?",2022-06-17 18:39:00
sunt2i,Apparently 90% of all the Azure Data products are 7 years old in 2022. This is the job desc for a DE of a billion dollar pharma. It looks pointless to me to have 5+ yrs of exp into something that is just turning 7this year. Unrealistic tech expectations!,N/A,2022-02-17 12:36:29
si6cjt,What resources do you use for staying up to date with the data engineering landscape?,"Hey all, curious what you follow to stay up to date with the data engineering world? Here's some of [my favorite](https://twitter.com/iporollo/status/1488590270044131332?s=20&t=tgf2-5R7h7cQs-fwW-G35Q) communities / blogs / newsletters / YouTube channels that I follow:

Communities:

dbt slack community - [https://www.getdbt.com/community/join-the-community](https://www.getdbt.com/community/join-the-community)

Locally Optimistic slack community - [https://locallyoptimistic.com/community/](https://locallyoptimistic.com/community/)

DataTalksClub slack community - [https://datatalks.club/](https://datatalks.club/)

Newsletters:

[https://roundup.getdbt.com/](https://roundup.getdbt.com/)

[https://benn.substack.com/](https://benn.substack.com/)

[https://www.dataengineeringweekly.com/](https://www.dataengineeringweekly.com/)

[https://dataespresso.substack.com/](https://dataespresso.substack.com/)

[https://davidsj.substack.com/](https://davidsj.substack.com/)

[https://www.blef.fr/](https://www.blef.fr/)

[https://seattledataguy.substack.com/](https://seattledataguy.substack.com/) 

[https://letters.moderndatastack.xyz/](https://letters.moderndatastack.xyz/)

[https://scientistemily.substack.com/](https://scientistemily.substack.com/)

YouTube Channels:

[SeattleDataGuy](https://www.youtube.com/c/SeattleDataGuy)

[DataCouncil](https://www.youtube.com/c/DataCouncil)

[Monday Morning Data Chat](https://www.youtube.com/playlist?list=PLIlqnK97FLdtQEad5pi22BefNuaSeBt1e)

TikTok Channels:

[the.data.guy](https://www.tiktok.com/@the.data.guy)

[SQream](https://www.tiktok.com/@sqreamtech)

[Sidcodes](https://www.tiktok.com/@sidcodes)

Curious to see what other people are reading / watching / listening to in this space!

EDIT: Found a few more!

EDIT 2: Fixed broken dataengineeringweekly link",2022-02-01 21:36:20
13ptsio,Microsoft announces Fabric data platform,"Looks interesting.. What do you guys think?

https://azure.microsoft.com/en-us/blog/introducing-microsoft-fabric-data-analytics-for-the-era-of-ai/",2023-05-23 16:43:17
13cprs6,How Instagram handles data?,"I was looking at Mr. Beast’s new post for his recent giveaway. The number of comments are staggeringly high. It has more comments than the likes for the post. 

I was wondering how these comments are stored or handled within IG? Are they using SQL tables for this? And every time, I go the comments section does it read the database every time? Do we see real time numbers? 

If you have worked in such a system. Can you give your two cents please? Thank you.",2023-05-09 13:23:20
yrjwcf,"Out of work for 8 months, trying something new with my resume. I have to imagine this is easier for hiring managers to look at. And much easier to have in front of anyone conducting your interview. But then again, I'm not a hiring manager. What are people's thoughts on this format?",N/A,2022-11-10 16:20:22
mvdteu,/r/dataengineering hit 30k subscribers yesterday,N/A,2021-04-21 11:41:22
1anokrg,Why Data Engineering is less saturated than Data analyts/science?,"Hey, CS student here. I was looking at job postings, and i noticed that data analyst/science roles have between 200-500 applicants, while DE roles barely reach to 50 applicants, why is this?",2024-02-10 19:45:21
11y6b3o,Where can I find online projects end-to-end?,"Two years in the industry, came from a non-tech background, but landed a job as a data engineer. I have worked on small tasks such as maintaining an already built ETL pipeline.

But I want to learn more. I want to build things from scratch.

Data modelling, data cleaning, ETL, etc.

Midnlessly solving SQL and python problems won't get me there.


Any help?


Note: This is for LEARNING. I don't want to sneak ANYTHING into my resume. I want to get my hands dirty.",2023-03-22 04:04:35
yw3mk5,After Airflow. Where next for DE?,"Airflow was released in **2014** by Maxime Beauchemin (of Airbnb). In those 8 years it’s really dominated DE in the wild. However, many teams are putting out ideas to succeed Airflow.

\----\[Edit\] **BTW** \- Seeing this has turned into a vendor conference **(Sorry!)** 

We are designing to support:

* DAGs on AWS Lamdba
*  Airflow Transpilation to allow productivity gains with no migration risk.

[typhoondata.io](https://typhoondata.io/)

[https://github.com/typhoon-data-org/typhoon-orchestrator](https://github.com/typhoon-data-org/typhoon-orchestrator)

Any feedback on our very early stage project is welcome!

\----

I’d like to start a discussion:

Is DE collectively moving on from Airflow, or … i***f it ain’t broke, why fix it?!***

What could be improved on Airflow (even v2):

&#x200B;

* Lack of ability to test easily
* Data sharing between tasks (TaskFlow API is a step in the right direction, but still limited by underlying design choices)
* End up with NxM for sources and targets

How might the market develop? (not 1 winner, for sure):

* **Streaming, Kafka**
   * Backbone of many of the large tech now
   * Very advanced for many teams
   * Will it completely replace batch?
* **Fivetran and other tools that automate DE**
   * Reduces reliance on hard to find DEs
   * New pricing models based on credits make it more affordable
* **Airbyte and DBT, Simple Airflow and DBT**
   * simple model of commoditizing the \[EL\] and then DBT for the \[T\]
   * makes ‘analytics engineers‘ (analysts) much more productive but is limited for complex workloads perhaps (will adding python hooks change this)
* **Dagster & Prefect**
   * Better composability
   * Shared data between tasks
   * Big enough feature improvements to move from Airflow?
   * Is the community big enough yet?
* **AWS Lambda (serverless)**
   * Tooling underdeveloped
   * 15 minute limit per lambda run
* **Stick with Airflow 2**
   * FOSS
   * Move to Astronomer for managed services is growing somewhat
   * Despite some productivity challenges, it is easy to support

**What do you think?**

\- What do you plan (concretely) on using in next 6 months?- What are you using now?

&#x200B;

**Full disclosure:**  I hope this is an interesting discussion question! We are of course making [a new alternative (typhoondata.io)](https://typhoondata.io/)   but want to learn from the forum so I have not included our option directly in the list.",2022-11-15 17:20:40
nc8zxh,Great Resource! YouTube walkthrough of 50+ Leetcode SQL problems,N/A,2021-05-14 13:46:49
1883wyz,Doom predictions for Data Engineering,"Before end of year I hear many data influencers talking about shrinking data teams, modern data stack tools dying and AI taking over the data world. Do you guys see data engineering in such a perspective? Maybe I am wrong, but looking at the real world (not the influencer clickbait, but down to earth real world we work in), I do not see data engineering shrinking in the nearest  10 years. Most of customers I deal with are big corporates and they enjoy idea of deploying AI, cutting costs but thats just idea and branding. When you look at their stack, rate of change and business mentality (like trusting AI, governance, etc), I do not see any critical shifts nearby. For sure, AI will help writing code, analytics, but nowhere near to replace architects, devs and ops admins. Whats your take? ",2023-12-01 05:21:39
11jzbx6,"Pandas 2.0 and its Ecosystem (Arrow, Polars, DuckDB)",N/A,2023-03-06 13:44:14
11fyslh,Any other DEs here not involved in data warehousing / data modeling? Where's the love for the infrastructure and ingestion guys?,"I feel sometimes like this sub is a bit of a dbt / snowflake / sql love-in. Nothing wrong with that stack or skillset btw but I thought that stuff was more for analytics engineers?

I am more involved in the platforms & ingestion side of things. Lots of IaC for setting up data infrastructure, maintaining a streaming solution, working with SWE teams to ingest data from their apps and OLTP systems in a transactional and performant manner, and implementing things like data contracts and schema validation to stop upstream breaking changes. Some custom integrations using python and various AWS services to pull external data sources. I'm also pretty good with spark and do some initial validation, transformations & optimizations etc in the warehouse before handing over to the modelers.

Got to be other people like me here but I guess we're in a minority? Curious to hear where the boundary between DE and AE lies in your business?",2023-03-02 09:47:30
v7pgim,Pandas-like library to build SQL models,"Hi! We built an open source python library called Bach\[1\] that talks Pandas, and outputs SQL that can be run directly on a data store (as cloud data stores can handle this now). It’s available on PyPI\[2\].

For example, let’s say we have the following DataFrame (a representation of what’s in the table):

|Index|Column A|Column B|
|:-|:-|:-|
|0|1|None|
|1|None|'a'|
|2|None|'b'|
|3|2|None|
|4|None|None|

…we can run a Bach operation equivalent to \`pandas.DataFrame.ffill()\`:

    df = df.ffill(sort_by=[‘index’], ascending=True)

|Index|Column A|Column B|
|:-|:-|:-|
|0|1|None|
|1|1|'a'|
|2|1|'b'|
|3|2|'b'|
|4|2|'b'|

… which gets translated to SQL:

    ""fillna_partitioning___2db01eb2e4973434d297e8ccc20cccd8"" AS(
       SELECT ""index_0""  AS ""index_0"",
        		""Column A"" AS ""Column A"",
               ""Column B"" AS ""Column B"",
              cast(
    sum(CASE
                         WHEN ""Column A"" IS NULL THEN 0
                         ELSE 1
                      END) 
    OVER ( ORDER BY ""index_0"" ASC rows BETWEEN UNBOUNDED     PRECEDING AND CURRENT row) 
             AS bigint) AS ""__partition_Column A"",
             cast(
    sum(CASE
                        WHEN ""Column B"" IS NULL THEN 0
                        ELSE 1
                      END) 
                     OVER ( ORDER BY ""index_0"" ASC rows BETWEEN UNBOUNDED PRECEDING AND CURRENT row) 
        AS bigint) AS ""__partition_Column B""
             FROM ""your_super_awesome_table"" ORDER BY ""index_0"" ASC 
    )
    SELECT ""index_0""                                                                                                             AS ""index_0"",
    
    first_value(""Column A"") OVER (partition BY ""__partition_Column A"" range BETWEEN UNBOUNDED PRECEDING AND    CURRENT row) AS ""Column A"",
    
    first_value(""Column B"") OVER (partition BY ""__partition_Column B"" range BETWEEN UNBOUNDED PRECEDING AND    CURRENT row) AS ""Column B""
    FROM   
    ""fillna_partitioning___2db01eb2e4973434d297e8ccc20cccd8""

We started working on this over a year ago, as we wanted to:

* Not write sometimes very complicated SQL, but use familiar pandas APIs.
* Not deal with data store SQL dialects and other quirks. (Currently supported data stores are PG & BQ, later Redshift, Databricks, etc.)
* Not have to port all data models if we switch data stores.
* Not build pipelines to get (quickly outdated) sample data, but just choose to get a sample from the data store or run an operation on the full set when needed.

Once a model is done, getting it to production is one operation, by exporting the resulting SQL to e.g. dbt, a BI tool, etc. So no need to port a model to SQL first anymore.

We already support a subset of pandas operations\[3\], but did we miss anything you use frequently? 

\[1\] [https://github.com/objectiv/objectiv-analytics](https://github.com/objectiv/objectiv-analytics)

\[2\] [https://pypi.org/project/objectiv-bach/](https://pypi.org/project/objectiv-bach/)

\[3\] [https://objectiv.io/docs/modeling/bach/api-reference/](https://objectiv.io/docs/modeling/bach/api-reference/)",2022-06-08 13:17:47
v3xn34,How do you develop proficiency in Apache Spark?,"I've been working as a Data Engineer over a year, and hail from software engineering background. In my work role we have sorted life since we use Azure's ecosystem to process mostly structured data coming from different systems and blob sources. 

Recently, I gave an interview for an exciting role in an exciting startup and I realised I don't know spark in depth as much as I thought it would. 

The questions were trivial yet difficult, mosty situation based like: the architecture of spark, how data gets diistributed and processed in the framework, various edge cases of operations involving joins and data movements, the scenarios when data failure occurs (when the data is much bigger and skewed than individual RAM's of the worker nodes) and strategy to process data in such scenarios, how to monitor performance, failure handling etc and many more.

I have familiarity with big data ecosystems but not to the levels. Now, I have resolved to gain an expertise in spark and want your help with resources or strategies to achieve that goal. 

Thanks in Advance ❤️",2022-06-03 11:50:29
r8pa3i,Why is Data Build Tool (DBT) is so popular? What are some other alternatives?,"Hi, can some one please explain why DBT is so popular?",2021-12-04 13:04:59
ve2qbp,Why does dbt have so much hype/ metions in this subreddit?,I’ve been a lurker on this sub for a little over a 1.5 years and over this past year dbt has been popping up in every post/comment section it possibly could be. I don’t I understand the hype about it. Especially to the degree it’s being said in this thread. Of all of my friends and colleagues in the tech/data sphere none of them have heard or even used dbt at all. It almost feels like it’s an slow astroturf campaign to try and get people to use their product. You don’t need dbt to set up a system to have version control for sql files. Or to use those version controlled sql files to run in your Prod pipeline. I understand the benefits of ELT but you don’t need dbt to do that. It’s one tool of many. Can someone explain they hype to me?,2022-06-17 02:46:29
1bifhj9,O’Reilly data engineering reference books on sale! (Includes reference books on pyspark and scaling up pipelines),"Hope this post is ok, as I don't work for either O'Reilly or Humble Bundle. Given the number of questions on this thread for getting books on the topic, thought maybe some of you might be interested in this too! Personally, I'd been wanting to get, ""Data Algorithms with Spark,"" but had been hesitating due to the price. I was super thrilled seeing this included in the book bundle.

This is an organization that partners with others to offer books (and games) at a super low price. Part of the proceeds goes to charity. I've been a huge fan of them since discovering them a while ago.

[https://www.humblebundle.com/books/pipelines-and-nosql-oreilly-books](https://www.humblebundle.com/books/pipelines-and-nosql-oreilly-books)",2024-03-19 08:37:42
1b8e72j,Will Dbt just taker over the world ?,"So I started my first project on Dbt and how boy, this tool is INSANE. I just feel like any tool similar to Azure Data Factory, or Talend Cloud Platform are LIGHT-YEARS away from the power of this tool. If you think about modularity, pricing, agility, time to market, documentation, versioning, frameworks with reusability, etc. Dbt is just SO MUCH better.

If you were about to start a new cloud project, why would you not choose Fivetran/Stitch + Dbt ?",2024-03-06 23:02:56
11zh526,Magic: The Gathering dashboard | First complete DE project ever | Feedback welcome,"Hi everyone,

I am fairly new to DE, learning Python since December 2022, and coming from a non-tech background. I took part in the [DataTalksClub Zoomcamp](https://github.com/DataTalksClub/data-engineering-zoomcamp). I started using these tools used in the project in January 2023.

Project link: [GitHub repo for Magic: The Gathering](https://github.com/VincenzoGalante/magic-the-gathering)

Project background:

* I used to play Magic: The Gathering a lot back in the 90s
* I wanted to understand the game from a meta perspective and tried to answer questions that I was interested in

Technologies used:

* Infrastructure via terraform, and GCP as cloud
* I read the [scryfall API](https://scryfall.com/) for card data
* Push them to my storage bucket
* Push needed data points to BigQuery 
* Transform the data there with DBT
* Visualize the final dataset with [Looker](https://lookerstudio.google.com/u/0/reporting/ebdf68e1-27f7-435b-8add-a4018681f801)

I am somewhat proud to having finished this, as I never would have thought to learn all this. I did put a lot of long evenings, early mornings and weekends into this. In the future I plan to do more projects and apply for a Data Engineering  or Analytics Engineering position - preferably at my current company. 

Please feel free to leave constructive feedback on code, visualization or any other part of the project. 

Thanks 🧙🏼‍♂️ 🔮",2023-03-23 11:13:22
10z37l5,Valentine's for your data sweetheart 🫶,N/A,2023-02-10 21:12:02
z01v13,World's Simplest Data Pipeline - I wrote a post about data engineering fundamentals. What do you think?,"
https://dantelore.com/posts/simplest-data-pipeline/

The idea was to capture the important stuff with the absolute minimum code/complexity.

Genuinely interested in people's opinions...",2022-11-20 11:30:46
lnj4lh,"We Don’t Need Data Scientists, We Need Data Engineers - KDnuggets",N/A,2021-02-19 15:55:00
1abov9g,"Something for fun, what abilities would you give this card?",N/A,2024-01-26 18:12:55
11irw04,Is it true that Apache Spark (especially with Python) skills are in very high demand and paying well?,"I am checking stack overflow survey 2022 (https://survey.stackoverflow.co/2022/#top-paying-technologies-other-frameworks-and-libraries), and I see that apache spark is the highest paying framework under other frameworks category. I want to upskill myself and to be more demanded in job market. So is it worth learning Apache Spark (PySpark) in 2023?",2023-03-05 08:30:17
rva6nc,How did you become a SQL pro?,"I can solve all Hackerrank/Leercode SQL problems but I feel like I don’t understand SQL as a whole, as such, I would not be able to handle a data cleansing/wrangling project.

Are there any practice SQL projects available? What would you recommend?",2022-01-03 19:45:56
1ao15gx,"I built my first end to end data project to compare US cities for affordability against walk, transit and biking score. Plus, built a cost of living calculator to discover ideal city and relocate!","Found no site to compare city metrics score with affordability. So built a one.

Web app - [CityVista](https://cityvista.streamlit.app/)

An end-to-end pipeline -

1) Python Data Scraping scripts  
Extracted relevant city metrics from diverse sources such as US Census, Zillow and Walkscore.

2) Ingestion of Raw Data   
The extracted data is ingested and stored in Snowflake data warehouse. 

3) Quality Checks  
Used dbt to perform data quality checks on both raw and transformed data.

4) Building dbt Models  
Data is transformed using dbt modular approach.

5) Streamlit Web Application  
Developed  a user-friendly web application using Streamlit.  


Not the greatest project but yeah achieved what I wanted to make.

&#x200B;

https://preview.redd.it/zz09hu5cdwhc1.png?width=1893&format=png&auto=webp&s=d326943976474f484f4def373fa16d46c283a7a2

https://preview.redd.it/xe30su5cdwhc1.png?width=1827&format=png&auto=webp&s=8b29c34af31c1deae92958f14949609a5913c917

&#x200B;

https://preview.redd.it/24xzpwwldwhc1.png?width=4420&format=png&auto=webp&s=ddc293c0b23b1329fc345ee764efb136f0d167ac",2024-02-11 06:07:55
15iu5ya,"Don't fall for the ""Data is Beautiful"" post with the mug. It is an ad. Mods, is there anything we can do about shit like this?","I have been blocked on the post, but OP is clearly running a sock puppet network as I detailed in (rapidly downvoted) comments in the post.",2023-08-05 12:14:18
12anr2k,"COVID-19 data pipeline on AWS feat. Glue/PySpark, Docker, Great Expectations, Airflow, and Redshift, templated in CF/CDK, deployable via Github Actions",N/A,2023-04-03 15:24:44
xkb4h8,hmmm,N/A,2022-09-21 17:23:37
u99v1q,Data engineering blogs worth reading,"Hi guys!

I'm looking for valuable data engineering/data science blogs in English. Any recommendations?

Is there anyone you follow on medium?",2022-04-22 08:38:22
144hveq,GlareDB: An open source SQL database to query and analyze distributed data,"Hi everyone, founder at GlareDB here.

We've just open sourced GlareDB, a database for querying distributed data with SQL. Check out the repo here: [https://github.com/GlareDB/glaredb](https://github.com/GlareDB/glaredb)

We have integrations with Postgres, Snowflake, files in S3 (Parquet, CSV), and more. Our goal is to make it easy to run analytics across disparate data sources using just SQL, reducing the need to set up ETL pipelines to move data around.  Take a look at our [docs](https://docs.glaredb.com/docs/working-with-your-data/querying.html#querying-multiple-data-sources) to see what querying multiple data sources looks like. We've also recently merged in a [PR](https://github.com/GlareDB/glaredb/pull/1086) letting you run queries like `select * from read_postgres(...)`.

GlareDB is still early stages, and we have a lot planned the next few months. Have a use case that you think GlareDB is a good fit for? Let us know! And if you have any feature request for things you'd like to see, feel free to open up an issue.",2023-06-08 19:02:21
12th15p,Live coding interview hatred,I DESPISE live coding interviews. I’m a good engineer and I can talk through skills and whiteboard and data model interview just fine. But seriously ask me a basic select statement in sql live and I barely remember how to do that. Panic sets in immediately and I barely make it through. I promise give me an hour to code something real and it will be done but just don’t make me live code. I have almost 10 years experience and can barely write sql in a coding interview. It’s just really rough.,2023-04-20 22:15:30
12r8x5c,"Zillacode Premium finally done, Leetcode for PySpark, Spark and Pandas at Zillacode.com",N/A,2023-04-18 23:37:58
zamewl,"What's ""wrong"" with dbt ?","I'm looking to learn more about dbt(core) and more specifically, what challenges teams have with it. There is no shortage of ""pro"" dbt content on the internet, but I'd like to have a discussion about what's *wrong* with it. Not to hate on it, just to discuss what it could do better and/or differently (in your opinion).

For the sake of this discussion, let's assume everyone is bought into the idea of ELT and doing the T in the (presumably cloud based) warehouse using SQL. If you want to debate dbt vs a tool like Spark, then please start another thread. Full disclosure: I've never worked somewhere that uses dbt (I *have* played with it) but I know that there is a high probability my next employer(regardless of who that is) will already be using dbt. I also know enough to believe that dbt is the best choice out there for managing SQL transforms, but is that only because it is the only choice?

Ok, I'll start.

* I hate that dbt makes me use references to build the DAG. Why can't it just parse my SQL and infer the DAG from that? (Maybe it can and it just isn't obvious?)",2022-12-02 13:34:26
z52hou,Scaled to 1M cores in EKS,N/A,2022-11-26 08:34:28
xx5gv0,Rant: Frustrating employer and salaries,"Just need to rant for a second.

Employer led on a qualified, hungry coworker who would have been an excellent addition to my team. Ultimately didn't hire him as a DE because they thought he was ""absolutely qualified and worth what he was asking, but they want to pay someone much less"". Couldn't do anything to convince them otherwise, short of threatening to quit, which is a bluff I can't afford to carry out in these economic times.

I just feel apathetic about everything now. I'm taken care of, but my coworker and friend got screwed, and it's taken me from being hungry and ambitious to ""quiet quitting"" (hate the term). I have no desire to go beyond my duties/hours, even if it means more compensation, and certainly no desire to move forward with hiring a new Jr.

It's like being pregnant in a post-apocalyptic world.

Anyway, thanks for listening, rant over.",2022-10-06 13:36:39
vwcl1q,Data Science is like playing with Chiellini,N/A,2022-07-11 07:26:45
s8gegf,Graduating from ETL Developer to Data Engineer,N/A,2022-01-20 10:55:49
q0alnb,Please Critique my Resume: Data Analyst transitioning to Data Engineer,N/A,2021-10-03 04:23:45
l66fyz,8 Data Engineering Evangelists to Follow,"Hi folks, I want to share my list of scientists/business leaders who I follow to stay up-to-date about what is going on on the data engineering scene.

Feel free to share your bookmarks in the comments.

👉  [John Lafleur](https://medium.com/@jeanlafleur)  \- Co-Founder of Airbyte - writes about ETL /  ELT and his startup journey

👉  [Tristan Handy](https://medium.com/@jthandy) \- Co-founder of Fishtown, created by dbt -  about startups, trends in data analytics and a little bit about Fishtown

👉  [Connor Shorten](https://connorshorten300.medium.com/) \- Computer Science Ph.D. at FAU - about Computer Vision, Natural Language Processing, Graph Embeddings, Generative Adversarial Networks, Reinforcement Learning, and more

👉  [Jürgen Schmidhuber](http://people.idsia.ch/~juergen/) \- computer scientist, researcher, keynote speaker, co-director of the Dalle Molle Institute for Artificial Intelligence Research - about the science of AI

👉 [Sébastien Derivaux](https://dataintoresults.com/post/category/thoughts/) \- shareholder and board member of many startups - about data science and startups

👉 [Jesse Anderson](https://www.jesse-anderson.com/category/blog/) \- data engineering evangelist - all-around data engineering in a simple way

👉 [David Layton](https://medium.com/@dmlayton) \- former CERN physicists, data engineer & scientist - about agile, data management, tools

👉 [George Fraser](https://twitter.com/frasergeorgew) \- CEO of Fivetran - about trends in data preparation and management",2021-01-27 15:11:14
11xcy2g,I don't understand DuckDB,"I understand it's an in process OLAP database. Ok, fine, that's what everyone have been saying since its inception.  I understand it's great and everyone loves it.

I also have no ducking clue what I'm supposed to do with it. Is it supposed to be a drop in replacement for pandas? Or spark?

Or am I supposed to ship it alongside an analytics app to make fast calculations, like Hex does?

Or maybe run it in a pod and use that to perform the T in ETL? Or all of the above?

\---

Can you ELI5 it to me?",2023-03-21 10:27:40
xbvuul,Rewriting the data pipeline,N/A,2022-09-11 22:32:28
x5thfz,"Definition of the Data Engineer role, IMHO.",N/A,2022-09-04 18:07:31
vgwcgg,This is actually what broke her heart.,N/A,2022-06-20 21:28:48
1b96lr0,Dagster University | Dagster & dbt,N/A,2024-03-07 21:53:37
14xhi13,Is it normal to feel completely lost during initial months of your data engineering job ?,"I got into a data engineering role, it's my first job as a DE. And i am feeling absolutely lost, i don't understand what's happening, everything is everywhere, my team mates are very busy so no one properly explains what's happening and some structural change is happening in the whole section of DE teams. And I feel absolutely overwhelmed.
How do you tackle this?",2023-07-12 07:44:51
13ip8e5,"Is there something wrong with me, I hate dbt, what am I missing ?","Fairly self explanatory. It’s not very fast, no nice REPL for prototyping, large ETL seem to end up being a big mess of SQL in different models. I find developing using the tool extremely boring. Errors aren’t caught when run, dbt expectations is less expressive than if you just wrote your own assertions etc, etc. As far as I can tell the only benefit relative to a tool like DBR which also optimises DAGs is DDL statement automation. Could someone please tell me what I’m missing here.",2023-05-16 00:09:00
11xbpjy,Beware of Fivetran and other ELT tools.,"I posted this on another thread but felt like more data engineers should be aware of these issues with Fivetran and other ELT tools:

Fivetran is terrible for these reasons:

- slow to fix issues or problems when they are discovered
- they alter field names and change data structure thereby making it very difficult to migrate to other options if the need arises.
- for some data sources they force you to ingest all objects thereby increasing your costs - great for them as it makes them more money
- they constantly have issues - we would get emails very regularly identifying problems with their system
- within 6 months of us cancelling we identified an issue where Fivetran was incorrectly identifying primary keys with the Pendo trackevents object.  We raised this with the support team and they denied there was an issue.  Maybe 4 weeks later they sent out an email admitting they had an issue and refused to credit us for the reprocessing of data we incurred trying to fix it.  Their fix also took about 2 months to implement.  We later learned we had dropped over 1 billion rows of data due to this issue.
- lack of transparency with all the transformations and adjustments they make (yes I know they have schema charts but the transparency goes beyond this)
- enormous expenses for loading data - we were getting charged around 30k to reload Pendo data when we were able to do it ourselves for about 3k.
- SLAs are non existent.  They have a 12 hour buffer.  Most integrations get flagged as “delayed” and there are no clear answers why.
- They pick and chose what data on each object they pull in.  Don’t assume they bring in all fields that are available on all endpoints.

We used fivetran for a few years and got off it last November.  

If you have the skill set to develop and support your own integration framework (Python in our case) I highly recommend it. It is much cheaper, you have full visibility into your data, you don’t get locked into anyone’s architecture, you can troubleshoot issues very quickly, and you can validate the accuracy of the data you are receiving.

For reference we are supporting over 700 objects with only one headcount.  If you build out a strong well thought out foundation you don’t need a ton of people.",2023-03-21 09:13:30
107rt91,Azure Synapse Analytics is absolute trash for anything bigger than a few extractions,"I have tried to work with this platform. I have given it every opportunity I can, but never have I seen so much trouble from a tool or framework I had to work with. I started the new year with a new years resolution: to write down every problem as I encounter them. I don't know if I should continue with it, since I might fill up my e-reader before the year is over.

In no particular order:

1. Synapse expressions have no support for the case() function. Have fun chaining if else's!
2. Not all activities have retries. What do you mean you want to put a retry on a pipeline? That's crazy talk!
3. SQL scripts are saved as JSON. Not even JSON5. Fucking JSON. I hope you like diff checking one fucking long ass line of SQL!
4. Browser IDE... Whoever thought this up deserves fish hooks up their ass. You can't even save properly in the stupid thing, because every single thing is counted as a commit. And before some asshat suggests committing after the work is done: you deserve the fish hook too.
5. Pipeline variables do not include int's (and some other types for that matter) for whatever reason. Converting everything to string and back is fun isn't it?
6. Pipelines don't have output parameters, so even if I wanted to make reusable modules for missing Synapse functionality I literally can't unless I start using them as error parameters (and if you try to suggest that I WILL shank you).
7. No global parameters EVEN THOUGH DATA FACTORY HAD IT AND IT'S BEEN REQUESTED FOR ALMOST 2 YEARS.
8. 2022 and still no dark mode. This has been requested for data factory in 2016. Yes you're old and so am I.
9. The SQL editor has all the great functionalities notepad has.
10. I hope you didn't name one of your workspaces incorrectly. Oh you did? RIP, time to remake it.
11. Local timezones? You mean UTC? What do you mean + or -? You craycray!
12. Nesting loops or if tests can't be done. You need to make separate pipelines for it and before someone asks: no, just because it's a nesting it REALLY doesn't mean it belongs in a separate pipeline.
13. Speaking about nesting: if you use an if test in a foreach, you can't access the current item of the loop. Haven't you learned by now? You're using Synapse. Now eat shit.
14. Self-Hosted Integration Runtimes cannot be shared, while you can actually do that in data factory. This means you need to run three separate runtimes just to get to on-premise sources.
15. On the topic of self-hosted integration runtimes: why do you even need them at all? Why is it not possible to peer a vnet that contains synapse and be done with it?
16. What's even the use of the SQL scripts if you can't access them from pipelines. You get the script activity, but that's just another fucking place to dump your SQL in. And if you dare to suggest to use the API to trigger/request the SQL script. I will adopt a dog just to feed you to it.
17. Testing? HAHAHAHAHAHAHAHAHAHA- fuck you.
18. You want to rename a variable? Lol you little shit now you have to search for every instance to rename it. Refactoring? Just become a 100x dev yo.
19. Parameters in pipeline templates? Why would you want parameters in your pipeline templates? You talk like you want to reuse the code you wrote or something. Oh you do? ........oof.
20. Git....lab? Sounds like a dangerous cocaine facility. Hope you weren't planning to attach that to synapse! Or like literally any other option since you have to attach git to synapse in the first place.

These are the ones from THIS year, so after one week. I have had the displeasure of using it for a year now. So here's my advice after this long-winded rant: use it for a quick prototype of anything and don't use it for anything bigger than that. If you do, you have learned nothing of all the improvements that people have brought to the art that is development and you should probably touch grass instead of sucking cock on LinkedIn.",2023-01-09 22:12:14
vjkarw,"ELT of my own Strava data using the Strava API, MySQL, Python, S3, Redshift, and Airflow","Hi everyone! Long time lurker on this subreddit - I really enjoy the content and feel like I learn a lot so thank you! 

I’m a MLE (with 2 years experience) and wanted to become more familiar with some data engineering concepts so built a little personal project. I build an EtLT pipeline to ingest my Strava data from the Strava API and load it into a Redshift data warehouse. This pipeline is then run once a week using Airflow to extract any new activity data. The end goal is then to use this data warehouse to build an automatically updating dashboard in Tableau and also to trigger automatic re-training of my Strava Kudos Prediction model.

The GitHub repo can be found here: https://github.com/jackmleitch/StravaDataPipline
A corresponding blog post can also be found here: https://jackmleitch.com/blog/Strava-Data-Pipeline

I was wondering if anyone had any thoughts on it, and was looking for some general advice on what to build/look at next! 

Some things of my further considerations/thoughts are: 

- Improve Airflow with Docker: I could have used the docker image of Airflow to run the pipeline in a Docker container which would've made things more robust. This would also make deploying the pipeline at scale much easier!

- Implement more validation tests: For a real production pipeline, I would implement more validation tests all through the pipeline. I could, for example, have used an open-source tool like Great Expectations.

- Simplify the process: The pipeline could probably be run in a much simpler way. An alternative could be to use Cron for orchestration and PostgreSQL or SQLite for storage. Also could use something more simple like Prefect instead of Airflow! 

- Data streaming: To keep the Dashboard consistently up to date we could benefit from something like Kafka.

- Automatically build out cloud infra with something like Terraform.

- Use something like dbt to manage data transformation dependencies etc.

Any advice/criticism very much welcome, thanks in advance :)",2022-06-24 09:21:03
gpie3f,"Couldn't find a good comprehensive article on setting up Airflow 6 months ago. I wrote one here: a setup using docker-compose, and included instructions on setting up PyCharm too! Hope you could get something out of it!",N/A,2020-05-24 03:14:23
173fj4h,Anyone Else Seeing Salaries Collapse?,"Had to leav my $130K remote job in August after 6 toxic months.  Had to take a 100% on site gig for $110K last month at a bad company to keep food on the table. Been still interviewing heavily to find something better. 

When I was casually looking around late last year, jobs were around $140K - $160K at my experience level, but I couldn’t land anything. 

 Even though I make $110K now, I tell recruiters I make $150K in my new role. Most refute and say that their max budget is $120Kish and full on site. 

This is ~20% lower pay than what was being thrown around late last year, and on site. Anyone experiencing similiar?

I’m also might just be in a bad spot since I had to take this new gig, and people see my 2 short tenures in a row as a red flag. Advice?",2023-10-09 01:21:20
1497ngt,I missed you guys,That is all,2023-06-14 13:16:53
ug9r0w,The best SQL question you have been asked in a DE/DS interview?,"I have my interviews coming up, i would really appreciate if you could provide me with your favourite/ most interesting SQL questions you have encountered so far in interviews.

P.S - I think many of us will have this question
Thanks in Advance 🙂",2022-05-01 21:22:30
euuelg,Trying to architect a new dataset be like...,N/A,2020-01-27 21:05:20
150mfrd,Why do data engineers have so much to learn?,"I am a student preparing to get a job as a data engineer.

1. linux, Python, SQL, JAVA or Scala
2. Cloud Knowledge
3. Docker, Kubernetes
4. Server security + data security/quality
5. Database (""Cassandra"", ""Mongo"", ""Mysql"", ""postgres"", ""redis"")
6. ELK or (fluentd, Opensearch)
7. Kafka
8. Spark Stream or Flink
9. Spark or Trino
10. table format ( Iceberg/deltalake/ Hudi)
11. snowflake/ Redshfit / Bigquery   
12.  OLAP Data Modeling  
13. Airflow

  
I only wrote down the essentials.  
There are many good tools like dbt, lakeFS, etcBut in addition  
Companies may still require Hadoop Echo System (Hive/HDFS/Hbase).  
They might ask us to build a dashboard with javascript or python.  
They can also ask us to create the web.  


From number 1 to 13, each one has a very difficult and difficult concept to master.What I'm really curious about is how much I need to know and how much I need to master them to apply for a company.

I'm sorry. Actually, I was whining because I got hit with reality while studying.",2023-07-15 20:40:52
13xkeov,"Orchestration: Thoughts on Dagster, Airflow and Prefect?","I’ve read the articles, looked at the websites, but want to hear from people who’ve actually done it. How do the three compare? What are the downsides of each? What’s your thought process in choosing an orchestrator anyway?",2023-06-01 15:23:31
11mtgx0,What dataset would you pay good money to get your hands on?,"Just a lighter post for a change. 

I'll go first, I'd sell my car for an api access to the myFitnessPal data.

So many interesting findings out of it. What do people eat? Per country? Age group? How does it relate to their fitness goal? Such a gold mine.",2023-03-09 14:22:46
xwfkd6,"me irl: ""I've been using SQL for fifteen years! How hard could python be?""",N/A,2022-10-05 16:42:09
r2853m,Why is learning data engineering so opaque,"I am a full stack developer trying to learn more about data engineering, but so far everything is so damn opaque. I know front end has its own messes, but at least at this point everyone is unified under one programming language, a couple open source frameworks, and a couple open source package managers. It’s not hard to find tutorials frontend or backend that have you developing locally in minutes with commonly accepted tools on sound examples.

Meanwhile, under Azure there are so many services that seem like they should do the same thing. Synapse and Databricks. Data Explorer and Analysis Services. Data Factory and HDInsight. Iot hub and stream analytics. And learning any one of them is an annoying exercise in setting up an account and learning a user interface that will be swapped out in a couple years. There are no definitive starting places it feels like.

Where should I start? I know Python/SQL well and I’ve read Kimball and Designing Data Intensive Applications and want to start applying this stuff but can’t even begin to know what tech to choose.",2021-11-25 22:26:52
1bly2h0,Feel like an absolute loser,"Hey, I live in Canada and I’m going to be 27 soon. I studied mechanical engineering and working in auto for a few years before getting a job in the tech industry as a product analyst. My role is has a analytics component to it but it’s a small team so it’s harder to learn when you’ve failed and how you can improve your queries.

I completed a data engineering bootcamp last year and I’m struggling to land a role, the market is abysmal. I’ve had 3 interviews so far and some of them I failed the technical and others I was rejected. 

I’m kinda just looking at where my life is going and it’s just embarrassing - 27 and you still don’t have your life figured out and ur basically entry level.

Idk why in posting this it’s basically just a rant.",2024-03-23 17:43:51
1axd7cy,What are your Top SQL Query Optimization tips?,"Share your favorite tips for writing better SQL, your pet peeves and best practices.",2024-02-22 18:09:24
10vnmzj,I’m organizing an AMA with Joe Reis (Co-author of fundamentals of data engineering). Feel free to send in your questions!,"Send in your questions. If you’d like to join the linkedin live, DM me for the link.

Edit: Updating the link to LinkedIn Live: https://www.linkedin.com/video/event/urn:li:ugcPost:7028827342466662400/",2023-02-07 00:49:54
ymz0oh,Can you share some of the best software engineering practices you can apply to data pipelines?,"The ones you feel are critical, but are missing, especially if the pipeline was developed by beginners.",2022-11-05 16:36:30
rekhhn,Guide to read The Data Warehouse Toolkit to save you from reading cover-cover and outdated topics.,[https://www.holistics.io/blog/how-to-read-data-warehouse-toolkit/](https://www.holistics.io/blog/how-to-read-data-warehouse-toolkit/),2021-12-12 08:13:59
1al2r0o,One Trillion Row Challenge (1TRC),"I really liked the simplicity of the [One Billion Row Challenge (1BRC)](https://github.com/gunnarmorling/1brc) that took off last month.  It was fun to see lots of people apply different tools to the same simple-yet-clear problem “How do you parse, process, and aggregate a large CSV file as quickly as possible?”For fun, my colleagues and I made a One Trillion Row Challenge (1TRC) dataset 🙂.  

Data lives on S3 in Parquet format (CSV made zero sense here) in a public bucket at s3://coiled-datasets-rp/1trc and is roughly 12 TiB uncompressed.

We (the Dask team) were able to complete the TRC query in around six minutes for around $1.10.For more information see [this blogpost](https://medium.com/coiled-hq/one-trillion-row-challenge-5bfd4c3b8aef) and [this repository](https://github.com/coiled/1trc/)",2024-02-07 13:19:59
16iaaku,Love you guys!,"This is one of the most helpful subs I have come across personally. For me, I have learnt about how to progress in my career, help for interviews and certs also.
So, thank you to everyone who keeps commenting and figuring out ways to help others with their queries.",2023-09-14 05:59:35
p4lm79,My Path to Becoming a Data Engineer in FAANG,"I hope I'm not violating any rules by posting this here, but I wanted to share a blog post that I wrote that outlines my background, work experience, and the interview process I recently went through to become a Data Engineer at Facebook. The blog was written in response to some questions/inquiries I was getting in another post, so I apologize if you're seeing it twice.

The blog can be found [here](https://tibblesnbits.com/posts/de-interview-faang), and I'm hopeful that it helps at least one person thinking about applying to a role like this. Spoiler alert: you're more qualified than you think you are!",2021-08-15 02:56:10
t2mh65,Why this subreddit dislikes the so-called Modern Data Stack?,"Everything is in the title.

I feel like a lot of posts over here are gatekeeping data engineering. For example, I have seen messages where people do not understand dbt popularity (""this is just templatized SQL""). Another example I see is Datalake (S3+Spark) >>> Data Warehouse solutions (Redshift, Snowflake, Bigquery).

Why?

For me, they all help me to:

\- Avoid spending time on maintaining brittle in-house solutions

\- Avoid spending time developing boring ingestion pipelines

\- Avoid spending time setting up complex platform (Spark)

\- Focus on impactful projects

\- Reduce the time to insights

\- Facilitate collaborations with other data functions.

The ""ETL way"" requires a lot more data engineering people and effort vs the Modern Data Stack that can be nearly created by a data analyst. This is a big win for the company.

This is not a troll. I feel like I got hyped by the Modern Data Stack but for now, I only see wins. I'm interested in other perspectives and where the Spark and co. architecture would fit.",2022-02-27 11:41:16
1750zdx,Is Python our fate?,"Is there any of you who love data engineering but feels frustrated to be literally forced to use Python for everything while you'd prefer to use a proper statistically typed language like Scala, Java or Go?

I currently do most of the services in Java. I did some Scala before. 
We also use a bit of Go and Python mainly for Airflow DAGs. 

Python is nice dynamic language. I have nothing against it. 
I see people adding types hints, static checkers like MyPy, etc... 
We're turning Python into Typescript basically. And why not? That's one way to go to achieve a better type safety. 
But ...can we do ourselves a favor and use a proper statically typed language? 😂

Perhaps we should develop better data ecosystems in other languages as well. 
Just like backend people have been doing. 

I know this post will get some hate. 

Is there any of you who wish to have more variety in the data engineering job market or you're all fully satisfied working with Python for everything?

Have a good day :)",2023-10-11 00:37:34
167x47m,Data Engineers overcomplicate things,"As a DE with a few years under my belt, I am starting to believe that with the glutt of tools available, the industry is now over-saturated to the point that we are making it un-needlessly overcomplicated for ourselves. Trying to automate everything and get a one size fits all solution for data that is anything but. What is wrong with having one tool for one thing, and one for another? Why have an airflow instance that calls 150 different dependencies? Make 10 that call 15. Getting bored of the whole ""my airflow instance launches 300 notebooks and can't find the root cause"" posts. 

Let's get back to basics, simplify data, write good documentation and spend time managing 30 DPL's that work rather than unpicking one fucking giant one that never does.",2023-09-02 08:35:48
10rudcp,How do you handle increasing stress?,"I'm a junior DE working with a small team. Recently I was shadowing a senior DE who abruptly quit. I've been given their entire work load and feel completely overwhelmed. I also found out from my manager that the information the senior DE was giving me was wrong, to the point where my manager said he thinks they were sabotaging me but doesn't know why they would do that. The senior DE also deleted all of their data/workflows/processes and code.

So now were set back in some instances nearly two years and I'm working 14-16 hour days trying to rebuild things that are completely out of my area of knowledge and at the same time I'm getting pressure from different stakeholders to deliver data and products that I haven't even had enough time to rebuild yet or even learn about.

I hate to sound like a cry baby but I feel totally overwhelmed and like a duck drowning.

My manager is trying to intercept as many stakeholders as he can to give me time while nudging me along.

How do you all handle it? Any tools or tips?",2023-02-02 16:44:58
xw7vzv,Apache Iceberg Reduced Our Amazon S3 Cost by 90%,N/A,2022-10-05 11:19:43
vywuj2,Requirements Gathering is terrible. What is your best method.," Ok I’m just going to say it requirements gathering is terrible and you have to pry every bit of useful information. No this is not one of the hundreds of sales posts on this sub but a DE that has spent years fighting with users over requirements. I just crossed a decade, been in multiple lead/architect/ manager roles but still requirement are the bane of my work life all these years. 

Either I don’t ask enough questions and don’t spend enough time with the users which causes rework OR I I go overboard and they hate me for having to specify every minute detail. I have never any many attempts been able to get the requirements perfect and the users happy. What do you do? What are your best practices.",2022-07-14 14:12:01
uzr5ks,The inflated world of data: have you actually seen a business decision taken based on your analytics work?," Let me clarify a bit. I got experience as a Data Analyst and a Data Engineer, built ETL pipelines, wrote loads of SQL scripts, built a small ETL platform in Python, worked with all sorts of data tools and did more than healthy amount of data modeling.

All for the sake of analytics / machine learning.

Yet I have never ever seen or heard about the results myself. With my colleagues we created dashboards in Tableau, no idea how it got used. I built pipelines and dwh-s, no idea if actual actionable insight was ever made out of these beyond after I built it.

* How often does the moment when a decision maker looks at a visualization and says, ""aha! We gonna do this from now on"" exist?
* How often does an actual ML product makes it into active live prod state bringing in the money or saving on the expenses?
* How often do orgs build a DWH and then just ignore it, because they realize they don't know what to do with it?

Managements love to dream big and depending on managers and ideas, I can absolutely love it, because it's innovative and smart **or** I can hate it, because it's an abstract bullshit. I experienced both. But how often do we get to make these ideas into something tangible value? Do we, as data professionals, really provide the value business are hoping for?

Sometimes i'm afraid that the data world is significantly more inflated, than anyone can guess, but I hope I'm wrong, because I really like my chosen profession. What do you think? If you got a story to share for both pro or con, I would be really interested.",2022-05-28 16:44:44
uet6ty,Got a job!,"I wanted to say thanks - I'm mostly a lurker but I have been paying attention, especially to resume and interview advice and have just accepted a job as a Data Engineer (officially: Senior Software Engineer) with a 36% raise (to $140k from $103k) from my current Data Analyst role. I'm super stoked!

Sounds like they mostly use Python for ETL and aren't in the cloud so its a pretty simple stack for now...",2022-04-29 19:11:36
tnejkq,Really Detailed Company Interview Guide for DE and Other Data Jobs,N/A,2022-03-25 05:01:53
t0jn12,"So now dbt is worth $4.2b! Yes, that's a ""b"" for billion.","[https://blog.getdbt.com/next-layer-of-the-modern-data-stack/](https://blog.getdbt.com/next-layer-of-the-modern-data-stack/)

Seems all you need is 1800 customers (each is worth $2.3M), 25,000 humans on Slack and to call people ""humans"" a lot.

Is this not a supreme example of VC hubris?",2022-02-24 19:48:45
15h6qw5,"Just had a technical interview, got roasted on streaming, distributed computing and k8s 😬","So I just got out of a technical interview with the tech advisor of a small company I recently applied for, and while the first half went well, the 2nd I'm afraid not so much.

I was asked several questions about the aforementioned technologies, which I ""know"" only superficially, but I never really had to deal with them first hand.

I would like to deepen my knowledge about them, but admittedly at my current company we don't really have a use case for them.

So I'm asking you, what resources would you recommend to learn about them on my own?

Any contribution is more than welcome :)",2023-08-03 15:05:11
158sqwa,What's the best strategy to merge 5500 excel files?,"I'm working with a client that has about 5500 excel files stored on a shared drive, and I need to merge them into a single csv file. 

The files have common format, so I wrote a simple python script to loop through the drive, load each file into a dataframe, standardize column headers, and then union to an output dataframe.

Some initial testing shows that it takes an average of 40 seconds to process each file, which means it would take about 60 hours to do everything.

Is there a faster way to do this?

Edit:
Thanks for all the advice. I switched to polars and it ran dramatically faster. I got the total time down to about 10 hours and ran it overnight.

Answering a couple questions that people brought up:

* It took 40 seconds to go through each file because all files were in xlsm format, and it seems like pandas is just slow to read those. There are a ton of posts online about this. The average rowcount per file was also about 60k
* All files had the same content, but did not have standardized column headers or sheet names. I needed to rename the columns using a mapping template before unioning them.
* There was a lot of good feedback about breaking up the script into more discrete steps (copy all files locally, convert to csv, cleanup/transformations, union, db load). This is great feedback and I wish I had thought of this when I started. I'm still learning and trying to break the bad habit of writing a giant monoscript.
* It was important to improve the speed for two reasons: the business wanted to go through a couple iterations (grabbing different field/sheet/file) combinations, and it wasn't practical to wait 60 hours between iterations. There was also a very expensive issue caused by having a giant shitpile of excel files that needed to be fixed ASAP.",2023-07-25 00:49:22
shabud,DBT: Interactive search and selection of dbt models directly in your terminal,N/A,2022-01-31 19:51:42
rlmhvx,"Facebook's reputation is so bad, the company must pay even more now to hire and retain talent. Some are calling it a 'brand tax' as tech workers fear a 'black mark' on their careers.",N/A,2021-12-21 19:45:15
o1gfgc,"How Taco Bell Updates Over 7,000 Restaurant Menus Daily",N/A,2021-06-16 21:45:35
jym5v1,I'm Officially a Data Engineer!!!,"I recently just got the news for promotion from an Application Developer to a Data Engineer, any advice you guys could give me. My new role starts in about two weeks.",2020-11-22 00:40:42
1b427r1,150$ Coupon for Airflow certification ,"**\[ Expired !!!! \]**



Hey guys just wanted to share that you can grab 150$ coupon and take the airflow fundamentals certification for free

Use coupon ""m-fundamentals-free-cert""

Happy weekend guys !",2024-03-01 18:54:25
uwsm9g,Am I overdressed? Met engineers from our parent company and they were all in heavy metal t-shirts on Zoo.,"I just wear a collared shirt with a sweater or something, like old man.  These guys were older than me and dressed awesome. 

So thought this would be a fun morning post. 

For all I know, this could be the norm and I'm just naive.",2022-05-24 14:51:55
1ajq0td,Just got a LC Hard in an interview,"Not necessarily a complaint post.

Companies are obviously allowed to interview however they want and ask whatever they want. I’m a senior level DE and my background was perfect for what they wanted. 

For context, I got asked LC 84 Largest Rectangle in a Histogram. I’ll admit my LC knowledge is not great. I’ve been working on it but this one is beyond where I’m at right now. But I do think it’s a little funny that this particular question was asked. 

Leetcodes like 84 really make me question my intelligence sometimes. I could’ve looked at that problem for 3 hours and I might not have even been able to brute force it. Even the stack answer doesn’t make sense after seeing it, let alone the dynamic programming solutions.",2024-02-05 20:03:31
18l9aak,What are they looking for with title data science full stack engineer 😂,"How can someone with 2 years of experience with knowledge of frontend ,backend, data science, data engineering . 
And with a salary of fresher 😂",2023-12-18 13:56:29
10uu1j4,Best books or material to learn the basics of data engineering.,"Hi everyone, I’m currently a senior data analyst and I’m looking to make the move to the DE side. My day to day deals with SQL, Power BI and some python for the manipulation of datasets. Started being involved with some DE projects and it seemed very interesting. Can you guys recommend the best books or material for me to learn the basics of DE?",2023-02-06 01:38:14
wlyhrs,What can I do at home to become a better data engineer?,"I feel a sense of imposter syndrome. I have a title “Data Engineer” but most of what I do is sql to pull data from an existing data warehouse and tableau to visualize and make dashboards. I feel more like an analyst. Sometimes I use Python to move data from excel files into the dwh. 

I don’t want my skill set to stagnate. What kind of work or projects would be a good suggestion to keep up to date with the field of data engineering today? What kinds of tools or technologies can I use from home without having access to terabytes of data or a bank of servers. Does it make sense to get cloud certified?",2022-08-11 18:27:45
p6wvzm,What’s An OLAP Cube,N/A,2021-08-18 18:17:26
hi0tme,100% OFF Udemy Databricks & Apache Spark 3.0.0 Course,"Hi Data enthusiasts,

If you are new to **Databricks** and **Apache Spark** and want to learn it step by step, then I have a brand-new course on Udemy for you.

[The course](https://www.udemy.com/course/databricks-fundamentals-apache-spark-core/?couponCode=B0F1F71238845CB68C4C) covers all you need to know to get started with Apache Spark and Databricks

[ENROLL NOW FOR FREE](https://www.udemy.com/course/databricks-fundamentals-apache-spark-core/?couponCode=B0F1F71238845CB68C4C)",2020-06-29 14:27:07
1bdzrkh,Data Engineer vs Data Analyst Salary,Which profession would earn you most money in the long run? I think data analyst salaries usually don’t surpass $200k while DE can make $300k and more. What has been your experience or what have you seen salary wise for DE and DA?,2024-03-13 19:04:40
1ajx26t,How come so many people seem to think DE is incredibly easy? Is my perspective skewed due to my experiences?,"I've read a lot of people in this and other tech subs saying that DE is one of the easiest subsets of software engineering, some people saying it's just one step harder than regular frontend web design.

It's not that I want to pat my own back, but I've actually always thought that DE is an incredibly complex subset of SE. Even before I was a DE.

But then I compare the work these people claim to do with what the companies I've been at do. People who say those things seem to be working exclusively with ""pandas scripts that are done running in 1 minute"", like their business needs are just sending a csv with 3 columns to some sales people.

Hell, I even saw a guy saying that setting up a script to send 35000 daily emails with a csv file was a better alternative to setting a table with a consumer view to feed a dashboard, and it had a lot of upvotes, and it really made me wonder what did they think DE was?

At the companies I've worked at, the work has always been super complex, extremely heavy duty pipelines needing to process hundreds of thousands of events per second, designing scalable architecture, processes for data governance, working on external services integrations and constant feature requests. Yet it seems most people here only describe those pd.read_csv cases?

Am I out of touch or is it the kids who are wrong?",2024-02-06 00:57:50
15b85up,Who has worked with both Snowflake and Databricks and what do you enjoy/dislike about each?,"As someone who works in Snowflake day to day but has had little to no exposure to Databricks I'm curious to know from those who have worked with both, which do you prefer and what do you like/dislike about each?   
Yes, I know it's not exactly an apples to apples comparison but no one can deny these two companies are trying to compete with each other in the data marketplace.   
This isn't meant to be a comparison of which you think is necessarily *BETTER*... but more so which do you prefer working with, what have you enjoyed or disliked about either/both. Honestly just curious to hear opinions.",2023-07-27 17:37:53
13y5aks,What are some books that had an impact on your career?,"Doesn’t necessarily have to be technical. But things that helped you with productivity, communication, or business. I feel like my technical skills are growing but my business acumen, time management, and communication skills are falling behind. 

I want to ensure I have a long career.",2023-06-02 06:47:14
13ltiiu,What are the most advanced DE frameworks skills that DE employers value the most?,"This question is part of my upskilling strategy so any advice is appreciated.
I want to hear about technologies that not anyone can learn and master and is really sought after and can make me a standout",2023-05-19 12:03:43
10jjsfp,"Another data project, this time with Python, Go, (some SQL), Docker, Google Cloud Services, Streamlit, and GitHub Actions","This is my second data project. I wanted to build an automated dashboard that refreshed daily with data/statistics from the current season of the Premier League. After a couple of months of building, it's now fully automated.

I used Python to extract data from [API-FOOTBALL](https://rapidapi.com/api-sports/api/API-FOOTBALL) which is hosted on RapidAPI (very easy to work with), clean up the data and build dataframes, then load in BigQuery.

The API didn't have data on stadium locations (lat and lon coordinates) so I took the opportunity to build one with Go and Gin. This API endpoint is hosted on [Cloud Run](https://cloud.google.com/run/docs/overview/what-is-cloud-run). I used [this guide](https://go.dev/doc/tutorial/web-service-gin) to build it.

All of the Python files are in a Docker container which is hosted on [Artifact Registry](https://cloud.google.com/artifact-registry).

The infrastructure takes places on Google Cloud. I use [Cloud Scheduler](https://cloud.google.com/scheduler) to trigger the execution of a Cloud Run Job which in turn runs `main.py` which runs the classes from the other Python files. (a Job is different than a Service. Jobs are still in preview). The Job uses the latest Docker digest (image) that is in Artifact Registry.

I was going to stop the project there but decided that learning/implementing CI/CD would only benefit the project and myself so I use [GitHub Actions](https://docs.github.com/en/actions) to build a new Docker image, upload it to Artifact Registry, then deploy to Cloud Run as a Job when a commit is made to the `main` branch.

One caveat with the workflow is that it only supports deploying as a Service which didn't work for this project. Luckily, I found this [pull request](https://github.com/google-github-actions/deploy-cloudrun/pull/422) where a user modified the code to allow deployment as a Job. This was a godsend and was the final piece of the puzzle.

Here is the [Streamlit dashboard](https://premierleague.streamlit.app/). It’s not great but will continue to improve it now that the backbone is in place.

Here is the [GitHub repo](https://github.com/digitalghost-dev/football-data-pipeline).

Here is a more [detailed document](https://storage.googleapis.com/website-storage-bucket/docs/football-data-pipeline-doc.html) on what's needed to build it.

Flowchart:

(Sorry if it's a mess. It's the best design I could think of.

&#x200B;

[Flowchart](https://preview.redd.it/hfvokmmiy0ea1.png?width=2711&format=png&auto=webp&s=5eabbc6cdaa400e59a01b71881c37296ac6356f2)",2023-01-23 18:36:37
npxcqc,Quarterly Salary Discussion,"This is a recurring thread that happens quarterly and was created to help increase transparency around salary and compensation for Data Engineering. Please comment below and include the following:

1. Current title

2. Years of experience (YOE)

3. Location

4. Base salary & currency (dollars, euro, pesos, etc.)

5. Bonuses/Equity (optional)

6. Industry (optional)",2021-06-01 16:00:17
16076go,"If anyone is wondering what it's like working for a garbage company, then read this","I'm a seasoned data professional That has worked at mainly big companies throughout my career, so all of my experiences lie primarily in Fortune 500 firms. Most of those experiences have been generally very positive. My latest company treated me pretty fairly, up until about 2 months ago when it became very clear what they are really like.

**The reason I'm sharing this is not to rant. A lot of people seem to end up in a cruddy company and think: ""No one else ever experiences this, do they? I'm cursed!"" I'm here to tell you it's not just you. This DOES happen, and you need to look out for red flags.**


&#x200B;

&#x200B;

\- Mandatory social outings and extracurricular ""fun"" activities. For example, we were forced to go out to a park during the week and play soccer against another team in our department. Our manager showed up 40 minutes late and didn't even play. Just milled around, cheered our team on, and we lost.  Also had goofy t-shirts we were required to wear that said ""the eliminators"" or something like that

&#x200B;

&#x200B;

\- Tech stack extremely out of date and organization very siled. Some teams were very lucky to have Google BigQuery, and even though I had access to it, was often told I'm only allowed to use and create data sources in Microsoft Access, have to work exclusively out of Excel, have to be very very adept at using SQL, but I'm not allowed to create data tables in BigQuery, I have to do everything in legacy Microsoft Access because that's what the team is using and has used for many years, they're not able to or ready to transition into more modern data sources. We have Tableau, but we prefer to make everything in Excel. For example, using VBA to create tables repeatedly that could easily just be done in Tableau. Reinventing the entire wheel in Excel is absolutely insane

&#x200B;

&#x200B;

\- Manager was extremely lazy, unknowledgeable, unhelpful, and hardly ever did their job. This manager of course was included in the layoff, rightfully so. But the fact that they worked at the company for about 5 years is astounding to me. They would often schedule meetings and not even show up on time, arrive 20 minutes late, or they are driving in their car you can hear their turn signal, every single time, they have an excuse for why they are not doing their job during business hours. While you are sitting squarely at home being trustworthy independable, they are not doing their job. And their leaders were completely unaware of this for years? But they make it really hard for you to get promoted....

&#x200B;

&#x200B;

\- No in-role promotions. This was the first time I've ever heard of this in my life. I have never heard of a company that says you can't be promoted in the role that you are in. If you are an analyst, you can't possibly become a senior analyst. That's not possible. If you are senior analyst, you can't become the manager. A new entire role with different responsibilities has to be created by HR and you have to apply for it, compete against people throughout the company who probably aren't even in your department, you have like a 10% chance of getting hired into it, but likely you won't. So you're expected to stay in your job as long as possible even though they tell you after a year you can apply to other positions in the company, they frown upon that because they don't want turnover

&#x200B;

\- No advancement. I was working as data analyst, AND data engineer. Creating extracts, automatic table updates, data warehouses, BI stuff. Told I can mentor with other DEs, but I can never get that job because of glass ceiling bullsh\*t. I have to leave company, work as a ""real DE"", then re-apply. WTF? 

&#x200B;

\- Almost no yearly increase in salary. I was told that I was lucky, because as a hard worker, this year I was getting a 1.5% increase which is more than a lot of people were getting

&#x200B;

&#x200B;

\- No employee discount of any kind for the products that our company sells. Anytime this was brought up, the reason was that we get a bonus on top of that. Rivaling companies give discounts for their products, hours doesn't. So we have a 0% discount, and a bonus. But they sell it to us like we are so lucky to get this bonus, even though every single company offers a bonus

&#x200B;

&#x200B;

\- Laid off completely at random by our director, who read a message off a script in a monotone voice. Was told that I cannot apply to other positions as internal candidate, I would have to use the external career site and apply as if I am not an employee anymore, so in other words, follow the external applicant process. \*\*They also laid off another person on my team who was pregnant and about to deliver a new child\*\*. I considered myself very lucky. I found it extremely unprofessional, and downright evil that they would lay this person off Right before They are set to deliver a child. What kind of company could be so evil as to do that? There were other people on our team that were less qualified, and barely understood how to use Excel, and they chose someone who is extremely vulnerable and laid them off like that. Pretty crazy.... 

&#x200B;

&#x200B;

&#x200B;

\- Put me through a very rigorous application process like I am some random dude off the street. Admittedly, this is kind of normal I suppose, because they want to make sure they are making the right hiring decision. But some of the things I had to do and hoops I had to jump through were borderline insane. Take a full blown Excel test, take an SQL coding assignment, which is so weird because people in my previous department spoke to my skills and abilities. I was considered an expert in SQL, Python, Excel. So it was no mystery that I was extremely well versed in all of these things. Yet I had to do it anyway. 

&#x200B;

&#x200B;

\- I was working remote previously, but this one is fully in office 5 days a week. No possibility to be remote. So now, I have to go from being fully remote to fully in office, costing me time, and resources, 10 hours a week in commuting back and forth completely erased from my life

&#x200B;

&#x200B;

\- After I provided my start date for the job that would be included on my offer letter, was contacted by HR and asked to start immediately, and gave me an extremely hard time about a trip that I had already planned for next week, flights, hotels, everything booked and paid for unable to be moved around. Their solution? I could take unpaid vacation to take the trip. What is the real reason you might ask that they have pushed my start date up so I have literally 3 days after getting the offer letter to start the job? \*\*Because they would have to pay me out on my severance and then bring me back as a brand new employee.\*\* 

&#x200B;

\- Overall lack of respect for their employees. Lay me off completely at random, put me through external hiring process like I'm a nobody even though I moved here for this company and job, then push my start date further without any care of consideration about what I have going on in my personal life

&#x200B;

&#x200B;

\- Company leaders are often in the news for a very negative reasons. For example, contributing to political action committees on behalf of the company for legislation that Is negatively targeting people of a certain ethnicity, extremely pro-conservative far right ideology throughout the entire company, and contributes financially to those sorts of political organizations. I personally am not going to voice my political opinions, but I'm just going to tell you right now, any company that contributes to political action committees with company funds, or the owners are very active and pushy and do that themselves is a really big red flag

&#x200B;

&#x200B;

&#x200B;

&#x200B;

I honestly feel my skin crawl and just feel so wronged thinking back about the last 6 months with his company, even though the first year and a half with my team was actually generally pretty great, it just kind of traumatized me seeing how quick they were to throw people out of the company and then treat them like nothing, literal dirt, and then try and get them back in the company. Zero bargaining power or respect for employees, 100% of the respect for their own company",2023-08-24 16:45:05
11p2dqg,How good is Databricks?,"I have not really used it, company is currently doing a POC and thinking of adopting it.

I am looking to see how good it is and whats your experience in general if you have used?

What are some major features that you use?

Also, if you have migrated from company owned data platform and data lake infra, how challenging was the migration?


Looking for your experience.

Thanks",2023-03-12 02:13:05
10pqspk,Weekend Data Engineering Project-Building Spotify pipeline using Python and Airflow. Est.Time:[4–7 Hours],"This is my second data project. Creating an Extract Transform Load pipeline using python and automating with airflow.

# Problem Statement:

We need to use Spotify’s API to read the data and perform some basic transformations and Data Quality checks finally will load the retrieved data to PostgreSQL DB and then automate the entire process through airflow. **Est.Time:**\[4–7 Hours\]

# Tech Stack / Skill used:

1. Python
2. API’s
3. Docker
4. Airflow
5. PostgreSQL

# Learning Outcomes:

1. Understand how to interact with API to retrieve data
2. Handling Dataframe in pandas
3. Setting up Airflow and PostgreSQL through Docker-Compose.
4. Learning to Create DAGs in Airflow

Here is the [GitHub repo](https://github.com/sidharth1805/Spotify_etl).

Here is a blog where I have documented my project [Blog](https://medium.com/p/432dd8e4ffa3)

[Design Diagram](https://preview.redd.it/a6kh9au6nbfa1.png?width=2283&format=png&auto=webp&s=6296e53c9df7150048d40e89b5d9b6399b809628)

&#x200B;

[Tree View of Airflow DAG](https://preview.redd.it/7gqn7up8pbfa1.png?width=635&format=png&auto=webp&s=2870c95aa39fbd81b5b425093eb54d5ef5d678e3)",2023-01-31 06:26:46
z7o6cp,Meta fined $276 million over Facebook data leak involving more than 533 million users,N/A,2022-11-29 08:40:39
ol4l9j,Anyone else having trouble finding data engineers?,"Data engineering is such a unicorn skillset, and good ones are hard to find. We've been looking for a second data engineer for months. Most applicants come from traditional, enterprise backgrounds. They've never touched Docker. Never really used the command line. Don't really know any cloud tools. Don't think about testing and monitoring. It's really tough to find someone.

I feel like I can teach those things, but even then it's hard to find someone with the base skillset.

 

Anyone else having the same problem?

 

Those who have found good DEs, how did you find good candidates?

[EDIT]

Wow, did not expect this much discussion. Thanks everyone for contributing. Shared this with my team and we are learning a lot. Here are a few general thoughts to common questions and comments.
 
We cannot post jobs in this sub, but I'm happy to give a link in chat.

Will you teach me? I read this a few times. The most important skill for data engineers is the ability to learn new tools and processes. Everyone, jr. or sr., will need to learn a lot when taking a new DE job. That being said, most companies will require some base skills where you can be productive quickly.

I've never used X tool: I had little experience when I was hired with the tools I use most today (see above). My point in listing a few different tools wasn't to say everyone should know these specific tools. It was more about finding people who love technology and learning new things because they are interesting. Not just because they were assigned to learn. The fact that you are reading this on Reddit is a good sign that you want to learn.

DEs are difficult to find: I think we all agree on this. To get one, the company needs to offer great pay and benefits. A good working environment. Flexibility, time off, remote.... Unless you are a well-known company, that's not enough. You need to go out and find DEs. Build relationships, talk to people. DEs are not going to just walk through your door.

My best advice for companies is to start very early, long before you need to hire. Get on LinkedIn groups. Talk to people on Reddit. Have a list of people you would want to hire before you need it.

Will you teach me part 2: It would be great for this sub to create some sort of mentoring and training program. Something where we all contribute learning material and help each other complete real projects with real data. We should discuss this in a different thread.",2021-07-15 23:28:29
17gduz6,To my data engineers: what do you *not* like about being a data engineer?,"In contrast to my previous post, i wanted to ask you guys about the downsides of data engineering. So many people hype it up because of the salary, but whats the reality of being a data engineer? Thanks",2023-10-25 20:11:48
172ul0e,Data engineers in Europe,"Hello everyone, I was wondering if you guys would be interested in joining a Data engineers Slack channel for who are located or working for European companies. Would be interesting to meet new people, share experiences and maybe set meet ups.
Have a nice Sunday 😊

Edit: thanks for the great interest. 
Here is the invite link: https://join.slack.com/t/dataengineers-5fv9386/shared_invite/zt-24mmw5v85-Jg3EFRRgZpki1fBcgZxhBg",2023-10-08 09:06:13
yluu6c,"What are some highly recommended courses for data engineers (that are already working in this field so not ""from zero"")?","I'm looking towards expanding my knowledge in this field. Are there some courses (or even books) that are maybe not specific to one technology, but are very useful to improve in this field and you can recommend?",2022-11-04 10:15:22
16imcbc,How to approach an long SQL query with no documentation?,"The whole thing is classic, honestly. Ancient, 750 lines long SQL query written in an esoteric dialect. No documentation, of course. I need to take this thing and rewrite it for Spark, but I have a hard time even approaching it, like, getting a mental image of what goes where.

How would you go about this task? Try to create a diagram? Miro, whiteboard, pen and paper?

Edit: thank you guys for the advice, this community is absolutely awesome!",2023-09-14 16:06:46
12v2lcx,How do you feel about the return to SQL?,"Over the years people have started realising you don't need a distributed framework if you're not operating on that scale. SQL-first tooling such as DBT and others have also improved SQL-based workflows.

However as much as I like SQL before I start a project I always reflect on whether or not it's a good fit. Yes you can do everything with SQL but *should* you? There's times where queries are so far removed from intentions which is a no-go in most other places in software. Sometimes imperative paradigms are a better fit. 

Do you go for Python in these cases or does your shop stick to SQL for all tabular data? What are your opinions?",2023-04-22 11:01:21
xhe2bg,What we learned after I deleted the main production database by mistake,N/A,2022-09-18 11:21:09
xbh053,Experts found 10 malicious packages on PyPI used to steal developers’ data,N/A,2022-09-11 12:13:52
w3mvbc,How to stay up to date with latest advances in data engineering?,"Hi all,

In the same way we use r/dataengineering to stay immersed in any news and discussions on data engineering, is there any other sources of information that you know of that we could use to stay up to date with perhaps new advances in tools and software, or some sort of blog or community pages in which best practices, tips, examples of projects/code etc. can be shared and displayed? In the past 3 months I've introduced myself to programming, computer science, data engineering etc. through youtube, software documentation etc., but I'm looking to be involved in a way that I can find information that I didn't know I was looking for/just become immersed in DE so I can be the best DE I can be.

Appreciate any response, hope the question made sense.",2022-07-20 13:51:22
slolx6,What's your data engineering stack at your company?,"I wanted to know if there's a better stack than what we use at our company?

**Sources:** postgresql, 3rd party tools  
**Extraction:** Python scripts for API calls w/ 3rd party or other external sources; internal databases we schedule sql scripts   
**Loading/Transformation:** sql scripts (dbt thoughts?)  
**Storage:** Redshift (any thoughts on latency or other warehouses?)  
**Application:** Looker (migrated from mode)",2022-02-06 03:19:26
rg5rjp,Just got fired.,"Entry level. Offshore. Remote. 25 bucks an hour. Contract.

I signed an NDA. I am not mentioning the name of the company, the employer or any confidential or any specific information. If the client comes across this and  wants to have a fight about this he has my email.

So, I joined this 100% remote startup. They wanted to hire a fullstack developer but I begged and convinced my way into a Python+data engineering role. They had me on probation for a 2 week period.

*Day 0*

During the onboarding interview I was given a ""simple"" task where I needed to generate two analytics table each time they made a request to saturate their database for their data analysis stuff.

I was told that all I needed to do was run the `main.py` and everything was ready to go. They have a few lines of code on the README for setting up the repo. They were just docker configuration commands. I said, that config is never going to be that simple and the client was visibly surprised by that response. But as we know all know configuring a repo for the first time isn’t always plug and play type of deal. 

*Day 1*

So, I get started and things start to go south. First they didn't assign me with access permissions to the cloud. I waste some time figuring that out. Then the docker file shows that I need to setup some registry stuff for the cloud database access and setting up the machine learning models. That took a while to figure that out. I also needed to setup config dotfiles for that...... 

At this moment I have spent a day but I logged around 4.5 hours. The senior dev was super helpful and very kind but he was super busy. The client comes back and gets mad as all I supposedly needed was to run the `main.py` file and that was supposed to be it. I said we were trying to set up Docker for the python version, the machine learning libraries and python packages. He says run the requirement file. 

The machine learning libraries calls the database, processes and sends to another database. I needed to create an analytics log in the middle of that. But most of the functions simultaneously called, merged and uploaded data.

*Day 2 (Weekend)*

If you use multiple python versions in a project the requirements.txt installation using pip installing isn't exactly straightforward. After 2 hours of trying to figure out why I wasn’t able to install packages among other things, I discover that they were using two versions of Python and for the requirements file they used one package that wasn't supported by the python version they recommended me. I change the version of that package and finally install all the packages.

This might sound like an easy fix but I urge you to try this out.


Day 3

I run the file it doesn't work of course. Because this entire repo heavily depends on the Docker file and docker file sets up aliases for certain API calls and actions. Without docker configured the file will never work straight. I try my best to discover my way through the jungle that is that codebase. And even still I get permission issues. I get one permission issue which was sorted. And the entire file doesn't run anyway because I wasn't granted full permission of something I couldn’t figure out that day 

I say to the client that without cloud permissions I am not getting stuff done and he gets mad. He says that, my skills aren't that strong as I have indicated in my interview.

How the heck am I supposed to react to that. I said please re-read our conversation. How am I supposed to solve this issue without having permission and configuration issues ironed out.

In the begging of the project they have provided the schema for the analytics they needed, so they insisted it should be an easy task. I said, I can provide a code but I will be working be working blind without running the file top to bottom. They said, I have all the necessary things for me to get the job done.

Based on the schema alone, I comment out the sections that doesn't work and I write a code that outputs a CSV. But the client still isn’t happy. At this moment I let the client know let me know when I should leave. They indicate it is going to happen. And they ask me what I meant by the ""re-reading the conversation"" message 

Day 4 (Today)

I intentionally didn't reply to the ""re-read conversation"" message. Yeah. An unprofessional thing to say I know, but what am I supposed to say when the client thinks I am not getting things done while simultaneously locking me out from the very place I need to get things done in. And moreover questioning my ability just  triggered me a little bit.

I comment out each sections of code to find what is going on. They are simultaneously fetching and merging data and aggregating data upon layers upon layers function in multiple modules.

Then I discover they have another cloud service that they are using that I didn't knew I even needed access for. I sent a message saying that, I have found the problem for the issue and I need access to this service.

The response I get was, ""Pack your things and go"".

---

Overall I logged about 10 hours for this job but my idle time which I didn’t log was about double that time because I was stuck on stuff and I wasn’t getting any responses.  I obviously didn't get paid. Of the two task I was asked to do, the first one took me 20 minutes to do. But the setup and configuration and code discovery took up the entire logged time.

The first day when I joined slack, I came across a few profile and all of them were deactivated. Scrolling up I saw a guy quitting under 3 hours.

I don't plan to work for another company that doesn't provide some level of mentorship. I don't want to work with someone who isn't technically competent and is not willing to guide or hear me.

I understand I am supposed to be an expert because my rate is WAYYYYYYY too high for an offshore dev. But not again am I working without pair programming, routine video discussion or helpful seniors.

I really despise freelancing.

Edit: Was emotional made a ton of typos. It is a rant there is going to be a lot of typos.",2021-12-14 11:44:58
n5msew,The State of Data Engineering in 2021,N/A,2021-05-05 18:22:49
1ajnqf4,Finally landed my first DE role,"Im a data analyst who just signed his first offer letter for an DE role. My manager told me that I won’t be a “true” data engineer until about a year in, he said the first year will be a lot of learning and that a lot of my job will be closely supervised so don’t feel like Im being micromanaged but once he feels comfortable i will be able to handle projects on my own. Is this normal?",2024-02-05 18:32:02
124ozln,Big news! LambdaConf returns Sept 16-19th and is better than ever! 🔥,"Join us in the Rockies for an unforgettable conference featuring thought-provoking talks, workshops, craft beer tasting, hiking, and immersive experiences that will change the way you think about software development. Grab your Early Bird Ticket: [https://www.eventbrite.com/e/lambda-conf-colorado-usa-in-person-only-tickets-540887036687](https://www.eventbrite.com/e/lambda-conf-colorado-usa-in-person-only-tickets-540887036687)",2023-03-28 13:35:41
zltid3,Any really good end-to-end walkthroughs?,"I've been reading the data warehouse toolkit as well as data engineering blog posts but I'm still having a hard time fleshing out a full picture of how everything integrates. Are there any really good resources that show a ""real-world"" data pipeline being built out? From modeling the data to designing the ETL/ELT pipeline to distributing the data via a data mart or dataset?  

Ideally, a guide or two that show how this would be applied on-prem as well as in the cloud would be helpful.",2022-12-14 15:12:32
pscnk0,University of Helsinki is providing Big Data Platforms Mooc (https://big-data-platforms-21.mooc.fi/),"Finnish Universities(Helsinki & Aalto) are generous enough to provide some of the best courses for **absolutely free** on [https://www.mooc.fi/en/](https://www.mooc.fi/en/) in **English.** It only costs you money if you want university credits, or else everything is free including exams and certificates. 

[**https://big-data-platforms-21.mooc.fi/**](https://big-data-platforms-21.mooc.fi/) is one of them. 

Main topics are:

* distributed computing,
* Warehouse-Scale Computers,
* fault tolerance in distributed systems,
* distributed file systems,
* distributed batch processing with the MapReduce and the Apache Spark (PySpark) computing frameworks, and
* distributed cloud-based databases.

The course material will consist of lecture materials and exercises provided by the lecturer.",2021-09-21 05:53:28
1addsj4,What distinguishes production-grade data pipelines from amateur setups?,What do amateurs usually not do well?,2024-01-28 21:34:31
161zmp3,"Follow up on my previous post! Who are some of the no-fluff, not clickbaity data influencers you like and follow?","I’m looking to clean up my LinkedIn feed and would like recommendations on who to follow. 

Even if they have 500 followers, but post thoughtful, insightful content, I’m all for it. Let’s call it underrated gems lol. 

Netflix, Uber and LinkedIn engineering blogs are my go to. 

Medium, LinkedIn follow recommendations appreciated.",2023-08-26 16:25:24
xyeqfv,"Is there a list of SQL ""patterns"" or problem types for interview questions?","With some data structures and algorithm questions there's general topics and patterns to learn to pass interviews, for example array specific problems can be solved with a 2-pointer pattern.

Does anyone know of a list of patterns or even types of problems? I've seen 1 type of problem that's common and learned it's called the Top K Elements/by Group problem.

At the moment I'm doing random Leetcode DB questions and there isn't a good structure to my approach.

Thanks for any pointers.",2022-10-08 00:29:38
rem26j,Data Engineering Jargon - Part 2,"Hi - this is the next 10.

1-10 is [here](https://www.reddit.com/r/dataengineering/comments/rdw3b3/data_engineering_jargon/?utm_source=share&utm_medium=web2x&context=3)

11-20 is below

21-30 is [here](https://www.reddit.com/r/dataengineering/comments/rfbuu8/data_engineering_jargon_part_3/)

31-40 is [here](https://www.reddit.com/r/dataengineering/comments/rg5vr0/data_engineering_jargon_part_4/)

**11. Ingestion**

Generally, the first step in a data pipeline, where data is inserted in tables in the platform.

*A pipeline where customer address data is inserted from source A.*

**12. Extract, Transform, Load (ETL)**

A 3-step process of extracting data and transforming it (by applying some kind of logic like aggregation) and loading the new information into the destination. It could be used as ELT where the destination tables transform the data instead.

*An extract of customer address data is taken from the customer relationship management tool and is then aggregated according to their cities and this new information is loaded into destination B.*

**13. Data Models**

A way of organising the data in a way that it can be understood in a real-world scenario.

*Taking a huge amount of data and logically grouping it into customer, product and location data.*

**14. Normalisation**

A method of organising the data in a granular enough format that it can be utilised for different purposes over time. Usually, this is done by normalising the data into different forms such as 1NF (normal form) or 3NF (3rd normal form) which is the most common.

*Taking customer order data and* *creating granular information model; order in one table, item ordered in another table, customer contact in another table, payment of the order in another table.* *This allows for the data to be re-used for different purposes over time.*

**15. Star schema**

The simplest way to model data into different quantitative and qualitative data is called facts and dimensions. Usually, the fact table is interpreted with the help of a dimensions table resembling a star.

*A Star schema of sales data with dimensions such as customer, product & time.*

**16. Facts**

A data warehousing term for quantitative information.

*The* *number of orders* *placed by a customer.*

**17. Dimensions**

A data warehousing term for qualitative information.

*Name of the customer* *or their* *country of residence.*

**18. Schemas**

A term for a collection of database objects. These are generally used to logically separate data within the database and apply access controls.

*Storing HR data in HR schema allows logical segregation from other data in the organisation.*

**19. SCD (slowly changing dimension) Type 1–6**

A method to deal with changes in the data over time in a data warehouse. Type 1 is when history is overwritten whereas Type 2 (most common) is when history is maintained each time a change occurs.

*When a customer changes their address; SCD Type 1 would overwrite the old address with the new one, whereas Type 2 would store both addresses to maintain history.*

**20. Business Intelligence**

A slightly out of date term for a combination of practices to derive business insights from data by predominantly using data warehousing, analytics and dashboarding.

*Creating a management dashboard to show customer demographics across the country.*

1-10 is [here](https://www.reddit.com/r/dataengineering/comments/rdw3b3/data_engineering_jargon/?utm_source=share&utm_medium=web2x&context=3)

21-30 is [here](https://www.reddit.com/r/dataengineering/comments/rfbuu8/data_engineering_jargon_part_3/)

31-40 is [here](https://www.reddit.com/r/dataengineering/comments/rg5vr0/data_engineering_jargon_part_4/)",2021-12-12 10:06:17
mqa99k,The deck we used to raise our Seed round for our open-source EL(T) platform,N/A,2021-04-13 19:59:20
1bekue2,How to recover a deleted dataset from BigQuery? (Urgent),"So I’m in analytics and one of the seniors must have been high or something because he ended up deleting the Master dataset from the project.

The dataset had over 5000 tables that were used across the board. 

Most of the teams are panicking and there is a lot of chaos. Online articles and StackOverFlow don’t help.

Is there a way to restore it because we might lose the client at this rate? 

Sample id of a table: ‘project.Master.table1’

Update: Client directly got in touch with Google, they sorted out the mess but our reputation is tarnished. Lesson for everyone reading, MANAGE YOUR ACCESSES WELL. ",2024-03-14 13:12:05
1adsbnd,What happened recently with Snowflake?,"I am on a job hunt and I noticed that companies actively looking for someone with Snowflake experience.  I am not saying Snowflake was bad or anything, just never see that big demand/need for its use.

Do you use Snowflake in your company? Did you migrated to Snowflake recently? Any huge cons that made you this migration?",2024-01-29 10:34:43
wfk8l2,"One of a brainy friend of mine told me, redditors gives the best advice. So, here I am seeking advice/suggestions/feedbacks on my Backend/Data Engineer Resume.",N/A,2022-08-03 22:35:30
nquv61,What are great books that helped you to become a better data engineer?,"As title suggests I am looking for resources that you think were a valuable read and has helped you to become a better data engineer (or software engineer). Has anyone read a book on programming philosophy (or other topic) that has changed the way you think about writing code and overall helped with your work (e.g. write cleaner code, employ better practices)?",2021-06-02 20:01:15
ln4fi7,Data-Ops-ish Design and Workflow Walkthrough,"In [another thread](https://www.reddit.com/r/dataengineering/comments/lmjhv1/ideas_to_build_a_regression_test_suite_for_data/), [u/brendersplide](https://www.reddit.com/user/brendersplide/) asked about building a regression test suite. In response to my comment, there were a few individuals who wanted to know more about the system I am currently running. ( u/isleepbad and u/htrp) It's more than what should be in a reply so I thought I would do a post for anyone else interested.

WARNING: This is a long post.

DISCLAIMER: This is, by no means, the best way to do things. We are constantly evolving and tweaking.

&#x200B;

As a little context, I run data and analytics at a start-up-ish financial company and have a very small team. In order to combat that constraint, we have done a lot to automate our workflow. We also rely on open-source as much as possible. We've tried to infuse this thought process into everything we do - from onboarding new hires to reproducing the entire infrastructure.

I'll break it down into the following sections:

**Main Sections**

1. Storage
2. ELT
3. Testing and QA
4. Infrastructure
5. Data Workflow Automation
6. Development
   1. Analytics
   2. Engineering
7. CI

**Follow-up Questions**

1. Edit 1:
   1. Code Testing
   2. Data Governance
   3. Data Versioning
   4. BI Tool
2. Edit 2:
   1. terratest
3. Edit 3:
   1. Why Looker?
   2. How long and how many people did this take?
4. Edit 4:
   1. How do you handle data that fails validations defined in dbt tests or great expectations?
   2. Once these failed records are corrected, how do you re-integrate them into the destination tables?
   3. What is the data issue turnaround time?
5. Edit 5:
   1. Positions and Personelle

&#x200B;

# Storage

Tools Used:

* [AWS S3](https://aws.amazon.com/s3/)
* [Snowflake](https://www.snowflake.com/)

The storage is quite simple. We use S3 as a landing zone from the vast majority of sources. We then pick up those files and load them into Snowflake. I call this the **Lake House** as it serves as both our Data Lake and Data Warehouse. What is really beneficial about Snowflake is how it treats 'databases'. They aren't DBs in the traditional sense but more akin to workspaces. Because of that, you can have as many as you would like. This comes in handy when you want to practices [Gitlab's concept of Infinite Data Warehouses](https://www.youtube.com/watch?v=eu623QBwakc).

&#x200B;

# ELT

Tools Used:

* [Fivetran](https://fivetran.com/)
* [dbt](https://www.getdbt.com/)
* python

We use Fivetran for third-party vendors. dbt is used in the Lake House. Once data arrives in the raw databases, dbt takes over the management. If you aren't familiar with this tool, make it your best friend. It allows you to write SQL and YAML to define your data models. It also comes with super handy features like test definition, lineage, and document generation.

Because of Snowflake's awesome ability to scale, we bring all data in its raw format and then do our transformations with dbt.

For data from source systems, we keep it easy with CRON jobs running python scripts. This is very manageable because we are essentially just replicating the source data into the S3 Landing Zones. The only transformation that should be taking place at this step is the removal of PII.

&#x200B;

# Linting, Testing, and QA

# Tools Used:

* [dbt](https://www.getdbt.com/)
* [SQLFluff](https://github.com/sqlfluff/sqlfluff)
* [pylint](https://pylint.org/)
* [black](https://pypi.org/project/black/)
* [Great Expectations](https://greatexpectations.io/)

For linting, things are pretty straightforward on the python front. We use pylint and black to enforce a standard. From a SQL point of view, [SQLFluff is a new player to the field filling a much-needed space](https://www.youtube.com/watch?v=veYB9uh0RCM).

For testing, we use Great Expectations for data coming from source and arriving in the Lake House. Once in the Lake House we leverage a couple of dbt's packages to keep the workflow standardized. Firstly is a new port of Great Expectations syntax to dbt in [dbt\_expectations](https://github.com/calogica/dbt-expectations). This allows you to bake in tests as easily as

    models:
        - name:table_a
          description: Slightly modified copy of table_123
          tests:
              - dbt_expectations.expect_table_column_count_to_equal:
                  value: 27
              - dbt_expectations.expect_table_row_count_to_equal_other_table:
                  compare_model: source('raw_schema_a','table_123')
          columns:
            - name: month_value
              description: Month number [01 -> Jan, 02 -> Feb, etc.]
              tests:
                  - dbt_expectations.expect_column_to_exist    

We also use [dbt\_meta\_testing](https://github.com/tnightengale/dbt-meta-testing). This checks all your models to ensure all tables and columns have descriptions and that all required tests are defined. This is important because we then leverage `dbt docs generate && dbt docs serve`  which auto-generates our documentation and data lineage.

&#x200B;

# Infrastructure

Tools Used:

* [Terraform](https://www.terraform.io/)

We use Terraform of Infrastructure as Code (IaC). All of our infra is terraform'd, including our Snowflake instance. Terraform offers a [Snowflake provider](https://github.com/chanzuckerberg/terraform-provider-snowflake) which allows you to create tables, users, roles, schemas, etc. Given all of our managed infrastructure resides in AWS and Snowflake, everything is Terraformed.

&#x200B;

# Data Workflow Automation

Tools Used:

* [Prefect](https://www.prefect.io/)

There is a litany of tools out there for this (Airflow, Luigi, etc.) but we settled on Prefect for its simplicity. It focuses on data orchestration and does it very well. It also operates on what they call a hybrid model. This means that, if you are using their cloud UI, all they see is meta-data. None of your actual data is processed outside of your infrastructure.

You still need to have an agent running somewhere in-house though. For that, we are using [AWS ECS](https://aws.amazon.com/ecs/). Again this is Terraformed.

Prefect comes with tons of integrations, including Great Expectations and AWS. It is written in python so it works with our choice of analytics language. And it's Jupyter notebook plugin allows you to leverage the power of [papermill](https://github.com/nteract/papermill) to orchestrate, schedule, and run parameterized notebooks. Very handy for deploying models with all the benefits that come with notebooks.

# Development

**I give all credit for this idea to the GitLab data team**

All of our actual data modeling, analysis, ml work is containerized in [Docker](https://www.docker.com/). Depending on the work you are doing you spin up the appropriate image.  This has the benefit of not only being able to jump into an environment that has everything you need to do your work but also ensures that everyone and everything is running on the same baseline.

We also take advantage of Docker's volume bind so that changes made to the directory outside of the container are immediately reflected inside the container. This is important because it allows you to use your code editor ([VSCode](https://code.visualstudio.com/) in our case) and not have to operate in something like vim.

**- Analytics -**

We have an in-house `data-image` that comes pre-loaded with the familiar cast. Pandas, scypi, Jupyter, they are all installed and ready to be used. We also built our own helper libraries that make connecting to and extracting data easier. The goal here is to reduce all friction that exists when you want to start an analysis. Get it all out of the way so you can just do your work.

**- Engineering -**

Likewise, we have `dbt_image` and `prefect-image` which act in much the same way. They are all wired up so you can focus on the actual work instead of the technology.

&#x200B;

# CI

Tools Used:

* [Github Actions](https://github.com/features/actions)

Great so we have all of these tools - what about the dev-op-y stuff?

For that, we use GitHub Actions (but Gitlab is super powerful as well and I use it for my personal projects). As you may have noticed, everything we do has been codified. That allows us to create Actions that can handle:

* All of our testing and linting by defining steps accordingly
* Updating and provisioning our infrastructure with Terraform. Need to add a user? Cut a branch, add then to the `users.yml` file with the access they need and open a Merge Request.
* Need to create a replicated environment to try something new in AWS? Easy as changing the name.

But the real power comes in the use of the Docker images mentioned earlier. Because all of our workflows are containerized, we can then use those exact containers in both CI pipelines and production deployments. I know that if it works locally it should work all the way through.

Finally, using the concept of infinite data warehouses, you can have a Github action that goes up and replicates Lake House prod into a dev database for you. You can do whatever you want and it won't harm production. And then when you are done we can tear it down.

&#x200B;

You've made it this far. I am happy to answer questions. I am sure I have forgotten something. And I am positive that this will evolve and there are better solutions waiting to be uncovered.

&#x200B;

# Edit 1:

**Code Testing**

In regards to code testing, there isn't much to it. unittest for python is as simple as it gets.

For terraform our structure looks like below:

    .
    ├── main.tf
    ├── modules
    │   ├── module1
    │   │   └── main.tf
    │   └── module2
    │       └── main.tf
    ├── prod
    │    └── main.tf
    └── staging
        ├── main.tf
        └── test.sh

I know it violates some of the structure best practices but it makes it simple. The `test.sh`  file has commands to `plan`, `apply`, and `destroy` and the staging [`main.tf`](https://main.tf) has our staging environment variables in it. So GitHub Actions can just run that script.

**Data Governance**

We haven't done much in the way of Governance. But I am more of the mindset of Data Discovery than Governance. That is the next chunk I'm likely going to be tackling.

**Data Versioning**

Going back to the concept of Infinite Warehouses. Snowflake has an awesome `clone database` feature. And so every morning before the jobs run you can clone the database and name it something like `backup_raw_source_2021-02-01.` And then its up to you how many days you want to be backed up. Easily automated to create a lifecycle.

**BI Tools**

We use Looker. I am a huge fan of their explore feature but it also fits so well with our stack. Everything boils down to LookML which, as you may have guessed, is versioned. You then have the ability to create templates for reuse. And they also have what they call Blocks which are essentially just pre-made LookML files relating to a specific source (Youtube, Google Analytics, etc).

&#x200B;

# Edit 2:

Speaking of code testing, I literally just came across a Terraforming library called [Terratest](https://github.com/gruntwork-io/terratest/). I know nothing about it yet but it looks way more robust than what I had laid out above.

&#x200B;

# Edit 3:

**Why Looker?**

Looker was chosen for a couple reasons.

1. It sits directly on top of Snowflake. Most BI tools will extract the data onto their servers (on-prem or cloud) and then run the reports there. You are essentially paying for storage twice in this case. Looker leverages Snowflake entirely and so you are only paying once.
2. It is fully version controlled. Because everything is done in LookML we have it all available in our GitHub repo
3. It has an amazing explore feature which allows you to drill down as far as you are comfortable. This was critical because it added another access point for end-users. A focus has been on providing these access points where someone can get to the data and depth that they need depending on their capability. It isn't a - send me requirements - I build a report - you ingest. We really encourage other teams to get in, build, and explore. This gets them the Level 1 and Level 2 answers that would normally consume our time and instead frees us up to do more valuable and deep work.

**How Long and How Many People Did This Take?**

It took about a year from the start to where we are. A lot of that also included going down an Airflow route for a while before pivoting to Prefect, converting from Domo to Looker, and I had a baby.

Given how small we are I was wholly responsible for the architecture design and setup while the other members of my team focused on the analytics. The design was built specifically so that they didn't really have to learn any new technology.

Having said that, they did have to learn framework (dbt, prefect, terraform) syntax but that is reduced to SQL and YAML files for them.

&#x200B;

# Edit 4:

**How do you handle data that fails validations defined in dbt tests or great expectations?**

When something fails we have notifications in a slack channel dedicated to that tool. The reason for separate channels is minor, but it means we know exactly what tool and where in the pipeline the issue is.

We then create an issue for the error and include the lineage graph from dbt (looks like below) so that we know all the downstream potential impacts.

&#x200B;

Then, to fix the issue, we cut a new branch from our repo and go through our standard workflow. You spin up your container, create a dev DB, and fix the issue. You then push to Github and open a PR for CI and review.

On my list is to automate the issue creation and include that in the slack message.

**Once these failed records are corrected, how do you re-integrate them into the destination tables?**

Once everything is corrected, we just re-run the jobs. Dbt and prefect pull from the master branch of our `pipeline` repository. So once the PR is merged, the new code is available and the job will just pick it up.

**What is the turnaround time?**

It all depends on the scope and complexity of the failure. But it can be between minutes or until the end of the day (this is the case if we have to do a source -> Lake House data backfill so that we don't impact actual production during business hours)

&#x200B;

# Edit 5:

**Positions and Personelle**

What this framework does for the team is it eliminates the need for Data Scientists, BI analysts, and Data Engineers and replaces them with [Analytics Engineers](https://blog.getdbt.com/what-is-an-analytics-engineer/). At its root, this enables the analyst to build, maintain, and enhance our pipeline

A concept of Dev Ops that I love is the idea that waste is created in the system anytime there is a hand-off. Having 'Analytics Engineers' removes all the hand-off waste I've endured in the past when managing separate functional teams.

Eventually, there will be value in havering separate functions, but given how small we are, this allows us to hit all the needs.",2021-02-19 02:35:12
1b43j0z,Are we entering a boom time for Data Engineering?,"**Bull case:** AI is taking off. There will be new tools all over the place. Companies will need to port data around to use it in new ways not possible today. Data will become a big differentiator for products driving its value up. The need for data migrations is going to increase and the opportunity to streamline data management tools will increase accordingly.

**To discuss:** Do we agree with this bull case? Are the golden days of data engineering ahead of us? What can we do to capitalize on this opportunity? If we disagree then what am I missing?",2024-03-01 19:47:19
181mou1,"A takedown of Alteryx, no-code data as a concept and the people who force it on talented data folks.",N/A,2023-11-22 23:11:59
15oc8z5,Why is dbt popular for the transformation step?,"I did not work extensively with dbt, but it always seemed less appealing than having a databricks based pyspark/spark sql workflow + some orchestrator and some internal python libs to make things smooth. Sure data models and lineage are alright with dbt, but they can get out of hand with analysts chiming in. Maybe my DWH/etl usecases were not big enough for them not to be managed via an a priori sketched out data model, decent PKs and FKs, good naming conventions and job structure.

Making things performant and cost effective just seem more straight forward in databricks.

Maybe I just did not get the vibe. dbt cloud also kinda feels like the myriad of other scammy ""modern data tools"".

Curious about other perspectives!",2023-08-11 15:39:29
152yp2h,"Free copy of ""Fundamentals of Data Engineering"" to learn DE",N/A,2023-07-18 13:36:06
zgtrnk,"Dates are hard—we can relate to that, can't we r/dataengineering?","&#x200B;

https://preview.redd.it/s8llls4you4a1.png?width=460&format=png&auto=webp&s=0e7cccdde22e2da0be5d14dff0e0f7f02d664628

I love the irony of this :D 

&#x200B;

(and probably also the meta-paradox of being a jerk by posting this thus violating the very rule I'm citing 😉 )",2022-12-09 10:38:02
wo7cd7,FAANG Interview question styles for DEs,"When I check on the web, people usually suggest LeetCode for studying interviews for FAANG companies. That means it is mainly about data structures and algorihms. Is that valid for the data engineering field?

Although it is always good to know data structures, algorithms, etc., I don't think that this is the fundamental job of a data engineer.

**TL.DR**: As a data engineer who is targeting FAANG, do I start studying LeetCode? What kind of interview questions are asked by FAANG to data engineers?",2022-08-14 14:13:07
q37zfu,Does anyone know where to find jobs for companies doing good in the world?,I’ve never been one to really care much for money and I’d much rather just do some good in the world. How do you find these places?,2021-10-07 12:27:15
pbd9gm,Recruiter reached out to me about a potential Data Engineer position. 1 hour later she sent her Robinhood affiliate link?,N/A,2021-08-25 15:13:22
nac4do,How to present your data engineering work to 300+ persons from the whole company,N/A,2021-05-12 00:33:41
16rxj7v,Is there a great book on design patterns in data engineering?,"I've read ""Fundamentals of Data Engineering"" by Reis. However, as the name says that book covers the fundamentals. There are loads of books on software engineering design patterns in general. Not for data engineering in my knowledge.

I'm looking for a great book that goes through the popular data architecture patterns end-to-end. With code samples. Googled, but didn't find anything particularly exciting. Just a few blog posts. Which books do you folks suggest as essential reading for a data engineer?",2023-09-25 16:58:49
112l0h5,"Thoughts on ""Databricks ❤️ IDEs""?",N/A,2023-02-15 00:39:26
10z1ft9,Big Data is Dead,Clickbaity title but this former Google BigQuery engineer has some really interesting things to say about why most companies do not or should not utilize big data.,2023-02-10 20:01:42
v0emaj,Background conversation in a CIA safehouse in the last Jason Bourne Movie 😆 Does the CIA pay well? Asking for a friend…,N/A,2022-05-29 16:20:15
su180u,One thing that irks me about the software field: Many companies are not willing to give a candidate a chance if they do not know their specific tech.,"As you all know there are many versions of the same types of technology, and once you know one, you can fairly quickly pick up a new tech as long as you have a good baseline knowledge. This is even true for entire languages (Java, C#, etc)

I had a company I was working recently on a role ask me if I’ve used Airflow before in a professional environment. I said no but I know Python and I’ve used pipeline management platforms like Dagster and understand the concepts of it.

They got back to me and declined me. When I asked why (to see how I can improve), they told me they looking for people with more experience with the tools they use.

I don’t get this, a lot of DE tools need to be used in an enterprise environment and the only way someone ever learns in that scenario is by getting hired into it without prior knowledge of the exact tool.

Why is it that so many companies are unwilling to provide training or take a chance on candidates who have used other variations of the same tools they use? Have the people here experienced this?",2022-02-16 17:18:18
ru62kw,Please suggest a book for Data Engineering concepts.,"I think it would be a good idea to grasp more knowledge about DE concepts, terms and data pipelines. 

I am interviewing to be a DE (I was a SDE for 5 years) and I have worked with Relational and Non-relational DBs in the past. I have knowledge of NLP and ML concepts too. 

I can prepare for the interviews through google articles but it does not give me satisfactory wisdom with DE. In interviews, I get lost when they ask me to create a data model from start to end. I need to learn more. 

 Can you please suggest a book ? If not book, then some series of articles or anything else?",2022-01-02 09:18:47
1basru9,‘Data’ is essentially a consulting field in most companies ? Thoughts?,"I have been working as a DE for about a year and a half the following is my opinion on basis for my experience and looking at mu friends’ experiences. 90% of the roles in data are usually analytics, BI, data science. Even if it is a DE role it usually falls into one of the above. These roles typically exist in orgs which are not mature with and in data and execs work on excel. If this is the case, then the ‘data’ team’s priority is making a case for itself /selling itself with its initiatives add value to the execs. In my opinion this is very close to consulting. This causes a de-prioritization of DE work which can be lack of data modeling, no focus on data infra, data quality sucks etc. This makes DE a support role and a visibility lacking role. On the other hand, orgs which are mature with data, say Netflix, few mid sized startups and maybe few companies actually have real DE roles where focus is equal on infra, data quality, analytics, DS. If I want to get into these roles, it makes it tougher as there are so few of these. Would like to know thoughts of DEs/Senior DEs here who have been thru this/navigated/transitioned into something else from DE",2024-03-09 21:29:56
18420vg,What DBT hacks do you wish you knew sooner?,As per post title. Anything from macros to project config. Keen to hear and learn.,2023-11-26 04:00:36
1792sm4,How to make these diagrams,N/A,2023-10-16 10:23:23
135470k,Which ETL/ELT tools do you think have future in data engineering space?,"I'm looking to learn a tool but confused which to learn because there are a lot of tools and most of them are very expensive. I'm reasearching on Informatica, Ab Initio, Stitich. Any new tools you suggest which you think will have a future help would be much appreciated. 

I already know Spark, Databricks, Snowflake etc this question is for my friend who wants to get into data engineering who doesn't know coding.",2023-05-01 23:01:26
12hj52u,What problems does the “modern data stack” actually solve that have not been solved already?,"Help an old guy out here! I have been working in the data warehousing / business intelligence field for the last 25 years. I have probably been exposed to every traditional technology under the sun. I am honestly having a really hard time understanding what the big deal is with the whole data lake / Serverless X / spark / airflow / metric store / etc. 

It seems to me that these technologies are geared towards use cases that either involve data volumes that most companies will never reach or reinvent stuff that already exists (how is a metric store any different than an OLAP cube with calculations?)

What is it that’s so great that’s not solved by a file system, traditional ETL and a RDBMS?",2023-04-10 13:40:12
1bq6k5q,"What's your prod, open source stack?","Looking into creating an open source ELT stack from scratch: if you have one, or have had one that worked well, what were the stack components?",2024-03-28 20:59:58
ypfb1m,How much software engineering knowledge should a data engineer know?,"I’m trying to create a curriculum for the DE path to help other programmers prepare for Data Engineering. Some SE topics are required to learn because they are generally useful when developing software. I want to hand-pick some important topics for someone who is just getting started, It’s important to only pick topics that will be useful in DE.  
If someone wants to study data engineering, how much of the following SE-related stuff do you think is required?  


* Design patterns & Principles like KISS and YAGNI: books like pragmatic programmer have a handful of these.
* Testing: Mostly unit tests.
* APIs: I assume only basic REST knowledge is enough to get the concept, and people can learn other methods like Grpc and GraphQL on the job.
* Errors & Monitoring & Tracing: Being able to log correctly, send metrics where needed and write code that can be debugged if it crashes with traces & logs.
* System Design: Keep in mind the simple techniques like caching and scaling, Plus some other examples like how you design X(Do you know any good resource on this?)
* RDBMS: I never used more than SQL and ORM knowledge myself. Please suggest which areas you think are important. Is RDBMS internals relevant? Is ACID relevant?",2022-11-08 08:14:29
ybc8ml,Anyone currently using an Operational Data Store?,N/A,2022-10-23 08:30:47
xgq4dh,What is ideal or perfect ETL/ELT pipeline in data engineering? An interviewer ask this questions and here is my answer,"1.) Self-Heal
2.) should have auditing/error handing
3.) Self explanatory
4.) Properly documented
5.) De-couple
6.) Scale to handle any amount of the data
7.) Repeatable with same result. 
8.) Modular
9.) Cost efficient and should not incur any charge  while not using. 
10.) 100% availability
11.) Should have all the types of connectors for source and destination
12.) Easy to Deploy with CI/CD or any other process
13.) Granular production support.

What else Am I missing?",2022-09-17 15:55:46
t28jy7,"Introducing Kestra, infinitely scalable open source orchestration and scheduling platform.","Today, our team is proud to announce a first public release of [Kestra](https://github.com/kestra-io/kestra), an open-source platform to orchestrate & schedule any kinds of workflow at scale.

## What is Kestra?

Kestra is :

* **an orchestrator**: Build a complex pipeline in couple of minutes.
* **a scheduler**: Launch your flows whatever your need!
* **a rich ui**: Create, run, and monitor all your flows with a real-time user interface.
* **a data orchestrator**: With its many plugins, build your data orchestration directly.
* **cloud native & scalable**: Scale to millions of executions without stress or hassle.
* **an all-in-one platform**: No need to use multiple tools to deliver a complete pipeline.
* **a pluggable platform** with the option to choose from several plugins or to build your own.

As you can see, Kestra will handle **all your pipelines** !

## The History of Kestra!

Kestra started in 2019 with this [initial commit](https://github.com/kestra-io/kestra/commit/d57e30c0c0d450590a1eaac5df0e82e1ea94e562). At this time, Kestra was at the proof-of-concept stage.

[Initial commit of Kestra](https://preview.redd.it/nzevvwt339k81.png?width=779&format=png&auto=webp&s=92f58459a6db4d64367582590854590dd1d63577)

To provide a bit of a background: I was working for Leroy Merlin as a consultant. We needed to build a new cloud-based data platform from scratch (destination: mostly Google Cloud Platform). We tried a [lot of things](https://kestra.io/blogs/2022-02-22-leroy-merlin-usage-kestra) and failed with some of our attempts. The **biggest setback was the orchestration** software that we tried to deliver with Apache Airflow: a lot of instability (tasks that failed simply due to the Airflow scheduler), performance issues (unable to handle a light workload), and a lack of features (scaling, data processing). After many tests (Google Composer, Open source Airflow on Kubernetes), the decision was final: **Airflow was rejected by Leroy Merlin**.

I did some research on the orchestrator ecosystem; most are **proprietary and license based** (far from my mindset), some are open source (at this time, only Apache Airflow seemed to be active — and it was rejected). I was really surprised by this discovery and faced this challenge from a co-worker:

>If you think Airflow is bad, do better!

It was decided: I set myself the task of producing a proof of concept for our own open-source workflow management system. It took a lot of time to build this software, and the task seemed to be never ending; but I continued to work on it for several months by:

* Choosing [Kafka as database and queue](https://github.com/kestra-io/kestra/commit/b4d026574c2fb141a3c7dd5b7f1481a31063acb2)
* Implementing [storage](https://github.com/kestra-io/kestra/commit/bcc5798d7fdcbe3afe95c019c41ddc546b24f62d) for task processing
* Choosing [ElasticSearch as a repository for UI](https://github.com/kestra-io/kestra/commit/2ede1e692be50999bc16f011f6a4796ffbbb9e1a)
* Adding some dynamic templating with [HandleBar](https://github.com/kestra-io/kestra/commit/05f1e20a3cb1e9a623024f5674144b3934cd5874) and changing it later to Peeble
* Starting some [Google Cloud](https://github.com/kestra-io/kestra/commit/14e3384be2144a2bf6698439b5ae22106ac83914) plugins
* Introducing [the UI](https://github.com/kestra-io/kestra/commit/1fef7509bb2d04b24bf66fce19b35dd01411a1db) — built with [Vue.js](https://vuejs.org/)

&#x200B;

[Kestra user interface](https://i.redd.it/ymdkj8lu89k81.gif)

And so on !

During a thirty-month period I built a variety of features, numerous plugins, and countless bug fixes — mostly during the night as I was still working as a full-time consultant for Leroy Merlin. It took a lot of effort, investment, and time that I could have spent with my family.

But now we are really proud of what we’ve achieved!

## Kestra is Open Source!

I'm a real open-source enthusiast. As an architect, I’ve been interested in open source solutions in IT for twenty years. I started as an open source consumer (using it without adding contributions, as is the case with most users). I then decided that the time was right to start out with the permissive [Apache License](https://github.com/kestra-io/kestra/blob/develop/LICENSE).

Three years ago, I started another open source project, [AKHQ](https://github.com/tchiotludo/akhq), with the same license. Working with a successful project was an invaluable experience for me as I was able to learn how to build a community around a project. I've also learnt that an open source system won't pay the bills on its own. AKHQ required a lot of personal investment; Kestra has required a lot more and will continue to do so in the future! This means you will have to ensure that you have the financial resources in place to enable your project to be viable and sustainable — we decided to create a company alongside Kestra in order to raise the required funds to support the development of the open source software.

The open source license is not limited and allows you to install and run it as you want on your server on premise or your cloud. We have also built our **Enterprise Edition** , bringing added security and productivity to your Kestra clusters. In addition, we plan to deliver Kestra in the form of software as a service in the near future (don't hesitate to [contact us](https://kestra.io/company/contact) for more information).

## Kestra Plugins are also Open Source!

[Kestra plugins already available](https://preview.redd.it/qnhl80en39k81.png?width=1141&format=png&auto=webp&s=ab3fae64526502d52c953efe412866fc6d4d7f08)

When implementing the deep integration of the tools and databases you are using, the connectors (what we call “plugins”) can present the biggest challenge. Most orchestrators (even proprietary and licensed based) only talk bash or cmd. You have to manage all of your needs with simple commands, often requiring you to use another tool in order to have access to the underlying resource (such as Talend). With Kestra, we want to have a deep integration with your tools and let [bash](https://kestra.io/plugins/core/tasks/scripts/io.kestra.core.tasks.scripts.Bash) deal solely with edge cases a plugin can't cover.

An example for a query to Google BigQuery:

>with Bash

    DATE=$(date --iso-8601=seconds)
    bq --format=json query 'SELECT name FROM \`project.dataset.table\` WHERE shippedDate=${DATE} AND shippedCountry = \'FR\'' > /tmp/query.json
    jq -r '.name' /tmp/query.json

>with Kestra

    - id: query
      type: io.kestra.plugin.gcp.bigquery.Query
      fetchOne: true
      sql: |
        SELECT name
        FROM `kestra-prd.demo.salesOrder` AS s
        WHERE shippedDate = '{{ now() }}'
        AND shippedCountry = 'FR'
    - id: ""return""
      type: ""io.kestra.core.tasks.debugs.Return""
      format: ""{{ outputs.query.row.name }}""

Kestra avoids the rigmarole of installing the software on the system, handling dependencies and conflicts, dealing with Python, etc. — just install a plugin (a simple jar) and speak directly with your database.

We have a [number of plugins](https://kestra.io/plugins/) and the process of [developing your own](https://kestra.io/docs/plugin-developer-guide/) is very simple. We also hope that a community will help us to maintain new plugins/connectors ([contact us](https://kestra.io/company/contact) if you require help or support).

## First Public Release and Production Ready!

First public release doesn't mean that Kestra is not production ready. In fact, it has been **used in production since August 2020 at Leroy Merlin** — take a deeper look at the [case study](https://kestra.io/blogs/2022-02-22-leroy-merlin-usage-kestra) if you want more detail. Here are some figures to give a picture of Kestra’s credentials:

* **4 clusters** one for every environment
* **200+ users/developers**
* **2000+ flows** in production
* **350,000 executions** every month
* **3,000,000 tasks** every month
* **Equivalent of 1,500 days of task processing time** every month (yeah, that’s the equivalent of fifty days of task processing every single day)

So, your next question is: **why are you waiting so long for the first public release?**

The answer is simple: we want to deliver the first impression as best as possible and this led to a lot of work: missing features, missing plugins, new UI design, polish of documentation and website. Now we are proud and confident enough in our product to display the result of our labor.

The road is not finished; we still have a lot to do. Stay tuned for the journey.

Stay connected and follow us on [GitHub](https://github.com/kestra-io/kestra), [Twitter](https://twitter.com/kestra_io) or [Slack](https://api.kestra.io/v1/communities/slack/redirect).",2022-02-26 22:15:44
q8zdlk,changing your attitude,"I just got back from a 1.5 week vacation trip two days ago. I immediately blew through all my project backlog which I had let sit for the past three months (really). The next day and today was me going back to old projects that didn't work out and fixing them. And responding to requests that I had not seen before (with a little creativity). It's a great feeling. 

But if i had skipped the trip and just kept ""attending"" work none of that would have been accomplished.

Something to think about.",2021-10-15 22:47:29
1b0up0e,Read/Filter a 1.7 TB CSV File in Python,"I'm reaching a mental breaking point.

I have a 1.7TB csv file that I need to filter and store two columns from as a new csv based on if column 'ID' is in a predetermined set of ID's (roughly 135,000,000) . I've tried playing around with Dask to speed up the process but set the blocksize to 50MB and just had it run for 8+ days without converging.

I really don't know what to do at this point or if it is possible to make an efficient script to do this.",2024-02-26 22:20:31
1asc1gj,PySpark Tutorial in Jupyter Notebook,"I created this Jupyter Notebook when I started learning PySpark, intended as a cheat sheet for me when working with it. Since I started learning PySpark with the book ""Data Analysis with Python and PySpark"", this notebook can be seen as my learning notes focused on practical coding. If you want to fully understand PySpark, I highly recommend reading the book. Originally, I put it on Kaggle. I rediscovered it recently and want to share it. It can be a good starting point for beginners who want to learn PySpark.

Link to notebook: [https://gist.github.com/ThaiDat/81c3662801aa8410a65b94f3c993c377](https://gist.github.com/ThaiDat/81c3662801aa8410a65b94f3c993c377)

Some related posts that expand/explain the notebook:

[PySpark common operations](https://note.datengineer.dev/posts/a-practical-pyspark-tutorial-for-beginners-in-jupyter-notebook/)

[PySpark UDFs](https://note.datengineer.dev/posts/pyspark-udfs-a-comprehensive-guide-to-unlock-pyspark-potential/)",2024-02-16 15:58:48
17j3bt3,Why did DE go the vendor tooling (hell) route for most things vs SWE where the solution is language frameworks/libraries and do less (or more) code.,"Title. This is more of a disappointment with expectation vs reality more than a rant really.  When I first started in Data, I was an analyst and immediately found out that ""deriving trends and insights"" from data wasn't for me. I realized I liked and wanted to be a builder, making tangible things in production. A SWE fits the bill but I still wanted to be in data so Data Engineering felt like the best route. But I was honestly disappointed that left and right vendor tools are the prescribed solution to everything.

It also made me sad that wanting to build anything on your own was ""reinventing the wheel"" apparently. So maybe people can give insights here how a DE is just as technical as a SWE because I see people here become adamant that a DE can even be more technical as a SWE? For me a technical solution is making your API or your own server with Go or something as a backend engineer. Using vendor A product that has a connector between OLTP and OLAP databases aren't as exciting honestly. Custom built solutions are what I want vs throwing money at the problem.

 And how did all this happen in the first place? Is Data Engineer just too broad a spectrum? A good technical DE isn't worth the ROI for pipelines vs a SWE building applications?",2023-10-29 13:49:09
16l4r9l,Every other post in this sub seems to be “going from DA to DE?” or “should I learn X?”.,"It’s the same shit over and over, and it’s getting a little tiring. There is some great content and interesting discussions from time to time — I look forward to occasional article or post with lots of comments, usually get inspired by or learn something in there. But these newbie/career advice are the same thing over and over and over and over again. Do people not search prior posts? Do they just need some validation or words of encouragement for **their** particular situation? 

That’s it, rant over.",2023-09-17 16:14:08
13vrzlt,What does dbt Labs get wrong about dbt best practices?,"I've seen a bunch of scattered criticism of how [dbt's official docs](https://docs.getdbt.com/guides/best-practices) describe best practices for the tool, but I haven't come across anything centralized here or elsewhere, so I thought this would be useful as a discussion topic where people could make their points about specific flaws and propose alternatives (or say which parts they agree with).

The two overarching points that I see come up are that their best practices:

* Encourage lock-in
* Lead to a large proliferation in models that become difficult to maintain and expensive to run

Do people agree with that premise?

EDIT: To clarify, I am more interested in issues with suggested best practices than I am issues with dbt itself - they're obviously related but I think it makes sense to separate those discussions.",2023-05-30 14:35:04
u5pbql,"Is it me, or does blockchain have zero use cases in data infrastructure?","Blockchain has been a buzzword for years, but I still don't see the big deal.  The update/write times makes it unusable in production.  



Also, I hear a lot of people say blockchain can be used to keep a record of transactions.  That pretty much sounds like any data warehouse I worked with.",2022-04-17 15:27:12
suvukx,r/DataEngineering Buzzwords - Details in comments,N/A,2022-02-17 18:35:11
oquubj,What are some intermediate to advanced level SQL and Python topics that someone starting in Data Engineering should know?,"Someone being me, having good understanding and experience with most basics of SQL and Python, and some intermediate topics as well. Want to know what are the must-have intermediate to advanced level SQL & Python topics I should know when applying for Data Engineering jobs?",2021-07-24 17:55:14
1aldl7r,How do these companies make money?,"I am currently a student of data engineering, and I just built my first pipelines using terraform, airflow, dbt, cosmos, snowflake, bigquery etc..

But all the tools I used were free... How the heck does Hashicorp (for terraform), Apache (for airflow), DBT labs (for dbt), and Astronomer (for cosmos) make any money? 

Sorry just one of those embarrasingly basic questions but I still don't get it",2024-02-07 21:03:09
15bs36m,What's the coolest data you've worked with,I know we're all cool and blase and here for the money (except for us euro folks that are chronically underpayed) but i'm curious: what's the coolest data you've ever had to deal with in your projects? Maybe something bio related? space image data? massive network traffic data? ,2023-07-28 09:03:44
14kra4i,Are these terms irrelevant in the industry anymore?,"I am having interviews to hire someone who will work for me. I interviewed two people so far. Neither of them answered on questions:

&#x200B;

1. OLAP and OLTP systems
2. Star Schema vs. Cube
3. ETL vs. ELT
4. Window function SQL question

&#x200B;

It is a position for 3+ years in data analytics, business intelligence, or a related field and I didn't expect to get the full extent of complete answers. Am I asking too difficult questions? or am I becoming out of touch and those aren't relevant anymore?

&#x200B;

Edit: I didn't really make it clear what the role is for. The role is BI Engineer, but the candidates that the head hunter sent to our HR manager happened to have a data analyst background. ",2023-06-27 22:13:05
1449ezb,Most companies are rushing to build or incorporate #gpt in their value chain. #genai. Do you agree?,N/A,2023-06-08 13:32:17
12qldg7,Does data engineering not have as much interesting career progression as other areas of engineering?,"I've been a data engineer for almost 5 years, and I noticed that of the many data engineers I have worked with, almost all of them have stayed in data engineering.  They have either moved into more senior data engineering positions or became data engineering managers.




My friends who are software engineers have had more interesting career progressions.  For example, one engineer started out as a backend developer, moved into full stack development to pick up frontend skills, and is currently in devops.  I'm surprised my data engineering colleagues have not seen similar career progression since a lot of companies see us as specialized backend engineers, but for data.




I enjoy data engineering, but I don't see myself doing this my whole career.  I would eventually enjoy going into devops or becoming a software reliability engineer.",2023-04-18 12:26:42
v4xtu9,Remote data engineers deserve salaries based on the market where the company performs primary operations,"I am a data engineer who moved from SF to Seattle. Since I went fully remote, my salary has flatlined.

I have grown my team, improved productivity, improved output, and even fly down once a month to give in-person trainings.

I am a manager of a team of 5. I feel that I should continue to be paid a San Francisco-based engineering salary, including objectively-deserved pay raises, even while being fully remote.

I continue to hear about this push by employers to reduce the pay of engineers who work remotely full-time, citing reduced cost of living as the primary reason.

Excuse me?  My personal cost of living is my own business and not the business of my employer. That should not be leveraged against me.

The value delivered to the marketplace is roughly the same whether the code and management are written and performed from SF, Seattle, or the other side of the planet.

Using my cost of living as an excuse to pay me less results in more profit for the company at my full expense and nothing else changes. That is not a fair shake.

If they want they can hire someone else from Seattle for 50% below market rate, but it's not going to work for me anymore.

I have seen 1.5 years of no increases after requesting salary increases twice over (2) 6-month periods and showing expected deliverables were achieved for both periods. Still getting stiffed.

I think some employers just don't understand what data engineering is valued at. Before our systems were in place, seeing the problems their employees were dealing with was like eating glass and staring into the abyss.

* Hour long report refreshes that had a 30% reliability rate.
* Users having to manually integrate 2 ERPs in Excel each month for budget vs cost analysis.
* Never ending data schlepping for monthly, quarterly, and annual financial reporting requiring 2 dedicated FTE to manage both using 100% of their time.

4 years later our business has grown 2x even through the pandemic. If these systems weren't here and new ones not currently being planned and delivered, the business would crumble.

But they would rather lose millions than pay their data engineering team manager an extra $10k per year.

I have an interview on Monday for a job that pays $30k more, with $135-175k RSUs, unlimited PTO and 6-week paid sabbatical every 4 years. Currently I only get 3 weeks PTO. If I get this offer I will be jumping ship *immediately*.

Grateful for the experience at prior company, but they lost touch with my needs and made decisions that were ignorant and offensive to me. They said I was lucky I had my remote contract signed when I did, because soon thereafter new remote employees had their salaries cut (albeit on other teams w/ different operating models).

**But I reserve that I should earn the fair market rate geolocated to the company's primary operations, and commensurate with the value my role delivers to that market.**

Nothing about my cost of living should impact my salary.

What does Reddit think tho?",2022-06-04 20:52:58
q8mbpt,"If leetcode is for software engineers, what's it then for data engineers?","To get a high tc for software engineers they must grind leetcode extremely, but what is thd counterpart for this for data engineers",2021-10-15 11:19:01
1bl3n3r,Writing effective SQL,"Hi, r/dataengineering!   

Over the last ten years, I've written tons of SQL and learned a few lessons. I [summarize them in a blog post.](https://ploomber.io/blog/sql/)

A few things I discuss:   

* When should I use Python/R over SQL? (and vice versa)   
* How to write clean SQL queries   
* How to document queries   
* Auto-formatting   
* Debugging   
* Templating   
* Testing   

I hope you enjoy it!   ",2024-03-22 16:38:00
19difxp,Is the Data Space really this Complicated or am I just overthinking?,"For some reason, everytime I try to learn I see new tools and how they ease the existing work. And I end up wasting more time where if I spent that on actually learning, I would be way ahead. How do you know which tool to pick and choose(from the noise in the market) ?

https://preview.redd.it/ji5thy5f05ec1.png?width=2013&format=png&auto=webp&s=167f4e2afce621cc135d5a0ff7d5c484fedaa032",2024-01-23 06:55:02
167b3ep,Quarterly Salary Discussion - Sep 2023,"https://preview.redd.it/ia7kdykk8dlb1.png?width=500&format=png&auto=webp&s=5cbb667f30e089119bae1fcb2922ffac0700aecd

This is a recurring thread that happens quarterly and was created to help increase transparency around salary and compensation for Data Engineering.

# [Submit your salary here](https://tally.so/r/nraYkN)

&#x200B;

If you'd like to share publicly as well you can optionally comment below and include the following:

1. Current title
2. Years of experience (YOE)
3. Location
4. Base salary & currency (dollars, euro, pesos, etc.)
5. Bonuses/Equity (optional)
6. Industry (optional)
7. Tech stack (optional)",2023-09-01 16:01:00
10njfnd,(RANT) I think I'll die trying to setup and run Spark with Python in my local environment,"For the last 4 hours, one error or the other keeps popping up. Never faced something like this in any other software.

Update: I'm making do with Pyspark in my Jupyter Notebook. Had to configure some things but things working fine as of now.",2023-01-28 16:45:05
rvel59,"2021, will it change in 2022?",N/A,2022-01-03 22:57:42
mmrgpo,Looking for open source projects that use data pipelines and big data flows,"As a software developer planning to move into data engineering, I'm looking for open source projects where I can learn from and contribute to the actual building & maintenance of data pipelines.

I don't mean developing open source data engineering tools like Kafka or RabbitMQ. I mean projects that are actually \*using\* those kinds of tools to consume big data and build out data pipelines. I can and will learn on my own by building out pipelines on my own personal projects. But I'm trying to find projects where I can collaborate with others on production-like pipelines using real-world data and tools.

I know there are plenty of projects that use open government/open data/etc. types of sources. I figure a lot of them must have solid data architectures using pipelines, data lakes, ELT/ETL processes, data warehouses, etc. Do you know of any where people can contribute to those processes? Thanks!",2021-04-08 13:11:29
1bit81p,"Stretching the truth about being a ""Data Engineer""","Has anyone ever stretched the truth about them being a DE? I was reading a post on Reddit where someone was a Data Analyst, and just did cloud work and projects on his own. He said he put his job title down as ""Data Engineer"" and ended up getting a DE job.  


Does that sound like something common in the job field? I have heard horror stories of people being hired in and not showing competencies. ",2024-03-19 19:42:15
18gfpn9,Why do job descriptions demand skills that are not at all needed?,"Just accepted a data engineering job. Talked about graph databases, unstructured data, python, DAGs, etc during the interview. 

Day one I'm told the job will be making dashboards in tableau under the direction of senior data scientist. 

Why?",2023-12-12 06:36:39
168p757,Will Airflow become obsolete in coming years?,"I see a lot of new orchestration tools popping up. Especially in the last 2 years. A few prominent ones are:

1) Prefect

2) Mage

3) Dagster

All three projects look solid, and all of them cover the most common use cases of data engineering. Which revolve mainly around orchestrating and error handling batch data jobs.

Having personally used Airflow, I know how quirky it is. With its idiosyncrasies and an awkward learning curve. Not to mention a nightmare to manage, if you're handling infrastructure yourself.

Are we witnessing a Hadoop v Spark battle in the orchestration world?

Apart from legacy systems, are there any good enough reasons in 2023 to still pick up Airflow over other tools, if I'm starting a new project?",2023-09-03 06:08:34
13pvcss,What’s your opinion on today’s release of Microsoft fabric?,N/A,2023-05-23 17:42:58
137ij6s,What kind of work can a data engineer do if they can't find employment as a data engineer?,"I'm currently employed as a data engineer, with about 5 years of experience across this field and devops.  I decided to give applying in this job market a try, and was surprised to see how competitive everything is even at smaller/less well-known companies.  Companies whose interview process 2 years ago involved asking candidates to find the most frequently occurring element in an array are now asking Leetcode hard questions and exact experience with certain technologies (even though it can be learned quickly!).




Even ignoring my work experience, I had a much easier time as a very average new grad engineer.  How can a data engineer get employed as soon as possible, if data and devops engineering jobs have gotten so competitive even at smaller companies?  I am mostly talking about any job that is tech/IT related.",2023-05-04 12:01:59
y5n0oj,Has anyone built a data warehouse primarily using Databricks?,"I hear many cases of Snowflake and Azure Synapse serving as several company's main data warehousing platform, but interestingly I haven't heard much on Databricks serving as the central data warehouse platform even though Databricks themselves claim you can certainly do so. 

Has anyone been in a situation where you've built or come across data warehouses with Databricks used primarily?",2022-10-16 17:48:32
odgm3w,I've built online resource where aspiring data engineers can learn modern tools in production like env. What are your thoughts?,"Hi. I’ve noticed that most online tutoring platforms are focused on specific cloud vendors (AWS, GCP, etc.) and their data tools. There is no online platform where you can try vanilla Airflow or Kafka, you can only install them on your local machine, and I believe that’s a hindrance. That’s why I decided to build a platform where aspiring DEs can learn various tools without installing anything on their local machines.

Here how it works inside (in case of Airflow):

I manually created few servers that Airflow interacts with. One server acts as RBDMS, second server acts as REST API, third server acts as HDFS with preloaded data. Every 5 minutes servers receive new data.

User story:

1. User registers and selects tools.
2. User receives hands-on tutorials that explain how to setup working Airflow pipeline.
3. User works inside hosted jupyter environment where she writes code and has access to shell.
4. User builds real pipeline that ingests raw data from several servers and performs transformations according to business requirements.
5. User can schedule DAG to run every ten minutes and every run will produce new results.

What user gets:

1. Immediate access to cloud sandbox environment (which is quite comfortable)
2. Every tutorial has its own business quest and interacts with many preconfigured servers
3. In no time user understands how different data tools are used
4. Preconfigured servers emulate production environment, so that student knows what to expect in real life
5. Experience with Airflow, Kafka, PySpark, Hadoop (HDFS & Hive), Cassandra, Mongo, Postres, MySQL, DBT, Neo4j

Note, every tool has its own business quest.

Experienced data engineers please share your thoughts.",2021-07-04 08:58:50
ms33t0,Open source contributions for a Data Engineer?,"What are some good git projects that a Data Engineer can target to increase their skills? Contributing to which git projects have helped you so far?

Edit:

Listing down all the repos mentioned in the comments below -

* [Quinn](https://github.com/MrPowers/quinn)
* [Chispa](https://github.com/MrPowers/chispa)
* [Spark-daria](https://github.com/MrPowers/spark-daria)
* [Spark-fast-tests](https://github.com/MrPowers/spark-fast-tests)
* [Soda SQL](https://github.com/sodadata/soda-sql)
* [Spark-rapids](https://github.com/NVIDIA/spark-rapids)
* [Airbyte](https://github.com/airbytehq/airbyte)
* [Singer](https://github.com/singer-io)
* [Meltano](https://gitlab.com/meltano/meltano)
* [SQLfluff](https://github.com/sqlfluff/sqlfluff)
* [DataGristle](https://github.com/kenfar/DataGristle)
* [Perfect](https://github.com/PrefectHQ/prefect)
* [Metabase](https://github.com/metabase/metabase)
* [Superset](https://github.com/apache/superset)
* [Streamlit](https://github.com/streamlit/streamlit)
* [Skytrax Data Warehouse](https://github.com/iam-mhaseeb/Skytrax-Data-Warehouse)
*  [Dagster](https://github.com/dagster-io/dagster) 
* [Top 20 list by Data Council](https://petesoder.medium.com/what-are-the-most-popular-oss-data-projects-of-2021-84ef021bb5a2)",2021-04-16 13:26:06
1454ghq,Have you ever seen a successful “self-serve” implementation?,"My employer has spent sickening amounts of money on various “self serve” analytics/bi/de tools. I have been successful on some small projects like setting up tools to generate commonly requested excel files and dashboards to let people filter and play around with stuff that been largely defined I.e return on sales…. But that’s akin to letting somebody drive the car. You can drive it however you want, but we built the roads so you can only go where we say you can go and if you want to drive somewhere new, the engineering team is going to have to build you a new road.  

Have you ever witnessed the ever elusive “self serve” where entry level hourly workers are solving complex problems and generating value for companies? Or is somebody doing AVG/SUM in some carefully curated DB view in power bi and calling it ML the absolute ceiling?",2023-06-09 12:52:41
v5yp7l,Building a simple ETL for personal projects,"Hi, DE newbie here.

I want to build a simple ETL with AWS for a personal project and I'm a bit overwhelmed by all the tools that are out here. 

Basically, I have a few functions that get data from a blockchain via an API. I want to run them on a daily basis and store the data in a database that I can query with SQL for analytics purposes.

I did some research into architectures but it's unclear to me which stack is most suitable for my use case. Specifically, I have the following doubts:

1. Which tool to use for scheduling? Argo, Airflow, Prefect or simply a AWS Lambda function?
2. How to store the data? Directly append a Redshift table? Store the data files in S3? Or maybe first S3 and then have a AWS Glue job ingest data to Redshift periodically?
3. How to query the data? Do I even need a relational DB service or could I just query S3 directly? 

How would you design this architecture? Any help is highly appreciated! :)

Thanks a lot!",2022-06-06 09:14:33
jwh0qv,New courses on distributed systems and elliptic curve cryptography (by Martin Kleppmann),N/A,2020-11-18 14:45:38
1b04b8j,"Marry, F, kill… databricks, snowflake, ms fabric?","Curious what you guys see as the romantic market force and best platform. If you had to marry just one? Which is it and why? What does your company use? 

Thanks. You are deciding my life and future right now. ",2024-02-26 00:48:29
19cak0s,what is it that you do for work again?,"&#x200B;

https://i.redd.it/zhg215lraudc1.gif",2024-01-21 18:51:36
17qge22,Is SELECT DISTINCT really that bad?,"I have been pushing back on DBT (sql on Sowflake) Pull Requests that use SELECT DISTINCT and instead ask people to create a surrogate key and aggregate/de-dupe explicitly on the keys they want to define uniqueness by. This is a lot more work. Yet, we all know the urge to SELECT DISTINCT “just in case” to avoid the dreaded duplicates test error or find by a stakeholder. 

I find myself wondering lately if my blanket rule against SELECT DISTINCT and blocking people’s work because of it, is outdated and misguided, Am I unnecessarily asking more work without enough evidence to the value add or risk mitigation? Because I’ve been so strict on this my whole career (14 yrs) and my teammates along the way have also agreed, major issues resulting from its use has not come up in recent memory. 

But I am now in the process of transferring the ownership of these models and reviewer pool admin to a new group who disagrees with me on this point. 

Does anyone have any horror stories of how SELECT DISTINCT caused problems? Query runtimes, cost, troubleshooting issues? Or the opposite—Has using SELECT DISTINCT consistently, made your life easier? I of course can do some testing myself and query plan analyses but I’m also looking for anecdotes and others’ experiences.",2023-11-08 07:23:54
14lfqfu,Delta Lake 3.0,"Just announced, Delta Lake 3.0 now compatible with Hudi and Iceberg.

My life just got more interesting.",2023-06-28 17:33:34
10tjhve,"What did ETL look like before the ""modern data stack"" was a thing?","This is kind of a noob question: When I started working in this field I was already being bombarded with blog posts, ads and tutorials about dbt, Fivetran (or the like), Snowflake etc...

But I'm curious to know, what did ETL look like before this?

What did engineers have to do, before Fivetran or dbt were a thing, to move data from OLTP sytems to OLAP ones? And to model it?

Sometimes I find tutorials like ""Build an ETL pipeline with Airflow and Pandas"" and I think ""Pandas, Really?"".

In other words: If all services like Fivetran and dbt disappeared tomorrow, what would I need to learn / use to extract and model data in a production system?

Please do share your experience!",2023-02-04 15:48:44
ycbwv0,"What do you consider ""advanced"" SQL","Well it's Monday morning back to work. I'm finishing up some QA queries from last week. In my opinion QA is one of the most tedious parts of Data Engineering because it's rather time consuming and many times it seems like your ETLs or Pipelines are working just fine but they might be missing a key data element.

In an attempt to automate some of this I am creating stored procedures that can dynamically iterate through tables and check for specific data points (record count, columns with null values where there should be something, ratio of nulls to non nulls).

Got a little bit of everything in there, temp tables, variables, a while loop, dynamic SQL. The only thing missing is a cursor or any XML functions.

At what point do you consider SQL transitioning from basic to advanced. For me I consider that line when you start using the programmability features like making stored procedures or functions that accept parameters for inputs or can store result sets into variables. However some people still consider this basic SQL and don't think of it as being advanced until you start getting down and dirty with some of the features like CURSORS, Dynamic SQL, and all of that XML PATH stuff.

Personally I've only used the XML functions for string concatenation and manipulation but I've seen entire queries written with those XML commands and they are pretty complex. I am sure if I had a better understanding of XML they would appear much more simple but I never use XML for anything. Especially with JSON being widely used now.

 Anyway where does everyone draw the line on what they considered advanced SQL. I expect the responses to vary widely.",2022-10-24 13:55:36
qupb5w,Databricks responds to Snowflake (TPC-DS),N/A,2021-11-15 20:15:26
pved42,Data Engineering Mentor Network,"We've noticed a trend in members who are interested in mentorship opportunities because mentorship is one of the best ways to get personalized advice to help you reach your goals.

https://preview.redd.it/4jikhbxn2pp71.png?width=847&format=png&auto=webp&s=7d41763b890168227c48a0b8e45e939adcf48f30

That's why over the next few months we are creating a network of verified expert mentors.

**As a mentee**, you will be able to search our directory of mentors and schedule time to get 1:1 personalized advice, join a live event, or join a cohort-based course.

**As a mentor**, you will be able to engage directly with the community and offer mentorship as well as be able to set your own rates and earn revenue for paid content you create.

If you're interested in finding a mentor, [please fill out this form](https://airtable.com/shrVdtALd6aZkvMMQ).

If you're interested in becoming a mentor, [please fill out this form](https://airtable.com/shr7dzDz37OvrskX5).

&#x200B;

**What is the #1 topic you would like to discuss with a mentor? Tell us in the comments 👇**",2021-09-25 20:00:22
k7xo0q,Useful resources for Data Engineers,N/A,2020-12-06 17:16:39
1axyooe,Talend is no longer free," Now that Talend is no longer free, what other ETL tool would you recommend that has data transformation capabilities as powerful as the tMap component?

[https://www.talend.com/products/talend-open-studio/](https://www.talend.com/products/talend-open-studio/)

Thanks!

Edit: We need to deploy each ETL in client environments, which is why Talend was good for us, it generate the .jar files and a ready-to-run .bat file",2024-02-23 11:55:21
16snd4x,What whitepapers should every data engineer read?,Looking for suggestions that you consider fundamental/useful to any DE.,2023-09-26 13:02:14
11uiemx,Books that made you become a better engineer,As the title suggest I’d love to know what books you felt made you become a better data / software engineer. Ones that helped you either advance your career or changed the way you thought about data / programming in general.,2023-03-18 08:27:18
yhhl2y,What skill would you learn after Python and SQL?,"Hey everyone!

I already know Python and SQL and I’m wondering what would be a good skill to learn next for becoming more experienced with data engineering.

Thank you in advance!",2022-10-30 15:49:54
1b9ba5c,Data architecture diagram tools,"Hi! I've found gifs with this style on LinkedIn a couple of times and I wonder which tool was it made on. In some posts they say it's Excalidraw, but I can't find the moving arrows style.

But let's not make it so specific. Which visual tools do you use to diagram your data architectures/solutions? I almost always use **draw.io**

https://i.redd.it/wwy0b4byi0nc1.gif",2024-03-08 01:27:24
1935ykj,Reality check: How good are you at the skills in your tech stack?,"So let's start by saying this is by no means a post to complain but just to get a reality check and understand what you guys mean when you name a technology in your tech stack. I was navigating the sub and found the posts like the salary discussion or newbies asking for help, and you've got the usual comments throwing around a bunch of technologies like it's fresh water. Oh sure just learn aws, sql, python, powerbi, airflow, terraform and docker and you're good to go, took me a year. Obviously started questioning if I'm dumb since I've been at this job 4 years and I'm currently mastering SQL after spending 3 years on PowerBI only

So I want to understand when you name these tools and put them in your tech stack, how good are you actually at this and how much is it just ""I understand the basics and I can Google/ChatGPT the rest"" ?

Let's take SQL for example. There's a huge difference between ""Udemy course"" level of knowledge (you got the basic idea, can use SQL up to subqueries and Window Functions) and that one colleague that can write 1000+ lines of stored procedure from scratch and model JSON into tables level of knowledge. Or PowerBI: again there's a difference between ""I can drag and drop objects on the canvas and create a visualization, yaay let me add it to the CV"" and having read The definitive guide to DAX 700+ pages on PowerBI's programming language, understanding how Vertipaq engine works internally and so on. You might say it is overkill to invest that much time in one single tech but that's another topic I don't want to tackle now.

For example, I don't use Python daily at my job, but I can do some stuff with it with the help of Google and Chatgpt. I know the basics of programming, I've done a couple Udemy courses out of curiosity, I know what sets and dictionaries are, I can query an API, do some stuff with the common libraries for data manipulation and return the data. If I have to touch a Python script written by another DEV to modify something specific, I can do that. But I don't have profound knowledge of the internals, I wouldn't know how to optimize code, I probably couldn't do heavy tasks on Python-built infrastructure unless the task was very clear or build something enterprise-level from scratch myself. Do you think I should name Python in my tech stack? Is this an acceptable level of knowledge for you to name in the tech stack? 

So yeah I just need to know what's the idea of this sub on this topic because there's one of two possible outcomes:

1. I'm studying in the wrong way, and it's taking me a lot more than a normal person to really understand these tools
2. I am underselling myself and suffering from imposter syndrome or something like that

Cheers ",2024-01-10 11:15:25
17b6wdl,PyGWalker: a Python library for data engineer that turns your dataframe into tableau-like data app.,"PyGWalker is a python library that turns your dataframe (or a database connection) to an embeddable tableau-like user interface for visual analysis.

It can be used to explore and visualize your data in juypter notebook without switching between different tools. It can also be used with streamlit to host and share an interactive data app on web.

PyGWalker Github: [https://github.com/Kanaries/pygwalker](https://github.com/Kanaries/pygwalker)

[pygwalker in juypter lab](https://preview.redd.it/lor544wm82vb1.png?width=3002&format=png&auto=webp&s=993a09ffb21075b1a4a213e3988c09b9a2be1bdd)

A simple example of how to use pygwalker, you can also check more information at official doc of pygwalker: https://docs.kanaries.net/pygwalker

    import pygwalker as pyg
    import pandas as pd
    
    df = pd.read_csv(""you_data"")
    
    # then pass it to pygwalker
    pyg.walk(df)",2023-10-19 01:16:44
1460qft,"Streaming data engineering project: Apache Flink, Apache Kafka, Prometheus, & Graphana running on Docker.","
Hello everyone,

  I've seen (& written) a lot of articles explaining batch data processing. However, not much content explains the concepts to be aware of when designing streaming data pipelines. 

  With that in mind, I wrote an article that covers the fundamental concepts to know when building a streaming data pipeline. It covers concepts such as 

  1. State
  2. Watermarking
  3. Backpressure
  4. Joins in streaming systems and their caveats
  5. Monitoring

The concepts are explained as you build a streaming data pipeline that does [clickstream attribution](https://www.shopify.com/blog/marketing-attribution#3) (which is very common in marketing).

  Post: https://www.startdataengineering.com/post/data-engineering-project-for-beginners-stream-edition/

  Code: https://github.com/josephmachado/beginner_de_project_stream

  Appreciate any questions, feedback, or comments. I hope this helps someone.",2023-06-10 13:47:50
12w49y1,Is it a data structure and algorithm a must for data engineering roles,"Background

I am a 2 YOE data analyst from a finance background but I am doing lots of data engineering work, so I want to make a move on to a data engineer role. 

I am certified with AWS SAA and CCP, and I am going to get the Data Analytics Specialty certification. I have built some data pipelines with AWS services with python scripting.

My question is: Is it a data structure and algorithm a must for data engineering roles?  
I had a few interviews and there were coding interviews, but I failed so badly due to the data structure and algorithm questions. It sometimes feels like a software engineer role - perhaps data engineers are really software engineers who focus on data ?

Additionally, some role require ML knowledge in the coding interview, and again I did it not very well. 

I am not so sure if I should slightly leave AWS certifications and focus more on data structure and algorithm knowledge.   
It would be highly appreciated if anyone could share your experience, please?

p.s. I am seeking ways to improve myself or succeed in the interview.   
Thank you so much",2023-04-23 10:37:00
w0dz7f,Build Awesome Data Engineering Portfolio from Scratch in 2022 | Complete Guide,"Hi Data People,

📣 Data Engineering Project Alert 📣

👨🏻‍💻 The best way to learn anything is by doing hands-on practice and doing projects is one of the ways  


💻 Having done so many projects on my YouTube channel, I have come up with a new project challenge  


🤩 Build Awesome Data Engineering Portfolio from Scratch in 2022  


👉🏻 In this challenge, you will spend 45 days executing an end-to-end data engineering project  


What You Will Learn? 👇🏻  
✅ Researching problems and building dataset  
✅ Data Modelling  
✅ Working with Database (PostgreSQL/MySQL)  
✅ Cloud Data Services (AWS/Azure/GCP)  
✅ Dimension Modelling  
✅ Data Warehouse (Redshift/BigQuery/Snowflake)  
✅ ETL (Spark/Python/PySpark)  
✅ Dashboarding(QuickSight/Tableau/DataStudio)  


📈 No only this but you will also learn about Content Creation and Learning in public  


🕵🏻‍♂️ I have given a detailed explanation in the video

[https://youtu.be/UIZdjAKadc8](https://youtu.be/UIZdjAKadc8)",2022-07-16 10:55:54
qm6jpa,"CMV: Data Engineers should code, not build ad-hoc dashboards",Or call it Data Analyst.,2021-11-03 22:53:29
opt99w,Is it me or are beginner-friendly ETL pipeline guides that explain from the ground-up how to incorporate the use of various technologies notoriously difficult to find.,"So this is something I've been struggling with for a couple of weeks. 

I have a small windows EC2 instance (1gb RAM, so it's tiny) where I'm running a simple python script that does some ETL. 

It reads files in an S3 bucket, and then saves some data in a MySQL db. Really simple code that a beginner could have put together. 

However this little set-up we have going clearly is neither sustainable nor scalable, as sometimes it ends up crashing the EC2 instance it's running on. A better solution would incorporate the use of technologies able to perform auto-scaling, but we haven't had much luck finding guides about this that teach you how to incorporate those things from the ground up. Most of what we found assume a lot of background knowledge.

So my question would be this - given that the only things I have familiarity with are Python and MySQL, what would be a good resource that would hold my hand through setting up an alien process like Airflow/Docker/Spark/etc to perform this more efficiently.",2021-07-23 02:49:31
f9l209,Want to learn Data Engineering? Here are some Example Projects to get your hands dirty.,N/A,2020-02-26 01:19:45
1b3zatv,Quarterly Salary Discussion - Mar 2024,"https://preview.redd.it/ia7kdykk8dlb1.png?width=500&format=png&auto=webp&s=5cbb667f30e089119bae1fcb2922ffac0700aecd

This is a recurring thread that happens quarterly and was created to help increase transparency around salary and compensation for Data Engineering.

# [Submit your salary here](https://tally.so/r/nraYkN)

You can view and analyze all of the data on our [DE salary page](https://dataengineering.wiki/Community/Salaries) and get involved with this open-source project [here](https://github.com/data-engineering-community/data-engineering-salaries).

&#x200B;

If you'd like to share publicly as well you can comment on this thread using the template below but it will not be reflected in the dataset:

1. Current title
2. Years of experience (YOE)
3. Location
4. Base salary & currency (dollars, euro, pesos, etc.)
5. Bonuses/Equity (optional)
6. Industry (optional)
7. Tech stack (optional)",2024-03-01 17:00:46
18dvjbf,Rockstar Data Engineers making big bucks: what are you doing exactly?,"I'm wondering what is the responsibility and daily work like.

Edit: not Rockstar Games.",2023-12-08 20:14:08
184cqy2,What are your favourite data buzzwords? I.e. Terms or words or sayings that make you want to barf or roll your eyes every time you hear it.,What are your favourite data buzzwords? I.e. Terms or words or sayings that make you want to barf or roll your eyes every time you hear it.,2023-11-26 15:10:53
17asnwh,"Have you seen any examples of “serious” companies using anything other than Power BI or Tableau for their data viz, including customer facing analytics? Example: pro-code tools like Shiny, Python Dash, or D3.","
I get the (false?) impression that the visual end of the data stack is always Power BI or Tableau, but is that true?

Would love to hear from other DEs that serve data to **pro-code** visualization tools like Shiny, Dash, or D3.js.

Trying to get a sense of how common these pro-code tools are in an enterprise, and/or customer facing analytics, or if it’s just hobbyists and companies that can’t afford Tableau/PBI.",2023-10-18 14:52:12
110gth0,What do you personally do to improve as a DE?,"Side projects? Reading? Courses?

As someone with no formal background in SWE/tech, most of my improvement has come from the desire to learn more about things. Not the most structured approach though, just curious to know how everyone else keeps their skills sharp!",2023-02-12 14:21:44
10qdxjr,Feel a bit burned from a take-home assignment I completed during an interview,"Hey everyone, I'm sorry if this post is long...there's a **tldr** at the bottom. I just want a place to vent and get feedback - I feel discouraged and frustrated. About three weeks ago, I had an initial screening with a recruiter from a tech start-up based in Europe. It led to a technical interview for a remote data engineering role (US) with them. The first technical interview went well, and I was informed that the next step would be a take-home assignment. The assignment asked me to do pretty much what I would be doing if I received the position:

1. Write an API that connects to one of their datasets
2. include multiple parameters for aggregation, querying, and filtering
3. put it on the cloud
4. write documentation for it
5. Write a report of why you chose the tools you did and what you would do differently.

The time limit on the assignment was approximately three hours. This struck me as a red flag because I felt like this was a lot of work to accomplish in less than half a work day (I would love your thoughts on that, actually. Maybe I'm just slow.)? The assignment was also very open-ended. There was no mention of which dataset to connect to, how to connect to it, which tools to use, or really any expectations on their end.

I'm not a fan of take-home assignments because I've heard horror stories of people getting used for their free work. Since it was so open-ended, only three hours long, and the interviewer was super chill when talking about the assignment (""We just need something to get a feel for your Python skills"") I assumed it was ok to build a small application showcasing the fundamentals of API development.

I used one of their smaller datasets, coded my API in Python using Django and Pandas, hosted it on an EC2 instance, and wrote markdown documentation. In my report, I mention how I would use Spark, tokenization (I was working with text data), and a proper database if I were to make this app production grade. I included all the drawbacks of my current application and how I would fix them - because of course - the assignment was only supposed to be three hours long, and there was only so much I was willing to do for a take-home assignment. I felt like this was a lot of work as it is, and it took me roughly 6.5 hours to complete.

After submitting my assignment, I got an email the next day telling me I did a great job and they would like to proceed with the second technical interview. The second interview ended well, with the interviewer telling me they would schedule a final interview with the CTO. Instead, I received another email three days later stating that the company decided to terminate my interview process. They said it was because my app was not using a large enough dataset, my code wasn't efficient enough, pretty much telling me that it wasn't applicable for a large-scale system. ????????. Of course not? This is a take-home assignment that was supposed to take 3 hours to finish. I am honestly baffled. Is this normal? I understand there are people out there who probably can write an enterprise-level API in 3 hours, but even so, why do it for free during the interview process? At this point, I feel a bit sketched out and like I dodged a bullet. It's still discouraging to make it so far in the interview process and not get an offer, though.

&#x200B;

**TLDR**; *I completed a take-home assignment, and I feel kind of shitty about it. Considering avoiding companies that ask me to do this in the future...*",2023-01-31 23:25:33
103j5cq,"What parts of ""the data warehouse toolkit"" and ""designing data intensive applications"" are important to read?","The two books are regularly mentioned as must-reads for DE's ; but they're pretty long, and some parts are outdated. What parts are still important to read for someone who wants to get into the field, has a technical data background, but doesn't want to read 1000 odd pages?

&#x200B;

Also, would you recommend any other books? specifically any with practice problems?",2023-01-04 23:45:12
yy2qmg,Snowflake syntax now supports EXCLUDE & RENAME syntax,"[https://select.dev/posts/exclude-rename](https://select.dev/posts/exclude-rename)

https://preview.redd.it/2inizkec1l0a1.png?width=1626&format=png&auto=webp&s=1513a820971b80f542a1a75b6a46d3ab9cefb140",2022-11-17 21:52:31
w1z2at,"I start my first day as a Data Engineer next Monday, any tips?","Hey Everyone, I’m overly excited that I achieved this role.

I live here in the USA, MCOL (Atlanta, GA). My degree is in Bio-Chemistry and I’m all self-taught in programming/Data Science. I have roughly 1 1/2 YOE in Data Science, I was a Inventory Analyst my first year and Systems Analyst in the second position at my most recent company.

Total Compensation: $120,000/yr

I already work heavily in SQL and light to medium in Python. I’ve been studying AWS (S3, Heavy on Lambda, DynamoDB, & etc) and Spark jobs, as the that’s the platform the new company uses. I’ve been using Udemy courses heavily and they have helped a lot.

I would like some advice on what to focus on: 
- How to not be nervous with my first Engineering job
- Did I get a good salary for my first Engineer position
- How can I grow into a better Engineer or to even a Architect?
- Then any questions or advice you all can give.

Please and thank you for all help.",2022-07-18 13:13:11
11f0ckj,What is the role of Kubernetes in Data Engineering?,"The title basically. I am trying to upskill myself and Kubernetes is in my list to learn. Before I dive in I need to first understand how k8s is used in data engg. 
Can you share how you are using k8s in your etl pipeline? Anything is appreciated. Thanks!",2023-03-01 10:27:57
wimm53,best books on data engineering? (not necessarily technical ones),"I'm looking for more general books on data/data engineering (i.e. not a technical book on Apache Spark for example) - if there are more general technical books, or technical books you feel are transferable enough (or simply just must reads) then feel free to fire them in, just try be clear if it's technical or not

I'm keen to learn more from the product/platform/use case aspect, hence the more generalist request.",2022-08-07 18:34:46
vrucvw,ETL Pipeline Testing,"I'm new to data engineering trying to learn from diffrent ressources and tutorials online what i dont find easily is Testing pipelines  do you have any  good ressources  where to learn and practice ETL testing ? 

what do you exactly  test in an ETL  and what does those tests include  ?",2022-07-05 10:01:33
tj81in,Wrote a post about how we implemented The Modern Data Stack at ManyPets,"Wrote [this post](https://medium.com/data-manypets/how-manypets-implemented-the-modern-data-stack-35877715c0da) about our data architecture. We use a cloud data warehouse and the group of low/no-code tools which is now getting called The Modern Data Stack.  


Not everyone should copy this stack but it does work well for us. It's come up on this subreddit before if you can use these tools for higher volume work. So to give some context on that, we've hundreds of thousands of customers and our warehouse tables are typically in the 10s of GB scale. There's a couple in the 100s of GB but few over 1TB.",2022-03-21 09:46:00
iwzjm8,Data Engineering from the Ground Up - Part 2: Better Pipelines with Python and Idempotency,N/A,2020-09-21 12:35:07
1b6dgsp,I created an open-source microsite to help analysts and SQL-heavy devs get started with Spark,"I help companies build and scale machine learning and analytics applications, with Spark being a core part of our data processing toolkit. While Python is generally the go-to language for data processing, libraries like Pandas & NumPy come with a LOT of sharp edges. This is especially true if you're trying to read someone else's code. 

IMO Spark (& PySpark) has an edge over other data processing tools for a few reasons: it reads more like SQL (making it a easier to understand without digging through obscure documentation), it's well-supported across cloud platforms, and it's dead simple to scale to handle any size data you need to throw at it.

I wanted to create a resource that anyone with basic Python and SQL knowledge can use to get up and running quickly with Spark (you can probably learn 80% of what you need in a day or two). I also wanted to include some suggestions around code conventions to help with readability and avoiding gotchas that trip a lot of folks up (e.g. duplicating columns when doing joins).

You can get started with either a web notebook or locally with a batteries-included Docker container. 

You can access it at [SparkMadeEasy.com](https://sparkmadeeasy.com/).

This is still a work-in-progress, with more topics to come (e.g. Spark-ML). Happy to hear any feedback!",2024-03-04 15:19:58
11i4n2h,Why would I choose Snowflake over BigQuery?,"Apologies if this is already covered somewhere as it seems like a simple question, but I'm not clear why I'd ever choose Snowflake.

The costs seem to be higher, and the integrated tooling seems to lock me into a vendor in the way that using 3rd party or open source tools would not.

I have nowhere near enough data to come close to hitting size or performance limits on either.

What am I missing?",2023-03-04 16:32:02
113x4cb,Data Engineering Competition!,"Inspired by this [post](https://www.reddit.com/r/dataengineering/comments/11349lf/is_there_anything_like_kaggle_for_data_engineering/) and this [comment](https://www.reddit.com/r/dataengineering/comments/11349lf/comment/j8spjrg/), would r/dataengineering be interested in a project based competition? (mainly for learning purposes)

To keep things simple, we could use reddit polls to host it. We can decide on the project (and the winner) using votes.

We can hash out the details if there's enough interest, but I'd be willing to chip in the first $500 to the winning pot. My personal preference is to donate the winnings but will also defer this decision to a poll.

**Open questions:**

1. What should the scope of the project be? Data Engineering is a very broad field.
2. Do you see any downside to deciding the project using a reddit poll?
3. Do you see any downside to deciding the winner using a reddit poll?
4. How long should the competition run? 4 weeks should be max for building a production-ready project on the side (to account for DEs with full time jobs and give new DEs time to learn)

Let me know what you think :)",2023-02-16 18:16:19
10wcbp7,Leetcode for Spark and PySpark at Zillacode.com,N/A,2023-02-07 20:34:07
10uw3rn,End to End Pyspark Testing CI/CD Example Repo,"Over the past while I've been battling with some local pyspark and docker-compose setups to have better tests for CI/CD.   
To path to solve this problem was to try get the same level of depth of environment that runs in production in the company I work at (databricks, AWS EMR, S3, etc.) as locally  


Figured I'd share out some of wrapped up knowledge of battling with specific Jars and hive-metastore (which you don't actually need, just a SQL database set up in certain way!) 

The repo has the following features in it  
 

* Quick and Easy setup for local testing and development environment and abstracting the complexity of configuring up a pyspark environment in this way
* Full Pyspark Implementation
* Full S3 like implementation with Minio
* Read and write data to S3 and access them as tables and databases in Spark through metastore
* Ability to run pytest on the pyspark container
* Read and write with delta format
* Example of testing pyspark code with either unittest or pytest
* Consistent environment for testing and development with docker-compose and poetry
* Ability to run tests on push with github actions  


If I've missed anything please feel free to let me know! Hope a few of you find this useful

[https://github.com/emmc15/pyspark-testing-env](https://github.com/emmc15/pyspark-testing-env)",2023-02-06 03:15:18
zfv38r,Rust for Data Engineering—what's the hype about? 🦀,"This is a great article that tries to explain [why people everywhere are speaking about Rust](https://www.adventofdata.com/rust-for-data-engineering/). To be honest it feels more like a under the hood thing rather than a full switch to the language and the article depicts this super well.

Mainly it says:

* Why Rust: because Rust compiler is strict, easier to use than C/C++ out of JVM
* When Rust should be used: when you want speed and performance with data, Rust and Arrow are well integrated and with security about your data types
* When Rust should not be used: sometimes Rust type safety is too rigid for data (hf read a CSV), when you want to go fast or on a new project because the learning curve is steep

*🎄 PS: if you did not see this article is day 8 in the Advent of Data. Advent of Data is a advent calendar, everyday in December a new article about data is published.*",2022-12-08 10:18:28
xr1bk7,Databricks Vs Snowflake - User Experience,"Hello all,

We are currently evaluating **Databricks (on AWS)** and Snowflake for our company, can fellow DE's share some experience/thoughts for the below questions

Consider the below scenario where there are **no cluster/SQL warehouses** running on both platforms, a data analyst comes in and starts hitting a table with some queries and this is what happens on both the platforms (correct me if I am wrong with this)

**Databricks** \- The compute cluster / SQL warehouse takes 3-4 mins to spin up, and the first query that hits it takes considerable time to return some results.

**Snowflake -** The SQL warehouse startup time is within seconds and even the first query returns results fast

I know in Databricks we can create cluster pools and keep some clusters idle (warm) which can reduce the startup time but still are we not paying more money to keep servers inactive 24/7.

The question is how teams  are managing Databricks to be always up all the time and at the same time maintaining the costs, doesn't Snowflake has an edge over this as you are not always keeping the cluster/warehouse active

One of our main evaluation criteria is User experience (at the same time maintaining low costs), we don't want people using these platforms needs to wait for a considerable amount of time running their queries.

Also, for people using Databricks are there any guidelines for choosing the instance families for the cluster, as there are quite a lot of them, it would be great if you guys can provide some tips on choosing it as we feel the jobs are running slow because we are not choosing the right instance family for the cluster and do you guys recommend using SQL warehouses as they are spinning up quite large machines (which can cost more) even for a smaller warehouse.

Just trying to understand real-life examples from teams who are using Databricks now

Any help on the above is highly appreciated. Thanks in advance",2022-09-29 07:28:30
r58r49,The Data Stack Show: A Podcast on Data Engineering,"Hey everyone, 

I'm one of the hosts of a podcast show, called the data stack show. We are focusing on data engineering related topics but we never say no to a guest who has something interesting to say around technology. We even had a guest where we chatted about communities.

We've been running the show for about a year now and never shared its existence with any community.

 I'd love to get feedback and suggestions from you. What questions you might have and what people you would like to listen to. 

Of course any constructive criticism is more than welcome!

&#x200B;

You can check the show here: [The Data Stack Show](https://datastackshow.com/)",2021-11-29 23:29:43
qbyml5,Some Production SQL Database schemas for a few different businesses. (inc Airbnb and Twitter) as an example of some real world data models.,N/A,2021-10-20 11:26:02
1bfevg2,"Flat file with over 5,000 columns…","I recently received an export from a client’s previous vendor which contained 5,463 columns of Un-normalized data… I was also given a timeframe of less than a week to build tooling for and migrate this data. 

Does anyone have any tools they’ve used in the past to process this kind of thing? I mainly use Python, pandas, SQLite, Google sheets to extract and transform data (we don’t have infrastructure built yet for streamlined migrations). So far, I’ve removed empty columns and split it into two data frames in order to meet the limit of SQLite 2,000 column max. Still, the data is a mess… each record, it seems ,was flattened from several tables into a single row for each unique case. 

Sometimes this isn’t fun anymore lol",2024-03-15 14:12:01
1bf4aft,How do I future proof my career as a Data Engineer?,"AI at this point is inevitable and it’s become quite clear to me that the roles and responsibilities of a data engineer today will significantly change as AI tools become more common place. At this point it’s  all speculative but my questions are
A) what does the data engineer of tomorrow look like
B) how can I adapt to a changing landscape and essentially future proof my career

Any advice will be greatly appreciated!


EDIT:

Thanks for all the helpful advice and comments (even the neuralink suggestion haha). I think my biggest takeaway is that AI is a tool, and like any other tool will still need humans to apply it. But the biggest thing I can do to develop my career is to enhance my soft skills i.e. stakeholder management, communication etc… as well as keeping up to date with the latest trends and developments in the industry. Thanks everyone, I’m glad to be part of such an awesome subreddit!",2024-03-15 03:19:05
19501yg,My whole team hates DLTs and I don't blame them.,"We have been using databricks(aws) close to a year now and have started working with DLTs \[Delta Live Tables\]. I personally don't hate them as much as my teammates but I don't blame them, a lot of DLT limitations are in direct contradiction with the Databricks vision. Reasons listed below:

* You have to be on shared compute, this complicates reading data and writing back out to s3 if you need to drop a file (need to be in single user mode)
* BIGGEST COMPLAINT: You cannot ""hop"" catalogs or even schemas. This is so weird to me. They are rolling out DLT and UC \[Unity Catalog\] and are pushing customers hard on both, but DLTs directly contradict the medallion architecture. You want to have data land in a bronze catalog, then move it to a silver, gold, etc. Great, create a pipeline for each and kill your job runtime because you now have to spin up 3 different computes. They had a private preview that allowed you to write to multiple schemas but they killed it. Why?
* DLTs have to run from workspace notebooks, because GIT providers can only point to a users specific repo, not a config'd repo and branch like a notebook job. Luckily we have DABs to control our deployment and skirt this issue but it seems so odd to us. The DLT documentation recommends setting up a repo per pipeline, thats insane!
* Cannot share compute across multiple pipelines. To my second point, the limitation of not being able to hop schemas/catalogs wouldn't matter if I could specify DLT compute to use in the three pipelines processing the data. That solves a huge problem.
* Documentation, community support is still weak.

I do love the automation DLT brings with ingesting data, specifically CDC data. But there are some pain-points that make absolutely no sense. I think Databricks is doing too much too fast and needs to refocus on what they were initially, a data platform that provided one place to do everything.",2024-01-12 17:06:52
15umpgh,Where does the career go from Data Engineering?,"I've been a Data Engineer for some years now and wondering what kind of career possibilities there are from here onwards.

The way I am thinking myself is that I have two possible paths, (1) go towards a more architect role in a consulting company, meaning more of a technical sales role including tech selection, drawing archtecture blueprints and not participating so much in the actual coding/implementation. The other path (2) I see is to transition into a more business-focused inhouse product owner role including working closer with business to identify and suggest new use-cases for analytics and linking those investments and activities to actual business value.

What experiences does others have and would you rather go with option 1 or 2? Or are there other options I am not considering?",2023-08-18 14:52:16
14r4v10,I attempted to create the Ultimate Guide to dbt,"When I was first learning dbt I found a lot of really great resources (dbt docs, blog posts, slack threads etc.) and wanted to try to put that all together in one place for anyone else new to dbt, or anyone wanting to learn a bit more about it.   
The guide covers everything from ‘what is dbt?’ to advanced topics like model refactoring best practices.  
I hope it's useful! **And if you spot anything missing, or ways to make it better, please let me know!**  


Desktop link: [https://count.co/canvas/JpkaYdqr9oN](https://count.co/canvas/JpkaYdqr9oN)  
Mobile link: [https://taylor-count.medium.com/the-ultimate-guide-to-dbt-bad192ab4914](https://taylor-count.medium.com/the-ultimate-guide-to-dbt-bad192ab4914)  


Full disclosure: I do work for [count.co](https://count.co), the canvas in which the guide was built. ",2023-07-05 08:50:09
14hy0vb,What did data engineering teach you about companies and businesses that most people don’t know?,What’s the “insider knowledge” that you now have?,2023-06-24 17:05:16
11c06oj,Any examples of DE projects that you feel are the gold standard for how DE projects should be organized?,"I know this can vary significantly case by case, but mainly thinking in terms of the best structured DE projects that you’ve seen on GitHub.",2023-02-26 00:14:23
1014khn,Why does everyone on this sub hate to use Azure?,Are there any major pitfalls in azure that turns you off..i would like to hear why Azure is not that cool compared to AWS or gcp,2023-01-02 05:38:16
z31651,How are you incrementally testing your data pipelines as you develop them?,N/A,2022-11-23 21:30:46
uim1tl,How to not be exhausted after work?,"This post really isn’t a DE specific discussion. But, I’m wondering if anyone has developed strategies to not feel mentally exhausted after work. Usually after I log off, my attention span is shot and I don’t have the energy to do much except watch TV.

I noticed that not touching my phone all day helps, as well as taking periodic breaks. It’s easier said than done though

Edit: wow, so much great advice! I WFH and blocked out two 15 minute time blocks to lay down and close my eyes (not look at my phone). I also exercised this morning. It’s the end of the work day and I feel great! Thank you all!",2022-05-05 01:33:06
lwb26q,What is your salary and where are you from?,"I’m from San Fran area, I get paid 77,000 base with about 11k in bonuses/benefits. 

Your seniority/years of experience would also provide further Insight

Would love to contrast with other data engineers to figure out a median/average salary.",2021-03-02 19:59:45
163522r,Data teams right now,N/A,2023-08-27 23:02:12
149g5zk,why is Apache Pyspark documentation so...sparse?,"Just curious, whenever I look for examples and syntax, Apache has these one-liners like ""this is what it is and don't ask anymore questions"" lol.

[https://spark.apache.org/docs/3.1.2/api/python/reference/api/pyspark.sql.functions.row\_number.html?highlight=row\_number#pyspark.sql.functions.row\_number](https://spark.apache.org/docs/3.1.2/api/python/reference/api/pyspark.sql.functions.row_number.html?highlight=row_number#pyspark.sql.functions.row_number)

Compared to Pandas docs, for example, which are more descriptive and useful. Thoughts?",2023-06-14 19:06:16
12db1ol,Dozer: The Future of Data APIs,"Hey r/dataengineering,

I'm Matteo, and, over the last few months, I have been working with my co-founder and other folks from Goldman Sachs, Netflix, Palantir, and DBS Bank to simplify building data APIs. I have personally faced this problem myself multiple times, but, the inspiration to create a company out of it really came from this [Netflix article](https://netflixtechblog.com/bulldozer-batch-data-moving-from-data-warehouse-to-online-key-value-stores-41bac13863f8).

You know the story: you have tons of data locked in your data platform and RDBMS and suddenly,  a PM asks to integrate this data with your customer-facing app. Obviously, all in real-time. And the pain begins! You have to set up infrastructure to move and process the data in real-time (Kafka, Spark, Flink), provision a solid caching/serving layer, build APIs on top and, only at the end of all this, you can start integrating data with your mobile or web app! As if all this is not enough, because you are now serving data to customers, you have to put in place all the monitoring and recovery tools, just in case something goes wrong.

There must be an easier way !!!!!

That is what drove us to build Dozer. Dozer is a simple open-source Data APIs backend that allows you to source data in real-time from databases, data warehouses, files, etc., process it using SQL, store all the results in a caching layer, and automatically provide gRPC and REST APIs. Everything with just a bunch of SQL and YAML files. 

In Dozer everything happens in real-time: we subscribe to CDC sources (i.e. Postgres CDC, Snowflake table streams, etc.), process all events using our Reactive SQL engine, and store the results in the cache. The advantage is that data in the serving layer is always pre-aggregated, and fresh, which helps us to guarantee constant low latency.

We are at a very early stage, but Dozer can already be downloaded from our [GitHub repo](https://github.com/getdozer/dozer). We have taken the decision to build it entirely in Rust, which gives us the ridiculous performance and the beauty of a self-contained binary.

We are now working on several features like cloud deployment, blue/green deployment of caches, data actions (aka real-time triggers in Typescript/Python), a nice UI, and many others.

Please try it out and let us know your feedback. We have set up a [samples-repository](https://github.com/getdozer/dozer-samples) for testing it out and a [Discord channel](https://discord.com/invite/3eWXBgJaEQ) in case you need help or would like to contribute ideas!

Thanks  
Matteo",2023-04-06 06:12:06
11izyfy,The SQL Murder Mystery,N/A,2023-03-05 15:38:09
10wd2e0,What is the hype around duckDB that I don’t seem to understand?,"When we have moved to data lakes and data lake house based architecture, why should I care about an OLAP DB? At this point in the data eco system?

I seem to have missed the memo on this one lol",2023-02-07 21:03:03
10awbwj,What are the things that you want to know but it’s too late now to ask…?,"I will go first, how debugging works in IDE and how to use it? 
I’ve been coding for awhile but never debug anything or never thought of doing it…

Edit: what I was trying to mean is that I don’t get why it’s used and I didn’t mean that I don’t understand it…",2023-01-13 14:48:43
ucacvc,Do you think this AWS based personal project would be suitable and complex enough for a resume?,"Hi. I'm looking to add a 2nd personal project to my resume.

I recently created one using Airflow, Docker, dbt, S3, Redshift, and PowerBI. It's not perfect, and totally overkill with regards to the tools I used, but you can find it here: [https://github.com/ABZ-Aaron/Reddit-API-Pipeline](https://github.com/ABZ-Aaron/Reddit-API-Pipeline)

For the next one, I thought I'd take someone else's advice and follow a process like below:

1. Lambda function to extract API data and load into S3 (daily)
2. Lambda function to transform and copy S3 data into Redshift or use Athena to query data without loading into warehouse (daily). 
3. Visualise data with QuickSight or setup an EC2 Instance with Metabase so I can link to the dashboard from my resume (QuickSight only has a 30 day free trial).
4. Use Step Functions or CloudWatch to orchestrate the two Lambda functions
5. Use a bash script or CloudFormation to setup and tear down the infrastructure

Does this make sense? 

I've chosen AWS mainly because I recently passed my Cloud Practitioner Cert and it would be good to further develop my practical knowledge here.

Athena looks interesting, but not sure if running Athena daily to update a dashboard with new data is really its use case. Seem like it's more for occasional runs to analyse log files and that sort of thing, so maybe loading into Redshift and linking it to a BI tool is the smarter decision.

I've looked into services like AWS Glue and EMR. Don't know if anyone on here has experience with these. EMR caught my eye as it might help me develop some Spark knowledge.

All in all I'd like a project that'll help me develop some core data engineering skills and hopefully get my resume past the first stage (I'm still looking for a junior/enter level DE position).",2022-04-26 11:29:34
rnmumx,Kimball vs. Inmon vs. Vault,"This post does a good job explaining the nuances of the three methodologies

https://link.medium.com/Arq8VTkzfmb

TLDR:

Inmon: a stable warehousing strategy where data consistency is the highest priority. All user-facing data marts are built on top of a robust and normalized data warehouse.

Kimball: a dynamic warehouse strategy where quick development of useful data structures is the highest priority. All user-facing data are built on top of a star schema which is housed in a dimensional data warehouse.

Data Vault: a fast and asynchronous warehousing strategy where speed of both development and run time is the highest priority. All user-facing tables are built directly from untransformed source data.

I would love to hear your experience with using any of the three.",2021-12-24 13:57:23
pru6r1,"Data Engineering course - UC Berkeley, Spring 2021",N/A,2021-09-20 12:49:16
nolaf8,How to gain practical experience in Data engineering ?,I am fairly proficient in Python and know a fair bit of SQL. How do I go on from here. How do I gain some practical experience that I can showcase in my resume?,2021-05-30 21:39:55
m7v2tf,Azure Data Factory sucks,"A simple two-step process to hit a REST API, extract the JSON payload, and land it into a data lake takes like 3 hours of meticulous debugging through the illegible, buggy, half-baked mess of a GUI. I swear I need to do a special chant and sacrifice my pet cat to have any hope of getting it to work. Why the hell do businesses use this crap?",2021-03-18 16:46:59
1905tj2,Which tools are worth learning for an aspiring data engineer?,"I'm currently working as a data analyst, but in the near future I'd like to move into a more technical role and delve into data engineering. 

At the moment, I have quite good knowledge of Python and SQL as I work with these languages in my day-to-day job. I also have basic knowledge of general database concepts like normalisation/views/stored procedures etc.

But even looking at the entry-level job descriptions, I feel that I'm still a long way from getting a data engineering job, because the amount of tools required is insane.

And my question is, which of these tools are actually worth my time to learn? For example, are SQL Server tools like SSIS/SSAS important for data engineering? Or is it better to learn some cloud computing concepts? Or maybe something else?",2024-01-06 18:01:08
17qlnmi,What are the skills of an advance Data Engineer?,"What would u consider are the skills of an advance data engineer? This could be a technical or non technical skills.

For me it would be
- CI/CD on data pipelines
- Implementing tests
- Data Quality Checks
- Writing maintable SQL

Share your opinions",2023-11-08 13:29:03
164jb05,Catching cheaters in CS:GO using data,"TLDR, no you can't.   


Sorry for the baiting title, but my colleague wrote an article about extracting data from Faceit API and looking at the data to see if you can get any insights into who's cheating based on the data from Faceit. 

Thought some people might find it interesting. 

[https://medium.com/@twkuhn/analysing-faceit-cs-go-data-14847a3cd2a9](https://medium.com/@twkuhn/analysing-faceit-cs-go-data-14847a3cd2a9) 

&#x200B;

If you're more interested in learning how Valve works to catch cheaters in game, you can check out these two videos:   
[https://www.youtube.com/watch?v=hI7V60r7Jco](https://www.youtube.com/watch?v=hI7V60r7Jco) \- Anti-Cheat for Multiplayer Games

[https://www.youtube.com/watch?v=kTiP0zKF9bc](https://www.youtube.com/watch?v=kTiP0zKF9bc) \- Robocalypse Now: Using Deep Learning to Combat Cheating in Counter-Strike: Global Offensive",2023-08-29 14:00:40
14zpfq8,Do you backup your S3 data?,N/A,2023-07-14 19:19:43
144u8fv,I have a final job interview in a few days and I’m scared.,That’s all.,2023-06-09 03:41:15
135mhee,I'm a one man data team (pretty much). How can I be successful and as future proof as possible?,"In my organization, 1/2 of my responsibilities include pulling data from our financial system via APIs and on-prem SQL Server databases with Python scripts, making transformations if needed, then loading it in Power BI to build table relationships and create dashboards. There is another team member who also helps with just the dashboarding in Power BI. The infrastructure is all me.

Me being on my own, there is no one to bounce ideas off of or for someone to check me if what I'm implementing makes sense but here is where I'm currently at:

* Implementing version control via GitLab to store the Python scripts.
* Going to start creating documentation and an overview of the architecture.
* I'd like to implement a database to store the data then have Power BI read from there. Currently, the financial web APIs are being connected directly to Power BI and gives me no vision on whether a pull is working or if some are failing. It's also making the refreshes take a lot longer with the more connections we make because it's pulling all the data every single time. Still need to test this theory. This would also make me want to implement Airflow for orchestration.
* We deploy the dashboards via the Power BI Service and the file is stored in SharePoint so all we have to do is replace the current file with an updated one and it'll pick up the changes and re-deploy on its own.

... and that's it. Not sure if I am missing anything. I'm trying to learn as much as I can on my own but not sure what I could be missing as I simply might not know. I'd like for someone to come into this role in the future and be able to pick up where I left off.

I'm not a data engineer by the way and these responsibilities fell on me but am grateful to have this opportunity.",2023-05-02 14:16:43
11rm2wy,What has been your career path?,"I know everyone is different but I’m interested to see what jobs most of the Data Engineers in this sub have stopped at along the way to the posit hey are in now.

Example: Help desk -> ? -> ? -> Data engineer(junior/senior/etc…)",2023-03-15 04:31:13
11q6ouz,"Ideal Data Stack - architecture , whats your view?",N/A,2023-03-13 11:06:00
urbsr3,Why there are few data engineering blogs/bloggers out there?,I am wondering why there are few data engineering blogs? Unlike data science for example. (Ps: share your favorite blogs/bloggers),2022-05-17 01:57:39
ryg4e7,"a curated list of docker-compose files prepared for testing data engineering tools, databases, and open-source libraries.",N/A,2022-01-07 19:50:09
qsaxbi,How the fuck did I land this?,"So for context I am a senior in college and was a analytics intern at a Fortune 15 company in my junior year summer. Currently I am taking a graduate level databases class where I have learned a lot but not necessarily what one would consider to be data engineering. I interviewed for one of Microsoft’s consulting company for a data engineer and got the job at an insane 115k a year salary. The position is entry level and in new york and the first 5 weeks will just be training me before I take on my first project. Can anybody give me advice if I should just fake it till I make it? And point me to resources like textbooks and whatnot as to how one becomes a data engineer?

Edit: Thank you, truly for the amazing feedback, will look out into the sources all of you mentioned as well as be as active as I can in asking question!",2021-11-12 13:14:47
n12a23,Have you been able to use your Data Engineering skills to successfully start your own business?,"If so, please share your story.",2021-04-29 11:28:02
mcwast,A short and clean example of how to create memory efficient data pipelines with basic Python generators,N/A,2021-03-25 11:43:53
k3ygzc,What exactly does being proficient in SQL entail?,"Hello there. Pretty sure this is kind of a noob question, sorry about that! I recently transitioned to a role that requires me to be more hands on with data from a role that was more management oriented. Since then I've been beefing up my technical capabilities further - a few months of python, some ETL and SQL.

I keep reading on here about proficiency in SQL, and considering how that in itself comes with a lot of other DB related skills - I was wondering when I can decide that I am in fact, proficient? What exactly does proficiency in SQL entail?

Thanks v much! :)",2020-11-30 15:56:24
1bpjpcj,Follow up to last post on Management shaming me for not knowing Data engineering,"Crux of last post: I am a data analyst with skillsets on descriptive stats and dashboards, Management is shaming me for not knowing Data engineering.

 **Latest Update: The customer insists on a synapse setup, So my  manager tried to sweet talk me to accept to do the work within a very  short deadline, while masking the fact from the customer that I dont  have any experience in this. I explicitly told the customer that I dont  have any hands on in Synapse, they were shocked. I gave an ultimatum to  my manager that I will build a PoC to try this out and will implement  the whole setup within 4 weeks, while a data engineer will be guiding me  for an hour/day.  If they want to get this done within the given  deadline ( 6 days) they have to bring in a Data engineer, I am not  management and I dont care whether they get billing or not. I told my  manager that if If they dont accept to my proposal, they can release me  from the project.** ",2024-03-28 01:51:19
1amq8hs,What is the hardest interview question you got asked?,Drop the hardest interview questions you had,2024-02-09 15:08:14
17m0ioz,LeetCode for Data Engineers?,"I've been thinking about it for quite a while now.   


What is the alternate for Data Engineers when it comes to upskilling and showcasing their skills.   
Like, Developers usually have coding questions like Leetcode, Codeforces etc. 

What do the DEs have to practice or work on?   


I've seen few companies ask LC questions as well in interviews for DE, Analyst etc and these companies are legit Fortune 500 ones. ",2023-11-02 10:35:06
13lppxu,Any Data Engineering Podcasts?,"Want to ask if there are any data engineering podcasts to listen while working :) 

I am willing to listen more opinions and discussions about emerging trends on Data & AI as well ! 

Thanks <3",2023-05-19 08:50:38
138l4yc,How Do You Bring Your DE Team Out of The Shadows?,"Asking this question to open up a general discussion about how do we bring the DE team under the spotlight. The work done by DE team is critical to support the dashboards/ ML Models but rarely do we see recognition for the DEs. 
What are some of the ways through which you are handling this at your org?",2023-05-05 13:04:49
12gdiok,Anything else to read,N/A,2023-04-09 08:36:13
11s8x0q,I'm not getting it...what's the point of DBT?,"I've got the below synopsis of dbt...

 

1. Modularity and Reusability: DBT allows for the creation of reusable and modular data models, which can be easily shared across different projects and teams. This makes it easier to maintain consistency across the organization and reduces the duplication of effort.
2. Version Control: DBT integrates with version control systems like Git, allowing data engineers to track changes to the data pipeline over time. This feature is crucial for maintaining data integrity and auditability.
3. Testing and Validation: DBT includes built-in testing and validation features, which enable data engineers to test data pipelines and ensure that the output is accurate and consistent with the input data.
4. Collaboration: DBT's collaborative features allow multiple team members to work on the same project simultaneously, which increases productivity and reduces the likelihood of errors.

&#x200B;

...but I still don't get what the point of it is. What am I missing?",2023-03-15 20:53:15
zlvtte,"Recently laid off, please review my Resume and throw me some hard truths! Interested in Data Engineer/Science roles. Thanks! (~1 Year of FT experience)",N/A,2022-12-14 16:46:15
yq3nwa,Hot Take : QA is the most tedious part of Data Engineering,"I've noticed with most big Data Engineering projects that a whole multitude of things can go wrong in the development phase. Data types don't match, tables get renamed or disappear all together , new columns get added so the bad Data Engineers before you who used SELECT * instead of listing out every column explicitly inadvertently break your pipelines months later. And many many other things.

The QA processes for data work seem a bit more tedious than other types of development I've done before. Right now we are in the process of fine tubing our QA methods , I've made quite a few helpful views and stored procedures that print out key information on certain tables but it's still very much a manual process. I'm trying to use dynamic SQL with input parameters to automate as much of it as I can but in general I try to avoid dynamic SQL due to performance issues particularly with large data sets.

How does everyone else feel about the QA process ? Love it, hate it, equate it to the 7th circle of hell?",2022-11-09 00:37:10
xxuodv,Do programmers like it when someone asks you to code on spot while they are watching?,"I had an interview today via zoom and out of the blue the interviewer asked me solve a coding problem suddenly on the spot while sharing my screen to him. I found it very uncomfortable. The question wasn't that hard but I couldn't focus because I was being watched and judged by someone. He wouldn't even let me Google until I said I need to Google! Anyways, I didn't solve it because I was pissed off already and didn't feel like. What do you coders think?",2022-10-07 09:37:53
k1tqbs,Thinking of starting a Youtube channel on Data Engineering. Ideas?,"I've been working for more than a year in a data engineering team, and have been learning a lot of stuff in this domain - primarily Hadoop, Spark, data pipelines, etc. Recently I've been thinking of starting a Youtube channel wherein I'll stream while making a project or while how to learn some technology or something like that.

Suggestions and ideas are welcome.",2020-11-27 03:46:24
1at7mdg,Update to interview posts,"After careful consideration and listening to your feedback, we've decided to no longer allow interview-related posts because they take away focus from our community's main purpose.

In the past, although they usually weren't directly related to data engineering we've allowed interview posts like ""What are interviews like at XYZ company?"" or ""What should I prepare/study for XYZ position?""

These questions are more often than not either too difficult to meaningfully answer or have already been answered many times. Similarly to resume reviews, we will no longer be allowing these types of posts and instead point users to other resources that are better suited and focused on answering those questions like Glassdoor and Blind.

Thank you again to everyone who has been providing constructive feedback on this topic. We know it may feel frustrating to see the same type of content and it may not feel like progress is happening but it just takes time to carefully review these changes and hear all opinions. We appreciate your patience and for helping shape this community.",2024-02-17 17:44:48
16p1v2j,Anyone migrating away from Snowflake and back to AWS?,When I migrated to Snowflake my costs went up like 5x. Maybe more.,2023-09-22 05:21:39
149fhb7,1 year since I started data engineer and I found the job of my dreams while you guys were gone 😭,"Started a year ago with microsoft exams, started a minimum wage job doing DE and have been for 10 months and I've been offered an amazing job actually helping people and also exploring analytics/datascience and other stuff too. 

Complete DE freedom to engineer and explore and find ways to help people, bonus is its 1.5x my salary and offers senior level relatively quickly. 

I've always struggled and felt like an imposter but in such a short time I've come far and I can't wait to learn more. 

I suck at doing off job projects, I prefer having  data shoved in my face and told to fix or do something with it!

Had a rough year but this has worked out amazingly and I'm thankful for everyone's support!

(Sorry if it's slight brag)",2023-06-14 18:39:01
138vov9,Is it me or is the AWS Glue documentation kind of bad,"I'm going through the AWS Glue scripts of my company and I see the module ""awsglue"". What's inside it? I go in circles in their documentation and I search for it and I cannot find any details on it. I google ""awsglue documentation aws glue"" and find nothing. It's only when I look at the pip page do I see a link to the Github which lists the classes in it. 

&#x200B;

I see a ""DynamicFrame"" class in the Github page, so I go to the API reference in the documentation. Its  table of contents is massive and impossible to scroll through. I Ctrl-F ""DynamicFrame"". It doesn't exist. I search ""DynamicFrame"" in the search bar, and the documentation is put in some separate area called ""PySpark Extensions"" Why is this not in a central location?

&#x200B;

So now I want to find details on the ""Job"" class because its in the script im studying. Where do I go? There's no central documentation. I have no idea which section of the website to click to. I search ""Job"" in the search bar, and ofc, I can't find any details because the word ""Job"" is in EVERY SINGLE PAGE

&#x200B;

Like, i've been having issues finding stuff on documentation my whole life so part of this is probably my fault but still ahhhh im frustrated",2023-05-05 17:35:14
zc6lvq,What are some of the big problems Data Engineering currently faces / will face in the next 5-10 years?,"Basically title - I've been thinking about this a lot lately and am curious to hear others' thoughts.

So far my main thought patterns have been along the lines of:

1) Ways of working. In organisations I've been exposed to the ways of working for data teams have been all over the shop. Some are trending towards working more like SWE teams, some act as project teams, and some (most common IME) seem to exist on their own island inside an organisation. Does this create a risk that teams will by and large be working non-optimally?

2) Businesses not knowing what they want out of their data. This I feel is pretty self-explanatory - if the business doesn't know what they want is there risk of decision-makers opting not to utilise the data, making data work redundant?

3) Technology crossover and ambiguity. There is seemingly an unlimited number of technologies that can be utilized for various functions, with no clear use cases for which is best in what situations. Also, as technologies have expanded they have sought to expand functionality into other use cases, which further increases the permutations possible. Per examplur, for data orchestration there is ADF, Step Functions, Airflow, DBT, Flyte, Cloud Functions, Luigi, etc., etc. How can a business case be made for any of them without thorough testing, which is expensive and time-consuming?",2022-12-04 10:45:17
wgefn6,"Thoughts on the resume? Been looking for 6 months & no offers I'm happy with, yet.",N/A,2022-08-04 22:38:34
w7qzs6,"""The best"" Data Engineering BootCamp","Hi all, looking to transition from automation engineering into data engineering and was hopping to borrow wisdom selecting ""the best"" data engineering bootcamp.

There is a lot info online and frankly it feels a bit overwhelming to select the top tools that are being used or the ""right"" skillset, therefore was hoping that a bootcamp will help me out to direct/guide me through  the process of transition.

At the moment following this roudmap:

[https://c-nemri.medium.com/your-2022-data-engineering-roadmap-3bfe6691ec50](https://c-nemri.medium.com/your-2022-data-engineering-roadmap-3bfe6691ec50)

[https://dataengineering.wiki/FAQ/What+skills+do+I+need+to+become+a+Data+Engineer](https://dataengineering.wiki/FAQ/What+skills+do+I+need+to+become+a+Data+Engineer)

&#x200B;

Apologize if this topic has been covered and thank you all for taking your time,",2022-07-25 14:54:27
po7imu,Is Python + SQL + Spark enough to enter DE role?,"I am coming from database role and wondering what skills I need to learn, thank you in advance",2021-09-14 17:34:09
p2i0po,Loading 261GB of reddit comments into Snowflake,N/A,2021-08-11 17:40:10
1br2c2e,What would you learn if you were to start all  DE again ?,"What would you start learning with, what skills , technology , certifications and toolset would you learn ? Would really love your input as I am starting all over 

I am someone who has internship DA experience and AWS CCP and Tableau Experience 
 ",2024-03-29 23:06:04
18rjrrf,"Are data engineers, architects and programmers in general getting older?","Maybe it's the recent lay offs and hiring freeze for graduates but I've had two different jobs at 2 different companies in the last 2 years were I was easily the only person under 40 in the team.

Has anyone else noticed this?",2023-12-26 22:12:53
170otm0,Microsoft Fabric: Should Databricks be Worried?,N/A,2023-10-05 18:06:29
10iq2pk,Layoffs for Data Engineers,"Its been a blood bath of layoffs for tech companies as of late and it clear we are headed towards a recession. 

I'm curious to hear various thoughts and the subreddits observations as it pertains to Data Engineers / Data Engineering. Have you been affected by the recent layoffs? Have you observed DE teams being affected? My first instinct is that Data Engineers are such an important role to any company that they are largely insulated from these types of events but then again, you honestly never know. 

Please share your experiences",2023-01-22 18:13:31
y2r6ml,"Celebrating my first Data Engineering Project -- Fitbit data with PySpark, GCP, prefect, and terraform!","Hello!

I've been trying to learn about data engineering concepts recently through the help of this subreddit and the data engineering Zoom-Camp. I'm really happy to say I finished putting together my first functioning DE project (really my first project ever :) ) and wanted to share to celebrate/ get feedback!

[Fit-pipe DE Project](https://github.com/rickyriled/data_engineering_project_1)

The goal of this project was to just get the various technologies I was learning about interconnected, and to pull in/transform some simple data that I found interesting with them -- specifically, my fit-bit heart rate data!

In short, terraform was used to build a data lake in GCS, and then I scheduled regular batch jobs through a prefect DAG to pull in my fitbit data, transform it with PySpark, and then push the updated data to the cloud. From there I just made a really simple visualization to test if things were working on google data studios.

https://preview.redd.it/24wnijf2mit91.png?width=1566&format=png&auto=webp&s=b9b48e19ac442b5070c0fe07e1dd4fb17b20f3d1

Ultimately there were a few things I left out due to issues with my local environment/ a lack of computing power; e.g. airflow running in docker was too computationally heavy for my MacBook air, so I switched to prefect; and various python dependency issues held me back from connecting to big query and developing a data warehouse to pull from.

In the future, I wan't to try and more appropriately use PySpark for data transforming, as I ultimately used very little of what the tool has to offer. Additionally, though I didn't use it, the various difficulties I had setting up my environment taught me the value of docker containers.

&#x200B;

&#x200B;

I wanted to give a shout out to some of the repos that I found help in/ drew inspiration from too:

[MarcosMJD Global Historical Climatology Pipeline](https://github.com/MarcosMJD/ghcn-d)

[ris-tlp adiophile-e2e-pipeline](https://github.com/ris-tlp/audiophile-e2e-pipeline)

[Data Engineering Zoom Camp](https://github.com/DataTalksClub/data-engineering-zoomcamp)

&#x200B;

Cheers!",2022-10-13 06:16:27
svx88j,Data Engineer Salaries - Did they suddenly jump up?,"I'm starting to wonder if I'm under payed. I truly don't know so wanted to get everyone's opinion.

First of all: our team lost alot of people in 2021. When we attempted to hire replacements we had a very very difficult time. Candidates were asking for the same salary that our senior director was making, even with only a few years of experience. 

True senior level engineers were almost nowhere to be found.

I also was contacted by a former manager and was told he just got a 40% pay increase moving to a new company.

I did some googling just now on average salary for senior data engineer and found the following:

1. Glassdoor - 125k
2. Builtin - 131.5k + 14.5k bonus
3. Zip recruiter - 134k
 
These numbers appear to be about what id expect for a senior data engineer working in an average cost of living area.

However, I wouldn't expect websites such as these to keep up with sudden rapid wage inflation for such a specific role.

So. What do you think? Is there crazy wage inflation for data engineers right now? Or do these websites have it about right?",2022-02-19 00:50:17
m0mx2g,Ask Martin Kleppmann - the author of Designing Data-Intensive Applications,"Martin is answering our questions! We've already asked:

* Should a start-up immediately plan applications for data-intense use cases?
* Should we read the book if the cloud providers already implement everything?
* Are relational databases suitable for banking systems?

And other things.

Join us and ask Marting questions! More info: [https://datatalks.club/books/20210308-designing-data-intensive-applications.html](https://datatalks.club/books/20210308-designing-data-intensive-applications.html)",2021-03-08 18:52:39
l9anov,Building a personal data warehouse in Snowflake for fun and no profit.,N/A,2021-01-31 11:35:27
k0faxp,Introducing Amazon Managed Workflows for Apache Airflow,N/A,2020-11-24 22:27:27
18tjgp8,Zendesk Moves from DynamoDB to MySQL and S3 to Save over 80% in Costs,N/A,2023-12-29 09:43:54
180lef1,How is Airflow really used in real practice?,"Hello everyone, I'm currently learning about orchestrating data pipeline with Airflow. With all the free resources I have seen, it seems that a majority of the demos are showing developers to save the python code in the `dags/` folder and then call PythonOperator to run the task. Note that the python code would not only include the Dags and tasks code, but they also have the actual ETL codes in there. To be honest, I am not usually comfortable in jamming codes for different purpose in one .py file because I prefer separating codes for ETL from codes that defines a DAG and tasks in other places.

So, some questions I have, maybe a stupid question, are:

In real practice, are we really saving the codes in the `dags` folder?  

Say that I have a function that does simple data cleansing with pandas. in real practice, should I just define a function in a .py and then also define the dags and tasks in the same python file? Would the `dags` folder be flooded? Is there any ways to just save the dags and tasks codes in the dags folder, but import the actual ETL codes from other .py files saved in another folder?

I feel the question may be leaning towards a 'no', but what I'm looking to learn is the \`why\` and \`how is that being done in real practice\`.

&#x200B;

Thanks!!!",2023-11-21 16:47:37
1570ujm,What are some common data engineering mistakes you’ve seen in your career?,Related to any stage of the process. Or anything at all?,2023-07-23 01:05:20
14ufjuc,Why are there no junior or regular data engineer positions?,"Current data analyst and getting into engineering for the first time ever. Have never worked as a DBA or engineer in any capacity, but extensive experience working with data, SQL, data sets, etc. Would like to learn within my current company how data engineering works and learn the ropes. But there is no regular data engineer position, no junior positions. The only ones I have ever seen are senior data engineer. Why is this?",2023-07-08 21:11:30
14e7vaz,Polars cookbook (Jupyter),"I went through an old pandas cookbook and repurposed it to use polars instead. I found it pretty useful when I was learning pandas years ago, so I hope that someone learning polars might find this useful as well.

[https://github.com/escobar-west/polars-cookbook](https://github.com/escobar-west/polars-cookbook)",2023-06-20 10:53:05
13d8rlr,What is that one skill you would wanna master at work?,"Just got my first job as a DE, am super pumped!! I am trying to learn as much as I can before I start the job. Wanting to start this discussion:

If there are only one/two most important skills you could master at DE work, what would they be?",2023-05-09 22:47:42
1191ia8,what does your company's current data landscape look like? Which tools and technologies did you go for and why?,"We are currently on Azure datafactory (orcestration)  + Azure SQL database (ETL done using procedures + presentation layer). We tested databricks and liked the functionality so are utilizing that for newer ETL development. The company has decided to go to AWS so now we are exploring options there. 

So my question to you would be which orcestration tools, databases/data warehouses, CICD tools are you using and why?",2023-02-22 14:48:21
10dr0r4,"For engineers who primarily work in a data warehouse environment mainly using SQL (and very little python), what are your career goals?","I'm an analytics engineer who is confused on what to learn next or where to take my career, so I am curious about like-kind professionals who are in the same boat. Have used python but 95% of my day is spent performing transformations using dbt in a cloud data warehouse environment.",2023-01-16 20:41:16
zy8lrb,AWS,"Hi, I got a job as a data engineer. I am comfortable in SQL and python. But, no aws exposure. My upcoming team lead emphasised on learning cloud services. From where do I start? and what is the scope?",2022-12-29 15:52:46
up583y,Imposter Syndrome,"I recently joined a company as a Data Engineer, and I constantly feel like I don't know enough weather it's fundamentals of Python, Data Structures, Cloud Computing Services, Linux, and most importantly hands on experience. I'm going through many courses and not retaining much. Is it normal to feel like you don't know enough or you will not be able to do the projects or tasks given to you? Any personal experience stories?",2022-05-14 00:19:19
q9o6aj,Offered DE Job... Feeling under-qualified,"A recruiter reach out to me over Linkedin a couple weeks ago for a Data Engineer position at a startup. I wen through the interview process which is pretty short, only 2 interviews and one short take home assessment. They tested some basics stuff like SQL and a bit of Pandas. I apparently did well on those and now they offered me the job. The problem is that I feel super under-qualified for job. 

I have been working as data analyst for 2 years but I have mainly used SQL and a small amount of python. I have no experience with things like Airflow, Spark and I am pretty clueless about data structures and algorithm. 

Do you guys think I should take this job? Why am I even offered this job when there are so many qualified data engineers out there?",2021-10-17 00:35:31
isl3a6,Diary of a Data Engineer,N/A,2020-09-14 13:43:10
18hgf31,What are some things that have radically improved your data engineering skills?,"Other than ""creating data pipelines over, and over again"" or ""practicing"", what are some things that have radically improved your data engineering skills?

e.g.

* Making each discrete phase of a given pipeline dumber (i.e. simpler, easier to deliver, easier to replace)
* Starting with a dumb Python script almost every time, and making it more robust over time
* Just getting started, and making things pretty later
* Using a particular data pipeline orchestrator because <x>
* Analyzing data a particular way before getting started
* Creating documentation <y> before getting started
* Sketching data composition and lineage out with pen and paper
* Sketching out your data pipeline with pen and paper
* Rendering your data pipeline using GraphViz and DOT markup, etc. so you can better visualize the structure of your data pipelines

EDIT: if this is too vague, or a bad fit, I'll delete it, just let me know - AFAIK how you build things and the process by which you decide to build things is just as important as what tools you use to build those things - i.e., software architecture matters when developing software - but, I don't know - just, let me know.",2023-12-13 13:07:42
15hhvj3,Can washing machines be used for parallel processing?,"Seriously, Spark is built on top of Scala, which runs on the JVM. JVM compiles code to byte code which is readable to anything that has a processor in it(?). Washing machines do have a processor of some sort.

So, can I theoretically use my washing machine to do some work besides, well, washing.",2023-08-03 22:16:43
13xldpd,Quarterly Salary Discussion - Jun 2023,"This is a recurring thread that happens quarterly and was created to help increase transparency around salary and compensation for Data Engineering. Please comment below and include the following:

1. Current title

2. Years of experience (YOE)

3. Location

4. Base salary & currency (dollars, euro, pesos, etc.)

5. Bonuses/Equity (optional)

6. Industry (optional)

7. Tech stack (optional)",2023-06-01 16:00:29
13u5lly,Are SQL query optimizations skills important for an exceptional data engineer?,I am sure most people in DE do have sql knowledge. But how important is to know how to optimize an sql query? Is it something exceptional with very few people with those skills or those skills are as widespread and normal as other sql staff?,2023-05-28 17:09:37
12knal1,Interviewer wants me to go into detail about current company's architecture,"As the title says. 

I'm based in the UK and interviewing for a well known company.

I've been provided an outline of the interview and in it they want me to prepare a diagram of my current company's data architecture and spend 10-20 mins explaining it.

I don't know if it's an odd request or not - I understand wanting to test my knowledge around architecture but it still feels odd.",2023-04-13 12:23:49
119s7yv,Is dbt really necessary?,"dbt is getting pretty popular recently, but is it really that “necessary”? I mean what are the added benefit of introducing new tool when you can do all transformations using python (polars, duckDB…) + in python you can also do the “extract” step so basically you are able to cover entire ETL lifecycle with one tool? Also you can unit test your code better. As python disadvantage I see the dependency management. The only advantage of dbt I can see is you do not have to explicitly create tables as it creates it for you.",2023-02-23 08:02:08
10seqay,Using Polars over Pandas or PySpark,"I've been refactoring some code lately from using Pandas to Polars and I'm just blown away by the speed increases on locally run tasks (I'm running python 3.11.1 too which probably helps a wee bit as well).

I'm yet to employ it for work but it looks like the perfect library for AWS Lambdas or GCP Cloud Functions where billing is by the millisecond.

I've also noticed that Polars is much more similar to PySpark than Pandas with some of the naming conventions, lazy execution function-chaining etc, and can't help but think that's the benchmark or 'target' for lack of a better term.

The documentation is really coming together well and I'm just looking for stories or experiences of polars being used by DE's in industry.

I think it's incredible but of course, in our game adoption is everything.",2023-02-03 07:52:53
yzmsyw,How do you keep yourself updated about latest DE news?,"Do you have websites you check daily like you'd read a newspaper? Do you have a strategy to keep yourself updated? I can't really find an efficient way to keep myself documented on new technologies, major bugs on tools we use daily etc.",2022-11-19 21:36:41
y6n37d,"SQL Workshop recording: Making SQL more efficient, readable, and easier to debug","[Ergest Xheblati](https://www.linkedin.com/in/ergestxheblati/) is the author of [*Minimum Viable SQL Patterns*](https://ergestx.gumroad.com/l/sqlpatterns), and he's spent the last 15 years mastering SQL.  

In this [SQL workshop](https://youtu.be/UFiZx5NlzL4), Ergest teaches various SQL principles (patters) to help you take your SQL skills from intermediate to expert. More specifically, you'll learn about:   
🎯 Query composition patterns - How to make your complex queries shorter, more legible, and more performant   
🎯 Query maintainability patterns - Constructing CTEs that can be reused. In software engineering, it's called the DRY principle (don't repeat yourself)  
🎯 Query robustness patterns - Constructing queries that don't break when the underlying data changes in unpredictable ways  
🎯 Query performance patterns - Make your queries faster (and cheaper) regardless of specific database you’re using.

Toward the end of the workshop, Ergest answers over a dozen questions from SQL professionals all over the world.  

Watch the full workshop 👉 [**here**](https://www.youtube.com/watch?v=UFiZx5NlzL4) 👈. It's free, and don't worry, you're not being sold on a SaaS product 🤣  


If you enjoyed this video, please join Ergest and +1,000 like minded data professionals in our Slack Community, the [OA Club](https://www.operationalanalytics.club/)! We're creating content like this all the time!",2022-10-17 21:20:34
tmdqlo,Data Lake vs Data Warehouse,N/A,2022-03-24 14:28:22
rja8s6,What is Kubernetes used for in data engineering?,Im curious as do why people use kubernetes in this field.,2021-12-18 15:54:03
laibru,Databricks raises $1B at $28B valuation as it reaches $425M ARR,N/A,2021-02-02 00:06:17
18x2214,Optimizing a One Billion Row Challenge in with Rust and Python with Polars,"I posted this on /rust and I thought /dataengineering might find it interesting! 

I saw this [Blog Post](https://www.morling.dev/blog/one-billion-row-challenge/) on a Billion Row challenge for Java so naturally I tried implementing a solution in Rust using mainly polars.[Code/Gist here](https://gist.github.com/Butch78/702944427d78da6727a277e1f54d65c8)

Running the code on my laptop, which is equipped with an i7-1185G7 @ 3.00GHz and 32GB of RAM, but it is limited to 16GB of RAM because I developed in a Dev Container.  Using Polars I was able to get a solution that only takes around 39 seconds.


|Implementation|Time|Code/Gist Link|
|:-|:-|:-|
|Rust + Polars|39s|[https://gist.github.com/Butch78/702944427d78da6727a277e1f54d65c8](https://gist.github.com/Butch78/702944427d78da6727a277e1f54d65c8)|
|Rust STD Libray|19s|[Coriolinus Solution](https://github.com/coriolinus/1brc)|
|Python + Polars|61.41 sec|[https://github.com/Butch78/1BillionRowChallenge/blob/main/python\_1brc/main.py](https://github.com/Butch78/1BillionRowChallenge/blob/main/python_1brc/main.py)|
|Java [royvanrijn](https://github.com/gunnarmorling/1brc/blob/main/calculate_average_royvanrijn.sh)'s Solution | 23.366sec on the (8 core, 32 GB RAM) |[https://github.com/gunnarmorling/1brc/blob/main/calculate\_average\_royvanrijn.sh](https://github.com/gunnarmorling/1brc/blob/main/calculate_average_royvanrijn.sh)|

Thanks to @[coriolinus](https://www.reddit.com/user/coriolinus/) and his code, I was able to get a better implementation with the Rust STD library implementation.  Also thanks to @[ritchie46](https://www.reddit.com/user/ritchie46/) for the Polars recommendations and the great library!",2024-01-02 22:13:42
16cnpvq,"Accepted Data Eng offer, seems more like BI Engineer","My company is a large enterprise that sells automotive data. I recently accepted a role as DE and have sat around for 6 weeks without training or understanding what my role is.

I’m now learning that they want me to be the admin for their BI tool (Microstrategy) and that I will not be doing any work on any cloud platforms and I will just be doing reporting and sql. Zero Python, infrastructure, data modelling, or scripting.

Is this common? I feel like I’m being trained on a useless, outdated tool that won’t progress res my career whatsoever.",2023-09-07 19:23:48
12ctygq,Spark/databricks seems amazing?,"Looking at some foundational components for a new data platform. Looks like data lake + Spark is just all you need. Spark as a compute engine with its different APIs seems extremely powerful. 

What are the drawbacks? If self hosting I guess it's what comes with self hosting anything. If using a managed service like databricks I guess it's cost? 

Any insights?

EDIT: Looks like there's a DBT connector for Spark. Has anyone used the two in combination? I've used dbt with RS, BQ, and SF but not Spark.",2023-04-05 18:47:33
x90yf4,How do i become a data engineer?,"Hi everyone!

I'm looking to get some advice on how best to go from my current situation (zero experience and skills in data engineering) to getting a junior position as a data engineer. I've been doing my best to understand the industry/job, and would like to take the leap as it seems that data engineering is 1. in high demand, 2. growing quickly, 3. sounds like a challenging and interesting job and 4. is better paid than almost every other computer science field.

I have a computer science degree and a modest amount of professional experience in front end web development and game development, but zero experience/skills in backend and data engineering. I'm not entirely hopeless with backend and data engineering as i learnt a bit during my degree, but i figure i should just start from scratch. So where to start? Are there any good Udemy or online courses i should do? Are there any certificates or extra qualifications i should get?

I'm currently working full time as web developer and have 2 children to feed, so internships and unpaid work is probably not possible for me. I think i can manage to dedicate 10-15 hours a week to purpose of getting skilled up for a data engineering role.

What would you guys do in my position?",2022-09-08 13:46:19
skrkoj,What is difference between data warehouse and data lake,What is difference between data warehouse and data lake? Please elaborate with examples and in simple layman term.,2022-02-04 23:20:12
rymcs8,M1 macs are still riddled with compatibility issues,"I posted a while back asking about macs for data engineering, but neglected to specify that it is an m1 mac. After spending my first week troubleshooting tons of errors due to mismatching architecture in my packages, I would say to avoid m1 Macs for any local development unless you like spending weeks fiddling with every package",2022-01-08 00:19:42
mob52r,Data Engineering Landscape in 2021 with Tobias Macey,N/A,2021-04-10 19:19:55
17rpfn3,SQL versus Python?,"Though I am a data engineer by trade, I am often find myself as the in-house de facto data analyst.

My own experience has been that I often use about an equal mix of SQL and Python to work on data analysis requests. But it’s not set in stone. For quick requests, I usually go with SQL. For more complex ones, I will rely on Python and Jupyter notebooks.

I am the only one in this situation? Do you go for SQL, Python (R?) or some other language?

Just wondering. Thanks.",2023-11-09 22:51:42
15la6wi,Just got certified! - Databricks Certified Data Engineer Associate,"Just got certified! I am happy to say I have completed the Databricks Data Engineering Associate certification. 

The exam wasn't as difficult I expected it be, primarily revolving around the Databricks platform as expected. The exam focused on concepts like Delta, Multi-hop architecture, Repos etc. Some coding questions on very basic SQL syntax(CTAS, create views etc.) nothing too out of the ordinary. 

I'd suggest taking the certification, it's not a difficult exam nor does it take too much time(about 10 days of studying). 

The resources I used are:  
1. The Databricks Data Engineering course (Free): I used my customer account, anyone can sign up for it, there's even a 2 week trial. I'd suggest downloading the .dbc files and uploading them to the community edition workspace and playing around. That's what I did!

2. Udemy Courses: [https://www.udemy.com/course/databricks-certified-data-engineer-associate](https://www.udemy.com/course/databricks-certified-data-engineer-associate/) \- was just brilliant. The course isn't too long, the instructor condenses the information really well. Overall pretty good imo 

3. Practice Tests:

1.  [https://www.udemy.com/course/practice-exams-databricks-certified-data-engineer-associate](https://www.udemy.com/course/practice-exams-databricks-certified-data-engineer-associate/?expanded=1014944232) \- was good to identify weak areas and revisit them 
2. [https://www.udemy.com/course/databricks-certified-associate-data-engineer-practice-tests](https://www.udemy.com/course/databricks-certified-associate-data-engineer-practice-tests/?referralCode=102E37D6BA7C7B8B5532) \- really good practice tests, the questions largely resembled the actual exam - (*70% of the actual exam questions*). Only practice test needed (wasted a lot of money on other tests). 

4. YouTube Resources: 

1. Advanced Analytics: Really good to find videos on alot of concepts - imo he breaks down concepts really well, but doesn't do a deeper dive. [https://www.youtube.com/@AdvancingAnalytics](https://www.youtube.com/@AdvancingAnalytics/videos) 
2. Stephanie Rivera: Okay, this is actual gold in terms of knowledge. She uploads the paid skill-builder series from Databricks to YouTube (though I'm not sure how accurate this is; a buddy of mine works at a company that has access to these, and he says they're the same). This is extremely useful for gaining in-depth knowledge. [https://www.youtube.com/@stephanieamrivera](https://www.youtube.com/@stephanieamrivera/videos) ",2023-08-08 06:51:07
12jt9hl,What I have learned by solving (almost) all the SQL problems in Leetcode,"Hello,

I have written my first blog post in medium about what I have learned solving leetcode SQL problems. Shared some tips for those who want to start solving SQL problems too.

Also added some resources in the post which might be beneficial for you.

Happy reading!

[https://medium.com/@iamrafiul/what-i-have-learned-by-solving-almost-all-the-sql-problems-in-leetcode-670b8a2cb32e](https://medium.com/@iamrafiul/what-i-have-learned-by-solving-almost-all-the-sql-problems-in-leetcode-670b8a2cb32e)",2023-04-12 17:33:18
11t4j09,"Tracking the Fake Stars Market with Dagster, BigQuery and dbt",N/A,2023-03-16 19:59:19
zixrj6,"Data engineers, how do you approach ambiguity in your work?","I get a lot of vague questions at work, for example ""create a dataset so that so-and-so team can do this-and-that"" with little to no context. It took me a while to navigate this kind of ambiguity, and I'm not sure whether I'm there yet, but there are a few things that I do

1. **Ask clarifying questions**. Many times stakeholders have no idea what they're asking for and how they want the result to look like. It's worth getting into conversations with them because there's a chance that you can jointly agree on an end result that might look nothing like what they initially imagined.
2. **Start small, scale later**. I have analysts requesting data going back 5 years, but I know from experience that more recent data (going back 12 months) is actually better for answering business questions. Our product has evolved so much over the last years that the data collected pre-COVID looks nothing like the data we collect today, so analysts should be able to work with recent data, which I can readily provide. After I have done that and the analysts have worked with it, we jointly evaluate whether it's still necessary to use historical data. And no sooner than that do I actually look at that data far back.
3. **EDIT: Look at what's been done before**. I find that many DEs are more eager to reinvent the wheel and build something from scratch, rather than deal legacy codes and systems. However, I've found that it helps to at least pause and consider existing tech before deciding whether a new solution is really required. Oftentimes I've found that legacy stuff isn't actually all that bad, and even if I can incorporate even just 10% of it in the new solution, therefore saving myself some time and effort, I consider that a win.

How about you, how do you navigate ambiguity in your work as a data engineer?",2022-12-11 16:07:45
z0srs8,Why FAANG+,"Can anybody please explain to me why people are obsessed with working for FAANG+? These companies are notorious for being morally dubious, having shitty working environments and high burnout churn. Plenty of more attractive options out there. Just curious as I see a lot of posts on here and other subs with people asking for FAANG+ specific interview prep advice and it just seems odd. You wouldn't want to work for SPECTRE would you, very passé.",2022-11-21 07:34:27
xv0lfs,"Been working as a Jr. DE, but worried I am at a dead end.","I was hired last year in October as a Junior Data Engineer ( they have since removed the Junior portion, without a pay raise). I got lucky with random applications to places that looked like I could meet the qualifications before graduating with a Bachelor's.

I am getting paid 75K, pretty much no oversight, and no one looking over my shoulder and harping me if I happen not to get much physically done in a day. Pretty chill job, and I work fast and get many different things done. And my direct superior really doesn't care if I feel like working at home on my 'Office' days.

The problem I am having that has been slowly eating away at me for the past two months is that there is too much freedom and insufficient structure.

* I am the only person that knows any programming languages, let alone python (which is what I am using)
   * I am not counting SQL; my direct superior does know SQL very well.
* There is **NO ONE** to check my work.  - 
   * I have written multiple scripts as ETL pipelines to the best of my ability to grab data that the company has access to but no way to incorporate into their reporting--And they work...
   * I have no superior in the job I was hired to do.
   * I have learned **LOADS,** and I feel like the speed at which I have learned python has been, but when I see things about good coding habits or good ways to go about a problem, I get a little upset that I am the only person at my job. 
* There is no version control in place in any sort of usable form.
   * There is a GitLab, but my direct superior isn't quick on the commits, and he doesn't use it.
   * I have been using git on my computer for the last two months to get the habit down, at least for my work.

I have so much leeway and so much slack at this job. I don't want to be taking it for granted, and I am working hard and trying to be the most useful I can be. But, the freedom is going to come back to bite me, I know it. I am going to flounder if I go somewhere else with actual structure.

My superior is a vocal advocate for me and constantly encourages me to learn more techniques, programs, and platforms, if not for this job, then for the next job. But, it is just him and me, and he can't help or mentor me on the job I was hired for.

I am keeping my eyes peeled for potential next jobs, but I feel like I am less confident in my ability to get hired than I was before this job. I have no idea how to gauge where I am professionally, and I am not sure what job title to look for next. 

I know if you were to ask my superior how I was doing, he would say I am doing great, but I feel like I'm crashing. I don't know if it is imposter syndrome, burnout, realizing I made it to the deep end without knowing it, or something else. 

It just feels like the more knowledge and skills I gain, the less I know what it is I am doing.

If I need to put this down to a question or two here is it.

* Am I overreacting about the no supervision and lack of anyone else to mentor me?
* (I haven't provided much to show what I do and what I have done; I can if needed)  What should I look for in a different job?
* I am sure there is another question wrapped up in all this, but it isn't shining out to me.



Edit-- 

Thanks a lot for all the suggestions and encouragements. Got even more to think about now, but at least it feels directional now.",2022-10-04 00:45:30
w2zprg,What problems is Snowflake and co. trying to solve?,like title. What problems does it solve compared to a traditional Data Warehouse in Postgres or MS SQL? What is its Unique Selling Point and why is so popular?,2022-07-19 18:27:20
tzzxkj,Tell us your Position / industry / rates / salaries / location?,"I will start

Position: DE Manager
Industry: non-tech
Rate/Salary: $150k CDN / bonus and long term incentive
Location: Western Canada",2022-04-09 19:05:28
rofnm0,Is being a data engineer just a specialised software engineer?,Ive been thinking about how similar both jobs are and what not and how alot of data engineers had backrounds in designing websites. So am I right or wrong with this analogy.,2021-12-25 19:21:34
m8iqiq,My team's code gives me anxiety,"I recently joined a data strategy team within a finance department to help with their data infrastructures.

So far, they've been using Python scripts to load data from data sources to a relational database, which is fine but as the number of pipelines increase, a data orchestration tool like Airflow, Dagster etc would be useful. This is what I've mostly been building for them.

I've just recently looked at one of those scripts and I almost had a heart attack. No version control, he's got input variables and code commented out all over the place, he's got weird functions with inner functions in them (FOR NO DISCERNIBLE REASON) scattered within the whole script (which is like 500 lines long).

What the hell did I get myself into? Lol rant over.",2021-03-19 14:42:20
hqtu8r,Data engineering project on Github,"Howdy all - just posting this in hopes of possibly getting some feedback on a DE project I'm working on. 

Some background: I currently work as a data engineer but almost entirely in a Microsoft shop. The goal of this project was to get some experience with open source software including Airflow and the AWS ecosystem.

From my understanding, it's also just beneficial in general to have some personal projects up on GitHub when searching for a job so figured I'd kill two birds with one stone :)

The project and explanation of it is here:  [https://github.com/ilya-galperin/SF-EvictionTracker](https://github.com/ilya-galperin/SF-EvictionTracker) 

Any and all feedback is more than welcome - this is my first time doing something like this so I'm sure it's not entirely perfect.

Thanks!",2020-07-14 03:15:18
1bqmq9c,Anyone else struggling to keep up?,"Honestly, I am tired of learning new things and have zero motivation to pick up new technologies. I don't hate my work, I like solving interesting problems, implementing solutions and even figuring out ways as and when required.

But I know that just being able to apply is not enough and that one has to constantly endeavour to keep on learning latest tech otherwise one is obsolete in market. How do you guys keep yourself motivated to constantly learn something new? ",2024-03-29 11:16:38
17oe4z2,Discussion: Data Engineering has a title problem,"After working in this industry for 10+ years, I strongly feel like the data engineering space has a title problem. Data Science also has this issue, but I believe in Data Engineering it is even worse. Companies release vacancies for ""data engineer"" when the role can mean so many different things.

I feel like a new set of titles is required to make the industry more mature. Such as:

1. **Analytics Engineer (AE)**: Previously the data warehouse developer. Mainly works with SQL, Airflow, Python to transform data within the data warehouse. Person building analytics pipelines. A lot of DE work falls here.
2. **Data Platform Engineer (DPE)**: An engineer who works on the platform, but not the pipelines. Cross-over with cloud engineer and dev/ops.
3. **Data Streaming Engineer (DSE):** Specialized in data streaming, specifically; coding and patterns here are an entirely different ballgame from all of the above.

Too many times I see companies ask for ""Data Engineer"" - expecting an analytics engineer with enough experience to do what the DPE does. Or companies who mainly to DSE, and end up with too many AEs in the pipeline.

Some companies get this right by specifying that there is a ""focus"" area for a DE role. For example, Data Engineers with a heavy cloud focus, or Data Engineers focused on streaming (sometimes also called Big Data Engineer).

More specific DE roles would help both candidates and companies. Ideally the DE title should disappear completely and have several different roles falling under it.

&#x200B;

What is your opinion? What titles would you propose?

&#x200B;

&#x200B;",2023-11-05 15:20:27
15mhunt,Is our dbt project as bad as I think?,"I just started a new job and am shocked at the state of the dbt project. I've no idea whether I am used to too high standards and I am overreacting. Would appreciate to hear some other stories of dbt at your company!

So why it is so bad, we're two analytics engineers and 1 data engineer. The data engineer mainly manages airflow and databricks so they hardly work with dbt. So it's basically two people. And we have the following:

- 600+ models

- no tests for most of the models

- lineage is a mess. One of the core tables has 55 parents and 150 children. (Edit: wrong wording I mean rejoining of upstream concepts) Circular references all over the place.

- everything in the mart is materialized as a table. They run those tables multiple times a day. Costs have been increasing at a steady pace ofcourse.

- they use schemas to denote topics. The project.yml contains configs for each folder to materialize it to the correct schema. The file is 200+ lines long as a consequence.

Btw they managed to get to this state in less than a year :p

Oh and they are migrating to a new bi tool with deadline end of October. Work hasn't even started on that. So should I run? :P

Edit: fixed formatting",2023-08-09 15:03:53
14ws3o3,There is no Data Engineering roadmap,N/A,2023-07-11 13:55:15
13ahkh7,Is Redpanda going to replace Apache Kafka?,https://redpanda.com/blog/redpanda-vs-kafka-performance-benchmark,2023-05-07 08:30:50
10svt5y,Why do people re-invent wrappers for Airflow?,"For every company I worked for (3 of them), the data team managed to pull out a certain yaml/jinja based ""solution"" to wrap up Airflow. So basically developers write yaml instead of Python. But why? Everytime I had to learn a new syntax, and not everytime the ""solution"" has all functionalities we want. The guy who made it had a lot of fun for sure, but everyone else is not having fun. Why can't they just let people write Python?

Sure the reason might be -- oh BI developers don't want to learn Python or don't have best practices. Well the first reason doesn't hold up because they do (and do write in another repo), and the second reason...well I assume your yaml based solution has the best practices then?

I'm not even going to complain how little documentation each one has. One company managed to invent a second suite of wrappers when I was there, Jesus...

Sincerely, I'd like to know why. I don't really see any benefit. I mean whatever your ""syntax"" can do, Airflow can do that too. I don't see man, I don't really see.",2023-02-03 21:27:55
ydedn2,A Few Pointers for Boot Camp Grads and Uni Students From a Sr. Engineer,"So for the last 3 months, my time has been almost solely dedicated to interviewing, onboarding, and training new people at my work. Throughout the whole process, I’ve been able to identify a couple patterns that I think would be useful to work on before going into interviews or if you are looking to have better chances at getting hired at popular companies.

Needless to say, these are my opinions alone and should be taken with a grain of salt. I might be a Sr. engineer but I still have much to learn and my experiences might be not a true reflection of the market.

# Things To Consider:

* This is more specific for boot camp grads. An interviewer will see the same ""Airflow pipeline with <insert name of free API  here> data"" project a million times with the same structure, tests, and GitHub readme. A lot of bootcamps (and online courses) tend to steer their students to reuse projects from previous years over and over, which leaves you at a huge disadvantage. If you think this is you, take the time to work on other things.  

* I thought everyone knew this but obviously, some missed the memo. Data engineering is not exempt from TDD practices. If can't talk about testing during the interview, this is a massive red flag.  

* Unwillingness to learn SWE concepts/topics will hurt you in the long run. It will not make it easy to interview at companies where Data Engineers are treated as specialized software engineers, or where you are expected to work with other engineers (Backend, DevOps, Architecture, Networking, etc).   

* Focus on collaboration during technical interviews. Getting to the right answer is great, but a lot of companies will not consider you if you don't communicate with whoever is interviewing you. Being silent during the entire coding session makes it awkward and makes interviewers think you won't be a good fit in a collaborative team.   

* For the love of god, come prepared with questions before an interview. And something else besides ""what is <insert name of company>, what do you do?"".   

* If you are applying to a position but have no experience (internships, coops, freelancing, etc.) or just come straight out of an 18-week boot camp course. Please understand that you must put in 300% more effort in the interview and your portfolio than most candidates. I would love to hire folks like this, mentor them, and see them become great developers (and I have). Unfortunately, for 1 job application I publish, I get \~170 applicants within a couple days sometimes. It is hard to say no to experience when jobs in the DE space are so popular these days. Maybe get an internship first or find a small startup that is willing to give you a shot.   

* There are sooooo many companies out there misusing the term ""Data Engineering"" in job postings. Some are just glorified BI/DA positions with fancy titles or jobs where you don't code and end up becoming stuck in a career where you will have a hard time making moves to other engineering positions. In my opinion, data engineers shouldn't settle for such jobs. Ask questions to weed out these jobs during the interview process if you really want to become a proper DE (sorry if it sounds a bit harsh, I truly appreciate good BI developers and Data analysts)  

* Finally, weekend projects and open-source contributions are always cool to see. I support those who want to keep their personal time completely separate from doing anything related to development. But you know, it's cool to see folks passionate about learning or contributing to our community. 

These are just a couple things that jumped out at me during the past three months. I think paying attention to some of these points will make you a more well-rounded candidate and possibly give you better chances in the long run.",2022-10-25 20:08:35
tdnxzw,OOP in python ETL?,"Hello, I’ve recently started a new job as a data engineer (junior) coming from backend web development and I’m having a bit of a hard time adjusting to they way etl pipelines are coded.

So far, I’ve had to main airflow pipelines that call about 15-20 tables and aggregate/transform data from different DB’s (mssql and Snowflake). The code is very procedural, with some functions for reusability but that’s about it. 

I’m having a hard time finding good python ETL design examples on the internet that aren’t extremely simple.

My question is: do people usually try to leverage OOP architecture when coding pipelines? Or does it just make the scripts harder to understand? 

I’m trying to find ways to improve the code to make it more maintainable/easy to expand. Is there any example somewhere of advanced pipelines where I could learn some best practices? 

Thanks!",2022-03-14 03:22:52
qf5dsx,How do you test your pipelines?,"Junior engineer here trying to learn how to properly test data pipelines. Currently my team just has unit tests for our Scala code, but nothing to test the full pipeline run (SNS trigger + EMR + S3 write).

What is the best way to end to end test full pipelines?

My team has around 70 different transformations with 400+ source files, so it doesn’t seem realistic to create mock data and test every transformation.

Should we just have 1 test with mock data that flows through the entire pipeline? Test every transformation that was changed in git commits? What patterns does your team follow?",2021-10-25 00:52:17
ock6qr,Curious to know what a typical day in the life of a DE at FB look like,"I have heard DE roles at FB/AMZN are more sql focused. 
Wanted to know any pros/cons about it. 

Apart from the pay, what all can an employee be happy about when they start working for fb? Will the skills learned at FB help if one wants to make a switch later? 
Given the boom in Spark/ Airflow, Aws and more, for big data, how does working at FB ensure us of keeping up with the latest tech?",2021-07-02 21:42:16
mg6ldf,How To Use Window Functions in SQL,N/A,2021-03-30 03:12:41
1bjbdv3,I am planning to use Postgre as a data warehouse,"Hi, I have recently started working as a data analyst in a start-up company. We have a web-based application. Currently, we have only Google Analytics and Zoho CRM connected to our website. We are planning to add more connections to our website and we are going to need a data warehouse (I suppose). So, our data is very small due to our business model. We are never going to have hundreds of users. 1 month's worth of Zoho CRM data is around 100k rows. I think using bigquery or snowflake is an overkill for us. What should I do?",2024-03-20 11:40:29
1aofkv9,"[Updated] Personal End-End ETL data pipeline(GCP, SPARK, AIRFLOW, TERRAFORM, DOCKER, DL, D3.JS)","Github repo:[https://github.com/Zzdragon66/university-reddit-data-dashboard](https://github.com/Zzdragon66/university-reddit-data-dashboard).

Hey everyone, here's an update on the previous project. I would really appreciate any suggestions for improvement. Thank you!

## Features

1. The project is entirely hosted on the Google Cloud Platform
2. This project is ***horizontal scalable***. The scraping workload is evenly distributed across the computer engines(VM). Data manipulation is done through the Spark cluster(Google dataproc), where by increasing the worker node, the workload will be distributed across and finished more quickly.
3. The data transformation phase incorporates ***deep learning*** techniques to enhance analysis and insights.
4. For data visualization, the project utilizes D3.js to create graphical representations.

## Project Structure

&#x200B;

https://preview.redd.it/ew1cjp8870ic1.png?width=9426&format=png&auto=webp&s=502a9d668f0f7453f770cd9513ac33c041309e7a

## Data Dashboard Examples

## Example Local Dashboard(D3.js)

https://preview.redd.it/fdhivgm970ic1.png?width=4038&format=png&auto=webp&s=1bbac51ef3929b0b0ec1c7c21ea7b450bf0e6ed7

## Example Google Looker Studio Data Dashboard

[Looker Studio Data Dashboard](https://lookerstudio.google.com/s/hEfY-Q6G4Fo)

&#x200B;

https://preview.redd.it/m5imqa5b70ic1.png?width=2886&format=png&auto=webp&s=32ea902c25e4f16b55da2085912d7585c743c6c5

## Tools

1. Python
   1. PyTorch
   2. Google Cloud Client Library
   3. Huggingface
2. Spark(*Data manipulation*)
3. Apache Airflow(*Data orchestration*)
   1. Dynamic DAG generation
   2. Xcom
   3. Variables
   4. TaskGroup
4. Google Cloud Platform
   1. Computer Engine(*VM & Deep learning*)
   2. Dataproc (*Spark*)
   3. Bigquery (*SQL*)
   4. Cloud Storage (*Data Storage*)
   5. Looker Studio (*Data visualization*)
   6. VPC Network and Firewall Rules
5. Terraform(*Cloud Infrastructure Management*)
6. Docker(*containerization*) and Dockerhub(*Distribute container images*)
7. SQL(*Data Manipulation*)
8. Javascript
   1. D3.js for data visualization
9. Makefile",2024-02-11 19:02:29
18cnsae,Adidas Sales data pipeline,"Fun project: I have created an ETL pipeline that pulls sales from an Adidas xlsx file containing 2020-2021 sales data..I have also created visualizations in PowerBI. One showing all sales data and another Cali sales data, feel free to critique.. 
I am attempting to strengthen my Python skills along with my visualization. Eventually I will make these a bit more complicated. I’m currently trying to make sure I understand all that I am doing before moving on.  Full code is on my GitHub! https://github.com/bfraz33",2023-12-07 04:49:12
16thbm7,"I recently passed the DP-203, here are my notes to prepare","  

My Stats:  
Azure Engineering Experience: 2 years on and off  
Studied: 30 days – Ave 3 hours/day  
Scored: 892/1000

Exam Covers: Data Factory, Synapse, Stream Analytics, Event Hubs, Databricks, Data Lake Storage, SQL/Scala

Resources I paid for:   
Alan Rodrigues – Udemy - DP-203 Course - 21 hours long - Updated for 2023. Very good course, but not enough to pass the exam. 

Measure Up Test Prep Questions

You will get questions on:  
Parquet Files, Data Lake Gen2 storage, Slowly Changing Dimension Tables (SCDs), Fact & Dimension tables, Synapse Distributions (Hash, Replicated, Round-Robin), Synapse Partitions & Indexes, Security Options. I’d say half the questions involved Synapse. 

Final Tips:

• Make sure you’re getting near 100% on test preps before you attempt the exam.

• Have Azure open when you do the test prep questions, to practice for real.

• If you feel you’re not ready, re-schedule the exam, which is free.

• Book the exam in advance - Stopped me from procrastinating or chickening out.

• Organize your time, I needed 3 hours/day for 4 weeks to prepare. Depends on your skill level.

• Read the question & answer options first. Then look at the case study. Helps you find the answer quicker. 

Here are some free resources.

Free Test Prep Questions:  
 [https://www.analystlaunch.com/c/dp203-test-prep-download-landing](https://www.analystlaunch.com/c/dp203-test-prep-download-landing)

Video on passing the exam:  
 [https://www.youtube.com/watch?v=fRR5FLrv398](https://www.youtube.com/watch?v=fRR5FLrv398)

Sitting your first Microsoft exam at home:  
 [https://www.youtube.com/watch?v=hV\_DpwpzNZI&t=1s](https://www.youtube.com/watch?v=hV_DpwpzNZI&t=1s)

Good luck.",2023-09-27 10:52:00
16i3u4l,How to build experience in Kafka and Spark if not in a data engineering job?,"I've worked as a data scientist / engineer for the last 9 years but always at a scale a bit below where you really need distributed computing (i.e. SQL databases of a few terabytes). I'm interested in developing the skills that can take me to the next level of scale, but at my job we simply don't have that amount of data. Launching and running a cluster just for fun also seems like it would be a bit expensive. And if I'd want to make a shift to a senior data engineering role at this larger scale, they're going to want me to have some of this experience _before_ I get hired.

What's a good way to expose myself to problems that I can solve with Kafka / Spark (i.e. I'm interested in streaming algorithms and mapreduce-like problems)? I'm wondering if there are (for example) open source geo datasets and public servers that you can do some work on (though obviously those cost money as well, so maybe I'm naive to think that).

Obviously I'm a bit new to this area so please do let me know if I said anything dumb :) I read ""Designing Data-Intensive Applications"" and have a decent grasp of CS fundamentals, but obviously there's some specialized expertise to be had here.",2023-09-14 00:30:54
168208y,I've been trying to wrap my head around the use of Snowflake,"So I've been doing Data Engineering for a while now, and understand the cloud based Data Engineering tools from AWS, CGP, Azure, etc. I keep seeing a lot of job descriptions requiring or preferring knowledge of Snowflake, which I've never had to use in my career.

I've read about it a few times and I think I generally understand the concept, but what I guess I don't understand is why you would use it?

From what I have read and watched some videos of (please correct me if I'm wrong), Snowflake works as a SaaS Data Warehouse that integrates with the big three cloud providers. For AWS, for example, you would host the data in AWS S3, and the processing comes from AWS EC2 containers that it would instantiate for you. So it seems to do a lot of the orchestration?

So in this case, why wouldn't you just use Glue, Athena, RedShift, etc to do this? It seems like you'd be paying AWS for their services, plus Snowflake. I know that you could potentially integrate data from the different cloud platforms together, but I know there are at least ways of bringing in, for example Google Analytics data into AWS through things like AWS Appflow, if not programmatically.

So I've been wondering, is it cheaper, more abstracted and easier to use, more performant? A lot of companies seem to be using it. I thought I'd ask this here so I had the advantages in mind before I sign up for a free trial or something.",2023-09-02 13:03:34
11vv3jo,"For those of you who were self taught, what was your path into data engineering","I’m looking to get into data engineering myself and come from an analyst background in healthcare.

I am curious as to what your paths into data engineering were for those who didn’t come from a traditional background. What has your career path been like? How did you decide to get into DE and how did you do it?",2023-03-19 19:32:15
102d1fm,Writing a Python SQL engine from scratch,N/A,2023-01-03 16:55:33
x3bb11,Quarterly Salary Discussion,"This is a recurring thread that happens quarterly and was created to help increase transparency around salary and compensation for Data Engineering. Please comment below and include the following:

1. Current title

2. Years of experience (YOE)

3. Location

4. Base salary & currency (dollars, euro, pesos, etc.)

5. Bonuses/Equity (optional)

6. Industry (optional)

7. Tech stack (optional)",2022-09-01 16:00:11
u7rk6a,Is Data Modelling still an expected part of a Data Engineer skillset?,"I'm currently recruiting for a data engineer with 2-3 years experience to create pipelines to supply a coud data warehouse, with the expectation that they will then support the activities to decide how the data should be structured in the warehouse and process it to this format. I'm getting a lot of people applying with Python, Spark, Scala, with demonstrable expertise in pipelines moving data from A to B, but I'm also finding people either don't have any data modelling experience, or don't want to engage with a role that involves data modelling. 

I'm thinking this might be the growing separation between Data Engineering and Analytics Engineering, but are others finding this distinction to be so clear cut now?",2022-04-20 08:14:48
ssrt2b,Data Engineering O’Reilly Humble Bundle,"https://www.humblebundle.com/books/data-engineering-oreilly-books

15 books for $18. Been collecting tech humble bundles for years. Causally reading them has helped my career. Often the books aren’t perfect, a few years outdated, but generally worth it. 

First bundle I’ve noticed catered to data engineering. Ifyky O’Reilly is a legit publisher.",2022-02-15 02:17:42
ri2k6b,Can someone explain the big deal with dbt?,"I'm not sure I get what the craze is about? It just executes SQL using the already existing SQL engine in the database...

I get it's more observable than filling a database with a bunch of stored procedure scripts, but if I'm already orchestrating my SQL transforms with something like airflow or prefect anyways what do I need DBT for?

Is it like a citizen data engineer tool? So like there are data science tools that make doing some simple stuff into a pretty GUI. Is it that for data engineers?",2021-12-16 22:56:40
qhtox6,Is our coding challenge too hard?,"Right now we are hiring our first data engineer and I need a gut check to see if I am being unreasonable.

Our only coding challenge before moving to the onsite consists of using any backend language (usually Python) to parse a nested Json file and flatten it.  It is using a real world api response from a 3rd party that our team has had to wrangle.

Engineers are giving ~35-40 minutes to work collaboratively with the interviewer and are able to use any external resources except asking a friend to solve it for them.

So far we have had a less than 10% passing rate which is really surprising given the yoe many candidates have.  

Is using data structures like dictionaries and parsing Json very far outside of day to day for most of you? I don’t want to be turning away qualified folks and really want to understand if I am out of touch.

Thank you in advance for the feedback!",2021-10-28 18:45:30
hiyyhg,What do you want to see in a 6-month Data Engineering course?,"Hi all,

I've built the data infrastructure from the ground up at a couple of small, successful startups. I like teaching and mentoring and I believe my position as a lead and in startup world has given me the experience to really teach people ""core"" and future-proof data engineering knowledge.

I'm looking to write a 6-month long data engineering course with the goal of taking someone with no tech/programming experience to a place where they could get a junior DE position. I think there's a huge knowledge gap for someone to become a data engineer, and most online course I have seen are not great (including Udacity's).

My main questions are:

* Are most people wanting to get into data engineering complete beginners to coding or are they general SWE's looking to transition? What bucket do you or any of your friends who are interested in data engineering fit into?
* **More generally, what would you want to see in a data engineering course?**

Just to give an idea of what I'm thinking:

1. Curriculum will be: A heavy emphasis on SQL/Python for ETL and ELT, data warehouses and data modeling, distributed ETL tools (Spark and Hive in EMR, serverless tools like Athena), Airflow, some RDBMS, some BI tools/analysis, maaaaybe a little NoSQL. And general cloud knowledge (EC2, S3, RDS etc). Not sure about streaming.
2. Heavily cloud-based. We will work with rdbms, data warehouses, and ETL tools in the cloud. This will be billed on the students cloud accounts, so we can coast on free tier, or we can easily limit usage to under a $100 using certain instances. If something can be learned locally (like Airflow and Python), it will be. I will host data in S3 and snapshots. Leaning towards AWS.
3. The course itself might be $100 or broken into chunks so you can only pay for what you need to learn.
4. The course will be **very** project-based. I will provide helpful code, data, assignments, guidance etc. The course content itself will be videos/text. I may see if I can get short multiple-choice quizzes or code evaluation.
5. I'll create a discord to answer questions and to create a student community. I'm not sure how feasible real person evaluation of students work is possible. But if I can figure out a good solution, I will.
6. Students should graduate with interesting projects on their resume as a result of the course.

&#x200B;

Thoughts?",2020-06-30 23:35:09
e5vxuv,"Metaflow, Netflix's Python framework for data science, is now open source",N/A,2019-12-04 07:53:35
18gn43u,How do you make working with SQL enjoyable (or less tedious),"Although I work as a Machine Learning Engineer, sometimes I'm requested to build some queries, for instance for dashboarding purposes.

However, I find it really tedious when working with SQL. My main reasons being:

1. Most of the times we work with tables in the DataLake we don't own, hence it takes an awful lot of time to get an understanding of how that data is structured (and what kind of data problems it might have)
2. I feel really unproductive having to wait for query results. It really slows down any kind of exploration one might want do with the data like one would with tools like Pandas or Polars (and loading data locally is not an option as we're talking of billions of records here, which we're handling with Spark)
3. Sometimes queries grow to be extremely complex, which makes it harder for team mates to review
4. Along the previous point, I really feel SQL is extremely unreadable as compared to a programming language",2023-12-12 14:28:59
186zm6c,My team only uses Excel to manage all of our critical data and I’m struggling to fix it,"This is my first job out of college and I’m more of a data analyst with little experience in any data engineering.

My manager tasked me with finding ways to improve our team’s data management practices.  We use one Excel workbook with 40+ sheets as our central hub for how we store, manage, and interact with our data critical to day-to-day operations.

Most of our team uses this Excel file, oftentimes simultaneously, which causes mistaken data entries, conflicting filtering, and so on.  It has “worked” so far but continues to grow beyond its useful limit.

My issue is I need to find a solution that my team will accept and be able to continue to maintain should I leave. They do not have rich technical knowledge, so I have to be careful about developing too crazy of a solution.

I’m just totally stuck.  On one hand, it seems stupid to have all this data laying around in Excel.  On the other hand, I don’t know how to find an acceptable solution that balances sophistication and user-friendliness for the “business users”.

On top of that, I have to develop this all alone and self-guided.  This is one of my first projects and I don’t want to fail.  Btw, this is a fortune 50 company in a highly regulated industry 😭.

Any suggestions would help greatly for my own sanity",2023-11-29 20:46:44
17pvhbv,Is it a must to be very good at SQL for a data engineer position?,"The question does sound silly. I was a programmer and loved web development and making something out of nothing but the heavy coding around the business functionality, CI/CD, Elaborate testing, is not appealing.

&#x200B;

What I love: 

data visualization, cleaning, statistics (the numbers not the math) and generally love information and DB design and optimization.

What I hate: 

my mind would rarely be able to wrap itself around sql queries that have more than a couple of joins, specially if its a query inside another. I hated reading those. I also despised functional programming and recursion because I couldn't visualize it

Why am I considering Data engineering? 

I mentioned my love for data and data cleaning, not to mention salary and I imagine with new tools the querying would not need to be SQL style. Is it realistic to do this job well without that skill?",2023-11-07 14:27:33
15kyl33,Is it hard to find Data Engineers with good SWE skills?,"At my company we have open Data Engineering positions but almost everyone falls short on their coding skills.

Then, people we’ve hired have needed quite a bit of hand holding on the SWE aspect of tasks (testing, coding, IaC, devops, etc).

Are we asking too much? I thought of SWE as a requirement for Data Engineering and the Fundamentals of Data Engineering book even defines SWE as an Undercurrent of Data Engineering.

Edit: We are only hiring in a single South American country so we can’t interview most of you.",2023-08-07 22:01:49
15gf97e,Is traditional data modeling dead?,"As someone who has worked in the data field for nearly 20 years, I've noticed a shift in priorities when it comes to data modeling. In the early 2000s and 2010s, data modeling was of the utmost importance. However, with the introduction of Hadoop and big data, it seems that data and BI engineers no longer prioritize it. I'm curious about whether this is truly necessary in today's cloud-based world, where storage and computing are separate and we have various query processing engines based on different algorithms. I would love to hear your thoughts and feedback on this topic.",2023-08-02 17:35:21
15614vp,Data analyst/engineer at Tesla,"I just had 20 minutes interview (1st) with Tesla on a role called data analyst/engineer, which requires these skills below. I was asked right off the bat some technical questions without giving me chance to introduce myself. I was asked what confusion matrix is and I couldnt pull out from my brain what they are. I know it's very basic but I wasn't prepared. I told her I came in with DE readiness so they asked me on DDL, how to drop a column (I swear I never had to drop a column but I manage to give an answer that works lol). This interview makes me feel so rushed from their end and at the same time I feel underqualified.😭

What You’ll Do
Create and/ or enhance action-driven dashboards (e.g., using Tableau). 
Support ad hoc data, SQL query, analysis, and debugging requests. 
Create and maintain an optimal database schema and data pipeline architecture. 
Create ETL pipelines in Airflow for analytics team members that assist them in building and optimizing their reports. 
Communicate with stakeholders, gather business requirements, and brainstorm KPIs. 
Develop/ maintain internal documentation. 
Proficiency in SQL, and comfort with a scripting language (e.g., Python) is a plus. 
Proficiency with a data visualization tool (e.g., Tableau). 
A good understanding of relational databases and database engineering concepts. 
Familiarity with data pipelines and a Workflow Management Tool (e.g., Airflow) is desirable.",2023-07-21 21:41:29
13k9fvu,What have you learned the hard way?,"Describe your experience & position and tell us about something you learned the hard way. I'll go first...

DE with 2 YOE. Background was non-technical. I design and implement pipelines to serve analytics.  


**Story**: We believed a service provider (service desk and software maintenance) was fudging numbers in various ways to make their service look better than it was. We had access to their service desk ticket data, and the business logic behind whether or not a ticket met the service target / SLA.

Our data analyst is almost done with his report for leadership on the matter. Asks me to add a calculated field to the materialized view that I provide him so he can finish up and send it out that day. I define a new CTE, left join it to the existing data, and define the new field. Briefly manually scroll through the new column and it looks good. Recreate the view and tell him to refresh on his end. He runs his calculation and starts finishing up his report.

Later on, he hits me up on Teams with the dreaded ""something looks off"". Long story short, I did not de-duplicate the CTE I created, and the join resulted in some entries having many duplicate records, making his numbers inaccurate. Disaster averted thanks to him catching this.  


**Lesson**: You need metadata capture and observability at every. single. step. in your pipelines, including the view layer.

I could have caught this manually if I had taken 5 extra seconds, but it's just better engineering to implement CDC or trigger function + metadata table, then put together a little metadata & lineage dashboard.  

So...what have you learned the hard way?",2023-05-17 18:09:46
136rwag,"Are there database design Standards out there? As in, formal documents listing exact best practices for OLTP database design?","When collaborating with Software Engineering, Product, etc. there are always things that come up regarding best practices in a production database.

* timestamps should always include a time zone and be stored in UTC (right?)
* Foreign key constraints should always be defined on foreign keys
* Column names should be descriptive
* Boolean columns should begin with is\_ or has\_

You get the idea. There are dozens or hundreds of standards I could think of if I kept going.

I see a few nascent attempts, but I'm surprised that with decades of SQL usage gone by, there aren't some standards that seem more.... authoritative at this point. Does anyone know of any semi-official standards, or have thoughts here?

 \- [https://ovid.github.io/articles/database-design-standards.html](https://ovid.github.io/articles/database-design-standards.html)  
 \- [https://fdotwww.blob.core.windows.net/sitefinity/docs/default-source/content/it/docs/standards/databasedesignstandards02042016.pdf?sfvrsn=115b611e\_0](https://fdotwww.blob.core.windows.net/sitefinity/docs/default-source/content/it/docs/standards/databasedesignstandards02042016.pdf?sfvrsn=115b611e_0)  


It would be really handy to have something authoritative, at least as a starting point, instead of arguing about these from scratch and not getting anywhere.",2023-05-03 16:18:10
11v9z1q,Manager denying use of Git,"Hi All,

I have been recently moved to project where we are developing application for client using Azure ecosystem. We are 25+ memeber  team where each team takes care of different part. I work with a sub-team of 3 members (manager,senior resource and myself -analyst)  where we do transformation using Azure databricks. Our manager is not from core development background and he was DBA before this project or involved in work around DBA. I have been trying to convince him to use Git for version controlling for reasons which are basic and every developer knows. But he is convincing us to work on same folders ( In databricks we can access same folders / shared access) . I tried my best to  convince him but no luck. Can you guys suggest any other ways to convince him  or has anyone faced similar situation before?

Update : 
Tried again with suggestions but no luck. Cannot highlight it to higher management as it will create other issues and will make the environment more bad for me. Planning to leave the team in the upcoming days.",2023-03-19 03:26:08
zpt1ri,From /recruitinghell - (Name and Shame: NielsenIQ - Interviewer ends interview 12 mins in.),"Recruiter ends interview because candidate has no experience with Databricks but the job description is not explicit about it being required. The unprofessionalism that some of these companies display is astonishing..

&#x200B;

>The first interview was scheduled on Tuesday and 20 minutes before, I receive an email from the recruiter asking to postpone for Friday because the hiring manager had an “emergency”. I was extremely annoyed, but I accepted, nonetheless.  
>  
>We start the interview on Friday, and I immediately notice the hiring manager avoiding eye contact. I found that to be strange and it made me uncomfortable. Then, 12 minutes in, he asks if I have experience with DataBricks which I promptly told him that I do not, and he proceeds to say “I have to end the interview because DataBricks is a requirement”  
>  
>I pressed him by stating that the Job Description clearly states *(Airflow, Databricks, Snowflake, Serverless Functions, Cloud storage preferred)* and that I never included Databricks on my resume.  
>  
>He then says “Well, we are looking for someone Senior” which I replied by saying that the recruiter assured me on 3 different conversations that this was not a senior role. We went back and forth for some 5 minutes, and I eventually end the call.  
>  
>I emailed the recruiter right away to share my displeasure of the session and she calls my phone. She explains to me that the hiring manager had decided a few hours before that he wanted someone senior which angered me because he basically wasted my time twice.

[https://www.reddit.com/r/recruitinghell/comments/zpflbr/name\_and\_shame\_nielseniq\_interviewer\_ends/](https://www.reddit.com/r/recruitinghell/comments/zpflbr/name_and_shame_nielseniq_interviewer_ends/)",2022-12-19 14:28:10
ym47ac,Snowflake Architecture Overview,N/A,2022-11-04 16:58:42
xojb5f,"Data Engineering Concepts: Definitions, Backlinks, and Graph View","If you are like me and daily confused about new terms in data engineering, I started a [Data Engineering Concept](https://glossary.airbyte.com/term/data-engineering-concepts) page. You can click on each of them, dig into details, and learn more about related concepts (everything is open on [GitHub](https://github.com/airbytehq/glossary)).

**Data Engineering** is still not well defined; However, in the latest book on Fundamentals of Data Engineering by Joe Reis, Matthew Housley tries and does probably best as of today, and it's getting clearer. Besides several boot camps, universities are starting to get a degree in Data Engineering as Data Science did before. 

Data Engineering is a discipline that has shifted over the years from a Database Administrator (DBA), ETL Developer, and Business Intelligence Specialist and merged with Software Engineers to a Data Engineer with the growth of data. Today you might call it a Data Engineer, a Software Engineering with business intelligence and big data capabilities.""

What is the recent term that confused you?",2022-09-26 13:28:13
x6mbt3,What’s the best paid courses to prepare for DE and certifications?,"I know that there is plenty of free resources but since my company offers a good budget for individual trainings, and th end of the year is approaching, I’m looking for some excellent paid resources to do DE trainings that also enables me to get prepared for AWS certification (ML Specialty, DA Specialty). So, have you guys any recommendations?

P.S.: I’m an data analyst, half scientistchy, for about 3 years now. Work with fraud detection. I know a lot of python and SQL.",2022-09-05 17:43:27
vtwt3q,I want to move from startup to big company. where I'm always seeing spark as a requirement how to learn it in home I want a local setup but seems very resource extensive ?,Please let me know what can I do to practice and learn it,2022-07-07 23:59:36
r893rw,Why is Snowflake so popular?,"Hi all,

I am just wondering why so many companies use Snowflake. As far as I know, it is a Cloud Datawarehouse, which means that the main competitors are platforms such as BigQuery, Redshift, Synapse. What makes Snowflake better than the rest?

Do you use it in your work? In that case, which kind of application do you have? And which tools do you integrate it with?

Thanks!",2021-12-03 21:04:00
mb004b,Monte Carlo Banned from Locally Optimistic Data Community,N/A,2021-03-22 22:40:53
1b5986w,[AMA] From FAANG Data/Software Engineer to YC-Backed Startup Founder - Ask Me Anything!,"Hey Reddit! I'm a former engineer at FAANG who decided to take the leap and start my own venture. I'm now building Quary, a startup in Y Combinator's Winter 2024 batch. The journey from a mere concept to securing a $500k investment from YC has been filled with a rollercoaster of emotions, countless challenges, and a steep learning curve. I'm here to share my experiences, insights, and the realities of transitioning from an engineering role to entrepreneurship.

Feel free to ask me anything about the journey, the lessons I've learned, or any advice you might need for your own career or startup aspirations!

(And hey, if you happen to check out Quary and find it interesting, [a star on my GitHub repo would be a nice way to show support. No pressure, though](https://github.com/quarylabs/quary)!)",2024-03-03 05:18:26
17fgte6,vendor confession: there's just too many ETL/ELT tools,"Pulling back the charade for a moment here... as a vendor, I really empathize with all the DEs in the thread trying to sort through the noise.

I'm not sure if it is the vast amount of VC funding that came into the space, the potential level of nuance to every pipeline, or something else (act of god?)... but it is borderline unimaginable how many new/different vendors exist for creating data pipelines.

Just taking a basic example, googling say Postgres to Snowflake will quite literally yield hundreds of distinct possible vendors. I scrolled so long waiting for repeat vendor domains that I actually got bored and stopped. And this is just revealing all the companies that have put in the effort to try and hack Google SEO results (stitch even bought hundreds of domains of [https://postgres.tosnowflake.com/](https://postgres.tosnowflake.com/) \-- IMHO: Google Search for B2B is increasingly a failed product for surfacing quality products/content... but I digress.)

Add in all the open-source, native tooling, or code-based ways to create a pipeline... and I think there might literally be 200+ legit ways to ETL from Postgres to Snowflake.

How is there possibly \*SO\* many solutions? Is massive consolidation of point-to-point ETL tools coming immediately?

While I think what we do is somewhat unique ([estuary.dev](https://estuary.dev)), there is still a ton of overlap, and to my partially trained eye.... it feels like Rivery/Fivetran/Hevo/DMS/Stitch/Talend --- just all basically the exact same 'sometimes good enough' solution w/ up to 99% the same features

END RANT

&#x200B;",2023-10-24 16:26:15
16u1sxr,"How do you ""know"" your architecture is correct?","I am always wondering how can a data engineer know, and then convince product team or business that their architecture or design is correct? There is not exactly an exact science to data engineering. There are companies which are happy using out of the box tools and make it work, and there are engineers putting up with complex legacy codebases using microservices et al. Besides reading up and doing courses trying to get as close as possible to best practices, your architecture suggestions are subject to either lessons learnt from experience (takes 1-2 years for a big data project to take real shape) or googling and doing the best possible back of the envelope tradeoffs. While other non technical team members expect you to give them a silver bullet or quickly lose trust in you. How do you establish this confidence in a solution without seeming like a tinkerer?",2023-09-28 00:54:38
14ltv6p,"Which are the most inefficient, ineffective, expensive tools in your data stack?","With all of the buzz around the high costs of various platforms and tools used for building data pipelines, including data collection, data warehousing, data processing and transformation, extracting insights out of the data - 

Which are the most inefficient, ineffective, expensive products that you have experienced?

Top 5 or 10 products listicles in various categories are just paid marketing campaigns and provide biased information.

What is the tribal wisdom about the worst offenders in data tools and platforms that you would recommend staying away from and why?

Share away and help the budding data engineers out.",2023-06-29 03:29:59
zo7609,Any good resources to learn Apache spark?,"I’m learning Apache Spark through “Learning Spark” book. But I would prefer good video tutorial to learn.  

Can you guys please share good resources to learn how things work under the wood.",2022-12-17 14:15:23
zn35zt,How do you use Airflow 'properly'?,"My manager has tasked me with setting up Airflow on AWS. Depending on the cost I guess my options range from ECS, MWAA or simply deploying it on EC2. In any case my question relates to writing DAGs: Airflow is merely a scheduler and is not supposed to perform the actual ETL work. I want to deploy Airflow and write DAGs in a way that will be scalable and isn't going to incur unnecessary costs, so with that in mind, are there any tips/best practices to keep in mind when writing DAGs? Any operators I should avoid in keeping with the principle that Airflow shouldn't be performing the business logic?

As an aside, where do you typically store connection credentials in prod? Is the Airflow UI sufficient or is it better to use environment variables?

Appreciate the help.

Edit: For anyone curious, I decided to use Terraform to provision a VPC + network infra, IAM role and MWAA environment twice daily (when our DAGs are typically running) on a schedule. Given that other members of my team aren't so familiar with AWS, MWAA was probably the wisest and most straightforward deployment option. Regarding the DAGs/tasks themselves I'll be looking strongly at using ECS and its operator in the future once I've moved all the DAGs to S3, as many of the answerers suggested. Thanks all.",2022-12-16 02:02:13
zl7gvh,Do you need to code in your job?,"What kind of scripts ir programs do you develop?

I'm currently a data engineer only using a low code cloud platform (informatica) and i am unmotivated with my routine.",2022-12-13 21:26:23
z90wtm,Great Expectations is annoyingly cumbersome,"You have to create a great expectation project, suite, checkpoint, data sources... and then several lines of code for everything you want to do, and it's not even trivial to validate an in-memory pandas dataframe.

Come on, I just want to have to do this:

    import pandas as pd
    import great_expectations as ge
    
    df = pd.read_csv(file_path)
    ge.check_duplicates(df, primary_key=""id"")
    ge.check_not_nulls(df, columns=""all"")

Why all the complications? I honestly don't get it. Maybe I didn't even understand the tool and what I say is possible to do, but then, oh boy, what a tutorial.

Is there any other alternative out there? Does it get easier when you have some experience with it?",2022-11-30 19:38:10
xlwc4x,Data Modelling part of Data Engineering?,I'm having a fascinating conversation with senior management around this topic - what are your thoughts? What I thought was initially clear cut is turning to not be so!,2022-09-23 12:34:14
v6xs2x,How are you guys validating your data?,"I saw similar posts but I also wanted to share the specific flow in our data pipeline. The data warehousing project started at the my company some months ago and we setup many ETL jobs in our pipeline using Spark-Scala to write the logic and Airflow to schedule and run the processes to deliver data to business teams on Redshift tables to be queried.

Currently everything is running smoothly and we never encountered any major problems within our pipeline, and now we're doing some research to setup a Data Quality project since we're not testing our data yet. 

I've personaly looked into the Great Expectations Library and it seems promissing to implement it at the end of some Airflow pipelines to do some basic validations, such as checking if a column value is Null and send us an alert. We don't need anything too fancy yet.

I wanted to know how are you guys doing Data Validation at your companies! Which tools are you using? Are you checking the resulting table at the end of your pipelines?",2022-06-07 14:51:23
v6ehhx,"You have total freedom for developing the data platform of your company, what tools do you choose?","Imagine that you are the first Data Engineer in the company for building a data platform, and you have total freedom for choosing the tools and a decent budget. It must cover the extraction from the backend/foundation sources (Kafka, Operational databases... whatever) to the Dashboard in a viz tool. 

What would your platform look like? The platform that you have always dreamed about, the one that will make every Data Engineer want to work in your team because of how cool the tech stack is.

I think I would choose something like:

Containerized Airflow for orchestrating  
Terraform for IaC  
Amazon as the Cloud (maybe there are needs for EC2, Lambda... whatever)  
DBT for the T of the ELT in combination with Airflow  
S3 for Storage  
GitLab for CI/CD and Version Control  
Python as the main language

And I have doubts about the DWH layer and the BI

I like very much Snowflake (more than Redshift and BigQuery), but maybe it makes more sense to use Spark in Databricks. Or maybe both? Or just use Spark in Amazon EMR? Or maybe some open source like Presto?

What about the BI? I don't know if it would make sense to don't go with the typical vendors (Tableau, PowerBI, Looker...) and maybe go to a more recent solution like Apache Superset or Metabase.",2022-06-06 21:42:49
uso1re,DE interview horror stories,"With Data Engineering being so hot 🔥 these days, I want to hear some interview horror stories. Gotta be some good ones out there.",2022-05-18 22:04:05
rdaclp,dbt Coalesce 2021 takeaways,"Hi r/dataengineering, few months ago I've shared my takeaways of the Airflow Summit and this time I'd like to share takeaways from the dbt Coalesce 2021 conference. I really don't know if dbt is used by people in this sub but still I'm trying.

So, for people not used to it, dbt is a framework to **organize** and **run** your SQL queries on top of your favourite data warehouse. More and more companies are using it, the core project is open-source but they sell a Cloud version with and IDE and stuff to ease the deployment of your projects.

Also from the panel at the conference we can see that dbt is not a tool used by Fortune 500 companies. I would bet they often already have a internal system doing the same.

This year at the Coalesce we got 5 kind of talks:

* Food for thought — to help us seeing forward
* 101 talks about dbt or other concepts
* Feedbacks from companies implementing dbt
* Promotional content (often from the sponsors)
* Diversity talks about how we can be more open in the data field

# Food for thought

* Erica from dbt explained why you should aim from self-service rather than Data as a Service, she also defined what is self-service and what you should measure to get it — [Scaling Knowledge over Scaling Bodies](https://coalesce.getdbt.com/talks/how-dbt-labs-is-building-its-data-team/)
* Tristan had a chat with Martin Casado about investments in data and [how big is this wave?](https://coalesce.getdbt.com/talks/keynote-how-big-is-this-wave/) — They talk about the fact that the Modern Data Stack is probably a déjà vu but still something changed because companies key differentiator now is data, before it was software. But after all isn't data software?
* [Down with ""data science""](https://coalesce.getdbt.com/talks/down-with-the-phrase-data-scientist/) — I liked it because the key concept is that words have a meaning, job titles matters.

# dbt

* [Git for analytics engineers](https://coalesce.getdbt.com/talks/git-for-the-rest-of-us/) and how to [build a mature dbt projects](https://coalesce.getdbt.com/talks/how-to-build-a-mature-dbt-project-from-scratch/)
* The metrics layer could be the next big thing when it comes to analytics. It means we have a middleware between our warehouse and our customer facing apps that holds metrics definition and that is able to answer quickly to every question. [Benn](https://coalesce.getdbt.com/talks/the-modern-data-experience/) described it and Drew (dbt co-founder) [gave us a glimpse of the dbt Server](https://coalesce.getdbt.com/talks/keynote-metric-system/) for 2022.
* Showcase of the [v1.0 version that was released this week](https://coalesce.getdbt.com/talks/dbt-v10-reveal/)

# What other companies are doing

* [Use dbt packages to encapsulate data logic](https://coalesce.getdbt.com/talks/so-you-think-you-can-dag-supporting-data-scientists-with-dbt-packages/)
* Use dbt macros to dynamically detect sources schema and then create dynamic sources in you dbt projects
   * By [Mattermost](https://coalesce.getdbt.com/talks/automating-ambiguity/)
   * By [Aula Education](https://coalesce.getdbt.com/talks/surviving-schema-changes-with-automation/), they use dbt\_utils to do it
* [Slido](https://coalesce.getdbt.com/talks/from-100-spreadsheets-to-100-data-analysts-the-story-of-dbt-at-slido/) develop [dbt-coverage](https://github.com/slidoapp/dbt-coverage) to get a documentation coverage number you could put in your CI/CD to naively push back analysts merge requests :D 
* Companies showcases also how they use the Cloud Metadata API to get lineage or run information about the their projects

&#x200B;

That is a small glimpse of what has been shown at the conference and also my favourite stuff, I've a more detailed post on [https://www.blef.fr/dbt-coalesce-takeaways/](https://www.blef.fr/dbt-coalesce-takeaways/) and all the talks can be found the conf website.

&#x200B;

Once again I tried to summarize the stuff to be readable on Reddit, I hope you like this kind of content.",2021-12-10 14:52:55
qyn24c,"Lesson learned: meme good, watermark bad. Here's another DE-flavored meme as compensation.",N/A,2021-11-21 04:29:28
pwq32y,What made you choose DE over DS?,"Just as the title states, what was the motivation for going down DE over DS?",2021-09-27 20:41:30
g69bpo,[META] We're Building a Wiki. Now what?,"Here's your chance to share your vision for the wiki. 

A few ~~weeks~~ months ago there was [some feedback](https://www.reddit.com/r/dataengineering/comments/ez9drx/data_engineering_wiki/) requesting a wiki for r/dataengineering. I offered to help bring it all together. 

Some unanswered questions on my mind:
- Does r/dataengineering even need a wiki? Other resources exist, e.g. [the GitLab Handbook](https://about.gitlab.com/handbook/business-ops/data-team/)
- What topics should be curated for the wiki? More broadly, what is the role of the wiki?
- Should there be limitations on who can/should contribute to or curate the wiki?

This isn't an exhaustive list. If something else is on your mind, please speak up.",2020-04-22 21:00:39
1bgct3c,When to use Spark vs Pandas?,"Hi guys, just wondering if some of the more experienced DEs have an answer to this. I’ve played around with pyspark before but I think there’s a df size threshold below which pandas is faster and above which pyspark is faster? Just not sure what this threshold is and whether anyone has tried finding out or have any heuristics.",2024-03-16 18:34:33
17592w5,I'm a beginner in the data engineering field with 1.5 years of experience in developing pipelines on ADF and working with SQL & Pyspark on databricks. Would you recommend this book for me?,Is it more advanced for someone like me with respect to my experience in DE.,2023-10-11 08:27:09
14hw7kj,What are your weekend side projects?,"Do many DEs have weekend side projects? Seems like so many software devs have side projects, but I hear less about it from DEs. Personally my side projects are birding, yard work, and being a dad 🤣",2023-06-24 15:47:01
135kadw,Experience with Data engineering Influencers on Linkedin.,Has anyone worked or have any real life experience with these data engineering influencers on Linkedin. Just curious how good as they in real life in term of their level of knowledge in data engineering.,2023-05-02 12:49:05
106ypk4,Recommendations for having Fundamentasl of Data Engineering,"Hi Everyone,

&#x200B;

I know this is a recurrent post, but in my case, I have been working as a DE for about 3 years, I have become a Ssr DE and I still do not have the basics knowledge (i feel ashamed lol) of how to develop a data warehouse, what is the best approach for a data modeling, how to orchestrate or which tools to use, so:  


\-would you please recommend to me where could I learn (possibly youtube or courses/boot camps) the basics of Data Engineer so I can level up my expertise and stop feeling like a Jr?

Edit: i know i am a shitty reader so if there are some youtubers or videos i would really appreciate the info :))",2023-01-09 00:01:17
yq0t6l,Introduction to Snowflake's Micro-Partitions,N/A,2022-11-08 22:35:12
ya7bly,Whats something that you don’t understand but are too afraid to admit because you don’t want to look like an imposter?,Be honest. Hop on that burner if you need to.,2022-10-21 22:56:54
y41mv5,How are you maintaining data fidelity throughout your data integration and transformation pipelines?,N/A,2022-10-14 18:34:58
vc4b2y,"Is all data engineering moving into SQL warehouses, or is there still a need for general purpose programming languages and systems?","It seems that modern data warehouses, exemplified by Snowflake et al, are good at efficient data storage, retrieval and transformation of everything from unstructured to structured data. In addition, these warehouses automatically scale and distribute query execution. With tools like DBT, it also becomes possible to manage and compose transformations expressed as SQL.

If that's true, then what is the remaining role of general purpose programming languages (PLs), like Python, and distributed systems like Spark for scale? It seems that PLs are at a disadvantage wrt SQL because they are much harder to automatically parallelize/make efficient/scale. It seems that distributed systems are at a disadvantage because they are harder to manage, and need more fine-tuning to work well. (I don't mean just setup cost of the system itself, which can be offloaded to e.g. Amazon EMR, I mean in actual day to day usage).

It used to be that heavily SQL-based code was a terrible mess, but it seems DBT has helped a lot with that (disclaimer: I have little actual experience with DBT), so ""modularity"" or ""maintenance"" of SQL is also largely solved, i.e. is not such a big argument in favor of using a general purpose language anymore.

In 5 years, will the bulk of data engineering be done via dbt-orchestrated SQL of some sort? Or am I missing some important area/use case/problem?",2022-06-14 14:08:07
rkhevl,What did you guys wish you knew before implementing everything for your company?,So I'm in charge of the dev/dataengineering/devops for our startup and it's daunting how many things need to be done. What did you wish you knew before building data pipelines and choosing datawarehouses/data lakes/delta lakes etc. Thanks guys.,2021-12-20 07:17:34
pc2yne,"If you could go back in time and tell yourself one thing before building your first pipeline, what would that be?","I'm about to build my first pipeline and warehouse, the ETL is pretty darn straightforward in this case... rip the data out of the shitty accounting platform architecture into something reasonably sane.

I plan to treat this as a learning exercise because I'll be responsible for building almost all the infrastructure in this firm for the foreseeable future.

I was hired as an analyst, when the firm had no DE or infrastructure to speak of, so I've been learning how to wear that hat and enjoying it a lot actually. I'm working my way through Kleppman's DDIA.

I don't need someone to hold my hand but I am curious, what sorts of things would you have kept in mind or done differently if you could do your first pipeline over again?

Note that I am the sole IC on this project so tips on making it maintainable are highly relevant.",2021-08-26 16:25:18
p6j1l1,Any mid-career coaches for people like me?,"I'm kind of a DE. I'm kind of a SQL developer. I'm kind of a visual report dev. I'm definitely a data modeler, and I'm also kind of a BA.  Can you tell my organization is under resourced?

In short, I do a lot of stuff, and I'm stretched way too thin. I feel a strong need to angle for depth over breadth because I'm painfully aware of how much better I could get at any of these things if I just had the opportunity to focus.  I'm concerned that my broad shallow skill set will make it hard to score a senior role in another organization if I decide to leave my current job, and I'm also afraid that I'll never be able to focus and really level up my skills in my current one. It's worth mentioning that I actually like all the things I do, and I would be perfectly happy focusing on any one or two of them, if it were only possible to make it happen. It's also worth mentioning that I am getting real close to burnout because of the chaos and overwork.

The last thing is - I work in a low-paying sector and I know I could make a lot more if I could figure out how to make a change (which means figuring out a path first). And also, I like my job and coworkers as much as I hate the shitshow we're all in. 

Are there coaches out there to help people like me decide what I should do?",2021-08-18 03:20:51
n916sy,What are the Golden Rules of Data Engineering?,"Hey r/dataengineering,

I'm starting an entry level data engineering job soon, and I wondered what the community thought were the ""golden rules"" of the field. These could be as broad or specific as you like - I'm just trying to get some food for thought in advance of my first day.",2021-05-10 10:25:35
n2jm1s,What are the most commond advanced SQL interview questions asked at FAANG?,I am going to have a data engineering role interview pretty and would like to know what are the most difficult advanced question they could ask for SQL? Could you please share your experience?,2021-05-01 14:38:44
jb6ody,How Airbnb trained 5K+ Employees to use data in their day to day job,N/A,2020-10-14 18:39:52
14scpwd,Ibis: The last dataframe API you'll need to learn? I hope...,N/A,2023-07-06 15:34:26
14rqj5o,Any self-taught data engineers here who feel like there are a lot of people who are focused on discrediting you?,"I get the sense during interviews that some people are more focused on trying to figure out anything I don't know, rather than talking about skills that actually pertain to the role. 

A lot of times, people with traditional computer science backgrounds talk to me in interviews like I'm an ""exhibit"". Getting comments like, ""Oh, I've always wanted to meet a self-taught person"", or ""You've done well so far, for a self-taught engineer."" 

Then proceed to ask me questions about something oddly specific like linting, or data warehouse processing algorithms. 

I feel like I have to be ""beyond perfect"" in a lot of situations, and even then people will still waste my time, and go with someone with a more traditional computer science background. 

It's really exhausting, and is making me consider doing a post-bach or masters, just so I don't have to deal with this anymore. 

Context - I'm a senior data engineer who leads a team with a lot of members that have under-graduate or masters degrees in comp sci or data science. ",2023-07-05 23:05:15
116a03p,What's the toughest DE problem you faced in your work career?,*Hoping for an interesting thread*,2023-02-19 13:13:50
10a07h5,Why Doesn’t the Modern Data Stack Result in a Modern Data Experience?,"Just wanted to get the community's take on this topic. Any thoughts?

*""The data landscape is exploding with tools.*

*As data professionals we have at our fingertips specialized tools for anything: from specialized databases (graph, geo, you name it) to tools for SQL-driven transformations (looking at you, dbt).* 

***Yet, a lot of data work is about provisioning, selecting, administering, and just maintaining those tools. Which is just a pain.""***

[https://www.keboola.com/blog/why-doesnt-the-modern-data-stack-result-in-a-modern-data-experience](https://www.keboola.com/blog/why-doesnt-the-modern-data-stack-result-in-a-modern-data-experience)",2023-01-12 13:55:04
zo4ntl,What did you screw up this year,"I will go first. Yesterday I deployed my spark script and it overwrote the table I been loading for a while.

Just started a new project this week and this project use older version of Spark. It was supposed to only overwrite the partition but instead, the whole table was wiped. 

Who knows that dynamic partition overwrite only work after spark 2.3

Now I am spending part of the weekend rebuilding the table.",2022-12-17 11:46:42
wou44a,Data pipeline project for beginners,"I successfully created my first end-to-end automated data pipeline, that takes youtube data (from Kaggle) as its source, transforms and analysis the data, and creates a dashboard in snowsight( offered by snowflake). I have briefly explained the architecture in my GitHub project repository.

Link: [https://github.com/Nupurgopali/youtube\_data\_analysis](https://github.com/Nupurgopali/youtube_data_analysis)

Feel free to let me know any type of improvement that I could have done to the pipeline architecture!",2022-08-15 08:22:31
urk1vw,Appreciation❤️,"Hi guys, I’ve been trying to pivot careers and I had a data engineering class for my masters. I did really well, and I honestly had to thank this subreddit group because y’all helped me out so much. I really appreciate it, and I hope you guys continue to be the amazing and supportive group that you are.",2022-05-17 11:12:01
ubghg8,Let’s study together!!,I want to be a data engineer and I’m at the basic level. Just learning sql buti need support. Who wants to form a study group?,2022-04-25 08:46:47
u6zarh,Biggest debates in the industry?,"I'm new to data engineering, and I'm curious about what people have different opinions about. There's only so much you can learn by searching Google...",2022-04-19 07:22:26
pfwuyg,Quarterly Salary Discussion,"This is a recurring thread that happens quarterly and was created to help increase transparency around salary and compensation for Data Engineering. Please comment below and include the following:

1. Current title

2. Years of experience (YOE)

3. Location

4. Base salary & currency (dollars, euro, pesos, etc.)

5. Bonuses/Equity (optional)

6. Industry (optional)",2021-09-01 16:00:16
n3h23y,Do most salary’s cap at around 120-150k?,"Noticing with SWE the salary can go to 300k-400k. 

Have not really been seeing that salary DE. 

Is it worth it to stay in this field long term? 

If you do have a salary that exceeds the 150k mark, can you disclose the YOE? 

Thanks",2021-05-02 21:59:49
l53wql,Hot take: You (probably) don't need high availability,"Seriously, I know you love writing your Kubernetes YAML and configuring  net topologies for highly resilient clusters, but before you embark into a months-long quest for high availability, ask yourself, what happens if your jobs don't run for fifteen minutes, or one hour, or even four! What's the real impact for the business? Nine out of ten times, one hour of downtime will be a minor nuisance, and you can recover a simple system from a backup in ten minutes or less, specially if you document and automate properly. I have, in the past, and with even basic monitoring, your infrastructure team can handle it and only warn you after the fact.

But you didn't listen to some random guy on the web and built your robust, high-availability system with a Kafka queue for your jobs and dynamically allocated Kubernetes instances. Great. It will probably hum along for much longer without failures, but when it does fail, you won't fix it in fifteen minutes. It may take a day. Maybe three. You will have to search the web trying to match this cryptic error spit out five levels deep with an incomplete Stack Overflow answer. Again, I've been there. It wasn't fun.*

I'm not saying you should *never* go for a spiffy, high-availability setup, but consider the cost, make sure you are comfortable with the tech, and have detailed plans to recover from failure, because as Douglas Adams wisely said, ""The major difference between a thing that might go wrong and a thing that cannot possibly go wrong is that when a thing that cannot possibly go wrong goes wrong it usually turns out to be impossible to get at or repair.""

\* It was HA Hadoop, though. That was a few years before Kubernetes became a thing, and we were still on-premise.",2021-01-26 02:20:57
1904k5j,DBT Testing for Lazy People: dbt-testgen,"[dbt-testgen](https://github.com/kgmcquate/dbt-testgen) is an open-source DBT package (maintained by me) that generates tests for your DBT models based on real data.

Tests and data quality checks are often skipped because of the time and energy required to write them. This DBT package is designed to save you that time.

Currently supports Snowflake, Databricks, RedShift, BigQuery, Postgres, and DuckDB, with test coverage for all 6.

Check out the examples on the GitHub page: [https://github.com/kgmcquate/dbt-testgen](https://github.com/kgmcquate/dbt-testgen). I'm looking for ideas, feedback, and contributors. Thanks all :)",2024-01-06 17:07:47
15trl9q,One company wants me to attend 5 interview rounds in 2 days. Even worth it?,"Hi there,

I already have a job and this position is paying 160k in MA area and is looking to conduct 5 interview rounds.

3 on one day starting 11 am until 2 pm. with three different individuals

and 

2 on another day from 10 until 12 with 2 other individuals.

&#x200B;

I had asked them to respect my time and have two one hour interviews but they sent this anyways.

I feel like just saying no to this. This is getting out of hand.

Unless they want to pay 300k, I feel like this would just waste of my time.",2023-08-17 15:59:08
13tffsk,Company wanted both Databricks and Snowflake so we have them (Airflow for data orchestration). Any advice on how best leverage these two platforms?,"So I started in in fairly new role at a new company. The tech lead who just left decided that we would be using both databricks and snowflake. Those tools are now both stood up and ready for use, but we have no plan or best practices about how to leverage the strengths of both these platforms.

My boss wants us to definitely have an architecture that is using snowflake and databricks because he thinks it would be a bad look if we went down to one platform after paying/pitching for both.

My starting thought is use Databricks as the data lake and transformation layers. Push “gold” datasets over to snowflake that you would put in a data warehouse or something that would be fed off BI tool traditionally. Use Databricks also for exploratory analysis and research, machine learning, etc. Use Airflow as master orchestration layer.

Any help or thoughts appreciated! I’m kinda being forced into this decider role, which is cool, just polling for some community support :)",2023-05-27 19:05:22
1331rzp,What tools do you “need” to do your job?,"My company uses a lot of cool stuff from the typical MDS but I’m honestly not sure if I have ever completed a ticket that truly “required” anything beyond SQL, a database vendor specific ETL tool, and maybe some python.  

I feel like most of the work I do is the result of bad business analysts and bad requirements gathering. Building data lakes, giant complicated models, calling API’s constantly, unpacking absurd amounts of flat files, all to build data pipelines to drown teams with “real time” data because that’s what they asked for, when they really just need a few KPI aggregated monthly.  

Wondering how common this is? To feel like everything you do is extreme overkill relative to the actual business problem that needs solving",2023-04-29 17:33:53
1289p1k,Which Data jargon or concept did you have a hard time grasping?,"For me, I couldn't for a while fathom what *Data Mesh* and its analogue, *Data Product* meant. For a long time I thought ""Aren't all software products data products? I mean, what product doesn't use data! Why do we need a new terminology?""

After much reading, I got to know that the term *data product* is being used in a different sense altogether - to literally treat data itself as a product & the resulting activities that a domain team would do to ship it (governance, infrastructure etc). And the Data Mesh (to put it succinctly) is the substrate that arises organically when different teams start using each other's data products!

Are there any such data lingo that you found cryptic?",2023-04-01 03:24:10
112dz5s,A short tutorial on running Spark with Jupyter using Docker,"Hi r/dataengineering

I wrote a [short tutorial on how to run PySpark in a Jupyter notebook](https://www.datain30.com/p/run-spark-with-jupyter-using-docker). Let me know if you try it out and are able to finish in under 30 minutes.",2023-02-14 19:33:39
10ghhal,"Has anyone here transitioned from Data Scientist to Data Engineer? What was your motivation, and do you regret the move now, or are you happier as a Data Engineer?","I posted the same in r/datascience, but thought it would get a better traction here instead, since many of you are actual data engineers. The original post is [here](https://www.reddit.com/r/datascience/comments/10fpmi2/has_anyone_here_transitioned_from_data_scientist/?utm_source=share&utm_medium=web2x&context=3) for anyone curious.

So basically, I'm curious to hear from people who work as data engineers that transitioned from a data scientist. What made you switch and do you regret you decision? Or was it one of the best decisions for your career? Thanks!",2023-01-19 23:59:52
102qkc6,What are some questions you would ask a DE Team in an interview to see how advanced the team is?,"Kind of flipping the typical question around. What best practices would you ask a DE Team you were interviewing with to know the maturity/skill level of the team?

I am thinking of technical questions but also how they structure their work?",2023-01-04 01:53:50
x1yski,Senior data engineers! What should junior data engineers know?,"Hi

What makes you a ""senior"" in the DE field?

Is it the way you program? 

Is it extensive knowledge of distributed systems?

Is it data governance or data provisioning?

What makes you a senior?",2022-08-31 00:07:22
v5i71i,"Design, Development and Deployment of a simple Data Pipeline",N/A,2022-06-05 17:30:16
ujpaxm,Designing a whole data infrastructure from scratch,"I'm a part-time Data engineer/analyst/architect/everything in an e-commerce grocery startup.
The responsibilities are taunting, however very interesting.
I have some experience in data engineering but I have very limited amount of resources to use. The higher management want to see the value added for the business at each step I take. Is there any guidelines to build a warehouse in a fast and easy to mantain way ?
The second question would be what's the best warehouse solution AWS Redshift, GCP bigquery or snowflake, in terms of cost and maintainability, taking into account that we are launching the app in a couple of weeks and we have 30-50 orders per day ?",2022-05-06 14:56:04
t7hdf9,What made you struggle as a beginner?,"I am writing several “Explain like I am five” blogs and would love to feature some of the community’s questions.

What were the most difficult concepts when you were starting out?",2022-03-05 19:53:27
r3au4g,Reasonable experience requirement,N/A,2021-11-27 10:25:53
qoxm2r,"how did you learn about writing ""optimized"" and ""clean"" SQL queries?","so I have a background in DS/ML, and although I have had previous coursework in database design, that is where it ends and I never used SQL in my projects seriously (outside undergrad course projects). I have no issue writing queries for Hackerrank, interview questions but my problem is that from a code-review perspectives, I have received this feedback that my codes are verbose and not optimized. For example, it seems like I am using too much subqueries, join operations, etc. when it is not necessary in the context of the question.  

I am interviewing for DE positions (along with DS/ML positions) and I want to brush on my SQL skills to be able to write codes that are of production-level qualities. Any advice is appreciated",2021-11-07 20:55:37
n6vmta,Suggested courses / material / books to start working as an DE,"Hi, I worked as a Data Analyst last year and I will start working as a Data Engineer.


In this new position I will work with Python, PySpark, SQL and Airflow, IoT platforms such as ThingWorx, container technologies and cloud solutions such as Azure, namely Data Lake and Machine Learning services.

My last year as a Data Analyst I worked daily with Python and some SQL.


I would like to know suggestions for courses / videos / books / material to learn more about PySpark and Apache Spark, Airflow and the rest of the technologies mentioned above.


Thank you very much

EDIT: I have recently bought the book ""Designing Data-Intensive Applications"" and know about the ""Awesome Data Engineering"" github (https://github.com/igorbarinov/awesome-data-engineering)",2021-05-07 10:45:31
ma51fb,Spark (Python particularly) learning materials,"Hello!!

Does anyone have some recommendations on how to learn Spark? I'm thinking udemy/coursera. I've already done awsdojo and that was super useful to get set up but I want to really dive in.

At the moment I'm essentially just porting python to pyspark which works, but probably isn't going to be the best way of doing things",2021-03-21 20:13:56
1bh2wha,How to become a good engineer ?,"Hi everyone,  
I am seeking guidance how can i become good overall engineer ? What i can learn to be better ?

**Experience**: 3 years  
**Age**: 27  
**Current role**: I am working on low latency data pipelines using scala, flink, cassandra, redis, s3 and kafka in a product based company.  
Other tools i am using prometheus, grafana for monitoring and kubernetes, docker for deploying apps.  
**Leetcode rating**: My Leetcode rating is 1960 and currently practing more on it. solved around 1200 problems. Using cpp.  
**System Design**: Completed Design data intensive applicaion book and Grokking the system design course.  
**Low level Design**: working on it this too using cpp.

Recently I read  design data intensive application book, reading it once again to get more out of it.  Next i am thinking of reading one os book (Galvin), microservices by sam richardson book. I also work on understanding how cassandra redis and kafka works internally in the free time.

I didn't try to learn kuberentes and docker in detail because they are vast in terms of concepts. My current job does allow only to used them as platform for deploying apps and there are other engineers for devops work.

I want your guidance on what can i do to become a good successful engineer and  join good company in future. I am not thinking of switching in few months becomes i want grind more in coming months and my current job able me to provide good free time.  **Is anything needs to be changed or added to what i am doing currently ?** Any comment would be much appreciated. Thanks in advance.

Sorry for my bad english.",2024-03-17 17:16:21
1b26kf9,Favorite SQL patterns?,What are the SQL patterns you use on a regular basis and why?,2024-02-28 13:53:22
188grde,Quarterly Salary Discussion - Dec 2023,"https://preview.redd.it/ia7kdykk8dlb1.png?width=500&format=png&auto=webp&s=5cbb667f30e089119bae1fcb2922ffac0700aecd

This is a recurring thread that happens quarterly and was created to help increase transparency around salary and compensation for Data Engineering.

# [Submit your salary here](https://tally.so/r/nraYkN)

You can view and analyze all of the data on our [DE salary page](https://dataengineering.wiki/Community/Salaries) and get involved with this open-source project [here](https://github.com/data-engineering-community/data-engineering-salaries).

&#x200B;

If you'd like to share publicly as well you can comment on this thread using the template below but it will not be reflected in the dataset:

1. Current title
2. Years of experience (YOE)
3. Location
4. Base salary & currency (dollars, euro, pesos, etc.)
5. Bonuses/Equity (optional)
6. Industry (optional)
7. Tech stack (optional)",2023-12-01 17:00:46
170rvz3,"Is the reason SFTP file transfer in banks, healthcare, etc is so ubiquitous just because they are using older systems without robust REST APIs...?","Apologies as my background isn't in DE but come more from BI systems administration. In my career I've seen a lot but the craziest thing I've seen is business users using email heavily to send Excel and CSV data dumps using BI tools...at a huge scale. Of course there are also some secure file transfers out there, stuff you would see with MoveIT and IBM Sterling, etc. Both of these models beg the question to me which is -- why do we need this?

Is it literally just because our system doesn't have a reliable REST API to pull the data easily? Is REST that insecure? So instead we rely on the other party generating a CSV and sending it to us in batch like it's 1995? Couldn't a REST API send a CSV over HTTPS anyway? Do we not trust it?

Is data integration really that dirty? What can be done to eliminate these file transfer overhead monsters? Is progress with good REST APIs just moving that slow?

I feel like there's something I'm fundamentally misunderstanding but not sure what it is...",2023-10-05 20:07:20
15yz47k,Do you truly enjoy data engineering?,"Just reflecting on my career and how fortunate I feel to have found something I really enjoy doing. 

I just think data is so important to civilization, and being able to build proper data pipelines, scalable infrastructure, and accessible data warehousing feels like my calling. 

Granted I’ve only been doing this for 5 years, and obviously some days it just feels like work. Anyone feel the same or feel differently?",2023-08-23 09:49:22
13v6dku,Is your boss technical in that he/she can help you with difficult problems or non technical?,"Is you boss someone who can help you with technical stuff or do they just manage you and you team from above? I'm just asking because my boss is completely non technical and sometimes its not an issue, but other times it would be nice to have someone who understands what his team is doing. It prevents hours wasted trying to explain something rather than a quick call or maybe an email. More importantly, it helps when you're trying to explain ""why"" something can't be done or why something might take longer than expected when you're dealing with someone who understands certain technical aspects of your job.",2023-05-29 20:58:56
zsprzt,"Organization wants to use SharePoint as a ""database""","So my organization has had O365 with the power platform for a few years with a handful of individuals getting really good at treating SharePoint as a database for there power apps(""front-end"") and power bi reports with power automate work around. The company doesn't want to invest in a proper database like SQL server or full Datavers.

I think this is a recipe for disaster and would like to get options for or against continue using SharePoint as a ""database"".",2022-12-22 16:12:23
sonq4y,Do you like your job as a DE?,"Do you like what you do? 

I’m debating switching careers.

I’m currently an analyst and realized idgaf about actually analyzing the data 😂",2022-02-09 20:56:49
pgsv68,"We Don't Need Data Engineers, Data Scientist Need Better Tools - A Data Engineers Response To A Data Scientist","A few weeks ago I came across an article titled ""[We Don't Need Data Engineers, Data Scientists Need Better Tools](https://towardsdatascience.com/we-dont-need-data-engineers-we-need-better-tools-for-data-scientists-84a06e6f3f7f)"".

Now, reading the article, this title was somewhat clickbait. The author was focusing more on the end portion the data workflow. Where data scientists sometimes need data engineers or ML engineers to implement their code.

I have many times ""productionized"" data scientist's logic.

So I get what they were going for. However, I also feel like they drastically minimized the role of a data engineer to being a support role of a data scientist vs. a key component for most businesses.

Regardless if they have a data science team or not.

I decided to respond to this article with my own content titled. 

[We Don't Need Data Engineers, Just Better Tools - A Data Engineer Responds To A Data Scientist](https://www.youtube.com/watch?v=z_sR2CVG8kk)

But I would like to know your thoughts? Do you agree with the authors opinion or do you see data engineers having a much larger role.",2021-09-02 23:05:35
oipivt,I LOVE BEING A DE. But I also hate it..,"If I wasn't paid so handsomely, I'd think twice about being a DE.  Maybe move onto Program Management or something.  I consider myself a pragmatic programmer: therefore am easily able to get out of pickles here and there.  But good God, do I hate being the bloody single point of failure for so many data assets (work in a small ""agile"" team).

You know.. thinking on your feet and crafting replies that would elicit ""tipping of the hat"" by politicians (to make sure your stakeholders leave you alone and STFU while you have to hose the fire).

I know this is dependent on team/org/company, but having been in various DE roles, stress will always be a part of the job.  For folks who are experienced:  does this ever get any better?  Or do you just develop a mentality of ""leaning in"" - e.g. leaning into stress/the job/the bullshit?",2021-07-12 11:40:51
nbua2l,ETL Pipelines Learning Resources,"I’ve been an Azure Cloud engineer for a bit and starting to get into big data in the hopes of making a move into a Data Engineering role. Anybody have any solid learning resources on properly designing end to end ETL pipelines using Spark/Databricks + orchestration tools + whatever else goes into making a production-ready ETL pipe?

EDIT for clarity:

So in software engineering there are design patterns and most fields have some sort of best practices. How would I learn these sort of things specifically for data engineering? Is there a learning resource that would help me understand how to approach various scenarios that require ETL in whatever shape and what the best practices in those scenarios would be?",2021-05-13 23:15:16
bs2a7p,Udacity Data Engineering Nanodegree Course Review,"# Overview

Udacity's new Data Engineering Nanodegree. The course is broken up into five sections, Data Modeling, Cloud Data Warehouses, Data Lake with Spark, Data Pipelines with Airflow, and a capstone project. Each section has different instructors, with each one bringing a different teaching style in a way that keeps things refreshing while still keeping you wondering if it happened simply due to lack of communication. The structure for each section consists of introducing concepts through lectures, reinforcing the material with demos and exercises (typically in a Jupyter Notebook), and concludes with 1-2 project(s) dealing with designing an ETL process using song data for an imaginary company called Sparkify.

# My background

I have about two years of professional experience wrangling data with Python and SQL and about a year and a half of web development experience. I have a bachelors degree in engineering and took a few introductory computer science courses. A few months ago I completed [Dataquest's Data Engineering Path](https://www.dataquest.io/path/data-engineer/) and have taken a few [DataCamp](https://www.datacamp.com/) courses as well as [CS50](https://www.edx.org/course/cs50s-introduction-to-computer-science) and [CS50 Web](https://cs50.harvard.edu/web/2019/spring/). I enrolled in this course due to its focus on cloud technologies, which I have been learning through trial by fire at a data engineering job I started a few months ago, mostly using AWS, Postgres, Python, and Airflow.

# Individual Sections Review

## Data Modeling

This section introduces what data modeling is, why it's important, and what the differences between a relational and NoSQL database are. It speaks on important concepts such as ACID transactions, what fact and dimensional tables are, and what the difference between star and snowflake schemas is. This section uses Postgres and Apache Cassandra and consists of a project for each of them where you design schemas and load song logs and song metadata into fact and dimension tables.

Pros:

* Introduces most of the Postgres and Apache Cassandra commands a data engineer would probably ever use
* Provides a good explanation on when you'd want to use SQL vs. NoSQL

Cons:

* Most lectures consisted of watching the lecturer read slides off her laptop
* This section's exercises seemed to have more bugs than the rest
* There were a few questionable practices in this section such as a try / except block around everything and always inserting rows individually instead of in bulk

## Cloud Data Warehouses

This section builds on the previous section and explains the need for a data warehouse and what the benefits of hosting it in the cloud are. AWS basics such as IAM, creating an EC2 instance, and security groups are introduced, as well as a brief introduction to infrastructure as code using boto3. Other concepts such as OLAP cubes, rollup, drill-down, grouping sets, and columnar storage are discussed. The project consists of designing tables in Redshift and loading data from S3 to Redshift.

Pros:

* Provides practical example exercises such as loading S3 files in bulk to tables in Redshift using the COPY command
* Makes creating a sandbox data warehouse environment much more approachable. Prior to this I always thought it would be too expensive and complicated to build one on my own and this section proved me wrong

Cons:

* Tries to cover too much ground. Topics like infrastructure as code are glimpsed over and overly simplified

## Data Lakes with Spark

Introduces what big data is and why big data tools like Hadoop and Spark are necessary. Provides a conceptual overview of how distributed systems like Hadoop and Spark work. Hands-on exercises consist of using PySpark to wrangle data. Explanations of why an organization may need a data lake instead of a data warehouse are provided. The project consists of ingesting raw S3 files, creating fact and dimension tables, partitioning them and writing them back to S3 all with PySpark.

Pros:

* Provides an excellent explanation on how distributed file systems and cluster computing works
* Gives a good explanation on when to use PySpark data frames vs PySpark SQL and how to port your data between the two styles

Cons:

* This project involved filling in a lot more blanks than the rest of the projects and I found it to be particularly time-consuming. The number of files to ingest from S3 seemed too large to run in a reasonable amount of time
* I wish it would have included more information and exercises about using PySpark on a cluster of machines instead of on a single local one

## Data Pipelines with Airflow

Data pipelines, DAGs, and Airflow concepts such as operators, sensors, and plugins introduced. The final project involves using Airflow to load S3 files into partitioned Redshift tables and perform data quality checks afterward.

Pros:

* The only tutorial I've found on how to use data quality checks with Airflow. I've started using this technique at work and it is a game changer
* Airflow is a bitch to deploy and someone they engineered a way for people to run it on Udacity's workspaces. Kudos to the engineers on that

Cons:

* This section felt a bit shorter and was more focused around a specific technology than the other sections. Not necessarily a con but I would have liked to have the lectures be more generalized around the concepts of a data pipeline

## Overall

Overall, I really enjoyed this nanodegree and learned a lot of practical things from it that I have already started using at my job. I would estimate I spent about 40 hours completing it so I definitely felt short-changed in content and think it is incredibly overpriced for what it is. What I don't like is how Udacity markets their courses as a way for someone to make a career change with no real-world experience. I find that incredibly hard to believe and can't imagine a company hiring someone with no real world experience after completing this nanodegree. I found the content to mostly be of very high quality and I think this is really the only intermediate-advanced data engineering course out there. If you have the cash and are interested in learning data engineering in the cloud I would highly recommend it.",2019-05-23 12:23:04
1bklthc,When NOT to use PostgreSQL? ,"Hello, I'm on my way to learn DE, and this sub has helped me a lot to understand better many concepts, but one thing I see very often is that Postgres seems to be the best DWH in most cases, so my question is, which are those cases where Postgres is NOT recommended? What alternatives are better suited for that situation and Why? 

Thank you! ",2024-03-22 00:09:41
1asim63,How do you prep for SQL heavy technical rounds?,"Leetcode SQL problems? Review concepts like window functions, joins, etc?

I wrote tons of SQL in a past job but it was data modeling heavy and more ""practical"" than what these interview seem to be asking.",2024-02-16 20:25:19
1asegcy,Blog 1 - Structured Way to Study and Get into Azure DE role,"There is a lot of chaos in DE field with so many tech stacks and alternatives available it gets overwhelming so the purpose of this blog is to simplify just that.

**Tech Stack Needed:**

1. SQL
2. Azure Data Factory (ADF)
3. Spark Theoretical Knowledge
4. Python (On a basic level)
5. PySpark (Java and Scala Variants will also do)
6. Power BI (Optional, some companies ask but it's not a mandatory must know thing, you'll be fine even if you don't know)

The tech stack I mentioned above is the order in which I feel you should learn things and you will find the reason about that below along with that let's also see what we'll be using those components for to get an idea about how much time we should spend studying them.

**Tech Stack Use Cases and no. of days to be spent learning:**

1. **SQL**: SQL is the core of DE, whatever transformations you are going to do, even if you are using pyspark, you will need to know SQL. So I will recommend solving at least 1 SQL problem everyday and really understand the logic behind them, trust me good query writing skills in SQL is a must! **\[No. of days to learn: Keep practicing till you get a new job\]**  

2. **ADF**: This will be used just as an orchestration tool, so I will recommend just going through the videos initially, understand high level concepts like Integration runtime, linked services, datasets, activities, trigger types, parameterization of flow and on a very high level get an idea about the different relevant activities available. I highly recommend not going through the data flow videos as almost no one uses them or asks about them, so you'll be wasting your time.**\[No. of days to learn: Initially 1-2 weeks should be enough to get a high level understanding\]**  

3. **Spark Theoretical Knowledge**: Your entire big data flow will be handled by spark and its clusters so understanding how spark internal works is more important before learning how to write queries in pyspark. Concepts such as spark architecture, catalyst optimizer, AQE, data skew and how to handle it, join strategies, how to optimize or troubleshoot long running queries are a must know for you to clear your interviews. **\[No. of days to learn: 2-3 weeks\]**  

4. **Python**: You do not need to know OOP or have a excellent hand at writing code, but basic things like functions, variables, loops, inbuilt data structures like list, tuple, dictionary, set are a must know. Solving string and list based question should also be done on a regular basis. After that we can move on to some modules, file handling, exception handling, etc. **\[No. of days to learn: 2 weeks\]**  

5. **PySpark**: Finally start writing queries in pyspark. It's almost SQL just with a couple of dot notations so once you get familiar with syntax and after couple of days of writing queries in this you should be comfortable working in it. **\[No. of days to learn: 2 weeks\]**  

6. **Other Components**: CI/CD, DataBricks, ADLS, monitoring, etc, this can be covered on ad hoc basis and I'll make a detailed post on this later.

Please note the number of days mentioned will vary for each individual and this is just a high level plan to get you comfortable with the components. Once you are comfortable you will need to revise and practice so you don't forget things and feel really comfortable. Also, this blog is just an overview at a very high level, I will get into details of each component along with resources in the upcoming blogs.

&#x200B;

Bonus: [https://www.youtube.com/@TybulOnAzure](https://www.youtube.com/@TybulOnAzure)Above channel is a gold mine for data engineers, it may be a DP-203 playlist but his videos will be of immense help as he really teaches things on a grass root level so highly recommend following him.

[Original Post link to get to other blogs](https://www.reddit.com/r/dataengineering/comments/1arpamc/guiding_others_to_transition_into_azure_de_role/?utm_source=share&utm_medium=web2x&context=3)

&#x200B;

Please do let me know how you felt about this blog, if there are any improvements you would like to see or if there is anything you would like me to post about.

Thank You..!!",2024-02-16 17:35:12
17heehr,How to earn the big bucks in Data Engineering?,"I have been a data analyst for the last 2 years and just managed to land a decent mid level data engineering (Thanks to my DP - 203 certification) role, which is way less compared to what I can achieve in this field. I am not complaining but I just want to know what would be required off me to get paid the big bucks.

I wanted to know what would be expected of me if I were to apply for a senior data engineering roles? Asking just so I could focus on those areas once I commence at my new role. Also, any certifications you guys would suggest? I see videos on Instagram where people claim to earn 450k as a data engineer.",2023-10-27 03:24:32
17aadnb,"I've got a DE Interview, but they're not letting me use libraries or SQL?","I applied for a DE role. After the first screening, HR shared some of the next steps in the recruiting process, among which is a technical interview. Their e-mail says they'll have me do ""big data"" code challenges, and explicitly states that SQL, Pandas, etc. won't be allowed (only default Python). 

I'm honestly confused as most interviews I've had expected me to use data-related technologies, I find it odd that they'd explicitly exclude them. Has anyone encountered a similar situation?

Maybe I'm reading too much into this, and their description was just a weird way of saying ""expect standard data structures leetcode""?",2023-10-17 22:16:02
160xwua,interview: this a red flag?,"During an interview for a Sr. DE role, the team lead told me:

""In this role, you will be using X,Y,Z technologies which you are not familiar with. This is an urgent position, and you will be expected to hit the ground running and deliver. There will be no KT. Will you be comfortable in this situation? I want to be transparent with you and not hide anything.""

I took this personally as a red flag for me, given how I am not familiar with the tech stack and I interpreted their comments as me possibly not being given ramp up time to get familiar with the tools.

Thoughts? Should I flee?

EDIT: Data Engineer role, not Data Analyst. Company has +60K employees. Tools in question are for migrations from on-prem to cloud.",2023-08-25 12:29:36
14irwgo,"I want to practice my skills in Airflow, AWS cloud provider, and Snowflake by working on a project, and I came across this project pipeline. Do you think this is a good project to enhance my skills and include on my resume? I am aiming to find an end-of-study internship at a reputable company.",N/A,2023-06-25 17:29:12
14b5t87,How old were you when you landed your first real data engineering job?,I’m going to guess early to mid 20s.,2023-06-16 19:33:29
13cb3xl,Future of DE,"Folks in the data engineering community like Zach Wilson have expressed that DE will change within the next 5 years or so because of AI, where the responsibilities of a DE will basically go into 2 buckets. One more focused on the business like data analysts and the other more technical like software engineers. I have a background in analytics and really thought that would lead to my first full time job, but I naturally got more and more into data engineering as I learned more. Do you think it’s still worth it, for someone like me who got offered an entry level Data Engineering role, to stay in this field of Data Engineering or should I start looking into Data Analyst type roles? 

My reasoning to accept the role would be to gain a lot of technical and business knowledge. Plus having a background in analytics and dashboards would also improve my skill set. Do you guys have any advice/career tips?",2023-05-09 01:20:36
11oml2t,"Are there any other platforms like Kaggle, but for data engineers instead of data scientists?","I see lots of competitions on data science specially on Kaggle, but i didn’t find anything for data engineer. I want to work on datasets to improve my Data Engineering skills. Is there any platform for data engineers, if no, how can I use kaggle to learn data engineering.

Thank you in advance",2023-03-11 15:14:07
115vbvp,Boss wants a data mesh,"My boss heard talk of “data meshes” at a conference and now thinks the future of our org depends on us building one now (we currently have a data warehouse and s3 lake for our data needs). Our organization currently has a centralized team of 4 data engineers and one architect.

Our conversation:

“So we’re giving ownership of data engineering and architecture to the business units?” “No, there’ll still be just the central data team continuing to fulfill requests in a round-robin fashion.”

“Why are we doing this?” “Because it’s better and we want to be forward thinking.”

Am I missing something here about the magic of data meshes if you’re not actually transferring data ownership to the business units? Isn’t that kinda the point?",2023-02-18 23:26:32
wby0c0,HUGE imposter syndrome as a junior. What do I do?,"I got hired as a junior data engineer 2 months ago after completing my bootcamp. For the past two months I've been doing Datacamp modules set out by my company. They were pretty chilled and straightforward.

This week I was moved to a team working on a project. OMG that was a difficult experience. I'm not the best coder, but I tried. I was given my first ticket and I really struggled, even on the basic of things. I can sense my senior just getting annoyed and pissed at me.

I can't help but feel I'm a burden/nuisance to my team. My first PR was full of mistakes, and honestly, I've lost my sleep. My heart literally hurts thinking I'm way out my depth.

I spoke to my seniors and asked them what they expected of me, they replied with ""magnificence"". Over the next couple of months I've been tasked with completing my AWS certificate and Terraform stuff.

Should I be feeling like this, is this normal? What advice do you have to improve my situation?",2022-07-30 13:51:14
w5xbku,What should a data engineer’s GitHub look like?,Please share examples or descriptions.,2022-07-23 06:48:15
vnbgc4,"Where can I get the hot or latest news about data lakehouse or dataengineering？ Are there blogs, websites, or media to recommend it?","I don't know where to get the information about data lakehouse, such as news, latest technology, industry analysis, etc. For example, the news about Google JAX is hot these days, but where can I get this information? I read about it on Reddit, but what blogs or news sites are they reading about it from? Could you recommend me some dataengineering or technology news websites or blogs? Thank you!",2022-06-29 09:14:12
trqki4,Is it worth me applying to Data Engineering roles with this Resume/CV?,N/A,2022-03-29 22:21:13
qqec8x,"Designing a Data Platform, when to choose Databricks over other DWH tools","Hi all:

I'm currently designing the data platform of my company and I have seen the following article that Databricks has shared in their social media:

[https://blog.denexus.io/databricks](https://blog.denexus.io/databricks)

With a description on why they have decided to use Databricks and their data lakehouse approach rather than other data warehouse based tools. 

According to what its said: if you are going to do a lot of machine learning, if your data could be of different types or formats and if your data volume is going to scale, the best option in the market seems to be Databricks.

What do you think? Did you have a similar scenario and used another solution? Why?",2021-11-09 21:54:03
pyr9vx,Your default tool for ETL,"For both small and big data. Whats your preferred tool for running ETL processes, and why?",2021-09-30 19:43:19
pl3pos,I'm a Data Engineer. How do I become a better Software Engineer?,"I became a DE after working in analytics and gravitating towards building infrastructure. I work with the modern data stack, so I'm comfortable building data pipelines using tools like Airflow and dbt. I've also worked on CI/CD, data quality, and observability within data.

However, I still have a large skill/knowledge gap when it comes to software engineering fundamentals.      I can write DAGs and data models, but when it comes to spinning up new services, provisioning infrastructure, or debugging complex software issues, I'm much less comfortable.

I recognize that since I can code and know my way around the stack, I'm starting from a good baseline. For someone with me background, what is the best way to learn software engineering?",2021-09-09 18:35:38
pkwx94,Is this group moderated actively? Can it be improved?,"As the title says. I love this sub, but not sure if it's just me, but everyday there are new posts created with people asking same questions (I.e. how to get in, get a job, doing x,y,z what are my chances) which creates a lot of unnecessary spam. 
Are mods looking into organising the sub and moderating it to avoid this? For example, create weekly/daily thread of 1. Help on Tasks; 2. Help on entering field, 3. Learning resources, 4. Leetcode problems, etc. 

I am happy to help the mods (or join them) if help is wanted/needed. Or am I missing the point somewhere and my expectations are not relevant here?",2021-09-09 12:34:45
p9uvp9,Trigger a data engineer with one sentence ? ( Fun ),Just wanted to try this trend in here. Let's see how it turns out.,2021-08-23 07:35:21
lih2xg,“Data Engineering is the new Data Science” DS Interviews: -15% DE Interviews: +40% 🚀🚀🚀,N/A,2021-02-12 18:18:35
kv1vc5,How To Become a Data Engineer in 2021,N/A,2021-01-11 12:41:00
1ac9fpf,What was your DE job from hell?,"I used to work at an extremely big company (actually one of the biggest corps in the world) as a data engineer. Their offer was great, great salary, fully remote and other nice perks.

I joined in and my first thought was how incredibly disorganized everything was. There wasn't even like an internal tooling to check your organization members. I only knew who my direct boss was, but I had no idea about any further hierarchy or even who my other teammates were.

It took them literally around a month and a half for them to assign work to me. Despite being part of an established team, I was ""loaned"" to another team led by a contractor and this guy was completely unaware about my capacities, so he was not able to properly assign work to me because he just couldn't find a task that fit me. The work I was assigned was literally just writing view definitions with SELECTs and JOINs. They'd give me a humongous Excel listing the columns they wanted, and their datatypes, but it was a major PITA because the amount of columns was huge, and every single one of them had an encoded name, so I was essentially modeling data from a blackbox. I NEVER knew what this data was, who was its consumer, or what could they do with it.

To this day, I'm still not sure what these people were doing, as in, what was being developed? What did the pipelines process? Were there even pipelines? No clue, all I I did was looking at an Impala UI and writing SELECT.

Needless to say, I left pretty fast. Happy to be at a place where you're actually aware of what is going on.",2024-01-27 12:05:53
18vdch8,Should I be offended? Project manager send me a code from Chatgpt,"I'm working on multiple things at the same time and last week a PM added some tasks and was pushy about it but other priorities are taking place, all the sudden he emails me a python code and asked me just to schedule it. I don't know how to react to this situation, and the code he sent is flawless, I'm at the point that I feel I can easily get replaced. Wanted to vent out with fellow DEs. What would you do if you were in my position?",2023-12-31 18:05:45
18i9w4x,Small Group of Data Engineering Learners,"Hello guys!

I am making a small group of people learning data engineering where we get on a call together every other week and talk about tools we're learning and other DE-related things. This will be good for everyone in the group to get better at DE and help each other out when needed. 

Thanks, and happy learning to everyone!

Edit: If more of you are interested consider making small groups with each other.

Edit, again: If you are still interested please reach out to other people who want to make groups.",2023-12-14 14:31:41
1860n25,What do data engineers want for christmas?,What do data engineers want for christmas? ,2023-11-28 16:52:17
17jg1x5,i'm seeing less star schemas,"Lately, i've done a few jobs consulting and one thing I found is a lot of teams are using the one big tabl approach as opposed to star schemas. Is anyone else noticing this or is it just me?",2023-10-29 23:44:51
10lyfl0,Got The Job!,"Good News, Everyone! 

Since September of last year I have sent hundreds of applications, been interviewing regularly, and turned down a few lackluster offers. This morning I received an offer from the best company I have interviewed with over this entire endeavor. 

I interviewed with \~10 people from the company from recruiter to director over the past couple of weeks. All of which have shown themselves to be intelligent and enjoy the work that they do, which is shockingly uncommon.  

The company mission is not just vapid corporate-speak, but something I believe in and it seems the entirety of the team gets behind. Without doxing myself, I can say they do research and analytics for Government entities and foundations with an overarching goal of public welfare.

The company has work on all three cloud platforms, has mature+modern tech infrastructure, and offers the ability to learn and experiment with building solutions from scratch. 

I couldn't be more ecstatic to move to get away from the ""use <ETL Tool> to move data from this place to <Datawarehouse> and create a view for analysts to access it"" type of *engineering*\--and I use that term loosely*--*work I was relegated to previously. 

Me: 2YOE, BA in Philosophy, M.Sc. in Information Management

Job: Software Engineer (Cloud Data Platform), Full Remote (USA), 106k , 4 weeks PTO, Casual down-to-earth work culture

A big thanks to this community for all of the advice and guidance over the past 2 years!",2023-01-26 18:15:45
zb329e,Where to Practice Complex SQL questions?,Hello every one. I am searching for websites where I can be tested for complex sql questions. Thanks in advance.,2022-12-03 00:40:55
y4ijk6,Layoffs?,"Are any one of you guys been laid off recently? I’ve seen many PO, analysts and some SWE getting laid off but haven’t really seen any DEs. What I’ve seen is an insane amount of demand for DEs lately. My company is still hiring DEs like crazy, same goes for other companies that our DEs go to lol.",2022-10-15 08:34:00
xkgc8v,Just landed my first role as a DE,"A quick background. I’m a full-time bartender and physics student and nearly done with my degree. I got involved with data and analytics about a year ago with the goal to land a data analyst role.

After having applied for numerous data analyst roles this opportunity fell on my lap. A recruiter happened to reach out to me on LinkedIn about a DE role within their company. Mind you, the only training I have are courses from Udemy and DataCamp in python and SQL. I looked into the role and the company and decided I would apply. To my surprise, I made it through the interview rounds and they made an offer that I accepted.

So here I am, nervous and excited all at the same time but I honestly have no clue what I’ve gotten myself into. Need some insight into common entry-level DE everyday tasks and some guidance toward resources you used when you had just started your journey.

&#x200B;

**Edit:**

I've seen a few people asking about compensation so I'll provide more info here. The initial offer was 87k base with 10% yearly bonus and 40k RSUs. Ended at 91k base, 10% base, 40k RSUs. I was also able to negotiate for a $750 stipend to get my home office set up.

If you wish to continue:

I took a step back and accounted for the 401k with employer contributions, HSA and the contribution, and a plethora of other benefits like health and fitness stipends, mental health assistance, ""unlimited"" PTO (taking this with a grain of salt), fully remote work from within the US, flexible hours and a few other not so glamorous benefits like pet insurance. It's possible that I left money on the table because I'm not the best at negotiating but for now as an entry-level employee I am not complaining. I'll be ready to negotiate my base when evals come around.",2022-09-21 20:40:29
v09zez,What should i practice for the PySpark Interview round?,"I have studied the concepts of Spark and practice few basic data frame, RDD and spark sql based questions. Can you list some important to cover / good to practice spark related questions for a DE interview? I have heard there are a lot of questions around Spark optimizations. Can you point out few important topics or techniques to cover that? Any link to blog or article would also help.",2022-05-29 12:17:13
u1w5fs,"Do data engineer's write a lot of code? Thinking of switching from SWE, but don't want to use GUI tools / drag and drop.","Title... how much code do you guys write on a daily basis? I enjoy programming, but also enjoy working with data.",2022-04-12 10:47:31
schltg,Beginner mistakes to avoid in building Data Pipeline,"Hi everyone,

I've recently been promoted to a Data Engineering position at work. That being said, my first project is helping migrate data from SAP ECC to SQL Server and solidify our data pipeline so my Analytics team can extract data in a more streamlined way for our dashboards and modeling.  
 

I don't have much guidance from technical leadership or access to technical expertise in this undertaking, and I wanted to see if there were any Sr. DE's that had common ""rookie"" mistakes they've seen in similar initiatives that I should look out for.   


Any insights are appreciated.",2022-01-25 16:27:54
r49lr5,I am looking for a roadmap on getting into Data Engineering. I can't hope to follow the popular roadmap shared on this sub.,"I know this roadmap https://www.reddit.com/r/TheInsaneApp/comments/pjt1le/data_engineering_roadmap/?utm_medium=android_app&utm_source=share

It is just massive and I can't hope to do all of that in 1-2 years (I have a full time job too). It looks like a multi year experience check list. Can anyone share a shorter curated roadmap for getting into the field?",2021-11-28 17:34:50
1bf2ntf,Steam Prices ETL (Personal Project),"Hello everyone. I have been working on a personal project regarding data engineering. This project has to do with retrieving steam games prices for different games in different countries, and plotting the price difference in a world map.

This project is made up of 2 ETLs: One that retrieves price data and the other plots it using a world map.

I would like some feedback on what I couldve done better. I tried using design pattern builder, using abstractions for different external resources and parametrization with Yaml.

This project uses 3 APIs and an S3 bucket for its internal processing.

[here you have the project link](https://github.com/edseldim/steam_prices_data_engineering)

This is the final result

https://preview.redd.it/139jgrlemeoc1.png?width=824&format=png&auto=webp&s=204769ad503885eef0153d565f9243e5c5f56add",2024-03-15 01:56:56
1aviw3v,"GPT4 doing data analysis by writing and running python scripts, plotting charts and all. Experimental but promising. What should I test this on?",N/A,2024-02-20 14:26:42
19aty1m,Data Engineer offer retracted after I moved cities.,"Hey everyone,

I don't know if this is the right place to post this but I finally landed a data engineering job after being a dashboard jockey for 4-5 years. It was everything I had dreamt of. I was due to start in two days and I find out that the company retracted the offer due to a massive layoff.
I moved to Toronto, Canada and  put down a deposit for a rental, and basically spent a lot of time, energy and effort.
Is there anything I can expect from the company? Should I go back to looking for data analyst roles?
Just a rant, any advice would be awesome.

Thanks!",2024-01-19 21:13:48
18m4e2i,"Non-technical boss, wanting to micromanage and kills our team","Throwaway because I know they are sometime on Reddit.

My boss is constantly asking for unnecessary details and micromanaging us. I am out of ideas on how to manage them.

It ranges from anything like wanting to be in meetings where we change the name of a cluster, up to asking why we need a tool to track bugs (and since we never had one before, why bother now?). Even not being technical they are:

- checking the SQL queries of my colleagues;
- they want to know if this or that has been documented for tiny operations;
- they are asking for ingestion of new datasets before we have even finished patching bugs or validating stuff, no unit tests because we are always short on time

I am trying to handle the stress of cascading requests by having a roadmap and shielding our team, but my manager will constantly challenge the roadmap and put pressure on my reports.

This is not tenable in the long term, do you have any advice on managing stakeholders above that aren’t technical?",2023-12-19 15:22:46
1725spn,What stack do the small players have here?,"For a few weeks I've been lurking this subreddit and most posts are aimed towards bigger stacks. Big datalake, etc. What do the small players here for their ETL pipelines? Most recent popular tools seem very big overkill for small tasks",2023-10-07 12:57:40
10uhjfg,8 Key Data Structures That Power Modern Databases,N/A,2023-02-05 17:09:33
zx8ss8,You guys ever puzzled by how some organizations are generating petabytes of data?,"Case in point, [this](https://www.reddit.com/r/dataengineeringjobs/comments/zwix9s/data_engineer_cloud_infrastructure_remote_110150k/) post from r/dataengineeringjobs for The California College Guidance Initiative. Like which part of their data is petabyte scale? how can i find out? am i underestimating how small a petabyte of data is?",2022-12-28 12:50:35
yhhqbf,Am I a Data Engineer?,"So I realize that job titles tend to be arbitrary - but I’ve recently gotten told by multiple people that my title is incorrect and compensation is too low based off my job responsibilities.

My official title is operations analyst and my total comp is around 70k - I currently have 3 years of experience working as an analyst. And my company is a mid sized saas tech firm based on the west coast. 

Responsibilities- I  spend 50% of my time in snowflake analyzing data and creating various scripts/schemas using dbt - and 30% of the time using etl/elt tools to load data from various sources into our warehouse.. and the remaining 20% of the time I’m creating dashboards with our bi tool. The business would love the bi portion of my work to be higher - but I really try keep my work to 60hrs a week - and the moving/cleaning of data takes up the majority of time.

I’ve been told my title should be anything ranging from data analyst, analytics engineer, data engineer and even product analyst.. so I’m just curious what everyone on the de subreddit thoughts are regarding my title and compensation? And if I’m not near or at the official data engineer title yet - what steps should I take to move myself in that direction?",2022-10-30 15:55:51
uenci9,GCP Data Engineer Certification Courses available for free (for a month),"Hi guys,

All the google cloud courses are available for free for a month on Coursera. I didn't check yet but they probably offer qwiklabs as well.

Link is here: [https://cloud.google.com/blog/topics/training-certifications/how-to-build-job-ready-cloud-skills](https://cloud.google.com/blog/topics/training-certifications/how-to-build-job-ready-cloud-skills)  (from April 28 - May 29, 2022)",2022-04-29 14:37:56
s490uh,[Novice here] For an internship is this too much? Or just a bit challenging,N/A,2022-01-15 02:04:35
rgyt7b,Data Engineering Jargon - Part 5 (Final),"Hi - here are the final 10 and some bonus ones as requested by the community.

1-10 is [here](https://www.reddit.com/r/dataengineering/comments/rdw3b3/data_engineering_jargon/?utm_source=share&utm_medium=web2x&context=3)

11-20 is [here](https://www.reddit.com/r/dataengineering/comments/rem26j/data_engineering_jargon_part_2/)

21-30 is [here](https://www.reddit.com/r/dataengineering/comments/rfbuu8/data_engineering_jargon_part_3/)

31-40 is [here](https://www.reddit.com/r/dataengineering/comments/rg5vr0/data_engineering_jargon_part_4/)

41-50 is below

**41. Sandbox**

Usually refers to an environment where extensive testing can be carried out without compromising the sanctity of the live platform.

*A sandbox to prove a concept of keyboard metric before getting this accepted in a live environment.*

**42. Subject Area**

A way of defining a data model by grouping the enterprise’s data according to known business directorates.

*A customer subject area containing all customer information that can be utilised across the business*

**43. Raw Data**

This is the data as it has been collected in its rawest format before it is processed, cleansed and loaded.

*Raw data of all the customer’s orders from the day of trading*

**44. Transactional Data**

This is data that describes an actual event.

*Order placed, a delivery arranged, or a delivery accepted.*

**45. Reference Data**

This is data that allows the classification of other data.

*Country code* *GB representing Great Britain.*

**46. Master Data**

This is data that is the best representation of a particular entity in the business. This gives you a 360 view of that data entity by generally consolidating multiple data sources.

*Best customer data representation from multiple sources of information.*

**47. Structured Data**

Data that is nicely organised in a table using rows and columns, allowing the user to easily interpret the data.

*Finance data in a database table, easily queryable using SQL.*

**48. Unstructured Data**

Data that cannot be nicely organised in a tabular format, like images, PDF files etc.

*An image stored on a data lake cannot be retrieved using common data query languages.*

**49. Data Quality**

A discipline of measuring the quality of the data to improve and cleanse it.

*Checking Customer data for completeness, accuracy and validity.*

**50. Data Management**

A discipline encompassing the end-to-end management of data lifecycle, including acquiring, transferring, securing and querying data.

*Combination of improving the quality of data, governing the data, enriching and cleansing the data.*

\-------------------------

**Bonus terms:**

**51. Metadata**

This is information about the data itself.

In DE this is likely to be table names, table size, data types and sizes, column names, even constraints like Foreign and Primary Keys.

*In a table with Country information. UK or US will be the data itself, whereas the Column name ""Country"" will be the metadata*

**52. Data Marketplace**

Concept of creating a marketplace where buyers and sellers come together to trade data. This has become even more popular due to IoT data that has rapidly increased over the past decade.

*Snowflake has a data marketplace where you can buy anonymised third party data to help with your use cases. Royalmail in the UK licenses PAF (Postcode Address File) to businesses, another way of buying data.*

*I expect with blockchain for this kind of use case to become more consumer-focused. A consumer maybe able to earn crypto tokens for willingly sharing their data, so instead of the ad revenue going to Google etc. it could come to you as an end consumer.*

**53. Data Product**

Solving a business problem using (mainly) data is defined as a data product. And using data could mean anything from simple dashboards to ML models helping with recommendations on products to buy on Amazon.

*A search function on Amazon is an example of a Data Product, without well-catalogued data, this function would be useless.*

**54. Scalability**

Scalability is generally defined as the ability of the application to scale in light of changing (increasing) demands.

In DE this could mean the datawarehouse or datalake platforms. Could also mean creating scalable data pipelines.

*There are many ways of scaling a database (datawarehouse): for example indexing a table would allow fast retrieval of information as your query would not need to search every row of data. This would be a simple change for significant benefits. Another scalability example is number 55.*

**55. Database Sharding / Partitioning**

A performance improvement technique of breaking up large data sets into small subsets. 

Sharding is when this break up of data set happens across multiple machines

Partitioning is when this break up of data set happens on the singular database

The point is, if the data is divided into smaller subsets and into different machines, you are not bottlenecking the same machine with your queries each time.

\-------------------------------------------------------------------

**Parting Thoughts**

This is the final post in the series, I hope you all enjoyed reading this as much as I enjoyed writing it. I also hope you at least learnt a few new terms, it certainly helped me clarify my thinking. Thanks also to everyone that took part in the comments and helped me improve some of these definitions.

Various people have posted the original Medium link of this article, where I usually write. So feel free to check that out.

Let me know what else you'd like to learn and I could write up a future series about it.

Thank You!

1-10 is [here](https://www.reddit.com/r/dataengineering/comments/rdw3b3/data_engineering_jargon/?utm_source=share&utm_medium=web2x&context=3)

11-20 is [here](https://www.reddit.com/r/dataengineering/comments/rem26j/data_engineering_jargon_part_2/)

21-30 is [here](https://www.reddit.com/r/dataengineering/comments/rfbuu8/data_engineering_jargon_part_3/)

31-40 is [here](https://www.reddit.com/r/dataengineering/comments/rg5vr0/data_engineering_jargon_part_4/)",2021-12-15 13:12:50
q9dgub,Is using pandas considered terrible practice in ELT?,"Hello,

all the python pipelines we have are using pandas. Basically final important step is always dataset.to\_sql

(and then transformation SQL script that will be executed as a transaction in our DWH/ Datalake itself.

Is that considered bad practice and I would be laughed off on DE interviews or something completely acceptable?

Average table has \~500k rows, with the biggest ones around 40M. So far I found this method as very effective, but I doubt it is also recognized in the industry.",2021-10-16 14:49:31
oeyu81,Data Analyst transitioning to Data Engineering: Need some tips on the courses and a few other things!,"Hi All,

I'm currently working as a data analyst, and I have good experience working with Python and SQL. I have basic to intermediate level skills in both. I'm looking to transition to Data Engineering and want to update my skills, especially w.r.t. to cloud computing, ETL, DBA, etc., which I think would be useful for mid-level DE jobs.

I checked Coursera and the IBM Data Engineering course ([https://www.coursera.org/professional-certificates/ibm-data-engineer](https://www.coursera.org/professional-certificates/ibm-data-engineer)) looked good to me. Before I apply for that, just wanted to check on forums if there are any better learning paths available, considering that I do have a background in Data Analytics. Please let me know if this one seems good enough.

That was the first thing. Coming to the next part, as I mentioned, I am currently in a job, so have to use the office-provided laptop. I can't really quit at the moment so it's my compulsion to use this laptop, on which admin access is restricted. And as far as I can understand, setting up the environment for DE projects, will require multiple scenarios where I would need admin access. Is there a way around that? Like can DE projects be done on a web-based interface somehow (like Google colab, as an alternative for local Jupyter notebooks)? Please guide me on this.

Note: I do have Anaconda Navigator, and SSMS installed (for Python and SQL purposes). If there's a  way to work with that, do let me know!

Thanks in advance!",2021-07-06 16:47:04
lbdnh4,Article/Tutorial: How to develop data pipeline in Airflow through TDD (test-driven development),"Hey folks I wrote an article/tutorial about how to develop DAGs using TDD (test-driven development) and how to setup a CI with Github Actions, you can read [here](https://blog.magrathealabs.com/how-to-develop-data-pipeline-in-airflow-through-tdd-test-driven-development-c3333439f358).

All the code is here: [https://github.com/marcosmarxm/airflow-testing-ci-workflow](https://github.com/marcosmarxm/airflow-testing-ci-workflow)

and for those just starting with Airflow/Data Engineering. I created a detailed step-by-step of the project: [https://github.com/marcosmarxm/airflow-testing-ci-workflow/blob/master/assets/how-to/create-dag-using-tdd.md](https://github.com/marcosmarxm/airflow-testing-ci-workflow/blob/master/assets/how-to/create-dag-using-tdd.md) 

Hope you enjoy! any suggestions are welcome",2021-02-03 02:46:47
f3az0z,An Awesome List of Open-Source Data Engineering Projects,N/A,2020-02-13 14:44:57
1b8l1nd,"As a data engineer, what do you find the most challenging task in modern data engineering?","As a data engineer, what do you find the most challenging task in modern data engineering?",2024-03-07 04:10:12
1b18mfk,I built an open-source CLI tool to ingest/copy data between any databases,"Hi all, ingestr is an open-source command-line application that allows ingesting & copying data between two databases without any code: [https://github.com/bruin-data/ingestr](https://github.com/bruin-data/ingestr)

It does a few things that make it the easiest alternative out there:

&#x200B;

* ✨ copy data from your Postgres / MySQL / SQL Server or any other source into any destination, such as BigQuery or Snowflake, just using URIs
* ➕ incremental loading: create+replace, delete+insert, append
* 🐍 single-command installation: pip install ingestr

We built ingestr because we believe for 80% of the cases out there people shouldn’t be writing code or hosting tools like Airbyte just to copy a table to their DWH on a regular basis. ingestr is built as a tiny CLI, which means you can easily drop it into a cronjob, GitHub Actions, Airflow or any other scheduler and get the built-in ingestion capabilities right away.

Some common use-cases ingestr solve are:

&#x200B;

* Migrating data from legacy systems to modern databases for better analysis
* Syncing data between your application's database and your analytics platform in batches or incrementally
* Backing up your databases to ensure data safety
* Accelerating the process of setting up new environment for testing or development by easily cloning your existing databases
* Facilitating real-time data transfer for applications that require immediate updates

We’d love to hear your feedback, and make sure to give us a star on GitHub if you like it! 🚀 [https://github.com/bruin-data/ingestr](https://github.com/bruin-data/ingestr)",2024-02-27 10:23:58
18xb4ug,None of what I learned is a job requirement. I am essentially skill-less.,"Hi there, here's an example job requirements I just found (shortened it), which has the same feel as the last 50 job adverts I've seen recently.

""Proficiency in Bash, Python, and SQL. Experience with Linux and Docker. Knowledge in Databases, Data Modeling, ETL, dbt, and Snowflake. Expertise in Spark, Databricks, EMR, Streaming, and Kafka. Familiarity with AWS services such as EC2, S3, Lambda, EMR, Glue, and Athena.""

So.. I'm about to graduate from a Master's in Data Science, where I took mostly Data Engineering stuff for my optional units. Literally all I have had is some exposure to Bash, Python and SQL, and data types. The only reason why I know Linux and Docker is because I started writing something on a Raspberry Pi to open my garage door when I was 16, with a few other small projects.

Yes the master's teaches lots of stats, modelling concepts, ML, DL, and some Data Warehousing etc.. but not a single job, not even entry position that I have found, require skills I learned in my Master's. Every student in my class is now great at R but useless in Python, literally never see job adverts with R on it. Feels like the Master's was a Bachelor's or an ""Intro to Data Literacy"" course.

Where do you even learn these skills? I doubt that you guys just bullshit-apply to jobs and watch YouTube before the interview.. Should I take a full year OFF after my Master's to just learn everything about Azure, Google Cloud, Microsoft Analytics, bloody software development practices even and empty all the Udemy/Coursera courses out there? Then maybe I can get a job?

Gee. I feel like uni has absolutely not made me job ready in any way.",2024-01-03 05:06:32
17puedp,"Spark, Dask, DuckDB, Polars: TPC-H Benchmarks at Scale","I gave this talk at PyData NYC last week.  It was fun working with devs from various projects (Dask, Arrow, Polars, Spark) in the week leading up to the event.  Thought I'd share a re-recording of it here

[https://youtu.be/wKH0-zs2g\_U](https://youtu.be/wKH0-zs2g_U)

This is the result of a couple weeks of work comparing large data frameworks on benchmarks ranging in size 10GB to 10TB.  No project wins.  It's really interesting analyzing results though.

DuckDB and Dask are the only projects that reliably finish things (although possibly Dask's success here has to do with me knowing Dask better than the others).  DuckDB is way faster at small scale (along with Polars).  Dask and Spark are generally more robust and performant at large scale, mostly because they're able to parallelize S3 access.  Really-good-S3 access seems to be the way you win at real-world cloud performance.

Looking more deeply at Dask results, we're wildly inefficient.  There's at least a 2x-5x performance increase to be had here.    Given that Dask does about as well as any other project on cloud this really means that \*no one\* has optimized cloud well yet.

This talk also goes into how we attempted to address bias (super hard to do in benchmarks).  We had active collaborations with Polars and Spark people (made Polars quite a bit faster during this process actually).  See [https://matthewrocklin.com/biased-benchmarks.html](https://matthewrocklin.com/biased-benchmarks.html) for more thoughts.

&#x200B;

This also shows the improvement Dask made in the last six months.    Dask used to suck at benchmarks.  Now it doesn't win, but reliably places among the top.  This is due to ... 

1. Arrow strings
2. New shuffling algorithms
3. Query optimization  

There's a lot of work for projects like Dask and Polars to fix themselves up in this space.  They're both moving pretty fast right now.  I'm curious to see how they progress in the next few months.

For future work I'd like to expand this out a bit beyond TPC-H.  TPC-H is great because they're fairly serious queries (lots of tables, lots of joins) and not micro-benchmarks.  We could use broader coverage though.  Any ideas?",2023-11-07 13:34:19
16lu20w,Process 250TB in 20 minutes for $25 using a 200 VM cluster,"Dear subreddit, there have been many complaints here lately about the quality of posts. I present to you this hidden gem that I found, which has gone under the radar. Please read the article and I encourage some healthy discussion. I also highly encourage criticism of this artchitecture because I am smitten by these numbers and I would like someone to slap me back to reality.

The article can be found here and it's from Coiled. It uses Coiled, Dask and Xarray:

Blog: [https://blog.coiled.io/blog/coiled-xarray.html](https://blog.coiled.io/blog/coiled-xarray.html)    
Code: [https://github.com/coiled/examples/blob/main/national-water-model/xarray-water-model.py](https://github.com/coiled/examples/blob/main/national-water-model/xarray-water-model.py)

Apart from Dask, I haven't heard of the Coiled or Xarray but they seem to be pip installable so I don't think there's not much overhead in using them.

Given that Spark might be the goto choice for this scale of data and given that I'd never willingly use Spark and admit to it, I'd have to come up with alternative strategies to process 250TB. So far, I have not been able to come up with an alternative strategy that can achieve the same time and cost efficiency.

**What would be your approach to replicate what's been done in the article? You can assume an alterate but similar problem. Please mention the following:**

* **Tools you would use?**
* **How much time it would take you to bootstrap this?**
* **Estimated time to completion?**
* **Estimated total cloud cost?**
* **Additional comments on why your strategy is better?**

In my opinion being able to download and process 250TB within 20 minutes is an darn good benchmark. I do want to point out that initially, I was repulsed by the fact that they use 200 VMs in their Coiled cluster. It seems like too much overhead. But when I thought about it further, it might actually be a pretty genius move. It wasn't mentioned in the article so I'm of the opinion the author doesn't realize it either that each AWS VM, ie. the r7g.2xlarge (64GB RAM) has a network bandwidth limit of 15 Gbps (1.875 GBps) but the r7g.metal (512GB RAM) instance has a network bandwidth limit of only 30 Gbps (3.75 GBps) which only 2x more than the r7g.2xlarge but is 8x more expensive. So this means it is optimal to use many smaller VMs instead of a few large ones if you want to actually download 250TB really quickly.

P.S. Not affiliated to any of the companies here.",2023-09-18 12:21:04
16e9iqw,Does anyone regret moving to Data Engineering,"I am currently a data analyst working in healthcare and looking to move to data engineering. For those of you who moved into the field from something else:

1. What was your previous role?
2. Why did you move to data engineering
3. Pros and cons of the move compared to your previous job
4. Do you regret moving to DE and why if you did

Any info would be much appreciated",2023-09-09 16:08:41
162utkn,"Just picked up this big boy for $1 at good will. Worth keeping? Wanted to get a refresher on Hive and cluster scaling for an interview, but this seems overkill and maybe outdated (4th edition is latest)",N/A,2023-08-27 16:23:37
15rws69,How much SQL do you use as a DE?,"I ask since I am an aspiring DE, currently studying the core skills, and I don’t want to spend unnecessary time grinding SQL exercises because

1) I know most of my sticky learning is going to come from hands on, real situations/projects 

2) I still need to make time to learn Python/ETL ect. 

I have been studying SQL for 3-5 hours a day for that last 2 weeks (I am so tired of my current career field that I have this burning desire to get out, hence the aggressive study schedule) 
And so far have been working on and can answer the easy and medium topics/concepts on stratascratch
 

1.	⁠SELECT and WHERE for filtering and selection
2.	⁠COUNT, SUM, MAX, GROUP BY, HAVING for aggregating data""
3.	⁠DISTINCT, COUNT DISTINCT for producing useful distinct lists and distinct aggregates
4.	⁠OUTER (e.g. LEFT) and INNER JOIN when/where to use them
5.	⁠Strings and time conversions
6.	⁠UNION and UNION ALL.
7.    Window functions like PARTITION
8.    CTE’s 
9.    CASE 
10.   ROW_Number and Rank 

So back to my original question, how much SQL do you use as a DE and is this short list of concepts above enough for me to move on to learn Python and other relevant skills?",2023-08-15 16:03:03
151npp7,Best way to execute Python scripts on a schedule?,"Our data warehouse is a SQL Server. We’ve been using Python to do a lot of scheduled ETL tasks. Currently I’m executing the tasks on a schedule (10 minutes) using Windows Task Scheduler and batch files on the same Windows server as the SQL Server. 

Is there a better way to do this? I’ve read that you can use stored procedures or scheduled events, but is that going to be faster? 

Currently 85% of memory is allocated to SQL Server.

Any pros or cons to consider?",2023-07-17 01:21:08
14syob6,Is data engineering the next data science?,"It very much looks like DS becomes over-saturated with passionate and (self)educated employees.  
Additionally, AI such as co-pilots and no-code environments make this domain even more competitive.

Look at the number of subreddit members r/datascience \~ 1 million, r/dataengineering \~ 10 times less

Do you think that DE will be the next DS (sexiest job...)?  
What do you think is worth to put time and effort in to get one step ahead - is this really platform engineering, distributive computing (Kubernetes) or something else?",2023-07-07 06:21:44
14nl01c,Introducing English as the New Programming Language for Apache Spark,N/A,2023-07-01 04:00:28
14az4gz,The Dagster Master Plan,N/A,2023-06-16 15:01:14
136d100,Sr. Data Analyst -> Data Engineer,"Got an internal job role, but as I'm moving from Sr. to Intermediate role, there's a pay cut from 115k -> 90k. Will you take it?  


I'm a 28 y/o unmarried dude, just fyi in case.

Edit: I considered applying for this new position as I thought it'd be a good chance to expand my skills, etc. and the hiring manager told me he would try to at least match my current salary, but I came to know from him today that it's not possible.",2023-05-03 06:58:35
11sq68k,"""ingestion is a solved problem""","I see this sentiment a lot in this sub. Seems to be mainly from analytics engineer types who are focused on data modeling inside the warehouse. The reasoning is normally along the lines of ""tools like fivetran and airbyte have connectors for everything, so no need to write integrations for anything, we can just deploy those and get on with the real work""

While on some level this is true, i really do feel like its missing a big part of the picture. For one it doesn't consider streaming and real time data but that's a debate for another time. The big problem with the above vision for me is it's overly focused on just lifting and shifting data from OLTP prod systems, with no consideration for things like data quality, schema validation, schema evolution, data contracts etc. To me it's overly coupled to the specific OLTP technologies used. Shouldn't we be looking to see OLTP systems wrapped in an interface that doesn't rely on internal implementation details?

Curious to hear other people's thoughts on this as it's something that's been bothering me lately.",2023-03-16 10:27:04
101k1xv,Dataframes vs SQL for ETL/ELT,"What do people in this sub think about SQL vs Dataframes (like pandas, polars or pyspark) for building ETL/ELT jobs? Personally I have always preferred Dataframes because of

* A much richer API for more complex operations
* Ability to define reusable functions
* Code modularity
* Flexibility in terms of compute and storage
* Standardized code formatting
* Code simply feels cleaner, simpler and more beautiful

However, for doing a quick discovery or just to ""look at data"" (selects and group by's not containing joins), I feel SQL is great and fast and easier to remember the syntax for. But all the times I have had to write those large SQL-jobs with 100+ lines of logic in them have really made me despise working with SQL. CTE's help but only to an certain extent, and there does not seem to be any universal way for formatting CTE's which makes code readability difficult depending on your colleagues. I'm curious what others think?",2023-01-02 18:42:49
zy7q2n,How do data engineering interviews work?,"Hi I am a “backend engineer” but truthfully I am more akin to a data engineer. My dumpster fire of a job has made me a de facto SQL, ETL, and SSIS/SSRS expert. Given that all my current experience in my current job is related to data engineering, I wanna make a shift to data engineering but I have no idea what to expect from these kind of job interviews

Any advice or tips would be appreciated. My educational background is Computer Science software development so I am curious how similar data engineering job interviews are to software engineering job interviews",2022-12-29 15:14:33
z0wzb0,Made a post about weird hybrid titles coming out. Who can guess the job duties without reading the full posting (which can be Easily found on LinkedIN for anyone interested),N/A,2022-11-21 11:45:56
ydwud8,Why is Data Engineering considered “not as attractive” compared to DS?,"I’ve been a Data Analytics intern coming up on a year now, but I’ve doing some work wearing the Date Engineering hat for the first time working on our company’s ETL process and creating a data pipeline to use in Azure to make departments more satisfied with the speed in which they receive their data, so I feel like I’ve gotten a decent view into some aspects of Data Engineering at least according to my supervisor.

What I don’t fully understand is how come they say Data Science is “sexier”? Is it salary? From what I saw they both seem well compensated and both of them have very high ceilings that seem to even breach 200k at a senior level in some areas. Is it the fact data science uses more forecasting for companies? I would think when you are in charge of the sources of huge volumes of data and you are essentially just as vital to these reports as the Data Science aspect of things I’m just wondering where is the line drawn between the two? 

Asking really just for curiosity but also trying to figure out which career path to focus on, going all in on DE or focusing some more on DS. Funny enough my internship in here Analytics doesn’t seem to be touching too much in DS, but seems to be putting me in more DE positions due to the lack of actual usable data that can be used",2022-10-26 12:25:51
wmpz5t,My first DE project about flight punctuality in Europe,"I want to build a career in Data Engineering, so I have built my first personal project. Please be so kind to leave some feedback on what I should improve on.

 

# About The Project

The  goal of this project is to display how flight punctuality changes over  time considering the temperature deviation from the average monthly  temperatures in European airports.  
The inspiration came to me from  recent headlines stating the unprecedentedly high flight delay and  cancellation figures across most of Europe.

# How to Read the Dashboard

A  flight is considered to be delayed if it departs 15 minutes after the  scheduled departure time. Flight punctuality shows the ratio of  non-delayed flights on a day.  
The columns represent how much the daily average temperature deviates from the historic average monthly temperature (from 1980).

&#x200B;

 [Link to the dashboard.](https://app.powerbi.com/view?r=eyJrIjoiMTk3MjQxY2QtNzM0Yy00OGI4LTk5N2ItYmRkOGIwMjVkNGYxIiwidCI6IjRiZWYyNmNiLWFhZTktNDg4ZC1iNTk2LWVkNzcyZWUzODBjYyIsImMiOjl9) 

&#x200B;

&#x200B;

[Architecture](https://preview.redd.it/dbsb4ko69bh91.png?width=1623&format=png&auto=webp&s=6f2f80fb3405d85236031efdad4e02e487567a61)

&#x200B;

 

# The Data

The flight data is downloaded in an xlsx format from Eurocontrol’s [website](https://ansperformance.eu/traffic/punctuality/).  It is updated daily with the previous day’s data, but unfortunately it is not retained in a day-by-day historic format, only in an aggregated  report.  
I chose the busiest airport from each country, to represent as many countries as possible, while keeping the list of airports at a reasonable level.

The weather data is taken from the National Oceanic and Atmospheric Administration’s [servers](https://www.ncei.noaa.gov/pub/data/gsod/). Each weather station’s data is stored in a yearly file, and occasionally small corrections are made on past days’ figures. Historic datasets are available going back for almost a 100 years.

Both data sources are updated daily, so Airflow runs the full ETL process each night, loading the flight data incrementally, and refreshing weather data for the full year. The historic average monthly temperature is also re-calculated daily, using observations starting from 1980.

# Tools Used

I wanted to build a completely free project, so I decided to run the whole process on my Raspberry Pi.

Orchestration — Apache Airflow  
ETL — Python and Bash scripts  
Local Database for bronze data — Postgres  
Cloud Database for gold data — Azure Data Lake  
Visualization — Power BI

The data usage on the Azure Data Lake is very small, so it should be in the free tier.

# Potential Improvements

* The  whole project could be migrated to the cloud. I would probably use  Azure Databricks and Azure Data Factory, as I have some experience with  those, and the visualization part of the project is already in the Azure ecosystem.
* The scale could be improved by adding flights in the United States, potentially from the [Bureau of Transportation Statistics](https://transtats.bts.gov/ONTIME/Departures.aspx).
* Additional aspects of the weather (visibility, precipitation, wind speed) are  already part of the bronze data, they could easily be added to the  visualization.
* Additional visualizations, potentially of the above mentioned aspects.
* Unit tests.

# Additional Notes

The  visualization tracks only one aspect, the temperature. I am fully aware  that the current situation is not caused by the higher than usual  temperatures in Europe, it is rather due to various circumstances, originating from the travel restrictions in 2020 and 2021, resulting in a staff shortage, and pent-up demand on traveling abroad. Nonetheless, if  the project goes on for a longer period, and we experience a return to normal situation, it might be interesting to see whether there is any  correlation between the temperatures and flight delays.

# Feedback

This is my first project, which is not based on a course material or guide, so it is rough around the edges. Please let me know what you think, how I can improve it in both technical and aesthetic aspects.",2022-08-12 16:46:29
u8xb20,"Those who transitioned from data analytics to data engineering, why?","I'm in data analytics and enjoy working with data, creating pipelines, and the visualizations, but I'm starting to get bored. I am, however, not a fan of the statistics and find myself enjoying the programming side the most. I'm curious why others in data analytics may have switched to data engineering?",2022-04-21 20:52:23
ty2662,What's the coolest problem you've ever solved?,Just was wondering what's the most innovative or largest problem you've ever solved on the job or for your own side project?,2022-04-07 01:58:26
qrq7mb,How and where did you learn SQL from and become good at it?,"I want to learn and become really good at SQL (and Python too!).

This sub seems to have both DEs and SWEs. Could I get some guidance on how and from where I should learn SQL and Python to become a skilled and competent engineer?


**Thank you everyone**


**Edit: I forgot to mention this. Most tutorials and courses seem to focus on the querying part of SQL. Where and how do I learn to design, structure and create Data and databases?**",2021-11-11 17:24:59
l2noa4,Data Engineer Interview at Amazon,Hi everyone! Who has had an interview at Amazon for this role? Can you share with me your experience and what to expect and some sample questions?,2021-01-22 13:25:39
1b91l3e,What's that one WTF moment you had that made you leave a company?,What's the worst data related decision a company made that left you speechless ,2024-03-07 18:19:03
1azl4lz,Landing a data engineering role with the help of this group,"Hey beautiful people, I recently got a data engineering role after trying for a while. It wouldn't have been possible with the support of this group. I would like to thank you guys for this. 
I would like to know what should my next steps be? Though I got the job, i still have that imposter syndrome even though my team members are super supportive. I want to take my learnings to a next level but I feel that I lack the basics so want to start from the very scratch like learning the alphabets.
So if you were to start a job in IT, how you would have started? I want to become an engineer in true sense. I am ready to devote sufficient time along with my job. 
Suggestions are most welcome.

Thank you!!
Have a great day!",2024-02-25 10:46:46
18zfz14,Preparing for DE Interviews at FAANG+ companies,"I will try not to dox myself but the end goal for me is to end up as a Senior DE at a large tech company. At the moment I'm ambivalent on whether this results in Data Platform Engineering or Data Analytics Engineering.

Here is my general framework for studying:

1. LC Easy/Medium (Arrays & Hashing, Two Pointers, Sliding Window, Stack, Binary Search, try to solve in 20-25 minutes with no/minimal help)
2. SQL Medium/Hard (Try to solve in 3-5 minutes with no/minimal help)
3. Data Modeling (Identify business needs using Product Sense and create a Star/Snowflake schema from this)
4. Behavioral (standard STAR answers)

I am decidedly not good at algorithmic questions, which is part of the reason why I transitioned to DE (also I think it's cooler, among other things). Is this a good framework to abide by to target dedicated DE roles at FAANG+ companies (I specifically have Meta and Amazon in mind)? Any comments or insight would be welcomed.",2024-01-05 19:59:59
17wragh,Running Python Jobs in cloud,"Most of our ETL jobs are python scripts that hit APIs and write to a blob storage. Data is small, so I don’t need any tools like spark. What are some easier tools to schedule and run these python jobs?

I tried using AWS lambda functions, but there are significant limitations like 15 minute max timeouts or maximum size is for uploading python environments.

Airflow on a server is ideal, but extremely difficult to set up and manage currently. Companies that manage it for you (like astronomer) are pretty expensive.

Anyone used Dagster?",2023-11-16 16:37:40
17t0d2p,Does anyone else hate the idea of writing code for job scheduling like airflow?,"It's a rant really about some of our airflow jobs having more lines of code than the actual code and also more effort in writing airflow code than the real work.

I tried building airflow factory models etc but it did not help. Hope something like autosys or control m comes up as open source for big data job scheduling.",2023-11-11 18:23:19
16qim1i,What is your background education?,"I work as a data engineer for a year now, but I graduated in chemical engineering. I had no prior knowledge in this field before (except for some code experience in matlab and very little in python) and I had to learn everything along the way. I consider myself extremely lucky, because this opportunity just appear out of the blue and it was a blessing of the universe. Anyway, iam just curious of how much of you guys entered in this field with non tech background.",2023-09-23 23:32:31
16c0p6h,Huge influx of recruiters messaging me on LinkedIn starting September 1st despite nothing changing on my LinkedIn… anyone else?,Is data engineering hiring ramping up all of the sudden? Could be that LinkedIn changed the recruiter search tool algo I suppose. Curious if anyone else is seeing the same.,2023-09-07 00:08:01
16b3k7m,Ok to not really use any python as a DE?,"I work in a DE role but find myself doing about 99% of my job in SQL. I’m building out dim tables and scripting the stores procs needed for the ETL. I rarely use any python and when I do it’s to throw my stored proc into a really simple DAG and schedule it. 

Most of our data comes into a GCP bucket and I’m just pulling that data from a external table.

I just don’t seems to have a strong use case to use python in my day to day.",2023-09-05 23:11:57
14ul4sl,Is Kimball’s data modeling really functional?,"Does Kimball's star schema really work in practice or is it just a theoretical ideal model that companies try to implement?
Have you ever seen an by-the-book star schema implementation that solves business needs?

This is a sincere question. I see star schema being required as “state of the art” and standard knowledge for analytics engineering (or other data warehouse related positions), but is there an alternative?",2023-07-09 01:16:43
14oqn1f,Seeking Guidance: Roadmap for Becoming a Competent Junior Data Engineer in 3 Months," I'm in need of your expertise and guidance as an aspiring data engineering student. Despite having knowledge in Python, SQL, Scala, Hadoop, Spark, and Airflow, I'm feeling overwhelmed and confused about where to start. Can you provide any advice or resources to set me on the right path?

If there's already a similar roadmap or guide available, please let me know where I can find it. Otherwise, I'd love to collaborate with you all to create a roadmap specifically tailored for junior data engineers with a similar background.

To kickstart the discussion, here are a few questions:

1. Essential skills and knowledge areas: What are the key competencies that a junior data engineer should focus on, considering prior knowledge in Python, SQL, Scala, Hadoop, Spark, and Airflow?
2. Recommended programming languages, frameworks, or tools: Are there any specific technologies that would complement my existing skill set and boost my growth as a data engineer?
3. Top resources for self-study: Can you recommend any books, online courses, tutorials, or other learning materials to further enhance my skills and knowledge?
4. Projects or exercises for hands-on experience: What practical projects or exercises would you suggest to gain hands-on experience and build a strong foundation?
5. Staying up-to-date with the latest trends: How can I stay informed about the latest advancements and trends in the field of data engineering?

I'm dedicated to investing my time and energy into this journey and excited to learn from your experiences and insights!

Thank you in advance for your support. Let's collaborate and create a comprehensive roadmap for aspiring junior data engineers!",2023-07-02 15:10:03
12iaaka,I have been recently promoted to Data Architect Role. I need help on learning resources/pointers,"I have experience in DE for about 7 years and currently promoted to Data Architect role. I understand my responsibilities which include laying out data models, architecture diagrams, suggesting the best tools for a particular scenario. I have worked majorly in Azure and Databricks .

Please suggest the learning path / thinking methodology for me to be better at my current role.",2023-04-11 05:53:27
zv2rzg,Best Place for Transformations: Python or SQL?,"I am a Data Engineer that has just started learning Python.  In my past experience, I always landed data in a STG table with as little changes as possible, then wrote SQL to load the Target and perform post-load logic.  As I'm working with pandas and data frames, I'm spending a lot of time on tasks like changing ""<NULL>"" values to NaN, parsing dates, setting datatypes, etc.

Before I go too far down that road, what are the opinions here about where to best perform these actions?  My alternative would be to define my STG tables with all ""Text"" columns, dump the data in raw, then perform all those actions with SQL on the way to my target.  Only downside I can foresee is possibly less options for error handling?  Is there a consensus on best practice for where to do these kinds of things?  Any insights are appreciated!",2022-12-25 17:48:03
ybm75x,What would you want to hear and learn about in a PySpark workshop?,"I will present a virtual 1.5 hour PySpark workshop at a popular data science conference in a couple of weeks. It's introductory in nature. The target audience are absolute beginners and those with moderate PySpark experience.

I want to ensure that it is as useful as possible for anyone attending. In addition to hands-on technical exercises introducing the basics, I will go over PySpark's relevance for particular use-cases. This should help people make an informed decision rather than picking PySpark just for the heck of it.  


- How does the above sound? Is there anything else that you would wish to be covered in such a workshop?
- Have you attended anything similar in the past? What was your experience like?

For added context, I am an author of an O'Reilly book about PySpark and have worked as a data engineer for a few years.

-----

There are a lot of great ideas in the comments. Thanks everyone!
Considering the audience's DS background and the time restriction, advanced topics around optimization, performance or query plans may not fit. However, I will try to incorporate some of the accessible ideas (read low-hanging fruits).",2022-10-23 16:33:26
xijit6,Overwhelmed by sheer amount of resources,"I'm preparing for DE interviews, planning to apply after 3-4 months of preparation, but I feel overwhelmed by the sheer amount of resources i.e courses, books, YouTube channels etc available to prepare for it.

Currently I'm brushing up my python skills and there are so many courses and books available, I get confused and end up going through multiple resources fearing that I'd miss some concepts.

I need help to shortlist 1 or 2 best resources to study and prepare for these topics below:
1. Python (only relevant topics for data engineering), Scala
2. SQL
3. DBMS, Data warehousing,  Data modelling
4. Apache Hadoop and Spark

And if I missed out any other important topic, please share resources for those too, it'd be a great help.",2022-09-19 17:52:50
x4yfed,layoffs in tech,Is ur company laying off staff? Is ur team reduced in size ?,2022-09-03 16:24:19
v94z8g,Being a Data Engineer: are we destined to do less coding in the future?,"I have been a Data Analyst for a year when I slowly worked my way towards being a Data Engineer, because I just enjoyed coding a lot more. In my next role I could develop a bit in Python, work in SQL, but it still wasn't much. Same for my current role: i was excited to work with Spark and with the Azure stack, but from an intellectual challenge point of view, feels a bit underwhelming. Now with about 2 years of DE experience I have the occasional sense of something is missing: being in the flow when coding. Can't remember when was the last time I was so lost in development that hours just passed by.

I am guessing many of us got oriented towards Data Engineering, because we could work with data which we are interested in and we could also code which we also enjoy. Well, at least this was the case with me.

Couldn't help but notice that with every role I had, coding wasn't particularly on the agenda (more like occasional scripting). It is mostly about heading towards being able to configure stuff, clicking together stuff, some minor scripts here and there. How processes and things click together, link together or affect each other. For example, this squad wants to have this dataset, so we use Azure Data Factory to get the data from the DB, then put it into the data lake for them to consume. That's it.

It is safe to assume that there are jobs out there which are Data Engineers, with a hint of software engineering, but overall: does the role of Data Engineer inherently veers towards Tool knowledge rather than coding knowledge?

P.S: I thought about getting into SWE, but here comes the disadvantage - with Python and SQL, the supposedly main, but underutilized tools of a DE, that's a hard rocky road to take.",2022-06-10 10:46:17
uvt1no,100+ Cheat Sheets for Data Science and Machine Learning,N/A,2022-05-23 05:25:54
nxm7mw,"A book like ""Cracking The Coding Interview"" for Data Engineer Interview?","Could our veterans suggest good books about the interviewing questions and how to answer them perfectly for data engineer interviews? something like the great ""Cracking the coding interview"" book.",2021-06-11 18:02:17
kbynb2,Is there a SQL set of questions equivalent to the 75 Leetcode questions for FAANG interviews?,Im specifically looking to get into FANG as a DE. I’m sure a lot of you heard of the well known LC 75 questions made by the Facebook employee which covers most of the topics for data structure and algo questions for a SWE interview at FAANG. But is there a SQL set of questions for FAANG interviews? Specifically studying for data engineering position at FAANG. Basically for DEs they test you on DS/algos as well as very advanced sql. So I’m wondering if there is a SQL version for FAANG?,2020-12-12 22:39:06
icu95c,We built a public PostgreSQL proxy to 40k+ open government datasets,N/A,2020-08-19 19:15:07
hbglth,Data Engineering Apprenticeship,"Professional lead DE here. I've been tossing an idea around for a while and wanted to get peoples' thoughts about it.  


I read or get asked quite frequently: ""What is data engineering?"" ""How do I start?"" ""What is important to learn?"" ""How would I transfer from a more traditional software engineering position into a DE role?""... and the list goes on.  


These questions are *really hard to answer* because the term ""data engineering"" can mean everything from database administration, to business intelligence, to dataops/devops, to data pipelining, to sysadmin, to just pure software engineering. It also means that ""data engineering"" is harder to teach, train for, or learn on a one-size-fits-all basis.  


My personal career learnings (software/data engineering for the past \~8yrs and a different career before that) have always been very hands-on, real-life, and immediately-applicable. I've learned ""why"" and ""what"" and ""how"" based on what companies need or want IRL.  


Which brings me to the ultimate question: ***Would people be interested in a*** [formal data engineering apprenticeship](https://www.bostata.com/training-data-professionals?utm_source=reddit&utm_medium=apprenticeship)***?***  


Is it scalable? Nope, and that's the point.  
Is it a proven model? Absolutely. Aristotle was taught by Plato. Philip Johnson studied with Breuer and Gropius. Most blue-collar industries have a significant apprenticeship-like component to professional development.",2020-06-18 15:19:47
1bd500u,I feel like such a fraud,"Title. I have worked my way up in my organization from Service Desk Analyst to the title of a Data Engineernand have been with this company for almost 7 years. My primary tasks are maintening an AWS Glue pipeline into our data lake and bringing new tables from various sources into it. I know python somewhat well (I can pass some medium leetcode problems as a reference), my SQL skills are very basic, I feel like my organization is so far behind things techwise I am getting away with it and I can use chatgpt well enough to figure out anything I don't know. 

Typing this out I feel like I'm describing myself more competent than I feel. I have no formal education besides highschool and a boatload of AWS certifications I always have to study my butt off for. 

I'm not sure what the point of this post is, I just kinda needed to rant and get it off my chest to someone and maybe get some outside perspective.

How do you guys deal with thoughts like these? Do my skills line up with what's expected of a DE? ",2024-03-12 18:45:14
1ap40zg,Data Engineering vs Data Engineering,"I've worked as a data professional with different job titles for about 10 years now and I'm noticing a pattern that I haven't seen explored before, and I'm interested in some structure.  I see two types of data engineering roles/teams and I'll try to describe them as best I can in a couple bullets.  


Type 1: the converted software engineer

* Comfortable in Scala/Rust/Spark/kubernetes
* Handles not just deployment but serving production systems
* Highly interested in optimizations as these save the company real money or just to make the problems tractable. 
* More likely to use streaming architectures.
* Further removed from the business problems.

&#x200B;

Type 2: the senior data scientist

* I've built a fancy model, now what?
* Has to set up their own OLAP architecture as the only dbs the software engineering team uses are OLTP, possibly even setting up their own data lakes as well
* Likely don't have replicated environments.
* Passes off data artifacts/exposes data products to the SE team for integration into core platform
* Focused on solving problems for the business, which necessitates data engineering.

&#x200B;

I could go on but I hope the distinction is clear.  Core skills SQL/Python/Data modeling/cloud are the same and of course many roles will be a hybrid.  Anyone have useful nomenclature for each of these archetypes?",2024-02-12 16:29:44
194d07l,"Data Engineer - What's the best course, certification or degree of all time?","Hello guys,

I hope you guys are well. I'm curious about your opinions. I'm a data engineer trainee. I want to learn A LOT. Not only SQL, Python, but PySpak, etc, etc.

But I'm curious: What's the best course, or certification (specialization) or degree of all time for you, that you can end the course and say: ""Wow, f\*\*\*\*\*\* hell! This was amazing! I learned so much with this!""

I want to know your opinions :)

You can also share books, share what really help you with to grow as a Data Engineer and as a professional :)

Have a good day/night",2024-01-11 21:28:02
18kt2fc,2024 Data Engineering Top Skills that you will prepare for,"Are you thinking about getting new skills? What will you suggest if you want to be a updated data engineer or data manager?

Any certifications? Any courses? Any local or enterprise projects? Any ideas to launch your personal brand?",2023-12-17 22:26:53
15nbhzf,dbt Labs to add usage-based pricing on top of their seat costs for dbt Cloud. $0.01 per model after free tier.,N/A,2023-08-10 12:43:39
155rksi,At what point do you give up on training a co-worker?,"I have a jr co-worker I have been trying to train and enable to reduce work on my plate.

However it has been almost a year, and despite multiple documentation guides, live enablement sessions, working groups, and daily training sessions, I have not seen this teammate actually improve.

It to be quite frank has gotten to a point where it is more time-saving if I just do the work for him. Something I have been forced to do b/c they continually make the same errors over and over again.

At what point do I just give up on this Jr Employee and throw in the towel here?

Also, not sure if this is helpful, but this Jr Employee is an off-shore resource. But the training techniques I have used has worked for other off-shored an on-shored Employees with similar profiles

&#x200B;",2023-07-21 15:41:36
11bhvux,Do Right Joins even matter?,"This is one of those out-of-the-blue thoughts that you get randomly. I am an expert in sql with many years of experience, but have yet to use Right Joins lol. Is there any specific reason or use-case for this type of join?",2023-02-25 10:29:14
ze798y,Interview coding question that I couldn't solve,"Hi,

I was asked this question for a Senior Data Engineer interview. A cycling race is composed of many legs. Each leg goes from one city(A) to another(B). Cyclists then rest for the night and start the next leg of journey from (B) to (C). If the legs are represented by Tuples (A,B), (B,C), (C,D)...and given a list of tuples out of order example \[(C,D),(A,B),(B,C)...\] can you print out the correct order of cities in the race (example ""A B C D.."")

Example \[(A C) (B D) (C B)\]

output: A C B D \[(C B) (D C) (B E) (A D)\] output A D C B E.

I was supposed to write code in C#. I was unable to solve this. This was my thought process. Treat it like linked list. If List-> next is null then it's the end of race and if List->prev is null it's the Start of race.

Can anyone guide me with the coding part?",2022-12-06 13:48:31
x494kp,Wanted to share my open source incremental ETL framework: Meerschaum,"Hey fellow data engineers! 👋

For the last two years, I've been building my ETL framework [Meerschaum](https://github.com/bmeares/Meerschaum). It's been an exciting journey ― I've deployed several Meerschaum stacks for some contracts, it's running in production at my last job, and I've been putting a lot of work recently to address some pain points in my current role. It's made building smaller pipelines a breeze, and I thought some engineers here might be interested in how the framework can help make your jobs easier.

In a nutshell, Meerschaum updates your tables according to the data you ingest. Here's a code snippet to illustrate what I mean (gotta say, I wish Reddit supported syntax highlighting in Markdown):

    import meerschaum as mrsm
    pipe = mrsm.Pipe(
        'foo', 'bar', # labels, more on this later.
        target = 'my_table', # optionally set target table name instead of ""foo_bar"".
        columns = {'datetime': 'timestamp', 'id': 'id'}, # indices
        instance = 'sql:local', # built-in SQLite for your convenience.
    )
    docs = [
        {'timestamp': '2022-01-01', 'id': 1, 'value': 1.0},
        {'timestamp': '2022-01-02', 'id': 1, 'value': 2.0},
        {'timestamp': '2022-01-03', 'id': 1, 'value': 3.0},
    ]
    
    ### Insert the rows into the table.
    pipe.sync(docs)
    
    print(pipe.get_data())
    #    timestamp  id  value
    # 0 2022-01-03   1    3.0
    # 1 2022-01-02   1    2.0
    # 2 2022-01-01   1    1.0
    
    
    ### Duplicates are ignored.
    pipe.sync(docs)
    
    print(pipe.get_data())
    #    timestamp  id  value
    # 0 2022-01-03   1    3.0
    # 1 2022-01-02   1    2.0
    # 2 2022-01-01   1    1.0
    
    
    ### Rows with existing indices (timestamp and id) are updated.
    pipe.sync([{'timestamp': '2022-01-01', 'id': 1, 'value': 100.0}])
    
    print(pipe.get_data())
    #    timestamp  id  value
    # 0 2022-01-03   1    3.0
    # 1 2022-01-02   1    2.0
    # 2 2022-01-01   1  100.0

This example demonstrates the expected behavior of syncing. But the secret sauce lies in how the datetime index is used. Whenever new rows are ingested, Meerschaum pulls a sample from your table in the given time range. That is, if your table is massive and you frequently have small updates, this behavior will save a lot of wasted computing time.

For example, suppose we sync the following document:

    pipe.sync([
        {'timestamp': '2022-01-01 12:00:00', 'id': 1, 'value': 123.0}
    ])

The underlying query to retrieve the existing sample would look like this:

    SELECT *
    FROM {target_table}
    WHERE
      ""timestamp"" >= CAST('2022-01-01 11:59:00' AS TIMESTAMP)
      AND
      ""timestamp"" <  CAST('2022-01-01 12:01:00' AS TIMESTAMP)

Note that the default behavior only bounds on the datetime axis. I've benchmarked other [experimental syncing methods](https://github.com/bmeares/syncx) for my [master's thesis](https://tigerprints.clemson.edu/all_theses/3647/) and found that the extra processing needed to reduce the sample size isn't usually worth the saved bandwidth.

The other major part of the framework is that pipes have connectors, specifically the [plugin system](https://meerschaum.io/reference/plugins/). For example, consider a file `~/.config/meerschaum/plugins/example.py`:

    # ~/.config/meerschaum/plugins/example.py
    
    from typing import List, Dict, Any
    import datetime
    import random
    import meerschaum as mrsm
    
    required = ['requests']
    
    def register(pipe: mrsm.Pipe, **kw: Any) -> Dict[str, Any]:
        """"""Return the pipe's parameters.""""""
        return {
            'columns': { # Indices
                'datetime': 'timestamp',
                'id': 'id_col',
            },
            'num_rows': 1000, # Custom key
        }
    
    
    def fetch(pipe: mrsm.Pipe, **kw: Any) -> List[Dict[str, Any]]:
        """"""Do some magic and return a list of documents or a dataframe.""""""
        import requests
        req = requests.get(""http://worldtimeapi.org/api/timezone/Etc/UTC"")
        if not req:
            raise Exception(""Oopsie!"")
    
        timestamp = datetime.datetime.fromisoformat(req.json()['datetime'])
    
        return [
            {
                pipe.columns['datetime']: timestamp,
                pipe.columns['id']      : i,
                'some_other_value'      : random.uniform(1, 100),
            } for i in range(pipe.parameters['num_rows'])
        ]

This Python module can act as a connector for a pipe, denoted by `plugin:example`. Calling a bare `pipe.sync()` will invoke the function `fetch()`and sync the returned documents:

    import meerschaum as mrsm
    pipe = mrsm.Pipe('plugin:example', 'foo')
    pipe.sync()
    print(pipe.get_data())
    #                      timestamp  id_col  some_other_value
    # 0   2022-09-02 18:20:25.034960       0         86.206002
    # 1   2022-09-02 18:20:25.034960       1         52.897848
    # 2   2022-09-02 18:20:25.034960       2          5.324236
    # 3   2022-09-02 18:20:25.034960       3         68.776668
    # 4   2022-09-02 18:20:25.034960       4          1.641710
    # ..                         ...     ...               ...
    # 995 2022-09-02 18:20:25.034960     995         89.157034
    # 996 2022-09-02 18:20:25.034960     996         22.009482
    # 997 2022-09-02 18:20:25.034960     997         71.547525
    # 998 2022-09-02 18:20:25.034960     998         48.389565
    # 999 2022-09-02 18:20:25.034960     999         38.341501

Because the behavior of the function is defined by `pipe.parameters`, you can create as many pipes as you like with this plugin as its connector, each with different behaviors.

One last feature I'd like to highlight is how you can set a SQL query as a pipe's connector:

    # Environment variable for the DB connector:
    # MRSM_SQL_FOO=sqlite:////tmp/foo.db
    
    import meerschaum as mrsm
    pipe = mrsm.Pipe(
        'sql:foo', 'bar',
        columns = {
            'datetime': 'timestamp',
            'id': 'id_col',
        },
        parameters = {
            'query': """"""
                SELECT timestamp, id_col, (val * 2) AS val
                FROM src
            """"""
        }
    )
    pipe.sync()

This will wrap the query definition in a CTE and only select rows newer than the most recent timestamp (you can tweak this behavior). This is a common pattern for incrementally updating time-series tables and will save you the hassle of writing and ETL script from scratch. Think of it as an incrementally materialized view.

There's a whole lot more that you can do with the framework, but this post is getting kinda long. Please check out the [project homepage](https://meerschaum.io/) for more details, and I'd really love know what y'all think! Can you see a use case for the framework in your stack?",2022-09-02 18:41:57
wvg8pf,Update: Journey to Data Engineering,"Original post: [Journey to Data Engineering](https://www.reddit.com/r/dataengineering/comments/mck2td/journey_to_data_engineering/) 

About a year and a half ago I made a post about getting a Business Intelligence Developer job and looking to move towards Data Engineering in the future-- now, I'm happy to update that I got an offer from my current company to move to a Data Engineering position in the analytics department.

According to glassdoor, maybe I'm underpaid at 80k for 1.5 YOE in the midwest US, but at the end of the day I'm happy to get the experience and the opportunity to upskill on the job. 

For those looking to break into data engineering, I am a firm (though perhaps biased) believer that the easiest route is through entry level business intelligence/data analytics roles.

Thanks to the community for helpful responses and words of encouragement!",2022-08-23 05:28:10
v54fpm,What data tools are you using for your daily work?,"Hi, Guys,

What tool are you using for your daily data work?  Are those tools in modern data stack popular?  For the list of the following tools, which one are you using?

Data Warehouses: Snowflake, Databricks

Data Integration: Fivetran, Airbyte, Stitch, Segment

Data Modeling: dbt

BI: Mode, Preset, ThoughSpot, Sigma Computing, Hex",2022-06-05 03:01:04
pvvtgj,What is a pipeline really ?,"I am primarily a ETL/Database developer and I am responsible for maintaining ETL workflows/ stored procedures that load data from applications to my Enterprise DWH. In my view, anything that is used for data transport is a pipeline. are ETL workflows also pipelines ? I just have trouble with this terminology . Can some one clarify what is and what's not a pipeline.",2021-09-26 15:16:25
ols39l,Repeated interview question: what do you if your ETL fails?,"So far I was asked this question twice in different interviews. It is a very generic question that comes in flavours like:

>\- what do you if your ETL suddenly cannot load the data because it lost the data warehouse connection?, what do you do?  
>  
>\- what do you if your ETL fails while transforming the data because of any reason?

I have some blur answers for this, but after two times still I feel I don't have an elegant reasoning about these scenarios.

Can someone help with this?. Surely there are multiple things to consider or useful examples, etc, but.... any help would be very appreciated. Thanks!",2021-07-16 22:53:22
iz2ucy,What tools will be a must-have for the data engineering stack in 5 years?,"Snowflake's IPO is validation of the increasing importance of the data warehouse/data engineering as a value add for any company serious about data. What tools will be integral to the DS/data engineering stack in five years? What technologies will be the ""cloud warehouse"" of 2025? 

This article has some thoughts on how data observability will play into this future: [https://towardsdatascience.com/data-observability-the-next-frontier-of-data-engineering-f780feb874b?source=friends\_link&sk=c6f791910928c8d0d59000884a91917e](https://towardsdatascience.com/data-observability-the-next-frontier-of-data-engineering-f780feb874b?source=friends_link&sk=c6f791910928c8d0d59000884a91917e)",2020-09-24 18:29:42
1avj2k2,What was your biggest challenge getting into data engineering?,"E.g. for me, one challenge is that I could never really practice on the cloud platforms unless I was using it at work (unless I pay for it myself). Also, it's hard to learn best practices in engineering with only fake / clean data (since the complexities, volumes, watchouts just arent the same)-- in which case, making pipelines with real data is just more effective, and hence only doable again in a work setting",2024-02-20 14:34:43
1advvbm,is programming in data engineering as complex as in software engineering?,in general,2024-01-29 13:55:59
1790jby,"You've just joined a new company who do everything in Excel, but....","There's a senior manager who's keen to modernize their approach to data, but doesn't know what they want. What are you asking for / putting in place?",2023-10-16 07:40:29
15i8nlo,How did you go about learning K8s,"Seems more and more necessary to learn K8s in this space.

I stumbled into data engineering from a data science background. By virtue of working in a small team I have been exposed to some basic cloud infra stuff like using the cloud provider's cli to create resources and set properties etc.

I don't understand very much at all about networking apart from that it's a pain in my ass. 

I do feel relatively comfortable with using docker.

Can someone who has learned kubernetes recently give me any guidance about how best to go about learning?

My primary use cases would be running workloads adaptively on kubernetes. Specifically it would be nice to learn enough that I can create a nice workflow using Ray (KubeRay) to run ML training workloads.",2023-08-04 19:02:12
14y3s4r,"Is airflow better for triggering jobs in a data pipeline, or actually running the jobs itself?","At work we’re building a lil’ pipeline. Nothing fancy, just reads data from a few API’s, normalizes them and sticks em in a table in our DB. 

We’re trying out airflow for this, and we’ve been putting all of the actual code into DAGs in airflow. So, all python.

I saw another post that mentioned how airflow is mostly a “job scheduler”, which made me second-guess keeping all of our code in airflow DAGs. So I’m wondering: do y’all use airflow primarily as a scheduler for jobs that are owned by other services, or do you also rely on it to run business logic?

If that’s too vague, here’s a specific example:
Ideally, I’d have all of my data pulling/normalizing code in rust. We already have a nicely setup rust environment, and that’s how I would handle our pipeline if it was just gonna be rust scripts and a bunch of cron jobs. 
But since airflow has so many easy integrations, we decided just to let airflow (and thusly python DAGs) handle all of the data pulling and normalization. 

Is the “correct” way to use airflow:
1) having airflow trigger rust scripts?
2) having airflow handle everything from within airflow?",2023-07-12 23:06:48
14trirs,"Noob question, but how do you deploy and automate a data pipeline?","I’m relatively new to data ops and have a learned a bunch out of necessity, but I really like DE and want to pursue it further. One thing that’s always baffled me though is how you actually deploy and automate a pipeline. I’ve done a few snowflake labs where I did this via python on a local machine, but what’s the next step up?

Let’s say I want to take data from Postgres in AWS RDS and send it to Snowflake. Do I write a python script and upload to EC2 then schedule it to run via a cron job?",2023-07-08 02:47:32
13rgq25,Is it normal for companies to retain all raw data?,"I work in the IoT manufacturing space and each machine can collect upwards of 50 million points per year. For display/analysis purposes that will be aggregated, however should the raw values still be stored somewhere? That seems like a lot to store. Is it acceptable to aggregate across much smaller intervals to reduce the amount of “raw” data?",2023-05-25 12:40:27
13622x1,The SQL Unit Testing Landscape: 2023,N/A,2023-05-02 22:14:01
133kbq3,"Which certification to get, Snowflake or Databricks?",We are given the choice to be certified in either Snowflake or Databricks. Which do you guys think is better in terms of career progression? We can only choose to specialize in one (in terms of company sponsorship).,2023-04-30 09:09:37
12otxy6,[Interview prep] Anyone in Zach wilson's data engineering bootcamp?,"Zach wilson is a data engineer at Airbnb and his linkedin post says that he is working on his first professional data engineering bootcamp.

Curious to know the reviews of it, if anyone's been there.",2023-04-16 23:52:32
117x39e,What lakehouse stack do you use,"Please share what lakehouse stack do you use. 

For example: 

* Storage: S3
* File formats: parquet
* Table formats: Iceberg
* Realtime: Clickhouse
* Modeling/Transformations: dbt, Spark
* Orchestration: Airflow
* Semantic layer: Cube
* BI: Tableau
* Data quality: Great expectations
* Data catalog: Amundsen
* ML: mlflow, kubeflow
* Other: lakeFS",2023-02-21 07:01:01
10esybb,DW toolkit book by Ralph Kimball,N/A,2023-01-18 00:40:43
yt56bf,"A step-by-step tutorial on how to build a Web application, combining the Streamlit Python library and Versatile Data Kit",N/A,2022-11-12 12:03:10
u6d2cd,What do you all think of vendors on this sub low-key trying to push their products,I've noticed quite a lot of those accounts while lurking the sub for interview tips and to see generally what everyone is working on/with.,2022-04-18 13:23:10
tpy5yd,Red flags to look out for when joining a Data team?,"https://eugeneyan.com/writing/red-flags/

This was written from the perspective of a Data Scientist. What would be red flags for a Data Engineer interviewing with prospective teams?",2022-03-28 01:32:24
sfme7l,Can someone help me understand why data batch processing and data streaming processing pose such different challenges in data management?,I am a ds and I see the differences mentioned everywhere when it comes to data management. I am having trouble understanding why this is the case if anyone care sharing some insights. Thank you!,2022-01-29 17:02:40
pmtenl,Data warehouse interview question,"Hi All,

In one of my recent interviews, I got this question -
How do you build the data warehouse from scratch?

My question is - 
What would be the sequence while answering this question?

Thanks in advance",2021-09-12 14:13:25
gm2rg7,Airflow: how and when to use it,N/A,2020-05-18 14:46:13
18lsb9p,I recorded a crash course on Polars library of Python (Great library for working with big data) and uploaded it on Youtube,"Hello everyone, I created a crash course of Polars library of Python and talked about data types in Polars, reading and writing operations, file handling, and powerful data manipulation techniques. I am leaving the link, have a great day!!

[https://www.youtube.com/watch?v=aiHSMYvoqYE&list=PLTsu3dft3CWiow7L7WrCd27ohlra\_5PGH&index=6&t=689s](https://www.youtube.com/watch?v=aiHSMYvoqYE&list=PLTsu3dft3CWiow7L7WrCd27ohlra_5PGH&index=6&t=689s)",2023-12-19 03:44:20
17yy1wp,How do you sell the value of Data Engineering at your org?,"Title is the question, how would you change the mind of someone who is ambivalent?",2023-11-19 14:13:07
175hslp,What the fuck *actually is* a Data Mart and why should I build one?,"I don't mean the definition of ""Data Mart"", that we all know. Subset of data... aimed at specific business teams... pre-aggregated... etc...

What I don't understand, and can't seem to find actual examples online that draw from realistic datasets, is what it is supposed to look like **in practice.**

Is it just a bunch of views that expose only certain tables to certain users?

Are there any aggregations and if yes, how does one decide which ones to build?

If we're building aggregations, thus losing in flexibility, does that mean that one has to aggregate for each necessary KPI the business users demand? Sounds like a massive PITA.

--- 

I often wonder, isn't it much easier to expose a single common layer (idk like OBT, or even a star schema if your analysts are good) to whatever BI software(s) one's using, and let the analysts build whatever aggregation/metric comes to mind over there, rather than at the end of ELT? 

Many thanks in advance for your help.",2023-10-11 16:08:39
11q1gyr,It's all gone... in a sec,"It's really sad and frustrating to lose so much hard work and a possible opportunity due to a stupid mistake.

For the past six days, I have been working day and night on developing an entire data streaming system using Docker (from streaming APIs and Kafka consumers/producers to the MLspark model). This was a task for a company I am interviewing with for a big data engineering position.

Every day, I told myself I needed to start a repository or back up the code somewhere, but for an unknown reason, I didn't. I kept procrastinating.

I was so excited to finish the project and share it with the interviewer on GitHub as soon as possible. But I told myself, ""One last test,"" and that's when I accidentally deleted all my code.

The main project directory was mounted by the Docker Spark container, which would write the output to the folder provided. However, it needed to remove or empty everything in the provided directory before writing. And that's how I lost everything.

I was so pissed off and spent three hours trying different methods to retrieve my work, but I couldn't. Now, I don't even feel like coding anymore.

(Note: I know I should have versioned the code with Git, which I usually do. But this time, I thought, ""What could go wrong?"")",2023-03-13 05:49:03
ytg56j,My company has around 400 tables in production and only 80 of them have a unique identifier (PK). What do I do?,We are currently going through a large data warehouse redesign and have discovered this issue. It has come up in the past when I’ve been asked to integrate data but haven’t been able to do so because I can’t make them relate. What do you do in this situation?,2022-11-12 19:30:38
wokrau,Advice on how to keep learning?,"I've personally found this subreddit and talking to other data engineers in real life to be some of the most useful tools for learning. I always get to hear how other people are doing things, new ideas, etc.

If I need to deep dive into anything, I usually just go on YouTube, read some blog post, or study some github repos. I also attend virtual conferences occasionally.

I feel like a may need to pick up a few books to get even more mastery, but I literally haven't picked up a book in like 5 years lol.

How do you all learn and keep up to date? Any advice or recommendations? Maybe my ways are a bit amateurish haha(I'm still young in my career)",2022-08-15 00:07:29
vl5d3d,Is a masters worth it?,"Hey everyone,

I’m considering applying to Georgia Tech’s online MSCS to further my CS/Data Eng knowledge.

Right now I work as a business intelligence analyst, starting to work with dbt and the logical next step In my career is moving into a data engineer role.
My background is in chemical engineering and I have no formal training in software. 

I know the MSCS may not be focused totally on data engineering but I see two benefits
1) Make me a better and more confident programmer 
2) There are courses that are still relevant to DE which I will benefit from 

Curious to hear everyone’s though/ alternative suggestions.",2022-06-26 14:00:57
ne3fuk,What tools and technologies do you use the most as a data engineer?,"In your case, what tools/technologies/programming languages do you use at your job?",2021-05-17 01:36:47
n8gloz,Bad coding practices followed in team,"Hi! So it has been nearly a year or so since i shifted in a data engineering team. The work there is quite fun. We build data pipelines, streaming solutions etc on cloud. There is always something new which is nice to have. But the issue is that there are some bad developers in the team and they are quite senior to me. They were working with data warehousing stuff where they mostly had to deal with more sql and less scripting. Now they work with more scripting side and they don’t care about the code as long as it is working. When I comment on their PR’s they get annoyed that why do I care about the code. It’s doing the task and that should be the main goal. I am not that picky but when there are 5 for loops for a thing that can be done in a single loop I can’t ignore that. Maybe it’s because I am from a SWE background and that’s why it bothers me a lot. What do you guys think? Have you guys experienced that? Any advice on how to deal with this situation? Should i just suck it up and not do anything about the code as long as it is working?",2021-05-09 15:32:49
lmod2e,Curated Github repository on how organizations around the world use dbt,"I've set up a knowledge repository of dbt best practices. The idea is to gather dispersed online resources and curate them in a single repository.

Contributions are more than welcome!

https://github.com/smomni/howtheydbt",2021-02-18 14:48:05
ibceit,Start writing better DAGs by discovering Apache Airflow Best Practices!,N/A,2020-08-17 11:31:47
hc9ogc,Data Engineering Youtube Channel!,"Hey guys!

I recently decided to create a Youtube Channel dedicated to discussing data engineering topics and just released my first video. It was just a way to start, more interesting, and hands-on videos will be released soon but for the time being it would mean a lot if you could check it out and let me know what you think.

 [https://www.youtube.com/channel/UCBT9cPM6LbCAIrFgg9Mi9YQ](https://www.youtube.com/channel/UCBT9cPM6LbCAIrFgg9Mi9YQ) 

Show some love! Thanks",2020-06-19 21:53:40
1borix1,History of questions asked on stack over flow from 2008-2024,"This is my first time attempting to tie in an API and some cloud work to an ETL. I am trying to broaden my horizon. I think my main thing I learned is making my python script more functional, instead of one LONG script.

My goal here is to show a basic Progression and degression of questions asked on programming languages on stack overflow. This shows how much programmers, developers and your day to day John Q relied on this site for information in the 2000's, 2010's and early 2020's. There is a drastic drop off in inquiries in the past 2-3 years with the creation and public availability to AI like ChatGPT, Microsoft Copilot and others.

I have written a python script to connect to kaggles API, place the flat file into an AWS S3 bucket. This then loads into my Snowflake DB, from there I'm loading this into PowerBI to create a basic visualization. I chose Python and SQL cluster column charts at the top, as this is what I used and probably the two most common languages used among DE's and Analysts.",2024-03-27 02:59:57
17qa1p6,How many hours do you work a week?,I’m new to my team and the field this year. We’ve all been working about 60 hours a week for the last 6 weeks. It seems like this may be the new norm. Is this normal in your experience?,2023-11-08 01:14:14
15hvadk,I created a chart to explain why 90% of data setups contain custom data pipelines,N/A,2023-08-04 09:30:43
154zbbu,What is the most impressive thing you’ve ever done as a data engineer?,"Did you perhaps singlehanded do [data engineering task]?

Or did you solve [difficult data engineering problem]?",2023-07-20 18:38:57
11qpfwo,What are some of the sticky problems in your data pipelines?,"Hi Folks,I am researching the challenges of data engineering to develop a deeper understanding of the problems and challenges faced by the teams building data products.

I used to be a data engineer between 2009 and 2014. In 2014, I got into product management and I am currently in a product lead role. I have always worked products which included a large amount of analytics, insights, predictive models, machine learning in software companies serving healthcare, surveillance, automotive, and ecommerce.

In my experience over the past 14 years, the data tooling ecosystem has expanded a lot.However, I am still in a scenario where the cost of data infrastructure and tooling is expensive.

* Projects are complex and long drawn.
* It is super hard to solve basic issues of data quality even reactively.
* Maintaining trust in the data assets is really hard.
* Data literacy of decision makers is not up to the mark in a lot of cases.
* Stakeholders expect miracles and stuff to just work.
* Very few people can explain the attributes, the calculations, the metrics, the insights, and the implications.
* Everyone is just promising stuff and punting the inevitable reality of face the hard problem and solve it properly.

I am just trying to research and gather inputs from the community on the nagging challenges of building products now to inform my product development and to inform a course that I am building to develop data product managers (because it is really difficult to find candidates to hire)

My question for you is:

* What are your top 3 challenges in engineering data flows and pipelines?
   * Is it the data inventory, quality, governance, accessibility, etc.?
   * Is the infrastructure, the complexity of building, deploying, administering the systems?
   * Is it the challenge of organizational structure, talent, capacity, leadership?
   * Is it communication, silos, lack of alignment?
   * Is it cost, performance, complexity of infrastructure?
* What is preventing you from building valuable data products?

For me, infrastructure cost, performance of existing tools, spaghetti code, lack of data expertise among leadership stakeholders has been the biggest headwinds to progress.

Last year, at one point, our AWS costs were $1.6 for every $1 a customer paid us. After working on a year and reducing substantial tech debt, we got to AWS cost of $0.6 for every $1 revenue. Still, there is no recognition, leadership is reluctant to fix data quality issues.

As a product lead, I have been able to influence some, but it's a lot of compounding challenges.

Does this resonate with folks?

What are the top challenges you are facing?

What are some solutions or workaround that have worked for you?

Looking forward to your responses.",2023-03-13 23:20:00
117djjy,I got certified recently and prepared some notes while preparing for Azure DP-203,"ps: I know that certificates are not really a very important thing. But I do AWS/Azure certifications to get some hands-on practice on the cloud through labs. I use AWS at work, so I took an Azure certification to get my hands dirty with Azure as well.

Recently I've cleared DP-203 and received the Data Engineer Associate certificate. I [shared a post on here](https://www.reddit.com/r/AzureCertification/comments/105oxza/passed_dp203/?utm_source=share&utm_medium=android_app&utm_name=androidcss&utm_term=1&utm_content=share_button) as well.

I prepared some notes on Notion while preparing for the certification. And I'd like to share it with others so that It could help others while doing revision for the exam.

Notes link: [dp203-azure-data-engineering-notes](https://github.com/jithendray/dp203-azure-data-engineering).

Tips that helped me:

- I did a decent course on the Udemy. 
- Made notes while watching tbe last lecture videos.
- The most important thing is - I spent lots of time on doing stuff hands-on than just watching videos. The main goal of this certification for me is not to get the certification, but to be able to use all the services really well.
- Finally, revised the notes that I made a day before the exam.

All the best, for anyone who is preparing for the exam. Feel free to add ⭐ to my repo ;)",2023-02-20 16:54:33
y9apc6,A good writeup on the newly-announced semantic layer for dbt,N/A,2022-10-20 21:34:48
xryruv,Data Quality Comparison on AWS Glue and Great Expectations/Updated with V3 API,N/A,2022-09-30 11:23:18
wztac0,"Getting hired as a DE (Advice from someone who tried to get hired as a DE, and failed.)","**YMMV!**

I am a self-proclaimed ""general purpose developer"". That means I mostly know one language, Python in this case, and I can do a lot with that language: webscraping, API development, data analytics, etc. So after trying a bunch of things, from the beginning of this year, I started to focus on DE, specifically Analytics Engineering.

If you are applying to an entry-level role and you are being interviewed exclusively by someone who is from the engineering or data team, they only have two criteria-

1. They need a few basic DE specific skills
2. They need something that they NEED

### 1. The few thangs!

The standard DE tools are Airflow, SQL, Python+Pandas, and dbt. And you need to cover a DE cloud stack. For GCP it is: BQ and Cloud Storage. For the ETL process, it could be anything: Cloud Function, Compute Engine or Dataflow.

Now, a bit of greyzone advice-

BI tool... to be honest, you don't need a BI tool that much in DE. So just repeat this after me, ""I have 'explored' streamlit, Looker, and dataflow in the past."" You need to visit their product page and just know what they are, period. The BI tool domain is a slippery slope for DEs because the list of BI tools is endless and BI is a distinct field from DE. A serious recruiter wouldn't demand anything other than that you have heard of the tool they are using. But it is always good if you have built a dashboard or something, but in my opinion, it isn't super necessary.

It's almost the same advice for Spark or Kafka. Look dead center in the webcam or their eye, just say you have explored them. Okay, do explore them, period. Get an understanding of what they do, and have some idea of the syntax. If the data team is using Spark or Kafka, they wouldn't expect you to touch those projects in your first 2-3 months.

### 2. The need

I think you can get operational knowledge of the basic DE skillset in 6 months. But the catch is the NEED factor. The NEED factor can be many things. Every company is different, and they all need that thing that makes them different.

Like Scala. If you know Scala and someone is using Scala in their stack, they will try their absolute best to hire you even if you don't have the hands-on Spark experience. If they are using something like Rust, where there are only a handful of DEs who use Rust, they will try to hire you.

If they are using a method extensively that is often not prioritized in DE, they will try to hire for it. In my case, it is web scraping. Because I am confident in my web scraping skills, companies with large web scraping operations try their best to hire me even though I don't have much DE experience. I also got a few interviews because they were dealing with obscure documentation. I said, I have built a project based on the OLD YouTube API docs (those who knows, knows). They tried their best to hire me.

Now the interesting part is that BI/Dashboard and Spark/Kafka fit in here. In my opinion, they don't fall under the basic needs criteria. These are specialized needs for some DE teams. If you have used them or built projects using them, you would have an edge in a team that uses those tools extensively. But they wouldn't give you an edge everywhere, as not all DE teams use them. That need factor is why some DE recruiters would prefer to have programmers instead of data analysts or vice-versa, because the skillset a recruit brings meets their specific needs.

Essentially, I am saying the need criteria is so diverse, you can't predict your way into it. So specialize in something that relates to DE that you enjoy. It sounds like feel-good advice, but that is just what I figured. Someone who is able to write Java shouldn't focus on building dashboards; they can focus on learning Scala or Rust. If you have data analytics or BI interests, you are wasting your time if you attempt to learn Scala without getting paid for it.

---

So, why don't I have a DE job? I mostly plan to do DE as a side project now. I am a DevRel at a Data first SAAS where I first applied for a DE role! DevRel fits me far better than DE.",2022-08-28 11:32:31
w335b7,Live Data Engineering sessions and workshops next week at Summer Community Days!,"If you're interested in getting hands-on education on data engineering, check out some of the live sessions available at our conference next week! 

Yes, it's free. No, you won't be getting aggressively sold to 😂

***👉 Workshop - All of data engineering in three hours*** *w/* [Pete Fein](https://www.linkedin.com/in/peterfein/?utm_campaign=Summer%20Community%20Days&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz---XAWgKpwAKhHmmRXud9l0Z4bw8zfDPsOweANTSoA_28MH4hlLpe6QeEJCcUyZtKQfZ8hD)*,* Consultant & Trainer @ Snakedev  
👉 ***Technical Talk - Analytics engineering with dbt: A fintech application*** w/ [Carlo Scalisi](https://www.linkedin.com/in/carloscalisi/?utm_campaign=Summer%20Community%20Days&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz---XAWgKpwAKhHmmRXud9l0Z4bw8zfDPsOweANTSoA_28MH4hlLpe6QeEJCcUyZtKQfZ8hD), Senior Data Analyst @ N26  
👉 ***Actionable advice - Data in a downturn*** w/ [Stephen Ebrey](https://www.linkedin.com/in/stephen-ebrey-456b264/?utm_campaign=Summer%20Community%20Days&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz---XAWgKpwAKhHmmRXud9l0Z4bw8zfDPsOweANTSoA_28MH4hlLpe6QeEJCcUyZtKQfZ8hD), Data Consultant @ Sawtelle Analytics  
👉 ***Actionable advice -*** **Delivering value as a data engineer** w/ [Sarah Krasnik](https://www.linkedin.com/in/sarah-krasnik/), data consultant and advisor)  
👉 ***Actionable advice -*** **Make events a first-class citizen with activity schema** w/ [Timo Dechau](https://www.linkedin.com/in/timo-dechau/), founder & data engineer @ Deepskydata  


If any of these interest you, register for the conference next week!  [🔗 Link](https://www.operationalanalytics.club/summer-community-days)",2022-07-19 20:50:02
vntp4f,Streaming the Reddit API with Pyspark + Kafka on Docker.,N/A,2022-06-29 23:36:44
u585sx,What are your thoughts on Data Lakehouses and Open Architecture?,N/A,2022-04-16 21:52:57
tt0yv7,Handbook for commom pipelines?,"Title. Is there a resource that is basically a handbook for common pipelines/use cases? For example, take a daily csv and upload/qppend to a database table. In my mind, what I want guidance on would be things like when and why to use staging tables, what to put in place to handle workflow failures, deduplicating, etc.

Like probably many of us, I just sort of happened into a role where I do data engineering, so I'm light on best practices.",2022-03-31 14:27:40
t6xni1,Learning basics Python and SQL: what's next?,"Hi, folks of reddit! 

I currently finished the Practical SQL (Anthony DeBarros) and Python Crash Course (Eric Matthes). Both great introdutory books to SQL and Python, but I don't know where to go next. Should I learn about ETL/ELT? Or should I learn how to use API's and get to know more about pipelines?   


What to do after studying Python and SQL fundamentals? 

Learned so far: 

SQL: 

1. Select, From, Where, Group By, Order By 
2. Subqueries, CTE, Index
3. Regular expressions

Python: 

1. Variables, algebraic expressions 
2. Lists, tuples, dictionaries 
3. For and While loops
4. If, else, elif statements 
5. Functions   


Should I study more deeply the topics above to get a solid knowledge before getting into Data Warehousing, Pipelines, etc?",2022-03-05 00:56:26
qmtkzj,Python and ETL,"Just spent the day dedicated to learning python, specifically pandas and numpy to get a better idea on how I could use this for ETL in the future (no immediate needs). 

I understand there are endless opportunities with a powerful language like this and I’ve literally only scratched the surface, but I wanted to gauge if I’m understanding more or less how pipelines are set up and also get some best practices.

Not to oversimplify, but is the basic gist of ETL in python reading files (or other sources) into dataframes, making manipulations on those dataframes, then spitting back out?

For the load aspect, how easy is connecting to an SQL Server? Does python have some username/proxy it will use on the server to read/write against the database? What will our DBA need to do server side to permit the connection?

Related to SQL, I’d say I spend most of my time there and as a result I find some aspects of working with dataframes weird syntactically speaking. My brain hates denoting fields like [‘Field’] and while using arrays to pass  conditional logic / corresponding choices with numpy is pretty cool, it isn’t as intuitive to me as CASE WHEN. Is it a good practice to get something like pandasSQL or sqlalchemy or is this generally avoided in DE? You can tell me to just suck it up and learn more python if that’s the case.

Thanks in advance to anyone who can help guide my thinking here. I’m completely self taught in this crazy world of data so I’d like to at least start off with python on sure footing.",2021-11-04 20:19:05
qev11p,How would you design a pipeline that ingests and processes TERABYTES of data every hour and delivers the data to DWH?,"My company is the developer of a web/mobile application that has 100k+ daily active users. The app is hosted entirely on AWS. I am a junior DE trying to build a pipeline that can do the following:

* The pipeline must be deployed entirely on AWS (like the app backend)
* Ingest and process between ~~1 to 2~~ 0.5 to 1 TERABYTE of data (mainly user action/event logs) per hour every day.
* Deliver the processed data to a DWH like redshfit in near real-time. For example, if a user does something, the action log should be in the DWH within 3 minutes-ish. This is a must for our use-cases, so hourly batch processing is not an option.

I don't have much experience and would love to hear how you would design this pipeline end-to-end. Thank you very much.

EDIT: it's more like 2 TB Maximum possible amount, and on an average day it's more like 0.5 TB - 1 TB per hour, but I guess that's still a lot...",2021-10-24 16:01:01
gmujcf,"DataCamp is completely free through 5/22, no credit card is required! There are 330+ courses on Python, SQL, Scala, and all the technologies you need mastery of as a data engineer. No risk, all reward!",N/A,2020-05-19 18:43:58
1arpamc,Guiding others to transition into Azure DE Role.,"Hi there,

I was a DA who wanted to transition into Azure DE role and found the guidance and resources all over the place and no one to really guide in a structured way. Well, after 3-4 months of studying I have been able to crack interviews on regular basis now. I know there are a lot of people in the same boat and the journey is overwhelming, so please let me know if you guys want me to post a series of blogs about what to do study, resources, interviewer expectations, etc. If anyone needs just a quick guidance you can comment here or reach out to me in DMs.

I am doing this as a way of giving something back to the community so my guidance will be free and so will be the resources I'll recommend. All you need is practice and 3-4 months of dedication.

PS: Even if you are looking to transition into Data Engineering roles which are not Azure related, these blogs will be helpful as I will cover, SQL, Python, Spark/PySpark as well.

&#x200B;

**TABLE OF CONTENT:**

1. [Structured way to learn and get into Azure DE role](https://www.reddit.com/r/dataengineering/comments/1asegcy/blog_1_structured_way_to_study_and_get_into_azure/?utm_source=share&utm_medium=web2x&context=3)
2. [Learning SQL](https://www.reddit.com/r/dataengineering/comments/1asxqdx/blog_2_learning_sql/?utm_source=share&utm_medium=web2x&context=3)
3. [Let's talk ADF](https://www.reddit.com/r/dataengineering/comments/1atuvav/blog_3_lets_talk_adf/?utm_source=share&utm_medium=web2x&context=3)",2024-02-15 20:29:02
16xxu15,"Any tool, you regret buying or deploying in the data infrastructure?","For me: I was pushed to use FiveTran and from there the pain started, our simple infra became complicated. Took me almost 8 months to convince my boss to replace that with ELT approach where sink to S3 -> dbt->Redshift",2023-10-02 14:17:15
12o0euv,Is data lake just a theoretical construct? How does it look on a code level when we say implement in GCP?,on an architectural level...,2023-04-16 07:56:57
12acdrk,The roast of the modern data stack?,"Just read this and loved it (who doesn't like a bit of drama?)! I'm not to well versed, so I'd be interested in hearing what other people in the industry are thinking? Given the previous posts about costs of data teams etc, I think it's quite interesting.   


[https://medium.com/@laurengreerbalik/customer-empathy-is-dead-10f412782b5e](https://medium.com/@laurengreerbalik/customer-empathy-is-dead-10f412782b5e)  


*Airbyte is such a joke that Reddit* r/dataengineering *users “MyDixonsCider” (say that out loud) and “ThunderCuntAU” seem to have done more due diligence on Airbyte than any venture capitalist willing to give tens of millions of dollars of other peoples’ money to Airbyte’s founders to make the twentieth-worst version of a data replication EL tool that exists on market.*  


*I should be firing Fivetran potentially, when all we mostly use it for is Postgres and Salesforce replications, and I should look into Keboola (or similar) which can give me the same thing with better SLA adherence and better uptime for $10k a year and allows me to “transform and denormalize” before dumping data into my Snowflake, which also will get rid of 25 dbt models and lower my Snowflake bills by a run rate of $30k this year.*",2023-04-03 07:04:24
125cd6n,Mountpoint for S3,https://github.com/awslabs/mountpoint-s3,2023-03-29 03:56:17
z2jh8f,Difference between Data Warehouse and Data Lake?,"Hi,

I'm still confused about the difference and use cases for a data warehouse and data lake. In my understanding what differs a database and data warehouse is OLTP and OLAP. While a database is more transaction and consitency focused, a data warehouse is optimized for big queries which makes it efficient for searching through big data. But why would I use a Data Warehouse like for example the Synapse Warehouse in Azure when I can create a Databricks solution with it's Lakehouse Architecture and Delta Tables that provide ACID? As far as I understand a Data Lake is just a dump for non relational data but you can still load from it since there a connector for Power BI also without the delta layer. So why not load directly from the data lake instead of putting the tables in a data warehouse as a intermediary step? Further, it is recommended to have around 3-4 stages (raw, curated, enriched), making the data lake also structured.  Another point is that a data Warehouse is very costy in Azure at least, while a data lake is quite cheap, so I don't really see the value. Can someone perhaps elaborate? Thanks!",2022-11-23 08:15:48
yndu7k,Data Engineering Project - Gmail Manager,"According to Statista, nearly half of the emails sent worldwide are spam. In 2021, it was estimated that nearly 319.6 billion emails were sent and received daily.

Though Gmail marks most of the emails as spam, still we receive bunch of marketing and promotional emails. I have tried to develope the datapipeline to see from what all domains, I receive emails daily. I have created dashboard where I can see all these stats and I can go and block the particular domains which makes my task lil easier instead of going through each and every email and blocking.

Tech Stack :

Python

Airflow

Grafana

&#x200B;

Dashboard Link : [https://snapshots.raintank.io/dashboard/snapshot/E3bVrLkkPYU0XzpfjPRbwZXsLCMlwg7t](https://snapshots.raintank.io/dashboard/snapshot/E3bVrLkkPYU0XzpfjPRbwZXsLCMlwg7t)

&#x200B;

&#x200B;

&#x200B;

[Dash Board](https://preview.redd.it/d7oeprsn09y91.png?width=2742&format=png&auto=webp&s=91692867b9920c391d5eeaa62085576f1ee89950)

GitHub :

[https://github.com/amrgb50/MANAGE-GMAIL](https://github.com/amrgb50/MANAGE-GMAIL)

&#x200B;

[App Flow](https://preview.redd.it/dck7scg8l8y91.png?width=1804&format=png&auto=webp&s=0b4400dfd0678a46ea01f00d295f8b7f7f7b3580)

&#x200B;

Improvements and next plan :

1. I am learning docker and kubernetes. So next step will be containerizing this app and run in cloud.
2. Implementing DQ checks.

Any and all feedback is absolutely welcome! This is my first project and trying to hone my skills for DE profession. Please feel free to provide any feedback!",2022-11-06 02:09:25
xmcxi6,Understanding the Snowflake Query Optimizer (one of the best article I’ve read in a long time),N/A,2022-09-23 23:39:35
whvtnh,5 things I wish I knew about Databricks … before I started.,N/A,2022-08-06 19:11:47
v70tod,What are your hottest dbt repositories in 2022 so far? Here are mine!,"Here are my top5! 🚀  


&#x200B;

https://preview.redd.it/4xun4kj898491.png?width=498&format=png&auto=webp&s=caa1bc10f10ad3cd902d622b4f282a068576a1a3

&#x200B;

\- ⚡️ [Lightdash:](https://github.com/lightdash/lightdash) Lightdash converts dbt models and makes it possible to define and easily visualize additional metrics via a visual interface.  


\- ⏎ [re\_data:](https://github.com/re-data/re-data) Re-Data is an abstraction layer that helps users monitor dbt projects and their underlying data. For example, you get alerts when a test failed or a data anomaly occurs in a dbt project.  


\- 📗 [evidence:](https://github.com/evidence-dev/evidence) Evidence is another tool for lightweight BI reporting. With Evidence you can build simple reports in ""medium style"" using SQL queries and Markdown.  


\- 🧱 [Kuwala](https://github.com/kuwala-io/kuwala): With Kuwala, a BI analyst can intuitively build advanced data workflows using a drag-drop interface on top of the modern data stack without coding. Behind the Scenes, the dbt models are generated so that a more experienced engineer can customize the pipelines at any time.  


\- 🐍 [fal ai:](https://github.com/fal-ai/fal) Fal helps to run Python scripts directly from the dbt project. For example you can load dbt models directly into the Python context which helps to apply Data Science libaries like SKlearn and Prophet in the dbt models.",2022-06-07 16:40:21
uhohlv,"Is Kimball's Dimensional Modelling dead in 2022? Is OBT (""one big table"") the way to go?","Seems like his approach to data modelling was more for relational OLAP databases.

Modern data warehouses are column oriented, which means a lot of issues that star schemas were supposed to solve isn't really a problem these days.

In fact, Fivetran released a study that found OBT is 25% - 50% faster than a traditional star schema, which makes sense because it reduces the number of joins you need significantly.

OBT would also make it extremely simple for end users to query.

So, why use Kimball's dimensional modelling in the modern data stack, when we can use OBT or other, newer architectures that are better suited for cloud based data warehouses?

This is a genuine question, not trying to cause drama.. I am new to all of this so I thought I would ask you folks who are experienced. Thanks!",2022-05-03 19:58:30
udxpmj,MLOps Zoomcamp from DataTalks.Club - free course about productionizing ML,"Our Data Engineering course is almost over and we're starting a new course about putting ML to production.

We'll cover:

* Processes
* Model training (ML pipelines, experiment tracking)
* Model deployment (web services, batch, streaming)
* Model monitoring
* Best practices 

&#x200B;

Here's more information: [https://github.com/DataTalksClub/mlops-zoomcamp](https://github.com/DataTalksClub/mlops-zoomcamp)",2022-04-28 15:31:34
t4clep,Quarterly Salary Discussion,"This is a recurring thread that happens quarterly and was created to help increase transparency around salary and compensation for Data Engineering. Please comment below and include the following:

1. Current title

2. Years of experience (YOE)

3. Location

4. Base salary & currency (dollars, euro, pesos, etc.)

5. Bonuses/Equity (optional)

6. Industry (optional)

7. Tech stack (optional)",2022-03-01 17:00:21
r1xvbe,The Missing Semester of Your CS Education,N/A,2021-11-25 14:34:31
jdp74r,/r/dataengineering hit 20k subscribers yesterday,N/A,2020-10-18 21:55:13
ixdtl2,What personal projects would you guys recommend for somebody who wants to do more data engineering?,"I will leave this open ended so that others can benefit too. Let's assume I have a website/GitHub.

Tell me some personal projects that can be done with data that is available which would impress you! Also be sure to mention all the technologies you would use to accomplish said project.",2020-09-22 01:23:35
1br8jrl,Transforming data with DuckDB in under 3 minutes!,N/A,2024-03-30 03:59:14
1b9oqr9,Stop Paying Snowflake for Failing Workloads,N/A,2024-03-08 14:05:52
18of52n,Was I unprofessional when I said I needed help?,"Im a contractor and I was asked by someone at my client's to help out with another project I wasn't originally contracted for. It's an ERP migration project, so I really only have to write SQL statements and extract some data based on the new ERP system's template from the legacy ERP's backend (SQL Server).

I've never done this kind of work before and largely unfamiliar with ERP systems, but since it's mostly just SQL work I thought I could help out. Project Manager gave me a list of fields I need to look for, but there are about 200 tables in the backend. I said I'd like to speak with the finance team managing this legacy ERP system to narrow down my search, and Project Manager says ""well I saw the table names, they all look pretty straightforward, I think you can just look through all those tables"". But I said ""I've never worked on this domain before so I would really appreciate some guidance"". And she didnt' respond.  I tried searching blindly but ended up finding a table with 10 addresses per customer and I don't know which of these are needed etc. 

Was I being unprofessional for wanting to ask for help to make sure I'm looking for the data in the right places? ",2023-12-22 13:39:17
18f0tju,What is DuckDB Used For?,"Sorry for the noob question if it sounds ignorant. Here is the situation. 

I joined a new company, and inherited a lot of legacy codes (ETL jobs, written in python) from another colleague who already left before I got to talk to him. Throughout his code, he made extensive use of duckdb just to query pandas dataframes, basically, something like this.

```python3
import duckdb, pandas as pd
df:pd.DataFrame

df.loc[df['value']>20] # Pandas form

query:str='SELECT * FROM df WHERE value>20;'
duckdb.sql(query=query).df() # SQL Form
```

So it seems duckdb gives a SQL like interface to pandas, but is that all it does? Any trade off between just using pandas semantics as opposed to writing a raw SQL query in the code? The pandas semantic looks a lot cleaner to me, and introducing another dependency seems rather superfluous. 

So the basic questions are 
* Why would a developer use duckdb? Does it provide better performance than pandas? Or is the SQL like interface the most important purpose here? 
* The examples are for some legacy codes that I am going through, but for myself, I use polars or dask for tabular data manipulation. Does duckdb work with those frameworks?",2023-12-10 10:35:19
17tqj7b,Fellow DEs how do you manage data quality?,"There are so many apps, products, and libraries out there that help with data quality; Some tools are marketed toward enterprise-level while others are for data-pipeline monitoring. I am currently in a position to consider and experiment with data quality tools for my team, so I am curious about what folks are using right now.

What is your set up and how does it help you?

&#x200B;

Given that data quality is a very broad topic, and I'm not necessarily looking for a comprehensive solution -  just whatever you've found to be the most effectively for your team / org. 

Personally, I've mostly used custom scripts and tables to track the metadata I need (pipeline runs, data source baseline metrics, unit tests) with a Power BI dashboard to visualize the most relevant information for that project.",2023-11-12 18:39:38
17d3xk3,What is data engineering *not*?,"Just curious what everyone thinks is more or less “outside” of the scope of a data engineering role (for example, that you may typically expect in a top 500 company, with the understanding that there is some natural variation in exact definition…).

There’s an endless amount of work to be done across any data team, but what sorts of things would you say no to in order to avoid scope creep of your duties as a data engineer?",2023-10-21 14:34:12
179x2d7,Leetcode and System Design for data engineers vs. software engineers?,"How does everyone interview prep, since it seems that all interview prep resources are more for software engineers than data engineers?



In general, software engineering interviews will ask more difficult data structure and algorithm questions.  But for data engineering interviews, I've noticed that the questions ask easier or moderately difficult Python questions involving string manipulations or dictionaries, followed by more difficult SQL questions.




Also, system design for software engineers, it seems they are asked to design a larger variety of services.  For data engineering, I've only been asked about data pipeline design.",2023-10-17 12:20:05
16vnj0c,"ELI5, what are the main differences between software engineers and data engineers?","May not like I’m 5 y/o per say, but how would you explain the difference to someone who has no clue what data engineers do?",2023-09-29 21:12:53
14tdmv2,Thoughts on the data janitor (youtube)?,"I recently discovered a YouTuber called ""the data janitor"" who articulates very clearly things that I've rarely heard elsewhere when it comes to getting into data engineering. He has very strong opinions on what are the ways of getting into data engineering and machine learning engineering. I was wondering if some of you know him and if, for those of you who are in a data engineer role, if his takes make sense or not from your point of view. I know the guy’s very assertive “no BS” tone is not everyone’s cup of tea, but I would like to have a discussion on what he actually says instead of his style or the fact that he also promotes his own education platform in his videos.  
  
  
Basically the takeaways from his videos are as follows:  
  
1) Data engineer is not an entry-level role. If you don't have at least one year of experience in a data-related role (data analyst, DBA, etc), there's 99% chance you won’t be hired as a data engineer.  
  
2) A person who wants to become a data engineer shouldn't try to become that first (almost impossible), but should focus instead on a real entry level role such as data analyst.  
  
3) Data roles (DE, DA, MLE, etc) are primarily SQL heavy roles. You can't get away from SQL. Because SQL is not sexy, bootcamps want you to believe that you’ll also need a significant amount of Python (more sexy), but 90% of the time, you don’t.  
  
4) Data roles are very different from software engineering roles. A data analyst is better suited at becoming a data engineer than a DevOps or a Back-end dev.  
  
5) Certifications and certificates of completion are totally different. Certificates of completion (Coursera, Datacamp, etc) that you obtain by simply watching videos and filling blanks are worthless to recruiters. On the other hand, certifications, i.e. you have to take an exam in a physical test center or online proctored and you pass/fail the exam, can definitely have some value, but mostly if they come from the big three (Google, Microsoft, AWS) or traditional tech corporations (Oracle, Cisco, IBM, …). Some of those certifications are very hard to get and thus very respected (example: MySQL 8.0 Database Developer 1Z0-909 from Oracle). Certifications are not worth as much as actual work experience, but it’s still a non-falsifiable signal that you know a tool/framework well enough for a job.  
  
6) He thinks that without prior data experience, if you want to get into data engineering, your primary objective should be to get into a data analyst role first, and to get this role, you need two skills, Power BI and SQL. To signal those skills, two recognized certifications can help if you don’t have any professional experience with them: **PL-300 Microsoft Power BI Data Analyst**, and (since Microsoft deprecated most of its former SQL certs) **DP-300 Administering Microsoft Azure SQL Solutions**. He claims that having those two certifications on a resume can definitely get you interviews for entry-level data analyst roles if you don't have any experience in the field.  
  
  
Thoughts?  
  
For those who are/have been data engineers, do you agree with him or not? Does it depend on the field we're talking (big/legacy tech VS smaller companies maybe)? Or is it broadly true/false?  
  
What I like about him is that he seems very frank and honest about his view of the professional data world, very different from the typical too-good-to-be-true takes that you see here and there that sounds like ""don't worry anon you'll find a job in data if you send enough resumes, plenty of opportunities out there :3"", either because people want you to sign-up to their bootcamp, or just not hurt your feelings.",2023-07-07 17:27:48
13695y9,Checking in: lake houses don't seem to be replacing data warehouses,"Casual observer was thinking the tooling and or feature sets around lake houses would be something to weigh against an actual data warehouse.

I can't seem to find much that would suggest a lake house comes anywhere close to the performance and or features, complex SQL syntax, of a data warehouse. 

Am I missing something?",2023-05-03 03:29:27
11fhmqu,"If dbt is the ""T"" part of an ""ELT"", what do you use for ""EL""?","I'm working on an API data ingestion project, and I usually use AWS Lambda or Python on Databricks for the whole process, but I was wondering if there are any better options/services for Extract and Load part of the process, like dbt is for data transformation.",2023-03-01 20:50:13
119oxil,Building a better local dbt experience,"Hey everyone 👋 I’m Ian — I used to work on data tooling at Stripe. My friend Justin (ex data science at Cruise) and I have been building a new free local editor made specifically for dbt core called Turntable ([https://www.turntable.so/](https://www.turntable.so/))

I love VS Code and other local IDEs, but they don’t have some core features I need for dbt development. Turntable has visual lineage, query preview, and more built in (quick [demo](https://www.loom.com/share/8db10268612d4769893123b00500ad35) below).

Next, we’re planning to explore column-level lineage and code/yaml autocomplete using AI. I’d love to hear what you think and whether the problems / solution resonates. And if you want to try it out, comment or send me a DM… thanks!

[https://www.loom.com/share/8db10268612d4769893123b00500ad35](https://www.loom.com/share/8db10268612d4769893123b00500ad35)",2023-02-23 04:55:25
10qzicp,Uber Interview Experience/Asking Suggestions,"I recently interviewed with Uber and had 3 rounds with them:

1. DSA - Graph based problem
2. Spark/SQL/Scaling - Asked to write a query to find number of users who went to a same group of cities (order matters, records need to be ordered by time). Asked to give time complexity of SQL query. Asked to port that to spark, lot of cross questioning about optimisations, large amount of data handling in spark with limited resources etc.
3. System Design - Asked to design bookmyshow. Lot of cross questioning around concurrency, fault tolerance, CAP theorem, how to choose data sources etc.

My interviews didn't went the way I hoped, so wanted to understand from more experienced folks here, how do I prepare for:

1. Big O notation complexity calculation on a sql query
2. Prepare of system design, data modeling for system design. I was stumped on choosing data sources for specific purposes (like which data source to use for storing seats availability)",2023-02-01 16:52:02
10g3g1r,What mistake have you vowed never to repeat?,Could be something very minor like a specific gotcha in a library or a major architecture decision that bit you in the behind.,2023-01-19 14:44:44
106f68v,Are you using an orchestrator like Airflow or Prefect for your project? Why?,"* Which use case/project made you choose a data orchestrator?
* How did you choose one? 
* How was your experience adopting it?
* How do you feel about it now?",2023-01-08 09:35:13
xfhvcw,How to move from BI to DE?,"Right now I mostly cobble sql queries together into stored procedures. This is using either a kimball style data warehouse or against transactional databases. These procedures are then called in ssrs or PowerBI for visualization.

What is next from here - how do I level up?

Should I go further into PowerBI or try to get more into the warehousing side? SSIS is used for etl.",2022-09-16 04:48:22
x0rxw1,My list of the best Data Eng podcasts,N/A,2022-08-29 15:42:08
wx484r,Is your job comprised of a lot of batch ETL work? How are you managing your workflows? How are you handling your parameters?,"Hello, I am a Data Engineer at a company that ingests data in batches from a few large clients. Our general workflow is:

Grab Data From S3 -> Transform -> Upload to our internal platform (mongodb with rest api frontend)

Our transformation process is typically composed of the following things:

- Data cleaning and normalization, which changes on a client-to-client basis depending on how the data was 
originally created. This is typically done with a few python scripts that are tailored to the client

- Enriching our clients' data by pulling from a multitude of different sources (in-house postgres databases, lookup tables, etc), these sources vary depending on the need of the client

- Some orchestration component to string all of the complicated transformations together. We're currently using Argo Workflows in Kubernetes, and each major ETL step is a container that spits out one or more files. We were using Airflow for a bit but felt that it was not fit for our use case. 

If this workflow is at all similar to that of your company, I'm curious as to how (if) you've solved a few of the following things:

- External connection configuration. Most of our ETLs require more than 5 different configuration files and credentials, typically loaded as secrets or configmaps in kubernetes 

- Handling configuration parameter complexity down to individual steps. Many steps in our transformation/cleaning process require 10+ parameters, which typically handle where data is saved, what directory files are loaded from, postgres connection information (see above), etc. It's a lot to orchestrate and manage. These are also typically managed as secrets or configmaps in kubernetes (we have a lot of secrets and configmaps)

- CI/CD: How do you go about intergration testing such complicated pipelines? We typically have a suite of test configs that connect our ETL workflow to dev sources that mimic our prod sources, but this gets complex as we're attempting to mimic the entire system as close as possible. What do you use to deploy changes to your batch ETL processing job? (This is getting more into DevOps, I know, but sometimes it feels like the whole industry is as well...)

This is more out of my own curiosity, as we have a few solutions so far, but I can't help but feel unsatisfied with them, as I often find myself in a sea of different configurations, or struggling to create a fully fleshed out integration pipeline for some of these workflows.",2022-08-25 04:12:01
vg3384,How much of python should I know to get a DE junior role?,"I'm currently learning python pandas, numpy,matplotlib.

What else should I focus specifically in python?",2022-06-19 19:51:18
t9i1fx,Have any of you done Data Engineering for small businesses?,"I am wanting to expand my toolbelt and become familiar with tech that my job doesn't allow for (shoutout big brother). Have any of you tried consulting for small businesses in your area? I would be planning to do a full data pipeline and reporting for the customer. I just don't know if they have software from their vendors that might already provide reporting or if their existing software wouldn't allow for a data pipeline to be built. I am also just nervous to be testing new frameworks to work in. 

I am wanting to raise my income and grow my tech stack by consulting small businesses, has anyone had luck doing this?",2022-03-08 14:30:52
t6lpul,Dbt Tests Vs Great Expectations,"Hello,   
Can anyone explain to me the difference between dbt tests and great expectations, what's the perimeter of each one in a pipeline, and how can I get most of them both.  
   
If anyone uses them in production , do you mind share your setup.

Thank you",2022-03-04 15:40:42
sqwcq6,Change Data Capture (CDC),N/A,2022-02-12 17:18:29
r7dyom,"What percent of your work would you say is done using Python, SQL, etc.?",What is your job title?,2021-12-02 18:34:47
pdt9z1,ETL guy trying to be Data Engineer,"Hello All, i am having 3.4 years of experience in ETL informatica and iam planning to upskill and change my company.

I got a week off from work as a break from burnout and planning to learn some data engineering tools or platforms, which can help me in getting better salary package during my on going job change..

what do you guys recommend and what's your opinions on this.",2021-08-29 11:02:55
o5jjig,Is it just me or ELT seems over hyped?,"I understand how ELT scheme can be beneficial over ETL, but ELT comes with ""conditions"" such as clean data ready at hand to be loaded directly to the DW, and what about the situations with multiple OLTP sources?

ETL scheme is here since the 1980s and it will always work for virtually any situations. Yes, it may not be the most optimal solution in some cases where ELT would shine, but the success assurance of ETL can not be denied.

Can we advocate for a rational approach in choosing the data integration scheme, where the words ""depending on specific business needs"" be bolded everywhere? I have seen ELT schemes that uses extensive Spark workload to clean/wrangle the data, then load to warehouse. If you are already employing Spark then why not use it for Transformation as well?? SQL transformation becomes very costly in MPP due to the sheer number of join operations, which could be avoided in ETL.

Are we going to continue promoting elegance of a solution over practicality?",2021-06-22 10:08:54
khmfqi,Almost done with my DE Project. Mind Taking a look?,"Hey, everyone! I am in the last part of finalizing my first DE project. I've been working on this for a while now trying to make it the best I possibly can. I know it is far from perfect but, I am super proud of how it turned out and I wanted to share it here so I could get some feedback. I am still new to the field so keep that in mind while you're roasting me for my code not being the work of a god haha.

&#x200B;

I welcome any and all feedback!

&#x200B;

[https://github.com/dylanzenner/business\_closures\_de\_pipeline](https://github.com/dylanzenner/business_closures_de_pipeline)",2020-12-21 18:00:24
gj722d,"Why GitLab is building Meltano, an open source platform for ELT pipelines",N/A,2020-05-13 20:29:17
1bscn19,Dealing With the Sunk Cost Fallacy of Giving up on Data Career?,"I have 4 years of experience in a smattering DS/DE roles. I have a BS in Information Science and an MS in Data Science.

 I’ve watched my junior salary of $130K depreciate down to $90K as I’ve dealt with layoffs/bad career moves yearly, and I am now scrapping the bottom of the barrel of jobs to keep money coming in. 

Data is so competitive now that I don’t think I can realistically rebuild and be competitive with the sea of math/stats/cs grads. 

I’ve put a lot of time into data. I don’t think I can compete in this field since I don’t really care about it, I have only made it this far out of sheer will.

I want to exit and become a game developer, which I study about and build games constantly. 

The reason I haven’t fully committed to becoming a game developer is the sunk cost fallacy of all the time and work I put into DS. Game dev is risky and competitive too, but I at least have the backing passion to maybe push through and be successful. 

Anyone else in a similar situation? ",2024-03-31 15:10:33
1bp4d61,From DA to DE unintentionally ,"So long story short I got hired as a Data Analyst and my manager said they’re changing my role to a Data Engineer. 

I’m slightly freaking out because I feel like I’m not ready or I don’t know anything. The team is super nice and helpful and so is my manager, but the fact that I’m still in school pursuing my masters on top of it makes me feel nervous. 

I feel like I don’t know the first thing about being a DE so I’m constantly on YouTube just watching videos.

Any advice to help me calm down?",2024-03-27 15:16:08
1bjvuwy,Data Processing in 21st Century ,"Timeline of Data Processing technologies covering from MapReduce to Polars. 

Covering distributed frameworks to single node libraries, mature and recent development. Let me know which one missed in the comments.

Let me know which one have you used, and which one I have missed.

https://www.junaideffendi.com/p/data-processing-in-21st-century",2024-03-21 02:29:32
1bg8vpi,A Definitive Guide to Using BigQuery Efficiently,"Hi, since I worked on several projects with BigQuery in the past months, I decided to share my learnings in a comprehensive blog posts, helping other Data Engineers to make the most out of their BigQuery usage, burning data rather than money to create real value with some practical techniques.

Looking forward to your experiences and feedback. Enjoy reading:

[https://medium.com/towards-data-science/burn-data-rather-than-money-with-bigquery-the-definitive-guide-1b50a9fdf096](https://medium.com/towards-data-science/burn-data-rather-than-money-with-bigquery-the-definitive-guide-1b50a9fdf096)",2024-03-16 15:40:38
1ablzb7,Why you're not getting hired -- Tips for those looking for a new job.,"## Context
I've seen a few posts/comments on the top of getting a new job. Talking about a tough job market, learning skills, etc. The comment I wanted to write on a few of those deserved a top level post IMO.

The market is somewhat saturated at the moment with the recent layoffs and hiring freezes/slowdowns, so you won't just get an offer or three thrown at you just because you applied a few places. Here's the simplest way to get hired.

## Differentiate yourself

A hiring manager or recruiter has to sift through tens to hundreds of resumes to fill a position; your resume needs to be have a reason why it should be chosen over your fellow applicants. **If your resume reads like every other applicants', then you're likely not going to get interviews**. The follow up question, then, is how do you differentiate yourself? Here are a few suggestions

1. Tailor your resume to the job description. At the very least, use keywords that the company is looking for. If the company isn't looking for it or it isn't relevant, take it off your resume; it's wasted space.

2. Write each bullet in your job descriptions demonstrating one of either two things (1) business impact or (2) differentiating skills. For business impact bullets, convey the outcome that your actions had on the business and not a description of what you did. For skills-related bullets, mention how you used specialized technology to solve a business problem. You're probably not being hired to be a technologist but to help drive business success.

3. Clearly demonstrate your skill sets!  Saying you can write Python is fine. Having a link to an easy to navigate Github.

4. Have side projects. This is an important carve out of #2 above. If your skill is passion for the industry or willingness to tackle unsolved problems, having a side project is the most effective way to demonstrate this and stand out from the crowd. 

5. Network. If you don't have enough experience to have differentiated job descriptions nor are you willing/able to put in time on a side project, your best bet to get interviews is to bypass the resume screening altogether. Go to events, meet people, tell them you're looking for a job, and/or offer to help them out however you can. If you can't travel, there are online meetups.",2024-01-26 16:13:39
169slfu,Been working as a data engineer for 2 years,"(Sorry for the long post….)

I don’t have a technical background but I started as an intern in my current company (IT Fortune 500 and my first and only professional experience since graduation) doing unrelated work.

I realized that I really liked programming so I started taking Udemy courses for that since my company was offering Udemy for free. I also realized that I like data so starting working on that too.

There was an initiative from the company to start developing Data skills, but It fell short because of the lack of interest from my co-workers (we are 1000+ just in my country). I soon took notice of this and knew it was an opportunity to make my way. 
Did the courses and talked to the chapter leader and, like a miracle, there was one POC project that they were working on for a contest to win a project with a client (almost like an auction) but the guy that was the data engineer, and only member of the team, left so they immediately let me work on it. I completed the POC and the client loved it but my company’s price for the whole services package was too expensive.

After that POC I was still working on my previous role (no data or IT related) so I started applying to jobs and got an offer to be a Data Analyst in an important cosmetic company in my country that is very well known for it’s great data department. I told my company about this and they told me not to leave and offered me a 50% increase in salary and the opportunity to lead the data engineering department when it’s founded, so I would be one of the founders of that department, but in the meantime I was gonna be part of the automation and innovation department. I accepted the counter offer and stayed.

Fast forward 1 and 1/2 years and now I’m part of the Data and Analytics team, actually there’s not really a department but the company really wants to start offering Data Consulting services to their costumers so we have one big project that we are working on right now (Databricks, Data Factory, Power BI) and one small one (Azure functions, Docker, Azure VM and Power BI), neither of those work with more than GB’s of data.

The thing is that I’m the only data engineer. There’s one data architect who is really great at data governance, two data analyst and many project managers. A lot of projects are coming our way with many interesting technologies like machine learning, AI, etc. and I’m already burn out because of all the hours that I have to put in only in those projects where I’m the sole builder of the Infra, pipelines, etc. it’s really not a problem because I really like it, but I’m starting to worry because of what’s coming.

So today I have two offers from two different companies to join as a data engineer. None of them are as big as my current company but aren’t small either, but work with bigger data.

I have to make a decision whether to stay here and be one of the pioneers of the data department, be considered for everything (learning and projects) and work on interesting projects because I’m the only one that is available with the skills to do those, or work for any of the other two companies that have a very mature data team that also work with cool projects and where I could also learn a lot from more experience data engineers/architects and work with bigger data.

What path would you take?

Edit: My current company is partner of the year with Google, AWS and Azure, so training and certs are covered.

Edit2: For the people asking what courses did I take for Python, I took Jose portillas Python course and Automating the boring stuff. That being said, I learned more by doing. I made two projects that earned me the offers from the other companies.",2023-09-04 13:53:01
143t8vc,Dashboards best practices - how much transformation should PowerBI or Tableau be doing?,"Our dashboards at work basically have powerbi doing all the merging and aggregation.

I am not positive if it's done by query or after pointed to the sql tables.

Is that normal, or best practices? I would have built a view and pointed to the view.",2023-06-07 23:54:49
1415jdb,How do I get better in SQL,"Apologize if this question has been asked countless times. As a newly hired DE I was advised by my Supervisor to improve my SQL knowledge from a 5 to 8/10 and be more versed in BigQuery and GCP

My previous experience as a Software Developer didn't provide difficult database tasks, so my SQL skills are limited to the basic CRUD and creation of schema and tables. 

Going back to my question, how do I proceed from here? I checked the links in the faq but I still feel lost.",2023-06-05 07:03:23
13492h6,"This reeks of ""we need free work.""","I already accepted a job offer, but emails from places I've applied before are still coming (and will for a while, I imagine). So fortunately I don't feel compelled to do their take-home assignment. There are some who will say that **all** take-home assignments are a scam. I'm not sure about that, but does this raise any red flags for anyone else?

[https://github.com/RiskThinking/work-samples/blob/main/Data-Engineer.md](https://github.com/RiskThinking/work-samples/blob/main/Data-Engineer.md)

The company only has 24 employees and all the data people are data scientists. There are no data engineers, data architects, or data governance. The ""homework"" has some extremely specific parameters, and all of your code (not screenshots) needs to be posted somewhere they can be easily swiped. Or am I being too paranoid?",2023-05-01 01:31:01
133qlow,Datasets for data engineering projects,"Hi Reddit,

I am new here. Recently I joined a course from udemy to learn more about databricks. Course instructor was using the F1 race dataset from the Ergast website. The website had a well documented table and all the database details. I have completed the course now and I am looking for some random dataset to test my skills. But it is hard to find dataset like that. 

In search of dataset I am writing this post to get views and comments from community who have been in this field for long time, 
Have you guys come across any dataset like that in your career?",2023-04-30 13:27:16
zf68rv,Expectations Vs Reality,"For current DEs, what is one thing you were expecting going into the DE field vs what actually occurred?

Both the good and the bad",2022-12-07 16:49:38
ykpth2,What's the cheapest online Masters Data Engineering degree?,"Hi everyone!

&#x200B;

I currently work as sort-of a data engineer in training and was hoping to solidify the foundations of my knowledge. I like what I do for a job a lot, but I don't come from a technical background (non-CS/non-STEM major) and just sort of got the position with loose knowledge of data, Python, and SQL.

My biggest frustration right now is how wonky the basis of my knowledge is and wanted to enroll in a program to both learn more about data and become a stronger programmer (all while working full-time).

Do y'all have any recommendations for affordable Masters in Data Engineering programs? I'm interested in Georgia Tech's OMSCS but wanted to have a bit of focus in the Data Engineering aspect of CS. Does anyone happen to know if there's enough wiggle room in the program to build that up? Thank you so much for your time!",2022-11-03 02:28:17
y979cd,"[Azure] Company is telling me not to use open-source tools because they are ""unreliable""","I've been an Azure consultant for several years in a mid-sized consulting firm. When we do solutioning and prototyping for our clients we typically exclusively rely on Azure managed services for data and analytics - Synapse, Data Factory, Azure SQL, Databricks, ADLS, etc.

And while that's all well and good, I feel like we are really constrained with what types of tools and technology we are allowed to work with. I suggested dbt recently to a client who wanted a way to validate their data automatically as part of their ETL pipeline, and my manager came back to me and slapped me on the wrist and said I shouldn't be suggesting open source tools to a client, because if anything goes wrong it becomes a huge liability and they cannot get support from Microsoft, so we would be on the hook for support. (She had never heard of dbt and had no idea what it does). Instead we should always use the official off the shelf Microsoft-supported tools with SLAs, etc.

As I learn more about the DE ecosystem of tools I am realizing that there's an entire universe of stuff out there that none of my peers at my company bother to learn or touch, and I feel like it's slowing me down and constraining me. Trino, Airbyte, Dagster, Airflow, Iceberg, Docker, Kubernetes, Terraform. If I mention any of these things to my peers or manager they look at me like I have two heads, and if I suggest using them it gets no traction because nobody wants to learn how to use a command line. It seems like the culture at my shop is just settling with what's easy and what makes the Microsoft partners happy, not with the best tool for the job.

Anyone else experience anything like this?",2022-10-20 19:19:53
y5bhzg,When To Use Object Oriented Programming in DE,Does anyone have a good example of when to use object oriented programming in data engineering and an example of when it was used but shouldn't have been?,2022-10-16 08:28:49
xfwogn,Useful libraries for data engineering in various programming languages,"Hi guys , So I am interested in knowing some really good libraries in any programming languages that you guys have used for data engineering/ devops / automation/ data quality/ data reconciliation etc. Like pandas => python => data analysis, .... etc",2022-09-16 16:56:45
x32ald,SCARED!! after first interaction with my manager.,"Recently landed a job offer as an analyst. Manager called to convey his expectations from me about this role

I'll be working on azure data lake, brick, factory and devops. My tasks are to address the errors which could originate from any .py files, code snippets, functions or data types and rectify those errors. I'll use pyspark.

My background : It's my first job. I have a background in statistics and economics and worked on few webscraping and forecasting projects and no background in computer science or data engineering 

Please tell me what I've signed up for?
What skills or book I should start grinding.",2022-09-01 08:39:07
v1xjub,"Sharing my data engineering project/idea, using jinja, docker, k8s, react UI.","Repo here [https://github.com/corpulent/fragments](https://github.com/corpulent/fragments)

Hi all, sharing a project I recently completed which was inspired by contract work I was doing in the past and dbt. The project itself is also an idea I have been exploring for a while, and actually building it is my best attempt at communicating it to others.

The idea is simple. Using jinja, I define a json structure that I want to expose over HTTP for GET or POST requests. Within these jinja models I can add connectors (databases, third-party apps, sheets) to fetch the data, and add custom functions/filters to transform it. Once the model is tested or deployed, the data is fetched, processed and saved to a cache. This cache is served over an HTTP endpoint which the project provides.

This is not a reverse-ETL. I slapped this project together starting May 1st, and pushed today, so the code is kind of loose. The goal is to contribute to the community, learn, network with others, and hopefully get some interesting work along the way.

Let me know what you all think, thanks.

https://preview.redd.it/k047pg0wzu291.png?width=3086&format=png&auto=webp&s=f2f6215f8751500c8d0946be07bdc47014772798",2022-05-31 19:02:43
uqh9k2,Responsibilities of a Data Engineering Manager,"Based on your experience as data engineering manager or based on your observation as a data engineer

What is the main responsibilities of data engineering manager and how technically deep should the manager be?

I assume that data engineering managers should be more technically involved than e.g. software engineering manager, is this valid assumption?",2022-05-15 22:29:40
tonk9l,Dbt Vs python scripts,"Hi all,
I hear great things about dbt, but from a first look at it, it seems a key use case is using SQL to automate lots of data transformations. Couldn't you do this with python scripts? Is dbt for SQL analysts who don't use languages like python?
Surely I've missed something!",2022-03-26 12:33:36
tebiol,Improving DataOps with Dynamic Indexing in Data Lake,N/A,2022-03-15 00:00:13
rfbuu8,Data Engineering Jargon - Part 3,"Hi - this is the next 10.

1-10 is [here](https://www.reddit.com/r/dataengineering/comments/rdw3b3/data_engineering_jargon/?utm_source=share&utm_medium=web2x&context=3)

11-20 is [here](https://www.reddit.com/r/dataengineering/comments/rem26j/data_engineering_jargon_part_2/)

21-30 is below

31-40 is [here](https://www.reddit.com/r/dataengineering/comments/rg5vr0/data_engineering_jargon_part_4/)

**21. Batch Processing**

An automated way of processing millions of data transactions at the same time. This is generally carried out overnight with the help of “batch jobs”.

*Loading all the customer’s data that bought a particular item on the day.*

**22. T-SQL**

SQL is a Structured Query Language or, simply put, a language used to manage databases. T-SQL is Transact-SQL which is a proprietary Microsoft extension of the SQL language.

*T-SQL can be used MS SQL Server or Azure SQL Database to write a statement as follows “SELECT customer\_name from tbl\_customer\_information where customer\_city = “London”. This provides the result of all the customer names where customers are based in London.*

**23. NoSQL**

Although SQL has been around for decades, NoSQL (not only SQL) is a concept designed for non-relational databases, particularly to store unstructured data like documents.

*Storing an Outlook email file in XML with key-value pair on a MongoDB document database.*

**24. BTEQ**

Batch Teradata Query (like SQL) is simply a utility and query tool for Teradata, which is a relational database system

*Creating a BTEQ script to load data from a flat file.*

**25. Cloud**

Delivery of computing services such as servers, networking, analytics etc., over the internet instead of using a dedicated data centre for an organisation.

*Storing data on Microsoft’s Azure Cloud service instead of on an on-premise solution.*

**26.** **Data Architecture**

The discipline of managing the people, processes and technologies relating to data; includes data strategy, data capture processes and technical patterns to derive insight from the data.

*A Data Architect creates a framework for an enterprise to manage its data flow end to end.*

**27. Data Visualisation**

A practice for visualising large amounts of data to derive key insights to drive business decisions.

*An executive dashboard that clearly outlines the sales performance of a certain team.*

**28. Data Centres**

A dedicated space (nowadays millions of sqft of space) which houses servers and systems for the organisation’s critical applications

*Microsoft Data Centre to host all the company’s critical applications.*

**29. Data Integration**

Usually, the hardest part of the project, where multiple sources of data are integrated into a singular application/data warehouse.

*Integrating finance and customer relationship systems integrating into an MS SQL server database.*

**30. Data Migration**

The practice of migrating the data from source to destination

*Migrating data from MS SQL server database to an Amazon Relational Database service*

1-10 is [here](https://www.reddit.com/r/dataengineering/comments/rdw3b3/data_engineering_jargon/?utm_source=share&utm_medium=web2x&context=3)

11-20 is [here](https://www.reddit.com/r/dataengineering/comments/rem26j/data_engineering_jargon_part_2/)

31-40 is [here](https://www.reddit.com/r/dataengineering/comments/rg5vr0/data_engineering_jargon_part_4/)",2021-12-13 09:24:44
rf9lav,Data Engineer Imposter syndrome,"Hello everyone! I'm currently having 3.5 years of experience in IT and I'm into my second company(moved out after 3 years with the first one).
So far I believed that I was a data engineer creating data pipelines to ingest, transform and curate data but off late I'm having an imposter syndrome.

At both my companies I am coming to realise that I never created any data pipelines from scratch nor wrote any code for this. They used to have their respective framework/jobs to ingest and curate the data. All I did was to add a custom Json config with the details of the source I'm ingesting into the repo and the framework used to take care of the rest by picking up data specified in the config and transforming it according to the parameters specified in the config.i have no clue on the logic/spark code doing all the heavy lifting in the background. Basically all my infrastructure is already in place and I'm just adding a few lines in a separate config for my source.

Is such a profile common in the data engineering domain or is my imposter syndrome theory actually correct and would negatively impact my career over the long run.

Any advice and discussion greatly appreciated.

TLDR: both companies I have worked in have infrastructure already in place for ingestion, transformation and curation. As I'm just adding custom Json configs with details around my source I believe I'm having imposter syndrome since I'm not writing any code to develop data pipelines from scratch.",2021-12-13 06:53:01
o744oi,Fivetan vs. Stitch vs. Singer vs. Airbyte vs. Meltano,"Edit: I've misspelled Fivetran in the title which I can't seem to change. Apologies in advance

---

We're building the data infrastructure from the ground up at my current company and we're looking to ingest third party data (i.e. Google Analytics, Hubspot, Zendesk) into our warehouse. The following five solutions are our ""most liked"" solutions so far:

- [Fivetran](https://fivetran.com)
- [Stitch](https://www.stitchdata.com)
- [Singer](https://www.singer.io)
- [Airbyte](https://airbyte.io)
- [Meltano](https://meltano.com)

**How do these services compare? If you have experience with any, please comment below I am interested to hear more about these services from an engineer's perspective.**

To give a bit of background about my company. The data team is me (data engineer), a data analyst, and the VP of Engineering who, until I joined the company, was leading the data engineering work. They don't take such a hands on approach any more.

I quite like the idea of rolling with a self-hosted OSS solution like Singer / Meltano / Airbyte with Airbyte looking the strongest to me so far. I can get it running locally quickly and the [public connector roadmap](https://github.com/airbytehq/airbyte/projects/3) shows promising signs of being a top competitor in this space.

Fivetran also looks like a strong competitor. As we're a small team, having the data ingestion handled for us would alleviate our workload, allowing us to work on ""more exciting things"". They do also boast an [impressive list of connectors](https://fivetran.com/connectors). However, I've heard that Fivetran can come with a hefty bill and removes control from our end of things.

So, I'm keen to hear more about what people think about the aforementioned services. If there are others I've missed, please comment them below!",2021-06-24 16:14:14
o46wci,Google's Data Engineering Certificate - Is It Worth It?,"&#x200B;

Over the last month I went over the Google Cloud Data Engineering preparation course to see if it was worth getting.

After going through it, I felt like course does cover a lot of great info specifically for GCP. However, I also felt like the course already assumed I work heavily in data(which was confirmed by their recommendation that people taking the course should already have 1 year of experience in python, SQL or data modeling.

If you're interested in getting a more in depth view, here are a few places you can jump into my video.

[Intro](https://www.youtube.com/watch?v=ZzUjt0TfHxc&t=0s) \- Honestly, I would just love to know your thoughts on the intro personally. Since the first 30 seconds of a video make a huge difference.

[Course Overview Google Cloud Data Engineer](https://www.youtube.com/watch?v=ZzUjt0TfHxc&t=202s) \- So There are 6 courses in the preparation program set up by coursera. Personally, I think only 1-4 are really relevant to data engineers. In particular, course 2 focuses on data warehouses and data lakes, which is only really focused on modern designs..which is a bit of a miss since plenty of companies still use non-cloud data warehouses but I get it. GCP wants to sell GCP.

Courses 3-4 focus on streaming and batch ETLs. Another key concept for any data engineer. So another great set of courses worth watching.

5 oddly focuses on AI and ML deployment. Which, I often find data engineers want to do, but rarely get to. So I don't want people to be disappointed thinking they are going to get to do AI and ML work as a DE.

6 Is a prep course for the certificate. So it's not relevant directly to data engineering. Of course, if you actually want to pass the program, then its great.

[Gaps And What I Liked About GCPs Certificate Program](https://www.youtube.com/watch?v=ZzUjt0TfHxc&t=597s) \- If you're expecting to go from 0-100 in terms of data engineering skills. This course won't provide you that. The course itself even recommends you have 1 year of some form of data modeling, python or SQL experience. Essentially, I think they already expect that you are either an analyst or a data engineer that is looking to add an extra layer into your skill set.

Also, this course is only focused on GCP so you're not going to learn any other tools other that Bigquery, Cloud Dataproc, Cloud Pub/Sub, etc. So if you wanted to learn Airflow or something similar, this is not the course for you.

Finally, in terms of ""gaps"" it's less of a gap and more a point of confusion. There were a few courses that focused on AI and ML deployments. Now, some data engineers will help deploy ML models, but I don't find it common, so this felt out of place.

**On the other hand, the course is great if you're already a data engineer and you want to learn how to apply your skills on GCP.**

So to answer the question. I did find value in the prep program, and it filled in a few gaps that I had in my GCP knowledge. But if you're looking to this program to make you a data engineer, you'll need to round out your skills with several other courses focused on python, classic data modeling, and classic ETLs.",2021-06-20 14:28:15
na0wmk,Using Postgres as a Data Warehouse,"I published a blog post yesterday about how to [configure Postgres as a data warehouse](https://www.narrator.ai/blog/using-postgresql-as-a-data-warehouse/).

We work with various data warehouses, so this came from the work we did to optimize a bit for Postgres. 

Here's the top-level advice

* don't use the same server as your production system
* upgrade to pg 12+ (or avoid common table expressions in your queries)
* go easy on indexes – less is more
* consider partitioning long tables
* ensure you're not I/O-bound
* vacuum analyze after bulk insertion
* explore parallel queries
* increase statistics sampling
* use fewer columns on frequently-queried tables
* at scale consider a dedicated warehouse

Anything else I'm missing? Would love thoughts people have. And of course, this assumes you're already using PG as a warehouse -- I'm not taking a position on whether or not you should use it vs dedicated cloud-based ones like Snowflake :).",2021-05-11 16:24:15
m5yufj,data engineering interview with reddit!,"Hi! I have a data engineering interview with reddit, and I've only had 2-3 DE interviews. The technical interview will be half case and half python/sql problems. What can I expect from a DE interview from the front page of the internet?",2021-03-16 02:26:42
i2922r,Wrote a book review of Designing Data-Intensive Applications. It was awesome!,N/A,2020-08-02 08:55:46
c17wry,The data engineering cookbook,"A beginner friendly resource to know about data engineering.
https://github.com/andkret/Cookbook",2019-06-16 08:49:40
1bqfb2c,What data architecture is most widely used right now,"I'm trying to see past the noise and what people are using. So would you say that you are using a data lakehouse, data lake + data warehouse, on-prem, or some other type of architecture for your database?",2024-03-29 03:29:53
1b912nc,From dbt to SQLMesh,"I'm excited to share my latest blog post, ""From dbt to SQLMesh"" 

Check it out here  [https://www.harness.io/blog/from-dbt-to-sqlmesh](https://www.harness.io/blog/from-dbt-to-sqlmesh)

We have been running sqlmesh in production in some way shape or form for a long time now, since the earliest versions of the tool. This post is focused on high level migration approach (which is the reason we were able to adopt quickly and drive innovation) & touches on the learnings and key takeaways too. Major points include simplification, no jinja (sqlglot macros instead creating valid and structurally correct sql syntax vs string interpolation free-for-all), better state management and simplified CICD+dev experience. Has been a rewarding experience and paid off manyfold in the quality, reliability, and maintainability of our data platform. And we are poised to scale extremely effectively. I hope this is useful for any folks looking for information on the topic or are just curious.",2024-03-07 17:59:07
1b4v030,Are Data Engineers underpaid?,"This though suddenly came up when I saw job postings for experienced individuals paying $25 an hour. This is not the norm, but still scary that companies even have such low expectations. Compared to most jobs out there, although the type of work may be fulfilling, And more often employers slap the ""Analyst"" title and get these people for a bargain. I feel that people in this industry are often underpaid compared to the work/skills they bring to the table. What do you all think?",2024-03-02 18:27:32
1ax4agw,Is there an industry that stands out as highly complex in terms of 'doing data engineering'?,"I'm not particularly experience in this so I have a feeling 'doing data engineering' is quite an annoying way to phrase it, but i'm too inexperienced to word it any other way. But is there an industry you'd stay away from in terms of data engineering? I have manufacturing/supply chain as highly complex in my head.

Is there a particular cloud platform that makes it easier? Would on-premise make it harder?",2024-02-22 11:32:24
18who2l,Ways to keep your SQL sharp with minimal effort?,"Hi, I'm at a job that does doesn't involve working daily with SQL as the project has matured and we're not making many changes to the business logic anymore. So I'm thinking that I want to keep working on SQL problems somewhere else so that I'm interview ready. 

Where would you recommend I can go let's say on the weekends and do some mini challenges, preferable problems and datasets that are closer to those in the real world.",2024-01-02 05:29:55
186fvwk,How are DEs using Docker containers for their ETLs?,Curious about containerization and ETLs,2023-11-29 03:38:27
177yh79,What's an easy/cheap data engineering pipeline+ warehouse stack?,"Hi friends, a total data engineer n00b here. So i got something im trying to sort out in my head for my own personal project, which I want to scrape and merge data from my social media profiles.

I have a Postgres DB that im hosting my application datas. Pretty typical software dev stuff. But I do need to essentially enrich the said data with other sources (from other PG dbs + social media + web scrapped things)  to make things more useful.

I was thinking to dump things into my applications PG, then use SQL to do data processing to clean up and merge data w/ my application data, but i felt this is not the right way to do it and that this should be done in a data warehouse. 

At my previous companies we spent $$$ on redshift + teams of data engineers to do this. I personally can't spend that so looking for a cheap/free solution to do all this, so I've been running airbyte locally to do the syncing. 

What should I use as a cheap/free data engineering pipeline + data warehouse? Should i stick with throwing things at PG? 

I guess at this point, I'm really looking for an easy to use + cheap solution for all this, running this locally via docker takes a lot of mental overhead...

&#x200B;

Thanks!

&#x200B;",2023-10-14 20:37:10
16oocc3,"Good Companies that use: Python, AWS, Snowflake?","Have 4 YOE. Current company is great but I’ve plateau’d. Is there a list of employers in the states that is heavily focused on Python, Docker, AWS Snowflake, Airflow? I’ve had experience with many technologies and languages. Over the years I learned that I LOVE writing Python or anything Python related. Any way I can find employers that seek stack heavily focused on Python, AWS, Snowflake, Airflow, Docker? Preferably in the U.S?
Which data engineering titles would help me find what I’m looking for? Or how can I find such roles/employers?",2023-09-21 19:08:17
16l1drf,What do you do after you make it to management?,"Curious on everyone's thoughts here. You're now officially responsible for the entire team, not just the code you're deploying. How do things change? You aren't primarily writing code/designing solutions anymore. Where is your focus now? How do you 'improve' and measure success?",2023-09-17 13:58:45
164hrx5,"Anyone considers themselves ""reformed data scientists""?","I started my career a few years ago, I started as a business intelligence intern, and my plan was to become a data scientist eventually.

However with a few months on the job I changed my mind and decided to become a data engineer instead.

First of all, I liked coding, building infrastructure and pipelines way more than I liked data analysis and modeling. But also, I realized that very few companies are capable to do data science.

To hire and deploy a data scientist effectivelly, all the planets must allign:

1. Most business simply don't need it. Most companies are good with just being able to do analytics and maybe some simple ML models.
2. Most companies don't have the data infrastructure to do that. For every company that have their data infrastucture built and ready to do ML, there are other 20 companies that don't know the term ""data warehouse""
3. Even if the company can benefit and is ready for DS initiatives, upper management needs to be bought into it. The thing is, DS initatives may just fail. Very often management doesn't want to invest resources into that type of thing.

So, I made the change to Data Engineering and didn't regret it one day since.

Then I picked up ""Fundamentals of Data Engineering"" and, in the introduction, the authors refer to themselves as ""Reformed Data Scientists"", and how they started their carreers as Data Scientists but migrated to Data Engineering due to reasons very similar to the ones I had already observed.

This made me curious about how common is the phenomenom of migrating from DS to DE is.

As a data engineer, what is your experience about this?  Did you start your carreer in DE, or migrated from DS or any other field? Have you witnessed this happenning to someone else?

EDIT: Grammar and typos",2023-08-29 12:56:53
15savgk,What's hot in the orchestration landscape right now?,"We have a few services (ETL, metrics, getting data to external services like CRMs, internal tools built with retool, etc) and everything runs on cronjobs. We've gotten somewhat stable over the last few months, wrapped http APIs around everything, and now want an orchestration service. 

I've worked with airflow before, and would commit acts of violence if required to do it again. From what I researched, [dagster](https://dagster.io/) is my favorite option for now. We have a good amount of engineering manpower and can go for unconventional alternatives too, if the team sees value in it - say, a very elegant API.",2023-08-16 00:59:44
13mk7e6,Question for senior engineers from a newbie,"This is a generic question for senior data engineers - Im very new to data engineering

What are some of the most importtant considerations when designing a data pipeline? What do you guys think of , from a design perspective before you create the pipeline. What are some of the performance issues you anticipate, and how do you actually gauge the performance of the pipeline other than -whether its getting its job done or how long it takes. Edit - My current stack is Python codebase. Use Pyspark. We use CLoud composer, Airflow, Dataproc,Bigquery,Cloud spanner, cloud mysql on gcp. Basically GCP tech stack.",2023-05-20 06:36:35
11qw5jt,How do you debug your pipelines?,"Junior data engineer here, currently i'm debugging my data pipelines by adding logging statements in between the code like a caveman. I'm sure there is a better way to debug pipelines, but I'm not sure debugging works in data engineering like it does for the rest of software development? Am i wrong? Is there a better way?",2023-03-14 04:08:27
zhm66m,A Conversation with chatGPT on Data Engineering,N/A,2022-12-10 07:35:50
171pbro,Why do data engineers use such short and ambiguous variable/alias names in SQL?,"Is it simply because it's easier to write:
s.column
than it is to write:
someDescriptiveTableAlias.column

I mean haven't you ever heard of multiline editing... or tab...

Yesterday I spent like 20 minutes trying to figure out what ""wom"" meant. Turns out it stood for weekOfMonth... why not just use weekOfMonth??",2023-10-06 22:10:03
16be6f8,Data Engineering Projects are Hard to Showcase than Web Development Projects,"When a web dev wants to showcase their project, they can easily build one and deploy it in the cloud and a prospect employer can easily appreciate it by diving into the web application itself. While in data engineering, all I see to showcase a project is to create architectural diagrams, enrich README file (yes, in github), and creating online dashboards (a means to an end to highlight the “data” part but how about the engineering part?). I mean, compared to web dev projects, DE projects will need the user/employer’s time and effort to get a feel of the project.

Any better way we can do to showcase our portfolio/projects?",2023-09-06 08:00:22
165p4sh,What do you think the near future of data engineering is?,"Current trends leading to a culmination in...?

What techs are on their way out?

What techs are on their way in?

Practices?

Attitudes?",2023-08-30 20:05:17
1438mkz,Airflow- am i missing something? Why does it need to be run on a large cluster with lots of workers?,"Our use cases for airflow will almost entirely involve it triggering other services that do the heavy lifting and have their own compute. I am really struggling to understand why I would need a full on cluster with separate workers, scheduler and webserver. Could I get away with deploying it on say a single ec2 instance or am I missing something obvious?",2023-06-07 09:40:58
13c0wi7,Working as a consultant,"Have any of you worked as a consultant? I really wonder if it would be nice. How long are your projects generally? Is it interesting to see many companies or are most tech stacks used not relevant? Are you actually doing data engineering work or does it more feel like you’re an analyst or architect? I feel like it’s interesting, but I also feel many companies are stuck with obsolete tools and it might feel like a waste of my time. I’m in Europe btw.",2023-05-08 18:47:13
1abfe9x,Whose job is it to add primary keys to tables?,"I’m a ETL developer. I have about 400/2500 tables that don’t have primary keys. I asked my manager why they don’t have primary keys and he said he doesn’t know. I asked if there was any documentation and there isn’t. I asked if I had to make them up for each table and he said yeah. I’m supposed to make them up? How? He said yeah it’s really annoying but it’s something we have to do…I need primary keys in the table for my code to work that migrates the data from source to target. 

Aren’t the DBA or the person who created the table supposed to do it? I’m genuinely curious because I don’t think I can sit there and go through 400 tables and make up a primary key for each table. Like I can’t mentally handle sitting there and going through each table like that. I’m considering finding a different job over this. Has anyone else done this before? It’s 400 tables for christs sake…",2024-01-26 10:33:51
184bse1,What Airflow hacks you wish you knew sooner?,Anything from CLI to deployment. Curious to know your answers 🤓,2023-11-26 14:23:47
160j6fd,"What are the hardest, most difficult tasks that Data Engineers do?","I stumbled upon this conversation earlier in the day on X and I will like to ask practising Data Engineers what the hardest and/or most difficult tasks for Data Engineers are with more emphasis on Mid level Data Engineering. 

I am currently looking to transition from Entry level to mid level and I am hoping to start ""looking"" the part so I can ""get"" the part. This is why I'm looking for problem out of my league that I can try to solve to build my muscles and grow. 

Thank you and I look forward to reading your responses.

https://preview.redd.it/blpawdlpd5kb1.png?width=1194&format=png&auto=webp&s=ce05044c42d49761bace238f0725ef5140cf763b",2023-08-25 00:15:02
15p4guu,Does anyone feel like learning business rules is useless since they are not transferrable?,"I'm a software engineer who's facing this dilemma and wanted to ask here as well. Am I the only one who thinks that?

The point I'm trying to make is not to discard them altogether, but to keep them at a bare minimum while expanding the arsenal of skills which are more transferrable in other domains - so, basically anything else.",2023-08-12 13:20:47
159g777,Do you all care about the type of data you're working with?,"I'm considering a job in insurance. It has all the tools and skills I want to learn, master, but I wonder what working in insurance will mean for my day-to-day. I come from a public sector background, so I'm used to feeling ""good"" about my work. Insurance isn't ""bad"" but it often has that reputation. ",2023-07-25 18:11:37
14b7yl2,"Happy Father's Day, data engineers!",N/A,2023-06-16 21:01:06
145kaqy,Introducing LineageX - The Python library for your lineage needs,"Hello everyone, I am a student working in the area of data lineage and data provenance. I have created this Python library called LineageX, which it aims to generate the column-level lineage information for the inputted SQLs. This tool can create an interactive graph on a webpage to explore the column level lineage, it works with or without a database connection(Currently only supports Postgres for connection, other connection types or dialects are under development). It is also implemented as a dbt package using the same core (also only Postgres connection, and an active connection is a must).

If you are interested, you are welcome to try it out and any feedback is much appreciated!

Github:[https://github.com/sfu-db/lineagex](https://github.com/sfu-db/lineagex), dbt package: [https://github.com/sfu-db/dbt-lineagex](https://github.com/sfu-db/dbt-lineagex)

Pypi: [https://pypi.org/project/lineagex/](https://pypi.org/project/lineagex/)

Blog: [https://medium.com/@shz1/lineagex-the-python-library-for-your-lineage-needs-d262b03b06e3](https://medium.com/@shz1/lineagex-the-python-library-for-your-lineage-needs-d262b03b06e3)

Thank you very much in advance!",2023-06-09 23:29:03
137cym8,Which supplementary tools are you using alongside dbt?,"Hi All,

I'm interested in learning about the supplementary tools you use in conjunction with dbt and how they enhance your workflows. As we begin with dbt core, I'm curious to explore the complementary tools that work well alongside dbt. Please share your experiences and insights regarding the tools you find beneficial when using dbt.

Thanks,

Mc",2023-05-04 07:11:53
11vim1y,Side hustle ideas,I currently live in a country where the single salary is not enough for me to buy a house in the next 4 years. I am looking for new income sources. Are there any good ideas for side hustles as a data engineer?,2023-03-19 11:13:50
18v0dn0,Do you really need to know coding algorithms in your job as a data engineer?,"Data engineer is a type of software engineer and everywhere suggests that software engineers should  know data structures and algorithms like insertion sort, merge sort,  binary search, etc.

I feel like I'm not using any of them at all (or maybe I'm using some but don't aware that I am), but I still able to write tons of Python programs in my ETL development and they work just fine.

As my background is not actually from com sci, I feel it will take me so much time and effort to learn and not worthwhile comparing learning other stuffs and concepts in data engineering like data quality, data modeling, data warehousing, SQL mastery, unit testing, distributed data processing frameworks, streaming, machine learning, MLOps, IaC, DevOps, etc. 

&#x200B;

Happy New Year :)",2023-12-31 05:26:52
16fmgwu,Are data engineers in charge of creating database infrastructure?,Long story short I’m being put in charge of building an entire database and our entire backend infrastructure. Is this within the realm of data engineering?,2023-09-11 05:02:27
15vy5wa,What skills should I pick up to make more money at my next job?,"Hey all, I have a ""Senior Analyst"" title at my company, but by day I masquerade as the sole analytics/data engineer for my company. I'm a one man data team managing the entire pipeline for about a 200-person company. Our tech stack for analytics consists of Fivetran and Stitch, dbt, Snowflake, and Looker. 

I make good money (145k) and the job is chill, but it's all too easy, and I'm getting bored. I've been there 2 years and there are no challenges. Learning nothing new, really. There's a Stitch or FT connector for everything we integrate. It's just SQL modeling day-in, day-out. I want to speed up my trajectory towards r/financialindependence by getting some new skills and a higher paying gig. In your experience and opinion, what skills could I pick up from here to get biggest bump in pay when moving companies? How could I right now change up my company's tech stack and learn new tools and languages? For context, I've been in finance mostly and have MBA, but I'm currently working for a small tech startup",2023-08-20 01:48:21
14zn3b0,"I’m the de facto data engineer at a small startup (50 employees), but I don’t feel like I could be competitive for a “real” data engineering job elsewhere.","Was hired as a many-hats data analyst.

For the last few years, I’ve:
-Worked with SQL/Spark/Python
-Built/managed countless ELTs in Databricks
-Designed architecture and built tables for BI and ML purposes
-Managed AWS Glue/S3 pipeline that replicates our prod database (eventually into Databricks)
-Built pipelines that read/write to multiple tools (eg Salesforce, Marketo, Google Sheets) via APIs

But the areas I’m lacking are:
-Terabytes of data. At this point Ive only worked with gbs.
-Best practices. My team was laid off so I no longer get feedback when I could be doing stuff better/more optimally. Not to mention data governance and all that jazz. 

But yeah, I’m wondering how much of my concern is legitimate and how much is imposter syndrome? Current title isn’t an issue—I could change that if I want. But when were you actually “ready” and competitive enough to go for data engineer gigs?",2023-07-14 17:47:05
14nshm9,Feeling drained and discouraged at my job. Anyone else?,"Feel like my current job is slowly draining all my energy and enthusiasm. 

The good parts: decent money, lots of trust and responsibility, good colleagues. 

The bad parts: things break all the time. Debugging is slow, often involves tedious and manual processes. Drowning in bugs and tech debt. Lots of things depend on us, and if a pipeline is on fire, you don't get to go home. Stakeholders come to us with an endless stream of requests, turn quickly impatient. Data producers need us all the time, we have to help them but we also have to educate them and ask them not to screw up the data. Add all the meetings and plannings and SCRUM stuff on top. Then we are expected to build and innovate, but I feel that less than 20% of my time goes into it. Always being dragged down by everything else, spending most of my time on bullshit. In this situation it is becoming very hard to keep up with the industry and experiment with new things and be creative.

Basically the [data engineer burnout report](https://www.businesswire.com/news/home/20211019005858/en/Data-Engineers-Are-Burned-Out-and-Calling-for-DataOps) from Data Kitchen is a reality for me, although I'm not burned out (yet). 

I feel like I don't have enough reference points to judge my situation. My previous job was much better, almost a holiday in comparison, but then again it was boring and nobody cared about data.

How is your experience? How much of this is inherent to the profession vs. my company and situation? Do consultants have it better, since their responsibilities are bounded and they are mostly called to do single projects? Or should I try to get out of DE (which I actually like) and move towards SWE? Is the grass actually greener somewhere?

&#x200B;

&#x200B;",2023-07-01 11:12:49
14bucxf,DE really more Meetings than SWE?,"I recently started as a DE.  I am shocked how less the job involves programming and software development. 

For me DE is SWE focused on data. At lot of time should be spent with databases and coding. Of course some time should also be meetings. 

In reality it seems like my current job is sitting in meetings 80% of the time, waiting for ad hoc work and 20% coding.",2023-06-17 15:48:59
140y77y,"In DE, is there a language that is actually worth learning besides Python and SQL?","Pretty much title.

Python and SQL are must-know-to-survive languages. Besides that, everything else's value seems dubious at best.

The next three on the list of importance would be Scala, Java and R. But like I said, the value of learning any of those three is very questionable.

C/C++ could be considered for writing python extension, and Go and Rust for their future potential.

What do you thing about this whole situation?",2023-06-05 01:47:28
124wcjb,"My 3rd data project, with Airflow, Docker, Postgres, and Looker Studio","I've just completed my 3rd data project to help me understand how to work with Airflow and running services in Docker.

# Links

* [GitHub Repository](https://github.com/digitalghost-dev/global-data-pipeline)
* [Looker Studio Visualization](https://lookerstudio.google.com/reporting/3710d6bb-25b2-4d64-b6e8-2889bc57c74b) \- not a great experience on mobile, Air Quality page doesn't seem to load.
* [Documentation](https://github.com/digitalghost-dev/global-data-pipeline/wiki/Global-Data-Pipeline-Documentation) \- tried my best with this, will need to run through it again and proof read.
* [Discord Server Invite](https://discord.gg/j2HEfpebuH) \- feel free to join to see the bot in action. There is only one channel and it's locked down so not much do in here but thought I would add it in case someone was curious. The bot will query the database and look for the highest current\_temp and will send a message with the city name and the temperature in celsius.

# Overview

* A `docker-compose.yml` file runs Airflow, Postgres, and Redis in Docker containers.
* Python scripts reach out to different data sources to extract, transform and load the data into a Postgres database, orchestrated through Airflow on various schedules.
* Using Airflow operators, data is moved from Postgres to Google Cloud Storage then to BigQuery where the data is visualized with Looker Studio.
* A Discord Airflow operator is used to send a daily message to a server with current weather stats.

# Data Sources

This project uses two APIs and web scrapes some tables from Wikipedia. All the city data derives from choosing the 50 most populated cities in the world according to [MacroTrends](https://www.macrotrends.net/cities/largest-cities-by-population).

* City Weather - (updated hourly) with [Weatherstack](https://weatherstack.com) API - costs $10 a month for 50,000 calls.
   * Current temperature, humidity, precipitation, wind speed
* City Air Quality - (updated hourly) with [OpenWeatherMap](https://openweathermap.org) API
   * CO, NO2, O2, SO2, PM2.5, PM10
* City population
* Country statistics
   * Fertility rates, homicide rates, Human Development Index, unemployments rates

[Flowchart](https://preview.redd.it/zz181kpt6iqa1.png?width=2112&format=png&auto=webp&s=5b93a45bf5848c3dcf8f8cacf6fa63b7579606d0)

# Notes

Setting up Airflow was pretty painless with the predefined `docker-compose.yml` file found [here](https://airflow.apache.org/docs/apache-airflow/stable/howto/docker-compose/index.html). I did have to modify the original file a bit to allow containers to talk to each other on my host machine.

Speaking of host machines, all of this is running on my desktop.

Looker Studio is okay... it's free so I guess I can't complain too much but the experience for viewers on mobile is pretty bad.

The visualizations I made in Looker Studio are elementary at best but my goal wasn't to build the prettiest dashboard. I will continue to update it though in the future.",2023-03-28 17:39:39
11f8yxo,Quarterly Salary Discussion,"This is a recurring thread that happens quarterly and was created to help increase transparency around salary and compensation for Data Engineering. Please comment below and include the following:

1. Current title

2. Years of experience (YOE)

3. Location

4. Base salary & currency (dollars, euro, pesos, etc.)

5. Bonuses/Equity (optional)

6. Industry (optional)

7. Tech stack (optional)",2023-03-01 17:00:34
18prs2x,"We Don't Test: ""Do you run tests in your data engineering codebase?"" -> ""No"" - 69%, according to [The State of Developer Ecosystem 2023 by JetBrains](https://www.jetbrains.com/lp/devecosystem-2023/big-data/#ds_engineer_tests)",N/A,2023-12-24 09:19:13
18l0c01,"Is it true that the demand for data engineers is higher, whereas the supply is low?","I have spoken with many friends in Canada and the US. Most of them have mentioned that the demand for professionals in the data space is quite high, and there aren't many proficient data engineers or scientists available. 

I wanted to confirm if this is an actual scenario. I would love to hear your opinion on this. 

Thank you.",2023-12-18 04:37:23
18bi2xj,Why do companies still build data ingestion tooling instead of using a third-party tool like Airbyte?,"In the [Metabase Community Data Stack Report](https://www.metabase.com/data-stack-report-2023#data-ingestion-in-house) 31% of responders said they're using in-house ingestion. 

Why do companies still build data ingestion tooling instead of using third-party tools? Wouldn’t it be more expensive, in terms of cost and time, to engineer and maintain your own ingestion pipeline? ",2023-12-05 18:04:33
17so76t,What are the biggest obstacles/painpoints in data engineering?,"Just curious on some of the biggest costs of time/money that holds back the ability for your team to do their jobs effectively. For example,  things that are huge pain daily.",2023-11-11 05:57:10
16t83hx,AWS Certified Data Engineer - Associate - New Cert!,"Brand new associate level cert just dropped into Beta!

Full details at : [https://aws.amazon.com/certification/certified-data-engineer-associate/](https://aws.amazon.com/certification/certified-data-engineer-associate/)

Exam guide : [https://d1.awsstatic.com/training-and-certification/docs-data-engineer-associate/AWS-Certified-Data-Engineer-Associate\_Exam-Guide.pdf](https://d1.awsstatic.com/training-and-certification/docs-data-engineer-associate/AWS-Certified-Data-Engineer-Associate_Exam-Guide.pdf)

USD 75 while in Beta",2023-09-27 02:25:43
164mu9m,Is anyone still struggling to bounce back from layoffs?,"I was laid off 6 month ago as a data engineer with 3 yoe and still despite having had tons of interviews, seems like I'm not getting selected.
I’ve been technical pre-interviewed couple of times to enter the main interview process, I’ve been contacted after 2 month after applying for the job, I’ve been ghosted after final interview, the job has been filled in middle of interview , or job has been put on hold in middle of interview process.

I just was wondering if anyone else is experiencing the same.

And I appreciate if anyone can help.",2023-08-29 16:18:33
163w3gf,"As a new data engineer, would you take a job where you're the sole data engineer?","Considering a role where they're starting to build out their data engineering group with an n size of 1. As someone who's been doing some data engineering and learning more, this kind of responsibility seems too much. I'd ideally find myself in a group where I can learn from a senior level rather than suddenly becoming a data architect... What do others think? ",2023-08-28 19:52:24
15nhu70,How does your team use dbt-core without dbt-cloud?,"I am wondering what the devops and production architectures look like for teams that have effectively in-housed dbt-core without paying for dbt-cloud.

Do you run dbt on a server that accepts http requests from a scheduler? If so, how do you define 'jobs' and 'environments' the way dbt-cloud does? 

Open to any ideas on the subject",2023-08-10 16:56:04
1575ma8,Rant-Microsoft Fabric is most annoying POS I have ever used,"Seriously, I don't know where to begin. It's so annoying. I am not understanding the UI part of Microsoft fabrics. It's so click heavy. I am completely lost where to start and where to end.",2023-07-23 05:08:11
11f95x7,Getting into Data Engineering and more!,"Getting into Data Engineering and more!

Hey fellow Devs,

I recently posted in this sub for some advice about a job switch,  but got a lot of queries and DMs about how I got into Data Engineering, how to get better and go from entry level to senior position or from Data Analyst to Data Engineer (DE)

I'm working as a Senior Data Engineer in a Unicorn Startup in India. 

Trying to give back to the community from my experience in Data Engineering since 2019. Still surprised that not many are aware about what actually are the roles, the expectations from Data Engineers are. Most of the stuff is available online, videos from many youtubers, still posted it for people who might not be aware! :)

Will answer FAQs about Data Engineering. Feel free to correct or improvise. 

**Warning: This is going to be a long post.**

> DE means Data Engineering


I. As a fresher, I'm interested in Data Engineering, but how to get a job in this domain?

A. Getting a job directly as a full-time DE is pretty tough. Try to apply for DE internships and maybe it will get converted to full-time or with that experience try to apply for Associate / Junior DE positions. Build a network on Linkedin with many Data Engineers and connect with them about their experience. 



2.  What are the required skill-sets to become a DE?

A. From my experience companies expect you to be good at any one language

* Python, Java or Scala
* Strong SQL skills
* Data warehousing
* Spark
* Cloud experience - AWS/Azure/GCP \[good to atleast have an idea on how to spin up a cluster in any cloud vendor, setting up Network rules, firewall, etc.\]



3. Is DE in demand, is it better than Data Science?

A. Even though all the hype on the internet is for Data Scientists, the role of Data Engineer is equally crucial and critical for companies to enable Data Scientists. 

* Even the pay is lucrative! 
   * Salaries may vary, but mostly ranges look like this in India
   * Entry level DE - 4 to 10 LPA
   * 2 - 4 years - 12 to 30 LPA
   * 4 - 7 years - 25 - 60+ LPA

4. How do the roles differ, Data Engineer vs Data Scientist vs Data Analyst

A. My understanding - In the Data ecosystem

1. Data Engineer - Process starts here, collecting, cleaning and transforming, ingesting data into Data warehouses or datalakes.

2. Data Scientist - With the collected data in DW/DL, understand business logic and build useful data science techniques / ML models to identify key patterns, insights that can drive revenue.

   3. Data Analyst - Final part in process, Visualize the insights from Data Scientists using BI tools like Tableau, Looker, etc. 



3. How to prepare for DE interviews, Most commonly asked interview questions?



* **NOTE: In most companies, even Data Engineers are expected to be strong in DSA, since first rounds can be OA tests like Hackerrank and F2F Coding rounds before you can enter technical rounds about topics mentioned below. So, still need to Grind Leetcode to some extent!**
* **That being said, there are still companies that focus mostly on SQL, Spark for interviews and pay lesser attention to coding skills.**



After attending close to 40 interviews in last 4 years, the most asked interview questions for 0-3 years of experience were mostly on the following.

**Must have knowledge on these concepts to crack any DE interview:**

I. SQL

* Aggregate functions - AVG, MIN, MAX, etc.
* Joins - **important!** types of joins and their output.
* Window functions - Ranking functions, LAG, LEAD
* what is <**following**\> how do they **work**, how to **create** this and why is it used, **pros** and **cons** for the **following:**
   * CTEs
   * Views, Materialized views
   * Index - also types of indexes, index behind the scenes.
   * Partitioning - types of partitioning
* Normalization / Denormalization - rarely asked but important



2. Data Warehousing (DW) and ETL

* Star vs Snowflake Schema
* DB vs DW vs Data lake, when to use appropriately
* Choosing Columnar vs row oriented Databases
* Facts, Dimensions - understanding, examples
* Steps to implement a Data warehouse (for example in Bigquery)
* Best practices for DW, reporting
* Slowly changing dimensions
* Handling duplicate records, inconsistencies in data.
* Understanding ETL vs ELT process, data cleaning, ingestion techniques. 



3. Spark

* Understading Architecture
* YARN basics
* Sparkcontext, session, worker, task, job, stage, etc
* Spark dataframes, actions, transformations, reading and writing data, specifying schema options
* Repartioning vs Coalesce
* Partioning
* Handling OOM error in spark
* Broadcast variables, broadcast joins
* Best practices of Spark, best tuning practices
* Different persisting strategies in spark


Cloud experience

* Not much questions but it is vital to have an idea on different big data tools and services available in any one Cloud platform and their use cases.
* Most commonly used services in Cloud for Data systems
   * AWS - S3, Redshift, Glue, RDS
   * GCP - Cloud Storage, Bigquery, CloudSQL, Dataflow \[for streaming\]

**Linux skills - I think this is also a very important skill and a basic requirement**

 

Other skills to learn to become a better a niche Data Engineer,  if have the above mentioned topics covered, check these out:

1. Orchestration tool - Airflow \[slowly becoming a must have skill\]
2. Streaming data - Spark Streaming / Flink
3. Pubsub systems like Kafka
4. NoSQL databases - MongoDB, Elasticsearch, Cassandra, etc.
5. System Design for Big Data



Resources: 

**Datacamp** is one of my most favorite platforms. It has skill tracks for Data Engineering, Python, SQL, Shell, Spark, etc.

https://www.datacamp.com/tracks/data-engineer-with-python](https://www.datacamp.com/tracks/data-engineer-with-python

https://www.datacamp.com/tracks/big-data-with-pyspark](https://www.datacamp.com/tracks/big-data-with-pyspark


**I would highly recommend this but this is a paid platform though :(

Feel free to explore Youtube, Coursera, Udemy for specific concepts / courses based on the topics mentioned! 

if you are a student, use your college ID and activate Github Student developer program, get free access to datacamp for 3 months!

**Other resources I used to prep:**

https://dataengineering.wiki/](https://dataengineering.wiki/



1. **Orielly books -** for any topic, check reviews, most of them are available as PDFs in github.
2. SQL - [pgexercises.com](https://pgexercises.com), data Lemur, Ankit Bansal on Youtube, hackerrank, Leetcode
3. Spark - Spark by examples, Datacamp, ChatGPT recently :P,  to understand concepts with amazing analogies. 
4. DWH - Ralph Kimball book

Notable YT channels: Ankit Bansal for SQL

Big data folks on Linkedin : Shashank Mishra, Seattle Data Guy, Zach Wilson


Data Engineering is gaining more importance everyday. Upskill yourselves and join the ride!

Feel free to correct / add on, ping me for any queries. 

Cheers!",2023-03-01 17:07:48
193xq76,Will you stop using dashboards?,"I'm hearing more and more about dashboards dying and moving to ""interactive data apps"". I wonder if this is vendor marketing fluff or if this is actually happening. Thoughts?",2024-01-11 09:29:34
1933ach,Someone finally did it! r/learndataengineering is live!,"After seeing the same basic question again, I was like “I think it’s enough. It’s time to create a new subreddit” and when I tried to create r/learndataengineering, it said community already exists. Voila!


My nominations for questions that should go there:
- What makes a good Data Engineer?
- Do you need <tool/skill> to be a Data Engineer?
- What do you all think about <tool/skill>?
- What <project/skills> should I work on if I want to be a Data Engineer?
- Is Data Engineering a good role for me to transition to from <current role>?
- Read my blog post about DuckDB/Polars/…


Since I had to choose a flair, I went with discussion. What questions/topics do folks think belong in that subreddit?


Edit: I guess it’s an old subreddit (3+ years). Regardless, since it exists, I think we should utilize it",2024-01-10 08:10:34
18fii0v,What is the best SQL environment you have ever worked in?,"I'm talking syntax highlighting/completion, auto-capitalization, easy interaction with schemas/tables/stored procedures, intuitive UX. What's the best SQL environment you've had the pleasure of working in?",2023-12-11 01:06:39
11upvix,How are credentials and secrets stored and used in Production with Python?,"Since learning Python I've been using environment variables locally to store credentials and sensitive information such as API Keys and credentials for databases.

How is this information stored in a live production server and kept safe? Are they stored in something like Azure Keyvault / AWS KMS? If so how are the credentials accessed?",2023-03-18 14:25:28
1ai2d2z,BI Tool Rant,"Is it too much to ask for a thin open source code-first BI Tool with modern UI that has a semantic layer and can read from parquet directories?

If there’s one out there, please point me in the right direction.

After a career working with Crystal Reports, Jaspersoft, Cognos, PowerBI… they all suck in bloated ways that don’t seem necessary anymore in these days of DBT, Parquet, Polars/Pandas, etc. for most common data loads. 

Give me something that doesn’t manage security, that should be done farther left anyway. Give me something that is a good code-first development experience where I don’t need a server to store reports, just Git. Make it YAML, SQL, Python, Git local hosting/static assets — these competencies should be replacing Excel as expected competencies for data practitioners if we’re ever going to truly elevate data quality and literacy.

This is what “low-code” should mean.",2024-02-03 18:15:14
19dry3i,What do you think about Apache Pulsar?,"Pulsar is a robust data streaming and messaging platform, similar to Kafka, but with more features out-of-the-box like schema-registry, multi-tenancy, geo-replication, and tiered storage.

It has been an Apache top-level project since 2018 ([https://pulsar.apache.org](https://pulsar.apache.org/)). Many big companies like Tencent, Discord, Flipkart, and Intuit use it.

Unlike Kafka, it processes messages individually, instead of just using offsets. Horizontal scaling is painless in comparison with Kafka, thanks to the separation of compute and storage nodes. Supports millions of topics.

When I first heard about the Pulsar, I thought my dreams had come true. 🌈🦄

Now I'm trying to understand why there is not much buzz in public about it:

* Is it a marketing flaw and people just didn't ever hear about it?
* Or is there something wrong or missing in Pulsar?
* Maybe it is just an overkill for most new projects?

I would greatly appreciate hearing about your experience and thoughts on Pulsar.",2024-01-23 16:12:17
16qwc2j,Upcoming Data Engineering Tools,"Back in the day when we had to scale an analytics pipeline over a large dataset, Hadoop/spark used to be the go to options and later pyspark became popular.   


Now if the dataset is big we have some really interesting tools which doesn't require complex setup like spinning up a hadoop cluster.   


* We have tools like [Polars](https://www.pola.rs/) which can stream data if the memory is not enough. 
* We have [Ibis](https://ibis-project.org/) which helps you write generic analytics pipelines and it supports multiple backends and you can either run these pipeline on a query processing engine or a pyspark cluster or even on Pandas or Polars. 
* You can create your own internal datalake using a couple of parquet files and pair it with [Duckdb](https://duckdb.org/) for superfast analytics.  


Hadoop ecosystem will still be relevant but I believe for Small and Medium Business doing data engineering is getting better and you don't require big complex clusters to solve problems every time you run out of memory. These new tools are so easy to work with that any python programmer can work with them.   


I remember how I struggled understanding this book on [MapReduceDesign Patterns](https://www.oreilly.com/library/view/mapreduce-design-patterns/9781449341954/) a couple of years back and thinking that ""*Damnn!! data engineering is going to be hard*"".

Rust and python combination has resulted in some amazing python libraries.   
One interesting thing I see is people developing different variety of query languages like [EdgeQL](https://www.edgedb.com/docs/edgeql/index) and [Malloy](https://www.malloydata.dev/)

What are some new things in data engineering that you are excited about?",2023-09-24 12:16:40
15mpm60,Too much to expect from a data engineer? Python + Dimensional Modeling + Power BI,"My manager thinks we can get/train data engineers to be ""full stack"", that is, write AWS Python lambdas to grab data from EventBridge and load to our DW dimensional models, and have them design those models, and be able to create Power BI reports.  And be able to write good DAX and efficient datasets as well.  Do you think that is too much to ask?",2023-08-09 19:54:24
15d0a1c,How important is Leetcode for DE interviews?,"I placed into my junior Data Engineer role (current) through a rotational development program. 
I've never done a leetcode-style interview before and was wondering whether Data Engineering job interviews usually require this type of testing.",2023-07-29 19:12:58
14mkrvv,Our Days Are Numbered ?,"What do you guys think ?

https://www.databricks.com/blog/introducing-english-new-programming-language-apache-spark

https://github.com/databrickslabs/pyspark-ai",2023-06-30 00:01:52
13oxcur,What are some unconventional uses of Airflow?,Surprise us with some uses of Airflow few of us have heard of!,2023-05-22 17:31:48
12yj6uv,How should production changes be handled? Rant/debate,"I currently work with a colleague who I swear to fucking god has the worst practices ever.  The stuff they do should be considered war crimes (in the data world, of course).

The person in question has never done CI/CD before and has been copying and pasting blocks of code into the next environment when they want to ""promote"" a change.  Naturally, this has left environments being massively out of sync.  

Since then, we've implemented CI/CD to move database meta data and schemas between environments.  It's consistent, it works, it's better than copying and pasting.  The person in question, however, always rejects the idea of CI/CD with the following talking point:

Them: ""We can't guarantee consistency between environments""

Me: ""We can because that's sorted through CI/CD""

Them: ""Yeah, but when I want to make emergency changes to production, it won't align""

Me: ""But why are we making changes in production directly? They'll get erased next time we release because they don't exist in the previous environments.  For us to be consistent, we have to be in the mindset of working through environments""

Them: ""I don't want to have to go through every single environment to make a single change""

They will then continue to argue forever.  For some strange reason, this happens a lot and I feel like I'm going insane because I feel like it shouldn't be up for debate.  I feel like the person I'm arguing with wants to patch everything, I want to do everything correctly.  They want to apply ad hoc fixes as and when they feel like it, I want to be consistent so when something breaks we can test it at a lower level before releasing it. 

I'm 100% ready to be wrong.  Do people regularly makes production only changes?  Am I misunderstanding the whole point of CI/CD? How do you guys facilitate emergency changes with CI/CD?

Thank you.",2023-04-25 13:18:14
1171ps4,No coding. Is it bad for career,"I just started as  data engineer in a consulting organization. My current project requires me to do very little coding or rather I'd say no coding at all. I am currenlty working on migration to gcp project. They just want me to work on tools that are already built. The thing is I had some training on hadoop, hive and spark etc. But none of those skills are required for the job. Is it bad starting project to work  on!!",2023-02-20 08:37:49
199lx3r,How do you spend your time waiting for things to run?,"Sometimes when I run a pipeline or a query, while waiting for it to finish I find myself getting distracted on my phone (e.g. right now writing this post). What are some things y'all do while waiting for something to run?",2024-01-18 09:03:31
18z9z3x,"In your opinion, what makes for an ""elite"" data engineer?","Personally, I think that communication skills are one of the aspects at the top of the list. Additionally, a depth in database technology, distributed systems, ETL/ELT, and business use cases. But for how long will those aspects remain true, since many things evolve over time, especially rapidly?

Do you think have intermediate ML engineering knowledge, high performance computing, or any other present tangential knowledge will become table stakes? ",2024-01-05 15:50:24
18ybkzq,Discussion: Is SQL all you really need? A perspective from an older colleague.,"Half rant, half a discussion point.

I having a conversation with my colleague the other day.  I'm quite a bit younger than them and have more ""modern"" skills in the sense I've mainly worked on cloud, use Python for automation, and, of course, SQL whereas they are very much on prem experienced.  Unfortunately, one of my colleagues, another DE, let's call them Angela, insists absolutely everything is a SQL problem.  If it can be done in SQL, it absolutely must be done in SQL.  Every piece of data, no matter the format, must be stored in a database.  Angela refuses to use anything else.

One of my colleagues asked about learning Python and how to go about it for somebody they know. I recommended for somebody trying to career change, doing a Udemy course for the basics and to get a taste of what's possible and then move into project building as soon as possible would be the advice I give.  My colleague then said thank you for the recommendation as there are so many courses out there it's really hard to pick a good one.

Angela then says very proudly, ""Ah, this is the problem with modern technologies.  See, when I learnt SQL 15 years ago, it hasn't really changed much so I've never had to learn anything new.  With modern technologies, it always changes and you could be using something completely different next year whereas I've only ever used SQL at all of my jobs"".

Rant portion: This made me feel rather uneasy because this isn't limited to just code bases.  Angela isn't a fan of CI/CD (the answer to all problems including cross server database deployments in the cloud is a SQL script).  Angela barely believes in source control (literally local development of SQL scripts is how they work despite being told by management to work differently).  From this anecdote, it's very clear that Angela sees SQL as the one true god which will outlast everything and therefore every other language is a waste of time.  What makes this doubly worse is, in my opinion, they have absolutely terrible work practices which makes it difficult for everybody else.  One of the most triggering things was that they added a lot of FK constraints to logging tables, this caused CI/CD to fail, and then they blamed CI/CD rather than kinda shitty FK assignment.

Discussion portion: It isn't a secret the opinion of SQL being the most important skill to have in the world of DE is probably the most popular one and I'm pretty sure we can all agree that CI/CD, source control, good software engineering practices etc. are non negotiable.  

How many people feel like Angela about SQL? 

I found it a really interesting perspective as my mindset has been, ""if something new and useful comes a long, guess I'll have to learn it"".  Whilst SQL is important, I don't agree with the notion that DE starts and ends with SQL, I think general purpose programming outside of relational DBs is just as important as SQL.",2024-01-04 12:04:16
18pyzsa,How do you version control a dataset?,"Hello everyone, 

I’ve got a dataset that I want to watch changes of. It’s got 4 numeric columns and 3 categorical columns. The 3 categories in combination designate an entity, and the 4 numeric values are measurements of the entity.

I want to version control the entire thing such that I can see every single change that ever occurs, like with Git. Are logs the best way to do this, or is there a more mature way?",2023-12-24 16:56:45
17xuptt,When is Automation “Not Possible?”,"I came from a company where the data engineer I was working across told me that automation was “not possible.” 

I am of the opinion that automation, at least partial automation, is a possibility in most data warehousing and ETL processes.

Could someone tell me when automation isn’t possible?",2023-11-18 01:18:27
173lca6,PySpark Tutorial for Beginners: 1-Hour Full Course,"🚀 Exciting News! Just released my latest YouTube video - ""PySpark Tutorial for Beginners: 1-Hour Full Course"" 🐍💡

Are you ready to dive into the world of PySpark and harness the power of distributed data processing with ease? In this comprehensive 1-hour tutorial, I'll guide you through the fundamentals of PySpark, from installation to hands-on coding examples.

🔥 What You'll Learn:
✅ Spark Introduction
✅ Spark Installation
✅ Setting Up Your PySpark Environment
✅ Spark RDD
✅ DataFrame Operations
✅ Spark SQL
✅ And much more!

Whether you're a beginner looking to kickstart your journey into big data or an experienced data engineer aiming to refresh your skills, this video has something for you!

Watch it now 👉 https://youtu.be/EB8lfdxpirM
GitHub Repo 👉 https://github.com/coder2j/pyspark-tutorial

Don't forget to like, subscribe, and share with your network. Let's spread the knowledge together! 📚💪
#PySpark #DataScience #BigData #Tutorial #LinkedInLearning #YouTubeTutorial",2023-10-09 06:52:36
16o5d6o,I made a free app to help you prepare for the data engineering interview,"Preparing for an engineering interview can be overwhelming, LeetCode, System Design, Behavioral questions, its a lot to manage. If you are preparing for a **#dataengieering** interview, give my free app a try. It gives you 2 tasks a day for 30 days to help you prepare.

Also I wanted to learn Swift.

https://apps.apple.com/app/interview-ace/id6465748534",2023-09-21 03:35:11
15r6k9i,Is Palantir as good as they say? is it worth our investment,"Hi everyone,

I work for a company who is looking into getting Palantir. I'm not the most familiar with the software and what the real life use cases are ( data visualization?) and wider operating system for data integration.

My role in my company is sorting out finance & doing a feasibility and what I can say it seems very expensive. VERY expensive. The execs and boss all swear by it & say it's a game changer that they've seen in other industries. 

Who has used Palantir? what do think of it? is this a vapoware or is it real and practical. 

&#x200B;

&#x200B;

&#x200B;",2023-08-14 20:47:45
13xb1m3,What is your data engineering philosophy?,"I had an interview with a mid-sized company, where the interviewer asked me, 'What is your data engineering philosophy?'. I was caught off guard by the question and just responded, 'The simpler, the better'. 

What would you say if an interviewer asked you this question?",2023-06-01 07:27:34
122eo0k,Why do Data warehouses don't have a Primary Key check that is enforced?,"Usually in OLTP systems, we have a PK constraint that is enforced. In OLAP side, I see that Snowflake, Databricks, GCP BigQuery - None of them have a PK that is enforced. I understand that it causes data performance issues, but given that all of them store data in columnar fashion, how does checking a unique id in column affects performance?

Even while working with MERGE statements, the system is going to compare the columns to check whether it needs to do INSERT OR UPDATE, then a PK column with corresponding index might increase the query performance right?",2023-03-26 07:36:44
11ymha3,Devops work as a DE,"Do any of you guys like doing DevOps work as a DE. I hate it so much, it’s just extremely boring for me, and for some reason all the DEs in my team are always prioritizing IaC (terraform), akyless integration for rotating keys, cicd templates, etc. I always vote to prioritize on data quality initiatives but I guess no on likes that lol. 

And is not that I like data quality, just that my mind thinks as a business stakeholder not as an engineer. Data quality is way more important for the business (better data aka more money) than doing all this fancy DevOps work. Off course, that work is important but there are more critical things to do. 

What are your opinions?",2023-03-22 15:28:15
1andkrm,"Passed DP-203 last week, details for aspirants in the post.","**Score:** 920  
**Study Time:** 2 Weeks (Studied for 8-10 hours everyday)  
**Relevant Azure experience:** 0 years \[Though have been studying from Youtube ADF & DBX for 2 months, but had no experience in Synapse and Stream Analytics\]  
**Hands-On Experience:** Had bought free trial of 1 month back in Dec last year but practiced hardly for 10 days.

**Paper Pattern**  
**No. Of Questions:** 41  
**Time to attempt:** 1 hours 40 mins

**Resources used for prep:**

1. Microsoft Documentations and Learn Path.
2. Exam Topics (Only Free Questions: [https://www.examtopics.com/exams/microsoft/dp-203/](https://www.examtopics.com/exams/microsoft/dp-203/))
3. Youtube Channels followed:  
a. Tybul On Azure \[[https://www.youtube.com/@TybulOnAzure/featured](https://www.youtube.com/@TybulOnAzure/featured)\]  
b. [https://youtu.be/0QUSK48YX04?list=PL0AYtrUw-NRQu89sbJNXPEo0dkBVTujY5](https://youtu.be/0QUSK48YX04?list=PL0AYtrUw-NRQu89sbJNXPEo0dkBVTujY5)  
c. Studying as you were \[[https://www.youtube.com/@studyingasyouwere](https://www.youtube.com/@studyingasyouwere)\]\[Go through the three DP-203 playlists\]

The questions dumps are still valid and I got around 25-30 of the 41 questions directly from the dump so do it by heart (but don't just learn the answer, understand the logic behind it). If anyone needs any guidance or has questions please comment here or reach out to me in DMs. There's not much help available on this so will be glad to help.

PS: Do learn to navigate your way around Microsoft documentation, online comments like it is a cut down version and things are hard to find are misleading, I had plenty of time left so for a few questions I was unsure about(around 7 questions), I went through it, and found answers for around 3 questions.",2024-02-10 10:39:29
192fhgg,Why we are ignoring Julia for data engineering and going for rust??,"I have recently observed this trend where people are getting curious about rust for data engineering especially after the whole polars, duckdb and apache arrow revolution. Apache arrow and parquet are amazing technologies but I am confused by craze of rust.

We all agree python has issues and major of them are
1. Its slow as hell
2. Packaging problem : I am sure we can agree than dyanmic languages have a bad packaging ecosystem for eg python and js. Go and rust has a much better ecosystem 
3. Two language problem : there are way more python users than contributors in libraries and a lot of libraries have expressed this concern

Because users have to learn an entire new language and learn to do memory management efficiently in c++ to become contributors 

I agree rust is better than c/c++ but still we are creating the same two language problem and writing libraries in rust doesn’t solve the packaging problem of python

Why don’t we just use one general purpose language with a good type system and almost as fast as C and users can become contributors",2024-01-09 14:06:39
1866ogd,Python versus ETL tools,"I am trying to break into the world of data engineering and I am seeing that you need to learn all these different tools like ADF, dbt, etc. to perform ETL. 

To me it seems like Python does all this but clearly I am new to this world and there is something I am missing. 

What are the benefits of using tools like the ones listed above as opposed to Python?",2023-11-28 21:04:47
16kcti2,Formula 1 streaming pipeline,"As a data engineer, I like to spend my free time building data pipelines. This is a data pipeline I built using streaming databases and Formula 1 data because I find it very interesting. What do you think about it, any input is highly appreciated.

[https://www.kdnuggets.com/building-a-formula-1-streaming-data-pipeline-with-kafka-and-risingwave](https://www.kdnuggets.com/building-a-formula-1-streaming-data-pipeline-with-kafka-and-risingwave)",2023-09-16 17:17:34
16ayrif,400M rows table : how to insert 4M and delete 4M rapidly without overwriting full table ?,"I have a postgres table with ~500M records on 20 columns. The table is updated / created from a spark job which use a parquet and load it to postgres. Every day, I get a new parquet file with 500M, but among all records, only 5M are new, and 5M disappear. At the moment, I overwrite all the table, but this is really long. I guess this is a classic issue and there must be a workaround to only append new rows are delete existing rows. My table does not have any primary key nor id column, but 3 columns together can form a unique id. What should I do to update my table in a better way ? (notice that I cant write the parquet file to a temp table because it would be at least as long as overwriting the table).

At the moment, the only solutions that seems to work is to create an id column with the 3 columns concatenated, index this column and use it to compare new data and old data to perform the delete. What is the common way to do ?",2023-09-05 20:14:30
15y76e4,Negotiated compensation and they gave it to me!,"I made a post last week(I ended up deleting it out of doxing paranoia)about if I should negotiate my offer letter. This being despite already being happy with the offer and knowing it was fair through multiple benchmarking sites. The general vibe for the community I got was to negotiate. But there was a good chunk saying you shouldn't in this market.

As a result, I ended up negotiating. Majority rules win

I am just a sample size of 1, but they did not rescind the offer and they did increase my compensation. In my case, the adage to always negotiate ended up being true! 

I did take the communities advice to tackle the negotiation from a benefits perspective over pure base salary",2023-08-22 14:17:48
15gnctu,What replaced cubes?,"I’m fairly old school with lots of on prem and ssis/ssas experience. Been doing a ton more with cloud now but more on the E/L and infra and haven’t gotten around to dbt yet.

But I’ve wondered what has replaced the idea of a cube or enterprise star scheme with multiple facts to power reports and dashboards from a single source? 

Additionally is the idea of self service via a cube style interface dead?

Is it mostly dbt now and a sprawl of models users pick and choose from?

I’ve done a lot with power BI, but even that seems to focus on creating mini data models per report, so wondering how the reporting/dash data sources are scaled now.

Apologies if it seems like I’m dense, just used to a semantic layer that creates common metrics and dims for people so they’re all looking at the same things.",2023-08-02 23:20:21
14jg3xi,People of Data Engineering,"A curated list^(1) of the people in Data Engineering:

* **Dipankar Mazumdar**: Dremio - Apache Iceberg
* **Maxime Beauchemin**: Father of Data Engineering
* **Mehdi Ouazza**: Awesome written content, also now on YouTube. Creator of Data Creators Club.
* **Benjamin Rogojan**: Seattle Data Guy
* **Ananth Packkildurai**: Functional Data Engineering, Creator of dataengineeringweekly.com.
* **Zach Wilson**: Data Engineering Challenges at Hyperscale
* **Marc Lamberti**: Airflow
* **Wes McKinney**: Pandas / Arrow
* Andy Grove: Apache Arrow PMC. Creator of DataFusion & Ballista (Arrow) query engines)
* ThePrimeagen: Rust, Netflix, programming, Neovim
* **Nick Schrock**: Dagster, data orchestration
* **Denny Lee**: Delta Lake, Rust, OSS
* **Simon Whiteley**: Databricks, Data Engineering), popular YouTube
* **Matt Turck**: Creator of MAD landscape
* **Jacek Laskowski**: **ApacheSpark** DeltaLake Databricks ApacheKafka KafkaStreams ksqlDB
* **Matt Housley**: Creator of Fundamentals of data engineering
* **Joe Reis**: Creator of Fundamentals of data engineering
* **Erik Bernhardsson**: Building a simple version of Kubernetes Modal
* **Matei Zaharia**: Chief Technologist at Databricks
* **Adi Polak**: Author of Scaling Machine Learning with Spark
* **Andy Petrella**: Writing on Data Observability
* **Peter Marshall**: Druid Advocate
* Alexey Grigorev: Manages DataTalksClub which has a blog, zoom camps and GitHub tutorials
* **Joseph Machado**: Lots of great how-tos and projects on Start Data Engineering
* Chris Riccomini: Essays on tech, data, and streaming
* Christophe Blefari: A combination of aggregate newsletters and one-off articles on data engineering
* **Itai Yaffe**: Druid use cases: Streaming with delta lake and Druid
* **Wayne Eckerson**: Author, keynote speaker, and consultant Eckerson Group
* **Andreas Kretz**: Creator of The Data Engineering Cookbook
* **Tobias Macey**: Data engineering podcast
* **Darshil Parmar**: Popular youtube
* **Michael Kahan**: Popular YouTube and Content on DE
* **Matt Weingarten**: Data Engineer at Disney Streaming Services. Previously at Facebook and Nielsen.
* **Robert Sahlin**: Data Platform with Google Cloud
* **Jérémy Ravenel**: Naas, Jupyter Notebooks into powerful automation, analytical, and AI
* Chad Sanderson: Data products, contracts, and captivating articles
* Sarah Krasnik: Great for infra and solutions insights
* Daniel Beach: Broad range of data engineering topics
* Benn Stancil: Prolific writer on his blogs, and they usually start with it Friday, let's fight...
* Barr Moses: Great articles on Data Observability.
* **Thalia Barrera**: Excellent post on date engineering
* **Stephen Bailey**: Exploring the world of data and its adjacencies at Data People Etc.
* **Shane Gibson**: Data modeling, in data for 30 years. Not technical, but about agile and data modeling.
* **Petr Janda**: Awesome blogs on petr@substack now working on Synq
* **Jonathan Neo**: Creator of Data Engineering Bootcamp
* Sandy Ryza: Dagster and passionate about Partitioning and Backfill.
* **Xinran Waibel**: Personalization Data Engineering at Netflix
* Simon Späti: Lots of open-source data engineering

Who is missing?  


^(1)[^(https://www.ssp.sh/brain/people-of-data-engineering/)](https://www.ssp.sh/brain/people-of-data-engineering/)^(.)",2023-06-26 12:33:35
14h0b8x,Need Recommendations: Azure vs. AWS,"Hi all,

Apologies if this has been posted before. 

Do you think one should learn AWS or Azure first? - I understand that several companies use multiple providers, but which provider would yield the best ROI in the current climate while considering the context below:

**Some context:** I possess a background in Data Analytics (PowerBI, Python, SQL) and am interested in embarking on my DE journey. Thinking of starting off with Udacity and supplemental textbooks (maybe even exploring the famous youtube/GitHub Zoom course down the road). - Focus will be on understanding foundations and building projects because I think this is a great way to learn.

Would love to know your thoughts. Thank you! xD",2023-06-23 14:39:56
14dxeco,I can’t terraform my company’s Databricks environment and I’m going insane.,"Started at a new company a couple of months ago, was tasked with terraforming our databricks environments. I’ve never had experience aws, terraform, and databricks. The deployment is going well I have dev and prod workspaces setup but we cannot get authentication through to external tables. I’ve read all the docs and watched all the videos with no luck. Have the metastore created, the metastore access role, the external table with pass role. I have unity catalog enabled with grants and privs defined. 

Terraforming all of this and I’m fucking lost.",2023-06-20 01:43:31
13l0fcc,What is the most ridiculous project/task/request that you have handled as a DE?,"As the title suggest, what is the most absurd projects or task that you have handled in your DE career? 

Mine was back when I was working in a local IT consulting. We have created pipelines and dashboards for this one client, but somehow they also want a PDF version of these dashboards to be sent to chat groups and DMs of their employees on different level and region (they use Telegram). Because our BI viz tool doesn't have the capabilities to do that, the consultant and DE lead that handled that client proposed an idea to recreate those dashboards one by one in excel using python scripts, export the excel files into PDF, and finally creating a chat bot to send this PDF to various chat groups and DMs.

The development phase was not a pleasant experience either. We didn't have a git repository and CI/CD pipeline, so all of code was stored in Dropbox and the 'deployment' was done by sending the scripts to a VM via SFTP and ran it via cronjob.",2023-05-18 14:18:17
17mzdka,How do you build great data teams? What is the best advice or lessons you have learnt?,"How do you build great data teams? What is the best advice or lessons you have learnt?

As a manager and a leader this is something I would love to learn about so welcome your thoughts and discussions.",2023-11-03 16:46:04
170vj3u,Backend Skills for Data Engineers,"Dear fellow Data Engineers

Yesterday, I had a Job Interview for a Senior Data Engieer Position at a local Healthcare Provider in Switzerland. I mastered almost all technical questions about Data Engineering in general (3NF, SCD2, Lakehouse vs DWH, Relational vs Star Schema, CDC, Batch processing etc.) as well as a technical case study how I would design a Warehouse + AI Solution regarding text analysis.

Then a guy from another Department joined and asked question that were more backend related. E.g. What is REST, and how to design an api accordingly? What is OOP and its benefits? What are pros and cons of using Docker? etc.

I stumbled across these questions and did not know how to answer them properly. I did not prepare for such questions as the job posting was not asking for backend related skills.

Today, I got an email explaining that I would be a personal as well as a technical fit from a data engineering perspective. However, they are looking for a person that has more of an IT-background that can be used more flexible within their departments. Thus they declined.

I do agree that I am not a perfect fit, if they are looking for such a person. But I am questioning if, in general, these backend related skills can be expected from someone that applies for a Data Engineering position. 

To summarize: Should I study backend software engineering in order to increase my chances of finding a Job? Or, are backend related skills usually not asked for and I should not worry about it too much?

I am curious to hear about your experience!",2023-10-05 22:30:43
152own3,"I am a new grad “data engineer” who barely writes SQL, how will my career be impacted?","I work at a big tech company as a new grad and my title is technically data engineer, however much of my work is building distributed Python applications for ML and data replication, spark, Java, devops pipelines and infrastructure automation (ansible). If I write sql, it’s always within a Python application and is nothing more than an insert, table creation or simple select query. With my experience, would I be more suited to go into software engineer roles moving forward? Scared of applying to companies that use modern data stack or no-code tools and I am stuck having to re-learn SQL or failing an interview.

Edit: I’d consider myself average at SQL, I’ve passed all-SQL interviews, just worried about my skills atrophying as it’s not something I really enjoy to use and my role doesn’t require it",2023-07-18 05:05:17
13mzqi0,The Missing Manual: Everything you need to know about Snowflake optimization,N/A,2023-05-20 16:41:03
11wc273,[Code]: Comparison between Rust (Polars) and Pandas | Basic Benchmark,"Sharing the code to draw comparison between Pandas and Polars (Rust). Results are pretty impressive for Polars.

    import pandas as pd
    import polars as pl
    import numpy as np
    import time
    
    # Create a random DataFrame with 500 MB of data
    n_rows = 250000
    n_cols = 400
    df = pd.DataFrame(np.random.randn(n_rows, n_cols), columns=['col_{}'.format(i) for i in range(n_cols)])
    df_polars = pl.from_pandas(df)
    
    # Benchmark Pandas - Apologies for not using 2.0 version
    start_time = time.time()
    df_filtered = df[df['col_0'] > 0]
    df_grouped = df.groupby(['col_1', 'col_2']).mean()
    df_sorted = df.sort_values(by=['col_3'], ascending=False)
    df_merged = pd.merge(df, df_grouped, on=['col_1', 'col_2'])
    pandas_time = time.time() - start_time
    
    # Benchmark Polars Rust
    start_time = time.time()
    df_filtered_polars = df_polars.filter(pl.col('col_0') > 0)
    df_grouped_polars = df_polars.groupby(['col_1', 'col_2']).mean()
    df_sorted_polars = df_polars.sort(by=['col_3']).reverse()
    df_merged_polars = df_polars.join(df_grouped_polars, on=['col_1', 'col_2'])
    polars_time = time.time() - start_time
    
    # Print results
    print('Pandas 2.0 time: {:.2f} seconds'.format(pandas_time))
    print('Polars Rust time: {:.2f} seconds'.format(polars_time))

If you want to increase dataframe size to 1GB:Change`n_rows = 10**6n_cols = 1000`

Results:

Pandas : 23.45 sec

[Pola.rs](https://Pola.rs) : 5.66 sec

If you want to increase dataframe size to 100GB:",2023-03-20 07:35:51
19d18sj,How often do you get bored?,"I work at a company I would consider to be top-tier. Great pay for where I live, nice work culture, great people, and good upside potential (company could sell or do IPO in the next few years). But I just feel bored. It's not challenging anymore. When I came on board 6 months ago, I rebuilt everything from scratch and now it's all working almost flawlessly. Our team is small, and we've scaled efficiently so not created more work for ourselves. I do 2-3 hours of real work every day. I have 1-2 short meetings a day if that, answer questions, fix small things that break. I like to feel like I've done something at the end of the day, and more recently the past few months, this is rare. Honestly, most days I feel guilty for not working more. I take time during the day to listen to audiobooks (while working), I work out, take lunch with friends, or similar. It's the opposite of burnout. It's unnerving to not be busy or feel like I'm adding value.  

Is anyone else's shop like this? What are your recommendations?",2024-01-22 17:36:51
18qjniu,Delta Lake without Databricks, Is it practical to use Delta Lake in an on-premises environment? Are there any detailed tutorials for this? I need to write real-time data from Kafka to Delta Lake using Spark Structured Streaming. What do you recommend? ,2023-12-25 14:26:57
17j7o18,Third world data engineering,"Quick fact: 95% of the world is not eligible for the 95% of the jobs on this subreddit. This is to show my frustration with an existing job market for data engineers. Being a third world national I feel it is hard to get a decent job in a field of DE. Is the only way to become good DE is to migrate to the US, cause I feel there are small chances of getting good project here. I was outstaffed to a US company, but my work was without challenges. Sometimes I feel like third world nationals don't get interesting things. Am I the only one who feels frustrated on my DE path? Getting through textbooks may be interesting, but outcomes are unknown and it is very time consuming. Getting certified is extremely boring and also it's imo useless without hands on experience. Has anyone managed to break this wall? **What are the steps a third world national should take to become a distinguished data engineer?**",2023-10-29 17:18:06
17hlscx,What’s the future of the data engineering job market?,"It seems like there’s more people interested in pursuing data engineering than there are positions out there. A lot of kids taking boot camps and coming out of college trying to “climb the ladder”. 

Realistically someone could learn enough to get in front of a hiring manager in 3-6 months if they’re determined. And with ChatGPT, Git, etc it’s very easy for people breaking in to fake it into a role if they’re being dishonest. A lot of initial HR recruiters won’t know the difference, and the hiring managers will be required to sort them out. That might make getting to the hiring manager really difficult. The sole barrier to entry seems to exist at the hiring level without any external constraints (ie don’t need a cert or higher education degree).

Maybe this is being pessimistic, but where do you expect the data engineering industry to be in 10-15 years? 

Will 2023 data engineers need to shift to data science? Will an oversupply of data engineering hopefuls influx data engineering skill sets into data analysis roles and raise expectations for data analysts?

Another aspect with the rise of GPT and the so-far failed attempts at low-code ETL platforms- eventually they’re going to get better. I remember in the 2000’s, “web designer” was a well-paying career. It’s hardly a college internship now. Everyone moved up and on to better things so it’s all ok, but the role has basically vanished. Web developer seems like it was the next evolution and required more learning and increasing skills. 

Full disclosure- I’m a data analyst who is totally content with where I’m at. I just like learning data engineering for the sake of learning, but every other discussion is someone else trying to break into DE from a job perspective. How does this overwhelming interest in such few positions with low external barriers to entry affect the hiring market? These same issues affect data analysts careers too. For us, possibly learning “big” data engineering will be a standard to simply stay competitive for data analysts in 10 years. I’m not worried since we’ll always be employed and we’re not our job titles (which may change in 15 years). 

Change isn’t a bad thing, we just can’t ever stop learning new skills if we want to adapt WITH (not in response to) job market changes.",2023-10-27 11:51:03
1641tum,What's Stopping You from Using Open Source Tools?,"I’ve seen lots of people here using OSS (or Commercial OSS) tools, but I also see many that aren’t. For those people in the IBM, SAP, Microsoft, Informatica, etc., commercial worlds out there, why, exactly? And for which parts of your stack?",2023-08-28 23:32:59
15p2yze,Does anyone actually watch Data Engineering podcasts/youtubers?,"Personally, I've never been interested at all in watching DE content on youtube/podcasts, so I'm curious if others actually want/enjoy this kind of stuff?

I had a look, and it seems hard to find any decent quality content that isn't vendors or influencers shilling their own tools. There's so much software eng, front end dev, etc. content out there, but very little about DE.",2023-08-12 12:09:44
13jzt1u,What are the other names for data engineering jobs,I came across some jobs that have the same descirption as DE but have different titles (exp ETL developper). I was wondering what are the rest of the possibilities.,2023-05-17 12:05:01
126xzgl,Data Quality Dimensions: Assuring Your Data Quality with Great Expectations,N/A,2023-03-30 20:15:44
1ahjv8r,How do you prove SQL proficiency with projects?,"I am currently a Data Analyst but want to change to Data Engineering. However, several of the jobs I want to apply to require proficiency in SQL. I am proficient in Python, since it's what we use in my company, but I have no experience with SQL so I want to add some projects to my GitHub to show that I can write SQL queries and make data analysis with it. Do you have any recommendation?",2024-02-03 01:14:11
19aeuyr,It seems that a bunch of US work has shifted overseas to EU.,"I noticed that a lot of people from the US have trouble finding a data job in the US(and people with substantial experience). 

In the EU the situation seems fine, I get recruited at least 5 times every 3 months. Maybe I've been lurking here for too long and started to take this as the real world, and its a different thing in actuality. For me I cannot imagine someone with a lot of experience to not have a job in tech, no matter the technology or how ""old"" it is  there is always something to maintain or develop. 

What is happening in the US? Is this jobless crisis coming to the EU soon? Is it only Data Engineers that are affected or all data people(or all tech people)?",2024-01-19 09:00:36
194rp77,Best way to learn pyspark.,"Hello Everyone, I am having 5.5 years of experience in data engineering mostly worked on teradata. Currently in gcp services. Before making another switch I want to be fundamentally good at pyspark by having some hands-on experience. What will be best way I can follow to learn pyspark or and good courses that I can take if any. Please suggest.",2024-01-12 10:11:08
18a8q2w,Is there any resource for medium to advanced Python for Data Engineering?,"Title. This might be a silly question. I am fairly competent with Python, however I was wondering if there are any data engineering specific Python resources to learn from.

I know that some compsci basics, algorithms, structures, design and so on are always good, but, since Python is the most widely used language in our job, maybe there's something more specific.

Thanks.",2023-12-04 01:15:27
17l1crr,Why do organisations use abbreviations for table and field names that don't save much space and reduce clarity?,"To give some examples,  I see table names such as applctn, prmtn, cstmr, thrshld. Is there any legitimate reason for doing this? To me, it reduces clarity and conversely makes typing and understanding them more difficult since one is not used to such abbreviations.",2023-11-01 01:48:57
17enoxk,How to unnest a json recursively,"Suppose I have a JSON structure like this-- and honestly, this is an oversimplified version of what we're dealing with at work:

My goal is to create a Python script that can recursively unnest this JSON, such that each array in the JSON is flattened/normalized into its own dataframes. We want it in separate tables because the granularities arent the same across arrays (not shown in this sample)-- some are on order level, some on order-item level (i.e. one row per item purchase,), some on customer level, all within the same object.

Other nuances is that the JSON nesting isnt always consistent-- right now, we see for example that 'dob' has entries for month-year-date, sometimes that can be empty or saved as a list with an array inside. 

Technically, I can unnest without going through this complexity, but just wanted to be 100% sure I've explored all options

Thanks in advance for the help",2023-10-23 15:53:05
17dnmln,Migrating away from Snowflake?,"Has anyone migrated away from Snowflake? It seems to keep getting more and more expensive for reasons we can't justify.

1. Did you move to redshift? What has people's experience been with it lately - do you use / try the serverless version?
2. Were their substantial cost savings from it?
3. What are other options to consider here? MotherDuck?

&#x200B;

Update:

Thank you for the reply, we're a really small company with not a ton of data and do try to follow most of the best practices. One of my main frustrations has been that how can snowflake be comparable to our main AWS bill :/

One of our main requirements is that we need a small warehouse running all the time as some systems need near realtime data which is what makes the stable billing a lot more tempting. ",2023-10-22 07:16:23
16ysjx8,How do I get away from devops,"Iv been a DE for around 5 years now. I love working with data, building pipelines, ML platforms, supporting BI/DA/DS teams, etc. But I HATE dev ops.

I work for a company with a pretty complex cloud environment setup. Handling permissions and networking between microservices takes up about 80% of my daily energy.

It seems like a lot of DE jobs require quite a bit of devops work. Is this true?

How do I transition to a role that focuses less on devops and more on software design/performance? Iv been thinking of Skilling up and trying to transition into an ML. Engineer role. Would a role like that theoretically have less devops responsibility?",2023-10-03 13:53:46
14pzuls,What skills is top 1% in data engineering?,"Basically the title, I wanted to know if smart ETL’ing, or great cloud skills makes us into the top 1% in our field. What in your opinion is the skills required to be in the top 1% of data engineering field?",2023-07-04 00:30:20
14g775a,Uncovering Stack Overflow's Shocking Architecture,N/A,2023-06-22 15:54:15
14aanke,What does a data engineer need to know other than Python and SQL?,"[https://dataisle.io/posts/data-engineering-technologies/](https://dataisle.io/posts/data-engineering-technologies/)

Hi all! I've noticed a lot of questions like this one around here, so I tried to give my take on it in this post. I understand it's hard to write such lists as everyone has a bit of a different opinion and experience and would like to put something in or take something out. My goal was to put in things I think are essential, especially for a new data engineer or someone with 1 to 2 years of experience.

Feedback and questions are appreciated. :)",2023-06-15 19:16:21
12b6fgb,A dbt killer is born (SQLMesh),"https://sqlmesh.com/

SQLMesh has native support for reading dbt projects. 

It allows you to build safe incremental models with SQL. No Jinja required. Courtesy of SQLglot. 

Comes bundled with DuckDB for testing. 

It looks like a more pleasant experience. 

Thoughts?",2023-04-04 02:24:48
1273nls,Is there really still any entry level data engineer job in market?,"I have been casually browsing the LinkedIn/Google but it is so disappointing that I can’t find any entry level data engineer job. Most “entry-level data engineer” jobs require 2-3years of experience, I mean is it really still entry level in that case? My goal is to become a data engineer in financial services/banking industry, it just adds more difficulties into the job search having a particular industry to get in. Do people just ignore the experience and send their resume in for application anyway? I would imagine that that will get rejected by the ATS.",2023-03-30 23:55:35
126r6zx,Build a data warehouse on top of Excel,"[dbt-excel](https://dbt-excel.com/) seamlessly integrates Excel into dbt, so you can take advantage of the dbt's rigor and Excel's flexibility.",2023-03-30 15:59:34
1absxeh,Geoglify is now Open Source,"Geoglify is now open source! Explore tools for understanding the maritime universe in GIS space. A new tech stack, a whole new world for me, geared towards the geospatial maritime universe. Where I'll bring all ideas to life. The code is now available on GitHub. The first version is open for testing. I think everything you need for this exciting journey is there.

[https://github.com/geoglify/geoglify](https://github.com/geoglify/geoglify)",2024-01-26 21:05:38
18zljvi,Is there such thing as a “product data engineer” who isn’t just a slave to the reporting team?,"I’m still relatively new to the field of data engineering, and I’ve noticed some patterns about the kind of work that data engineers handle and who that work benefits.
  
I did a data engineering internship this last summer at a midsized fintech company, which was my first time seeing “real” data engineering firsthand. The work itself was pretty interesting and I liked the people I worked with, but came to the conclusion that the only benefit that my work had on the company was getting data into the hands of the reporting team. 

I came to this realization because as I was updating my resume with my new internship experience, the only way that I could really quantify my work was essentially “saved time and effort for the reporting team.” I had no clue what my impact was on the business or my work was being used. It was kind of an alarming feeling.
  
I think I’d like to ask: is this normal? Is most data engineering work geared toward just being data stewards, and getting the data to other people?
  
Is there such thing as a data engineer that works on a “customer-facing” product, where I’d be able to tangibly explain how I impacted the product or business?",2024-01-05 23:51:43
15lr89o,Honest question: How do I know if I'm good enough to be a data engineer?,"**My background:**

Currently I'm a senior data analyst, and I'm one of those people that works in the department where the data analyst does just about everything. I do ETL, reporting, bi stuff so piping data from Google BigQuery into stuff like Power BI, looker studio, AWS, snowflake, using Python of course. I spend more time typing in SQL than I do in English in emails. I find myself doing recreational projects with Python and SQL quite often, for example this week I wrote in API that retrieves data from Kaggle, cleans it in pandas, loads it into BigQuery, also loads it into snowflake, Cassandra, Then using even more Python, retrieves some of the data using SQL. 


**Question:**

How do I know if I'm ready for a data engineer position? Will I ever be ready? I was contacted buy a recruiter for a couple positions. The one I applied to is data analyst, but I asked about the data engineer position because the job description said almost nothing about it. She said it was a little bit more technical than what I've probably done, preceded to list off basically everything I've already been doing. Said I have to be comfortable doing object-oriented programming in Python (didn't know they used objects in data engineering). Said that it's a lot more of the piping data, working in AWS data sources, snowflake, Hadoop, etc... This is all stuff that sounds insanely complex and technical but here's the thing.... **How do I know if this is stuff that I'm capable of learning on the job or doing?** Like no, of course I haven't set up a data source from scratch with 300 million rows or a terabyte of data myself. I can't just go and make a 300 million row data set on my PC at home. No matter how much I want to, It would be next to impossible for me to simulate the work that a data engineer actually does on the job in a personal setting. Because any work I do as a data engineer will be in a live environment for a big company with important deadlines and I don't have that right now.",2023-08-08 19:07:47
1490edf,Red flags in job hunting,"On my quest to find a new job, I need your hilarious insights. What are some unmistakable signals or alarm bells that scream, ""Run for your life! The job is a horrendous nightmare or managed by Captain Chaos himself""?

Edit: Thanks for the responses. Definitely, many of these will help me make better judgments!",2023-06-14 06:25:38
136i2v1,Architect and design data pipelines that can handle billions of data events per month,"Hi guys, can someone tell me what would you do to ""Architect and design data pipelines that can handle billions of data events per month"". This is just a premise/ requirement I saw in a job ad, so no more info.

&#x200B;

I think I would use kafka to process the data events and store them into Postgres. But not sure how to set up postgres to handle the volume.

&#x200B;

Can you give me some insights? 

&#x200B;

Thanks :)",2023-05-03 11:40:58
12rockb,🎧 Random grab-bag of podcast episodes that I've enjoyed recently,"* Joe Reiss and Matthew Housley chatting to Felix GV about Venice and Designing Massive Distributed Systems at LinkedIn [https://overcast.fm/+wcMoWgOXw](https://overcast.fm/+wcMoWgOXw)
* Benn Stancil's talk from Coalesce 2021 about the Modern Data Experience: [https://overcast.fm/+w94XLSJUU](https://overcast.fm/+w94XLSJUU)
* Joe Reiss and Matthew Housley talk to Neelesh Salian about building data infrastructure [https://overcast.fm/+wcMp5SpsI](https://overcast.fm/+wcMp5SpsI)
* Jean-Georges Perrin on DataEngPodcast talking about DataMesh in real life (not theoretical!) at PayPal [https://www.dataengineeringpodcast.com/building-a-data-mesh-at-paypal-episode-364](https://www.dataengineeringpodcast.com/building-a-data-mesh-at-paypal-episode-364)
* Maggie Hays and Pete Soderling on DataEngPodcast talking about DataCouncil and building an intentional data culture [https://www.dataengineeringpodcast.com/data-council-data-culture-track-episode-365](https://www.dataengineeringpodcast.com/data-council-data-culture-track-episode-365)
* Mark Rittman talking on DrilltoDetail with Stewart Bryson, Keenan Rice, and Jake Stein about the Modern Data Stack past, present & future  [https://overcast.fm/+Iexdb0bH0](https://overcast.fm/+Iexdb0bH0)
* Tristan Handy and Julia Schottenstein talking to Justin Borgman about Starburst, TrinoDB, and data access patterns and querying [https://roundup.getdbt.com/p/ep-27-to-move-or-not-to-move-data](https://roundup.getdbt.com/p/ep-27-to-move-or-not-to-move-data)

What are your favourite episodes that you'd like to share with others?",2023-04-19 10:17:45
12i0nh1,Is ETL Developer still a legit and in-demand career? Or is it just housed under Data Engineering now?,"Hi, college student here, I’m currently interested in working in the Data space. One job that seems interesting to me is ETL Developer, however, many are telling me that ETL Developer is an outdated job, and that ETL development is generally in the domain of the data engineer nowadays. But on the other hand, there are others who tell me that most data engineers hate doing ETL, and that a lot of companies still have so much ETL work to do that they hire specialized ETL Devs on top of data engineers. I’m not sure if I’m interested in the entirety of data engineering. I’ve only shown interest in pipeline development and data 
movement so far, so I’m a bit lost as to what career direction I should take. Do you guys think ETL developer is still very much a legit role that is worth aiming for? Or should I switch gears to data engineering jobs?",2023-04-10 23:33:28
122qe2z,Is it correct that OLTP systems are highly normalized and OLAP systems are not normalized? Can someone please explain the reasoning behind it.,Noob here. I heard it somewhere and have been wondering why this ks the case.,2023-03-26 15:48:06
11x4g6x,I see a lot of courses on data engineering that also incorporate what some call analytics engineering. I don’t see many software engineer-data platform / data infrastructure/ data ops courses,"I I have experience building pipelines (mainly batch) data modeling (dbt, data vault ect ) and lightly managing infra (terraform for snowflake or AWS/gcp, docker) and setting up ci/cd (GitHub , circleci ). It just seems all the online course go heavier on python / sql etl dev that won’t get you a job as a swe-data platform or data ops engine or ml ops ; like no clear path or structure to those roles and a decent amount of gate keeping where like I have so many adjacent skills/ experiences.

Anyone have a good class or mentor to help me get to one of those roles that’s not just recommending general related topics",2023-03-21 02:39:10
11nuumt,"Suddenly I can't find that much of a ""meaning"" in DE","I have been working as a Data Engineer for more than a year now (since I graduated from university) and all of a sudden I can't find meaning in my job.

I feel that all I'm doing is taking data from some data sources, transforming them and loading them in another data source (I'd be tempted to say that we have a data warehouse, but we simply don't).

While I am in the midst of the action I seem to be enjoying myself, but when I attempt to make a connection between my job and how it does have an impact / contributes to something meaningful I feel desperate.

I'm not completely sure if it's the sector that the company I work at is involved (user indoor location validation) or it's the job itself. When drawing comparisons to other software-related things I've done in the past (e.g. solving a Vehicle Routing Problem) I feel that what I'm currently doing has less of an impact.

Could you share what it is in this profession (or hobby for some) that you find meaning at? Also, if someone at some point in their lives were at a similar situation, please do share your thoughts / what helped you keep going.

Not sure if it's the right place to post, but I'm so immersed in it that even the process of gathering my thoughts, transforming them to a somewhat coherent text and posting here seemed like an ETL to me.

Thanks in advance!

&#x200B;

\[EDIT\]: I'm trying to reply, but unfortunately my account is 1 day old and I should ask for approval each time. I have read each and every comment so far. Thanks for sharing your thoughts!",2023-03-10 17:12:58
11ckktt,How to break into Big Data Engineering jobs without having Big Data experience?,"Hey folks - a lot of the roles that I'm interested in happen to involve big data technologies and almost always has one or both of the following requirements:

* ""Must have experience working with datasets of Big Data volumes""
* ""Deeply versed in one of the following big data technologies: Presto, Spark, etc.""

I feel a bit trapped because I have 4+ years of data engineering (7+ YOE total) and I qualify for a lot of Senior Data Engineer roles, but I've had recruiters decline me because of not having real-world working experience with big data technologies (per their words). This is despite having read books on various technologies, doing small projects implementing and using them, etc.

I don't believe in resume-driven development at work so I'm curious how one can get out of this rut. Should I be applying to non-Senior Data Engineer roles that involve Big Data tech stacks, despite exceeding their YOE requirements? Or should I step up my side projects and fork over some money to replicate a real-world data system within AWS? Or am I striking bad luck with the recruiters?

*Disclaimer: And no, this is not about wanting higher salaries nor perceiving these technologies as better than others. It's mostly an interest of mine to break into this field and work with these volumes of data. I hope that makes sense.*",2023-02-26 15:55:57
17evkku,What’s your opinion on working for low paying F500 companies?,"This is for US roles.

It’s becoming apparent to me that there are a ton of F500 companies in banking, retail, construction, etc. that pay 130k for “lead data engineer” positions that require +7 years of experience and a bunch of different things. Not to mention the jobs are on-site, and expected raises are 3% yearly. 

I’m a mid level data engineer at a late stage startup. I make 130k with great work life balance, and work remotely from a MCOL area. My last job was at 115k remote as well with decent work life balance(I left to work with a cooler company/tech stack). 

I wouldn’t consider myself a top candidate either. I don’t have a quant background or crazy credentials. So I’m just curious, do you all consider these positions? Or are they mostly for people who need sponsorship, were laid off, trying to escape a terrible job? Etc.",2023-10-23 21:24:01
16lkxuz,What’s the most “against best DE practices” things you have ever done at your job?,"For example, I once manually deleted a partition of a table by going to the AWS console and permanently deleting an S3 folder. What’s something similar that you have done?",2023-09-18 03:48:21
1661eg1,There's a problem with data engineering and nobody seems to realize it,"Hi everyone,

I am a data engineer with 5+ years of experience, EU resident.

I don't really know how to say it without offending/shocking anybody, but I think the data engineer job has lost most of its prestige and pursuing a career in it is not worth it anymore, it is basically undergoing a [deskilling](https://en.wikipedia.org/wiki/Deskilling) phase.

Back when I started, the data engineer had to do a lot of work, first they had to create a data infrastructure, either on the cloud or on premise. Once all of this was ready, it was time to develop ETL pipelines on top of it and finally to support data scientists/analysts in their job.

&#x200B;

Fast forward 5 years and I really don't understand why media keep saying that DE is still one of the hottest jobs in hight demand: MDS removed the need to create infrastructure from scratch, data integrations can be done with one click using Fivetran or similar tools, data processing is just writing a SQL query on Snowflake.

Assuming this is still a hot job, the inevitable consequence is that you need few DEs in your company if data integration and ETL have been made so easy.

You could argue that now the data engineer can focus a lot more on adding value, by having more time to perform activities related to data modelling and data quality, but let me say that while this is all true, it also means that unless companies have infinite use cases related to data, 90% of the companies will be happy with just 4-5 DEs if not less.

&#x200B;

In addition to this, the job market is terrible since the end of 2021, and I suspect many companies hired DEs just because they were in extremely high demand, and now they don't know what to do with them.

&#x200B;

I am not sure if I am just discouraged by the fact I only see few open positions (seniority level doesn't matter, they decreased a lot in number), but this might as well become the new normal and you need to hold on the work that you have because finding a new one might become incredibly hard. In addition to that, wages will most likely be suppressed.

&#x200B;

&#x200B;",2023-08-31 04:51:28
15nxf1q,/r/Databricks has been relaunched as a public subreddit,"Given this is by far the subreddit that has the most discussions about Databricks (good, bad,  and ugly) ...)

Myself and a few other Databricks employees have reclaimed /r/Databricks which was previously private and made it public.

Feel free to come join us there to ask questions and discuss all things Databricks and the lakehouse!

Or stay here and compare us to Snowflake some more, we love that.",2023-08-11 03:26:58
14wv6f6,Data Engineer isn’t really just data engineering,"So many people think data engineers are only responsible for building data pipelines.

But in reality, if you are doing a data lake project, you may also need to understand the cloud infra (VPC, IP, DBA infra, Terraform, K8s).

As a data engineer, I think being a cloud engineer is better than being a data engineer.",2023-07-11 15:56:04
14n7vya,Dumbest requests from end users or code you’ve seen.,"It’s definitely been a day so I wanted to poll the community, what are some of the dumbest/most frustrating requests you’ve received from end users or dumbest queries etc from coworkers? Can’t wait to hear these responses.",2023-06-30 18:17:24
13xlb49,UI for Apache Kafka - An open-source tool for monitoring and managing Apache Kafka Clusters - v0.17 release,N/A,2023-06-01 15:57:46
13h30py,NoSQL database use cases,What are some uses cases that require a noSQL database like MongoDB? I have been a data engineer for over eight years and never had to use one.,2023-05-14 04:56:53
138n3s4,"Overall, does Data Engineering provide a good work-life balance?",I also posted another version of this in r/Biostatistics. I'm currently deciding my next path in my career and I'm not ashamed to admit I value stability and good work-life balance over a higher salary (especially coming from a high stress career). Should I get an MS Biostatistics to be a biostatistician or an MS Data Science centered on DE (currently accepted but deferred to decide) to be a data engineer? I only want to work 9 to 5 and would like t to make at least six figures after some experience. Not sure if anyone has knowledge on how both fields compare to one another.,2023-05-05 13:58:12
124tspm,SQLMesh: The future of DataOps,"Hey /r/dataengineering! 

I’m Toby and over the last few months, I’ve been working with a team of engineers from Airbnb, Apple, Google, and Netflix, to simplify developing data pipelines with [SQLMesh](https://github.com/TobikoData/sqlmesh). 

We’re tired of fragile pipelines, untested SQL queries, and expensive staging environments for data. Software engineers have reaped the benefits of DevOps through unit tests, continuous integration, and continuous deployment for years. We felt like it was time for data teams to have the same confidence and efficiency in development as their peers. It’s time for DataOps!

SQLMesh can be used through a CLI/notebook or in our open source web based IDE (in preview). SQLMesh builds efficient dev / staging environments through “Virtual Data Marts” using views, which allows you to seamlessly rollback or roll forward your changes! With a simple pointer swap you can promote your “staging” data into production. This means you get unlimited copy-on-write environments that make data exploration and preview of changes cheap, easy, safe. Some other key features are:

* Automatic DAG generation by semantically parsing and understanding SQL or Python scripts
* CI-Runnable Unit and Integration tests with optional conversion to DuckDB
* Change detection and reconciliation through column level lineage 
* Native Airflow Integration
* Import an existing DBT project and run it on SQLMesh’s runtime (in preview)

We’re just getting started on our journey to change the way data pipelines are built and deployed. We’re huge proponents of open source and hope that we can grow together with your feedback and contributions. Try out SQLMesh by following the [quick start guide](https://sqlmesh.readthedocs.io/en/stable/quick_start/). We’d love to chat and hear about your experiences and ideas in our [Slack community](https://join.slack.com/t/tobiko-data/shared_invite/zt-1ma66d79v-a4dbf4DUpLAQJ8ptQrJygg).",2023-03-28 16:09:53
11g3w7v,Technologies you didn't expect to be so popular in the industry,"For me, it has to be (S)FTP. I would have never guessed that they would still be so popular and used for mission-critical data workflows.",2023-03-02 14:18:19
1aqsmoc,"When will ""we need to be on cloud"" statements stop with executive's?","I am at my 3rd company in my career that had executives release a mandate that ""we must move all of our infrastructure to the cloud""... Because ""it has faster integration""... And ""it will lower costs over *insert time horizon here*""... 

Then followed by those executives bosses saying... ""You promised me value with the cloud ... Show me... Because these bills are outrageous""...

Which is then followed by those executives (the ones who made the mandate and over promised to their superiors) laying people off because they realize they can't quickly unravel their cloud dreams (projects in flight and halfway or more to cloud already) but they can save money by laying people off... 

Just a sad state of affairs. Cloud is not for every situation and to sell top level executives on costs savings alone is basically setting all the IC's up for failure because those savings are never truly realized and should have never been used as a selling point. It's really just a shame ... When does it stop?",2024-02-14 17:35:06
195tn9u,"After finally getting my dream job, I was switched to other role against my will.","I was hired as a consultant and started to work as a Data Engineer on a startup. A lot of work but I really enjoyed working on it. However, they decided to fire a entire team and put me instead as the developer of that project. When I opened the files, it is just a mess. Spaghetti codes, multiple logics that just gets overwritten and no documentation at all. I discovered that the logic didn't work at all and I have been mostly finding issues with the code.

I asked the business analyst to help me understand all the logic and he doesn't know either. There is no proper documentation of the reqs of the project (the original business analyst was fired for this too), they changed the project owner since the original moved to another company and this new guy doesn't know either what to do.

Well, I suggested to spend time to try to understand the project itself but got called off by everyone since they need this done by February. It just happens that I am in a trial period in this job and it also ends in February. They are expecting me to finish this one but honestly I feel lost since there is no one to contact about the full scope of the project, I am working blinding and doing some ""screaming"" qa.

By the way, the original stack was Python Pandas, DBT, Airflow, AWS, Postgresql and they were planning to move to Snowflake, and now in this project is just plain SQL for MariaDB. The fricking test to enter the company were hard, I studied a lot of these data engineer tools to enter, even Kubernetes, Docker, Terraform and now I am just a SQL developer. Honestly, I don't mind working with SQL but because the incompetence of other people, they switched me over since they are out of people and they are no plans to hire a new team for this. I just hope that I can recover back my original role.",2024-01-13 17:49:57
17b1zaw,My company is implementing stack ranking,"I work at a company with 200+ data engineers, and a new Head of HR was brought in a few months.

They announced last week that in the Q1, we will be measuring individual performance using stack ranking with the intention to “support equitable and diverse career growth.”

Not sure if I’m crazy, but this is gonna be rough, right?  Everything I’ve read about it suggests that stack ranking creates strong incentives that outright harm collaboration (which is a current strength at my company).",2023-10-18 21:31:50
164wn9z,How many women are on your team?,"Obviously anecdotal, but just from interviewing a few years ago and seeing applications now, feels like there are hardly any women in this field. I know we’re in the minority, but I’m the only female on my data engineering team and I’m just curious if this is the case for many others as well? 

For background: transitioned to DE ~2 years ago from analytics. Completely unrelated STEM undergrad (no grad school)",2023-08-29 22:30:39
159cf8h,"Are data engineers taking 4X the amount of time to build, or working 18 hours a week maintaining data pipelines?","In a recent Data Engineering Podcast episode I heard that there are data engineers and architects working on distributed data pipelines processing large amounts of data and a bunch of tools including distributed messaging systems or pub-sub tools like kafka, kinesis and data processing like spark, flink among others to deliver analytics and predictive algorithms.

There were a couple of anecdotes that made me curious.

First one was that in a well known networking and tooling company they planned for 9 months to setup their analytics stack and ended up taking 3 years to build something that could do the job but not scale.

Second one was that these systems were so complex that the data orgs were often working 18 hours a day 7 days a week to operate and maintain the data stack.

I can relate to the first claim, and partially to the second one. I want to learn from the community.

This community is 117k strong, therefore a pretty solid sample size. I want to learn from your experience about the veracity of the anecdotes. Here are a couple of prompts/questions for you to share.

* What is the gap between planned work and the actual time taken to build the data pipelines that you have worked on?  
9 months plan and 36 months delivery is a 300% additional time required over the planned effort, I can't see that in startups, growth stage companies, even scale ups. What is your experience?
* On an average what would you average hours per week to support the data pipelines operations?  
18 hours a day 7 days a week amounts to 126 hour weeks, I can attest to 70 maybe even 80 hour weeks with on-call issues. is 126 hours per week, for real?",2023-07-25 15:54:44
14fv52f,How do you do documentation?,"My team is working on a complex ETL pipeline with 10+ sources, manual mappings, lots of business definitions and a set of final datasets for end user.

Lots of dimensions and facts needing to be processed differently from each sources to be correctly represented.

If I ask the engineer he says the code and the pipeline visualization is the documentation (visualization are nodes where transformation code is saved).

What would be the best way to create a documentation that can be shared with business? Is there ""one"" system or will there be a bunch of several artefacts, UMLs etc needed?",2023-06-22 06:05:46
14cs98f,What other communities do you follow for DE discussion?,"I'm likely discontinuing my use of reddit when Reddit Is Fun stops working, mostly because Reddit will no longer be fun. As my life has become busier as I've aged, Data Engineering has been the last bastion of why I stick around anyway.

So I ask, which other communities do you guys follow that fosters high quality data engineering discussions?",2023-06-18 19:05:27
115lbja,Which are the highest paying tech frameworks or programming languages in Data Engineering?,I am trying to be as effective as possible to not to waste my time learning or doing staff which is not demanded or isn't paying well in DE job market. So would you advise anything which should be my best bet?,2023-02-18 18:08:11
1anp313,Landing My First Data Engineering Role Without a CS Degree in Europe,"Hello,

I got my first job as a data engineer recently without a CS degree in Europe and I want to share things I learned.

For the record, and this is an important context, I did this transition into DE during a full year without being employed, I don't have a CS Degree (a Master's in Law), I only applied for jobs in Paris and I'm currently working there. I'll try to fill in all the details that I would have wanted to know beforehand when I started this journey.



# 1) Summary of my data engineering journey

Before getting interested in data engineering specifically, I knew a bit about programming, I did some personal-business-ish projects involving Python, Javascript and websites.

Last year, I started educating myself more specifically on the data engineering field. First, I took an online bootcamp for data engineering (Datacamp in the ""Data Engineer with Python"" path), then two small DE projects hosted in and using GCP tooling (presented on my github), then the GCP Professional Data Engineer certification. After this GCP certification, I felt a bit more confident in my abilities so I started applying for jobs for \~2 months. I got interviews with 4 companies and among those one going to the third round, but all ended in rejections. Then I completed 2 Coursera courses on Machine Learning (those from Andrew Ng, mostly for fun, and also because it was ""in the air""), then two certifications in a row from Microsoft, PL-300 (Power BI Data Analyst) and DP-300 (Azure SQL), then two personal Data Analysis projects applying what I saw in those 2 certifications (also added to my github). Just after this, I began \~2 months of applying to ""data engineer"" AND ""data analyst"" job postings, and I got my job as a data engineer, roughly a year after I started this journey into data engineering.

All this education phase took a lot of my time. Overall, in the 12 months that I spent trying to get into the data field, I spent 8 months educating myself, and 4 months actively searching for a job. I would do only one or the other without mixing the two.

During the 4 months of active job searching, I applied to 274 jobs, around 40% were ""data analyst"" job postings and 60% were ""data engineer"" job postings. I focused mainly on LinkedIn and WelcomeToTheJungle (a french job posting board for startups and tech companies). Around half of those 274 applications were made through mass ""Easy Apply"" applications on LinkedIn, where you can quickly apply with just giving your Resume and maybe a few quick questions (how much years of exp with X tool, for example). The job application that got me my job offer was an ""Easy Apply"" one.

Overall, I had 11 companies contacting me, 8 actually started rounds of interviews, and at the moment of getting my DE job offer, I was interviewing with 3 companies. I cancelled my application for the 2 others after signing my job contract.



# 2) Things I learned

- A lot of bootcamps do not emphasize enough how important SQL is. I guess it's because Python is more trendy nowadays, and SQL seems like an old language. But oh boy how wrong this is. Data Engineering is about data. All the data in this world are inside databases. SQL is the unchallenged king language for querying databases. Not knowing SQL as a data engineer candidate is suicide. As an aspiring data engineer, everytime you watch a tutorial on machine learning using Python (as I did myself \^\^), you should repent and flagellate yourself for not practicing your SQL.  But for the rest of the tools, online bootcamps + some Cloud-related courses seem to do a pretty good job at describing the typical type of tech stack there is out there. When applying I was already familiar with a lot of tools and practices that were mentioned in the typical DE job postings.

- I learned that I was maybe a bit too infused with Twitter and Reddit tech culture, and that it's not like that in real life for most of the people that will hire you. I don't know how it is really in America overall, but, from my perspective, if you live in Europe, go from the principle that everything you hear from America's tech culture, or even worse, Silicon Valley tech culture, needs to be taken with a big grain of salt. In my experience, companies I encountered did not seem ready to embrace the ""degrees are irrelevant, show me what you do"" kind of mood that you see a lot of tech bros promote. I had a senior person from a tech consulting firm tell me in an interview ""well you know, our clients freak out when we put people without an Engineering Degree on their project"". Well, bad news for me.Also, if you're like me and you don't have a CS degree, and your resume go through a HR department that is responsible for the hiring, don't expect HR to do you any favor. If you fuck up, managers will ask ""who chose this candidate"", and the HR person will be responsible. HR people usually don't want to take this risk, so they will usually choose the most reasonable and less risky junior candidates, and usually those have a CS Degree. So if you don't have a CS degree, you might have more chance for a potential manager/coworker actually reading your resume if you apply to smaller companies/startups, where the stakeholders are hiring directly themselves, without the HR filter. Of course, hiring an atypical candidate is always more risky for everyone, but keep in mind that the corporate environment is usually geared towards less risky options since numbers are bigger. Somebody who is hiring in a smaller company might have the time to actually look at you more precisely and ""feel"" you more, and the risk of an atypical profile might be dampened by this from their perspective. Whereas in a corporate environment, where HR could hire hundreds of people each year, things need to go forward and you can't take the same time to gently analyze every aspect of someone's personality. Risk need to be minimized, so usually weird profiles go to the bin just in case, and things like which college you went to, internships, recommendations, etc, all the things that ""look good on a resume"" usually prevail.

- Companies that are run by people outside of the social media tech culture mostly do not know the nuances of the data role definitions as we see it on this subreddit, on Twitter or on Youtube. Most people are not hooked on the latest social media tech trends, and the subtle nuances that you could see here and there between what is a ""BI Engineer"" or a ""Data Engineer"" or a ""Data Analyst"" or a ""Back-end Dev"" really just fly above most people's heads. Companies, especially the biggest and oldest which happen to employ a lot of people, have their own internal names for specific roles, missions, systems, etc... And usually that will not align with what you see on social media. HR department know that some titles are now trendy and they use it to attract candidates. But never forget that there will always be a difference between the social media ""standard definition"" of a given data role and what your actual job will be like. Companies have systems to be taken care of. They don't care about the internet's opinion about what is a ""data engineer"". Just go from the principle that you will be working around a database of some sort, using one or several of the data-related tools that exist in this universe. What truly matters in order to know what you will actually do is your team, where you are in this team, and who decides who does what. Not the actual job title on your contract. Companies use ""data engineer"" in their job postings and their job contracts, but in the end they just put you where they want. So just remember to ask specific questions about the actual job you will be doing, and people you will be working with, because taking for granted that companies know what is a ""data engineer"" is a very risky bet.

- About the kind of job postings that answered my calls, contrary to what I thought was going to happen, I received 0 (ZERO) responses for all the ""Data Analyst"" job postings that I applied too (around \~100). Before going through this experience, according to the many takes that I got from people working in the DE field on this subreddit and outside (including the youtuber I talked about in a previous post, the data janitor), I really came to the conclusion that Data Analyst was an entry level job, the easier step to take for somebody that would like to get into data engineering later on. Well, I don't know how much my experience is generalizable, but I experienced the EXACT OPPOSITE of what everyone was saying: the Data Analyst job postings completely ignored me, and the only answers I got from companies were for the Data Engineer job postings. The exact reverse of what I expected.

- About the applications themselves, contrary to the advice given by my own employment counselor (who told me I needed to focus on ""quality applications"" instead of mass applying), mass spamming job applications with the 'Easy Apply' option on LinkedIn (only my resume and no cover letter) proved very fruitful to me. My DE job offer actually came from one of those mass applications. Most of my interview rounds also came from mass applications where I didn't submit any cover letter. Over the 11 companies that contacted me, I made a cover letter for only 3, given that 2 of those were unsolicited applications because I found the companies cool (I made 24 unsolicited applications overall).Also, related to this, contrary to my expectations again, mass applications did not lead to me having contact with random shitty companies or whatever. The few that contacted me actually seemed pretty nice and quite open to my ""atypical"" profile. The one company that offered me a job was actually the one company for which I remember thinking during the interviews ""wow I feel very in sync with those guys, I would love to work for them"", even though it was one of those undifferenciated mass applications. So yeah, do not underestimate how mass applications can also increase the chance that the ""right company for you"" can find you. Whether your like it or not, a part of your personality is already engraved in your resume, and that might be enough for employers to distinguish the candidates who could fit inside their company or not.





This journey to data engineering allowed me to learn a lot in this last year. Some aspects were as I expected them to be, and some aspects were surprisingly completely the opposite of what I was expecting.

Also, remember to not generalize what happened to me here, as a lot of what I experienced could be linked to my particular context (a Master's but no CS degree, Paris/France/Europe, had a full year to work on my career change, current tech job market, luck, etc).

I hope this post can be useful to other aspiring data engineers seeking information about how to get into this field, especially to my fellow europoor tech bros :p

Feel free to ask questions even if you see this post long after it has been posted. I knew I looked at a lot of old posts when I was craving for advice not so long ago.

And for the final advice, as I experienced it myself, do not trust too much what people say on the internet lol",2024-02-10 20:07:05
19ab6dx,How true is this thing of getting “blacklisted” for wrong titles on resumes?,N/A,2024-01-19 05:04:06
17blv3d,A career in Data Engineering or Big Data,"Hi everyone,

I am an experienced ETL developer with over 10 years and I am skilled in using tools like SSIS, Informatica and Python for ETL development.

I am currently trying to Advance my career in the field and I was wondering which path to take. My major consideration is one that will still be in-demand for the 10 years or more and also one that has good pay grade as well as WFH opportunities.

I am considering going into full blown career in data engineering (pipeline design, a good programming and server experience as well as  Containerization. **OR** one that focuses more on BIG DATA development.

I know big data has been the buzz word since some years now but I am a bit concerned that its gradually not as loud as it used to be.

I will appreciate your advise so much.",2023-10-19 15:29:10
14a8ymy,My website with summaries of data engineering books and courses,"Hello guys,  
I want to share my website [https://technicalsummaries.com/](https://technicalsummaries.com/).

You can find there summaries of books:

* `Fundamentals of Data Engineering` \- Joe Reis, Matt Housley.
* `Designing Data-Intensive Applications` \- Martin Kleppmann.

Also, I am working on a summary of the course `Database Systems` by Prof. Dr Jens Dittrich, which is very underrated.

You can comment at the bottom of every chapter or edit the content.

If you have any suggestions regarding my page, I am more than grateful to take some advice.

I created the page because I can learn much more effectively when I need to structure the text I've just read, and also, it was mainly for my students, who are too lazy to read the whole book :D

Thanks.",2023-06-15 18:06:58
143f4n0,Does anyone learn without reading books?,"I feel dirty even asking this since whenever someone posts ""How do I learn about X?"", someone inevitably posts ""Go read 'The Big Book of X' by [very important person]"". It seems to be a very popular way to learn.

I have never once read any book related to data. I've read some blogs, articles, websites, watched some videos, but largely learnt from the others around me or through tackling the challenges I encounter in my job and projects on an ad hoc basis. In some cases, I've learnt through a few courses. But never once have I read a technical knowledge book.",2023-06-07 14:43:20
13auxhk,What's the best way to ELT from on-prem sql server db to snowflake,"What's the best way to load delta records from on prem sql server db to snowflake?

We are planning on ADF copy feature which seems use blob storage as intermediate stage - which I feel may cause performance issues, so are there any better ways people do? Is there a way to directly load to snowflake?  Delta extract would be based on record update date.. and need to cover 100s of tables.. with a hourly job

.",2023-05-07 16:13:27
12vl2p5,"Am I ready to be a ""Lead"" Data Engineer?","I may be getting an offer soon for a Lead Data Engineer position, and the more I dwell on it, the more daunting it seems. I have 5 years of building pipelines with everything from Sqoop, Nifi, Python, Informatica, and MySQL but I'm not sure I'm up to the task of what they need. On the other hand, every Lead Data Engineer started somewhere, I guess.

Here's the job description if you're morbidly curious:

[https://careers.auroramj.com/job/Ontario-Lead-Data-Engineer-Remote-Onta/568769017/](https://careers.auroramj.com/job/Ontario-Lead-Data-Engineer-Remote-Onta/568769017/)

I've always wanted to work with Azure stuff, but my coding is pretty meh and my proficiency with Spark/Databricks is lacking. I would hate to sign on and then get canned because I can't keep up.",2023-04-22 22:03:34
12bblq0,Great expectations?,"Hi, DEs.

I recently tried using Great Expectations on my data processing pipelines in Databricks (written in PySpark). I’ve heard about this library a lot, but in reality seems not promising. 

For starters, nearly all methods there can take only one column as argument - so for example you cannot check duplicates by multiple columns without concatenation. Secondly, it seems like it consumes a lot of resources - once we included it in script, it began utilise 5 workers at once. 

My question is: how do you approach your data tests? Do you write them yourself or leverage GE/some other open-source framework?",2023-04-04 07:09:56
11mq8pg,"Do i need to write ""highly optimized"" queries as a DE? Also, is DW knowledge becoming obsolete due to cloud?","Im an undergrad that aims to be DE and occassionally i check relevant job postings

 I see frequently that as a DE i should be able to write ""highly optimized"" queries. Well for starters theres the query optimizer in any DBMS, that will translate your query to an equivalent RA expression that gives better performance. So, aside from using available indexes ofc, i dont understand what this ""highly optimized query"" even means???

 Also i have a question regarding DWs, i see a lot of postings demanding deep knowledge of the topic and i have no issues to study hard about it. But im afraid that all this effort will be meaningless due to cloud. What do you think?",2023-03-09 11:52:20
187rd9z,Are SSIS Shops Always This Bad?,"Picked up a contract role for DE role at a large healthcare company. 

They use MSSQL, SSIS, the works. The architecture is very confusing since everything is pretty much on prem. Servers are split out running multiple jobs in illogical ways. 

What’s really jarring, is the rate of failures they have. SQL server executes the SSIS jobs. Instead of having immediate alerts, there’s another job that runs every 20 mins to query failures in sql server and SSIS logging table. The logging is abysmal. It’s so bad they have an on call rotation where someone sits and watches an email queue to notify people of alerts and triage them correctly. 

I am unfortunately part of this on call rotation. You’re guaranteed multiple high priority alerts everyday and because of the poor logging and infra you have to do a lot of digging to figure out what happened. This is very different from other jobs where I used airflow and the failed task just shows the failed status and logging right there. 

I’m about to just quit this contract cause this org seems like a dumpster fire. Is this how all SSIS shops operate or is this just a bad one?",2023-11-30 19:51:35
17qzmfw,Data Engineer Struggling with Docker? Best Practices and Info?,So I'm looking for learning and  training for data engineers based around docker? I'm trying to understand best practices and use cases? The tools I've seen seem kind of straight forward? I scoured the wiki within this group and only found one article dbt and docker so not really what I'm looking for. How pertinent is it to know and be familiar with? Being within the space I've never touched it since we primarily work in the cloud and our tools have never required it. How much weight does it carry within the space as a skill and are there any SMEs I should follow? Thanks,2023-11-08 23:57:45
17crmrr,Has anyone used Microsoft Fabric?,"Yes, I know there's much better tools for DE, but I work for a large organization that wants to provide enterprise capabilities to the whole org (think 25K plus).  So I've been evaluating SaaS offerings that can provide analytic capabilities to users of all levels.

  
Does anyone have experiences with Fabric, and if yes, what are some of the issues you've faced?",2023-10-21 01:56:12
175p2bq,Why are there data engineering internships if there is such a lackluster amount of data engineer junior roles?,"Please no roast. Just curious on your thoughts. I read some previous posts, and people mentioned that it's riskier to let juniors access your data, because they could be prone to mistakes. And that transitioning into DE is much more doable. But that still doesn't answer my question",2023-10-11 21:11:31
16q95ok,What ETL platform do you use and what are the pros and cons?,"What ETL platform do you use and what are the pros and cons? My business intelligence team is looking for new options for our healthcare company of around 8,000 employees. Currently we use a platform called Diver from a small company  called Dimensional Insight and there is a lot left to be desired in the way of data flow and ease of use; it is not intuitive.",2023-09-23 16:44:44
14qi60z,Anybody use Alteryx,I work at a small firm that is about thirty years behind in tech.  All of our data is in spreadsheets scattered across a vast and confusing folder tree. We are looking to use alteryx as a no-code ETL and eventually creating an azure database. I noticed most DEs use dagster or airflow. Does anybody have experience with a no code tool like Alteryx? I worry no-code equates to no flexibility,2023-07-04 15:24:23
12dwoe3,"How has ChatGPT helped you in your DE job? First hand experience only, plz.","We're already accustomed to hearing people say ""this is how YOU should use chatGPT"", but I rarely see people say ""This is how I use chat gpt.""  


If you've got first hand experience using chat GPT to add legitimate value as a data engineer, I'd love to hear about it!",2023-04-06 20:21:16
12cfomc,how much business knowledge do you *really* have,are you a python+sql monkey? are you basically a PM  and a DE as the same time? Share your experience in the comments!,2023-04-05 10:26:59
11tshrc,"Are most data engineers primarily working on improving the data warehouse compared to building extract-load pipelines, or vice versa? Will there be a trend pushing data engineers closer to analytics/business?","I fall within the former and have for several years between companies, as most of my work has been centered on building out a data warehouse environment and enabling the business to use their data. Recently have wondered if this type of experience is more common than the extract-load pipeline version?

The motivation of the question is to figure out where to take my career and how to spend my time on which topics to learn - for example, should I spend some time learning databricks or spark on my own, knowing most of my experience has been in GCP or Azure? Should I spend time on information retrieval? Or on information architecture as it applies to data/analytics? Do I get deeper into dbt?

What I am trying to understand is how to navigate DE going forward, and how to best spend my time when thinking of skills/education.",2023-03-17 14:22:50
11kdvkr,Insert data into DB best practice,"Lets say I have a python app that every day performs some data transformations and then loads the data into the warehouse (postgres). It loads parquet files using pyarrow and then using polars/pandas do some stuff and then it needs to save it to the db. 

What would be the best way to insert the data into the DB?

a. use pandas to_sql method

b. iterate over each dataframe row and send an insert statement

c. convert dataframe to e.g. list of dicts and send an insert statements (i suppose worse than b.)

d. asyncronously iterate over dataframe rows and send an insert statements

e. save dataframe to CSV and ?somehow import it into the DB

f. some better alternative?

Also - is it common to send the insert statements directly to the DB or should I use Kafka/RabbitMQ, send the data to the messaging system, create a consumer app that would take the data from queue and insert into the db?",2023-03-06 21:01:44
1ao6xbx,I don't understand the other post that said data engineering jobs have fewer than 20 applicants while data science openings have hundreds...,"The other day I came across a post essentially wondering why data engineering positions often had less than 30 applications after a week while data science positions have hundreds within hours.

&#x200B;

Yet any time I look on linked in, 90% of all data engineering positions have over 100 applicants (or clicks at least) within the first day...

&#x200B;

I'm wondering if I'm inefficient in my search? Essentially I look for jobs needing 4-7 years of experience, minimum 140,000 salary in the US. I look at remote (which obviously get lots of applications) but even when I look at hybrid or in-person roles in various areas (LA, San Diego, San Francisco and Denver metro), it's pretty much the same story. I sort by most recent and only look at jobs posted within the last 24 hours...

&#x200B;

So I guess what I'm wondering is, is the post I read totally out of touch with reality, or am I doing something wrong in my search, and if so, what should I do differently?",2024-02-11 12:31:14
1afnd1d,What Dagster Believes About Data Platforms,N/A,2024-01-31 17:31:33
198w13u,The Ultimate Guide to Unit Testing With dbt,N/A,2024-01-17 13:01:34
17hicz3,Data Engineering is more stable and secure than Machine Learning Engineering?,"I have held both positions over the last three years. This is only my personal take on the topic but I'd love someone else's perspective. 

I worked in a mid sized company as a Machine Learning Engineer. We were building models for big tech and big pharma. The tech stack was great and the team was very talented and decorated. They built quite sophisticated models. But the demand just wasn't there. People were still using dashboards for analytics. There was no appetite for predictive analysis or classification. I tried my best to serve the models so the clients had minimal to no pain in querying them. But the models just weren't taken seriously and the team was dissolved because of lack of revenue. 

I then shifted to Data Engineering in a F100 company. All they do is use dashboards. Same story, no appetite for ML models. There's one small team doing ML, but they haven't produced anything useful in two years and no one cares. But the demand on a DE is huge! I've been involved in up to three projects simultaneously, all with different data requirements. The tech stack is meh at best. The data is half baked. The work is more about attending meetings than doing tech work. But somehow this is taken very seriously and DEs have a good reputation of being important here, even though the amount of 'engineering' required is a fraction of what I used as an MLE. 

What is going on? Am I completely mistaken in my expectation from the industry? DEs seem much more in demand than MLEs whereas MLEs arguably have a harder job engineering wise?
Do you think this is true i.e. DEs are more in demand than MLEs right now?
Do you think this trend will last a while?

Any comments welcome",2023-10-27 07:47:41
16z8h6l,How to efficiently load ~20 TiB of weather data into a new PostgresSQL database? Is PostgresSQL even a good option?,"Hi everyone,

I should preface this by saying I'm a complete noob but I'd like to learn about relational databases and SQL.

I'm playing around with ""historical weather data"" produced by ERA5 which provides e.g. hourly temperatures globally at 0.25 degree resolution. The problem is that the data stretches back to 1940 so that's roughly (83 years) * (24*365 hours per year) * (360/0.25 * 180/0.25 grid points) = 754 billions rows per variable.

I'm finding it very slow to copy the data into Postgres even using: https://www.psycopg.org/psycopg3/docs/basic/copy.html#writing-data-row-by-row

I thought PostgresSQL might be a good option, possibly with PostGIS and/or TimescaleDB, but thought I'd start with just Postgres.

Am I taking a bad approach here? Should I consider another kind of database? Or am I just not loading my data in properly?

I'm also worried Postgres won't compress this data well, but haven't played around with this yet (might be where TimescaleDB helps?).

Thank you so much for any advice you guys might have!

EDIT: I ended up writing a blog post about this and in this case psycopg3 + parallel copy cursor + fsync off is the fastest: https://aliramadhan.me/2024/03/31/trillion-rows.html",2023-10-04 00:24:55
16m7ozt,What kind of Python is used within DE?,"Hey guys,

I've been working within the Data Engineering world (at least that's what I believe) for 5 years, and there was never any use of Python in projects I've been involved in. Even though we use ADF, Databricks (Spark), Synapse Analytics, and so on, what scares me is that anytime I search through job offers it is always about an experience with Python (PySpark). I went through a lot of books regarding that, and did some personal projects, but it was always only about reading dataframes, doing some basic aggregations, grouping data, putting them in some kind of warehouse/lakehouse whatever, which could easily be done using SparkSQL (which we do in our pretty HUGE project, with a small amount of Scala to save DFs/tables to other sources like ADLS and warehouse..), though I don't feel like that's enough to make into a DE role with Python, can you tell my how does it really look like in huge DE projects, based on Python? Is it really just that, or it's more about implementing something special idk? I'm just lost 😭",2023-09-18 21:19:25
16k8qaw,DBT Users - What Single Package Could You Not Live Without?,"Just curious. I was thinking about the DBT packages I couldn't live without and Elementary is the the one. It gives you stored metadata about your models, runs, tests and test results. Slack notification on test failures. Various custom tests. A test dashboard.  I could live without dbt_utils, but not Elementary. Just a ton of useful stuff we'd have to create on our own.",2023-09-16 14:20:34
15yv4bt,Switching from Machine Learning to Data Engineering: What Sparked Your Change?,"I've been hearing about a growing trend of professionals transitioning from machine learning roles to data engineering. If you're one of them, I'd love to hear your story. What motivated your shift? Was it the nature of the work, the tools, the opportunities, or something else? Sharing your experiences might shed light for others considering a similar transition.",2023-08-23 06:10:43
15qrd4l,Working for a Hedge Fund,"As the title suggests, I’d love to know if anyone here works for a hedge fund. If so, how do you like it? How’s the WLB? And do the additional stakes add to the pressure?

Thank you!",2023-08-14 10:53:25
14dg280,How's the job market like right now? (June 2023),Title,2023-06-19 14:23:48
13hbvf6,Any fun domains in tech to work in as DE?,"Im currently a DE at a tech company working in the Marketing Optimization domain. I honestly like my company and we use all the modern data stack including a CDP and a rETL tool. Seems to be the best place to learn since that’s were all the money is. BUT i found this marketing jargon and stuff really boring lol. The only other “domain” in my company for DEs is in manufacturing and supply chain which seems even more boring imo lol.

Anyone here working in some interesting companies or domains? I honestly prefer financial data but refuse to work in a bank (again lol). Mainly looking for tech companies. 

Any info or experience would be highly appreciated!",2023-05-14 13:11:00
12buxtq,Project showcase: sample Data Lakehouse,"Hello everyone, 

I know projects are not that important but I have a to fun building them and I thought maybe someone else is interested in some of mine. 

So basically this is a very simple Data Lakehouse deployed in Docker containers, which uses Iceberg, Trino, Minio and a Hive Metastore. Since someone maybe directly wants to play with some data I have built an init container which creates an Iceberg table based on a parquet file in the object storage. Furthermore there is a BI Service pre configured to visualize it. 

I thought this project might be interesting to some of you who have only worked with traditional Data Warehouses (not that I am an expert with ""new types"" of storages) or want a more real life like storage, without paying a cloud provider, for your own Data projects. 

Here is the Github repo: [https://github.com/dominikhei/Local-Data-LakeHouse](https://github.com/dominikhei/Local-Data-LakeHouse)

Feedback is well appreciated :)",2023-04-04 20:10:43
18vubnp,Happy new year engineers and data enthusiasts!,"What are you planning to learn this year? Are satisfied how the last year ended? What are some your personal goals for this year?   


If you have any ideas to follow any Udemy courses. Feel free to drop the link here. I am planning to do a couple more courses on Udemy. ",2024-01-01 10:44:10
18ilbto,What was data engineering like circa 2011-2013?,"People who’ve been in this game since when the hype was really getting started in the beginning of the last decade, what was it like working in the first companies that tried to build big data pipelines? What was the stack you worked on? I know that Hadoop and NoSQL were very trendy at the time, but can you share some more typical technologies that you ran into a lot? Are some of them supplanted by now or are they mostly still going strong?",2023-12-14 23:02:03
182w1a0,Costliest Mistakes,"Hi,

I'm interested what were your costliest mistakes in de? Mine was setting up databricks within a vpc, forgetting to set up a S3 endpoint and then working with tb of data. I wasted around 4k.",2023-11-24 16:33:37
182pl4i,Data engineering manager,"If you are starting a new position as data engineering manager, what would be your thought process or actions to be successful in the role?",2023-11-24 10:55:05
175b3z7,"Will data ingestion ever be a ""Solved Problem""?","A brief history and current state of the data ingestion landscape, and some predictions of where things are headed: [https://kestra.io/blogs/2023-10-11-why-ingestion-will-never-be-solved](https://kestra.io/blogs/2023-10-11-why-ingestion-will-never-be-solved) 

Are we doomed due to competing commercial interests between SaaS vendors and system integrators?

&#x200B;",2023-10-11 10:49:16
16f9z4h,Looking for data engineering personal project ideas for technical growth?,"I feel stagnated at my current job. I'm an architect in the biotech industry, but my actual responsibilities are closer to a product manager if anything else. In the past 2 years I've been hardly exposed to any technical challenges and when I am it's always at such a high-level that I don't feel any growth or satisfaction when completing projects.

In general I want to leave the biotech industry for more technically mature companies like Google or Microsoft, but I feel like I need to step backward to step forward. I'm interested in joining a new company as an engineer, be exposed to new ideas and best practices, and then continue from there. However, because I don't work hands-on in my day job, it's a catch-22 to actually pass the interviews!

 I'd love to have hands-on experience with tools and technologies like Iceberg, Hudi, Trino, ect. that I don't get to use in my day job, and ideally I could build some sort of personal project around them. But data engineering is different than something like frontend engineering. There's more startup and cost associated with building a data project, and I'm not sure if I can actually master some of these technologies without working with *big data*. I'm curious if anyone here has any recommendations on potential data engineering personal projects? Or recommendations on switching industries when you feel overleveled in your current one :)  ",2023-09-10 19:59:01
163jj1f,What are the best open source tools for ETL?,"Hey Guys, 

Quite new to the system and I wanted some suggestions of any open sourced tools for ETL. Primarily the landing zone would be GCP. Any suggestions on this?",2023-08-28 11:27:39
15sktrv,Why data engineering?,I am curious why data science students choose a data engineering position rather than a data scientist position.,2023-08-16 09:22:07
15l9rph,Did I bomb my live coding interview? Lol,"I managed to code out say 80-90% of the questions presented to me . However, I did not narrate my thought processes while I code But the interviewer just let me stayed quiet, though he did prompt me to talk about different variations of the code after I finish my code, and he did say ""you're on the right track"" here and there. I would say there were only 1-2 tiny instances where I couldn't really answer his questions.",2023-08-08 06:28:31
154qdwf,“Technical” PO driving me and my team nuts,"Just a quick rant. Thankfully I don’t have to deal with this woman anymore since I’m changing companies soon. 

We had a legacy process of sql queries running on notebooks done by non-engineers. 

Now, the company decided that because this product brings a lot of money, it should be maintained by actual engineers. My team started working on this “migration” and now that almost everything is done, this PO wants to have access to change code, etc. She is questioning every engineering decision Ive made like if she knew. Aside from not knowing what git is, I got a funny (not really funny) passive aggressive question “why do you have everything starting with feature on github, better rename it to something we all know what it means”. This is just the tip of the iceberg lol.

I honestly feel sorry for my teammates that will have to deal with her after I’m gone. 

Hope no one in this sub is going through this lol.",2023-07-20 13:04:27
14vcujs,dbt seems like such a cool concept,"fyi I’m a DE intern right now, but I’m learning about dbt for the first time and it seems really cool so far

The main benefits I see are:
* Time saving
  * You don’t have to worry about any CREATE TABLE, INSERT INTO, or UPDATE syntax. dbt takes care of all of it for you and converts your sql statement into a new table, which leads to the next point of…
* Easiness
  * dbt seems really straightforward. there are “models” (which are literally just sql files with select statements in them) that you can chain up with other models to create transformation pipelines. Instead of having to programmatically do the T in ETL, anyone who knows sql can create the data that they need (provided all the data is present in the source table)

I’m working on a personal project that was using mysql, but am migrating to postgres so that I can use dbt. I figure it will simplify things for me on the data cleaning side, as a lot of the logic is heavily embedded in the script and not easily testable/accessible.

I may be getting ahead of myself as I’ve not had the chance to play around with it in depth, but the switch from ETL to ELT with dbt seems like such a good concept",2023-07-09 23:08:15
14gjlti,what the hell is a data officer ?,"Hey guys, i'm a fresh data science graduate, i applied to a job that states data engineer, but i was surprised on the interview invitation that it's a data officer role,

i have never studied or worked in such area, nor there's enough information online to read, i don't know what the hell they gonna ask me on the interview, i was thinking of backing off, but it looks like a very big company , plus, i need the experience in this stage,

the interview is in 3 days

is that enough to learn about this? and if yes, can you guys suggest any good sources about this? a last question, it might sound dumb, but, is it harder than other data science roles (analyst, administrator, engineer )?",2023-06-23 00:17:02
14dgupv,New DE Community & Looking for Mods,"Hey Data Engineers,

There was a great [discussion yesterday](https://www.reddit.com/r/dataengineering/comments/14cs98f/what_other_communities_do_you_follow_for_de/?utm_source=share&utm_medium=web2x&context=3) about alternative communities to Reddit where one of our mods did an impromptu poll to gauge interest in a separate professional DE community. Over 100 people signed up overnight so we believe it deserves a standalone post.

Summary of potential community:

* Verified data engineers (i.e. no bots, spammers)
* Networking and in-person events
* Advanced technical topics and industry news
* Familiar Reddit-style feed

It's not meant to replace this community but it could in theory act as a backup. We don't have all of the details yet and are still figuring things out which means we are also open to ideas as to what the community would really find valuable here.

If you're interested, please [**join the waitlist here**](https://tally.so/r/nGK7pe). If you know other data engineers in your area, please share with them as well because we would be letting people in once there is enough interest in a location.

\---

We are looking for 1-2 new mods to join our team!

r/dataengineering has grown tremendously over the past few years and could use an extra set of hands and ideas. It's a volunteer position and generally speaking we are looking for folks with technical experience and community management experience. The benefits are you get to meet a ton of interesting people and it's a great way to give back to the community. If you're interested, [please apply here](https://tally.so/r/3xj669).",2023-06-19 14:55:33
13plvgz,Does moving to a company that doesn't use cloud services hurt your career?,"In this job market, there aren't many high paying data engineering roles that are open in my area.  However, I found some roles that interest me and pay much more than what I'm making now.  The only thing is that they don't use any cloud providers.






I already have 5 years of experience and tons of experience using one of the major cloud providers, so would these types of jobs hurt my career.",2023-05-23 11:32:22
13jx5k6,Is data engineering job market currently saturated?,"Hi. I was talking to people from DS/DE and they confirmed that after layoffs market is really saturated with more people, and it is really hard to find a job. Is that true or is DE still has a shortage of data engineers?",2023-05-17 09:51:59
12xfvnc,Personal laptop required at health tech company?,"This is quite poor practice, no? I received a job offer from a health tech company and I’d be working on pipelines touching PHI data. They do not supply a laptop and I’d have to use a personal device. 

I’m quite surprised in general that a tech company wouldn’t provide computers but especially shocked that a health tech company would not. Just want to sanity check that this is unacceptable, or if I’m missing something (coming from a bigger company and looking for a startup role so I know there will be changes, but was not expecting this).",2023-04-24 13:30:29
12sf3kg,Why is Redshift a no-go?,"Im currently in a team that uses Redshift heavily (1PB). We also have an S3 datalake in parquet partitioned by dates that we query through Spectrum or Athena. 
Why does this sub hate Redshift?
Is the feeling the same towards RA3 Redshift?",2023-04-19 23:59:04
1apz616,Passed AWS Data Engineer - Associate !,"Just got message, from credly, that my badge is available!

A bit surprised, exam was heavy & messy. Lots of questions from adjacent specializations like DevOps, Data Analysts, Solution Architect, and actually not much about Data Engineering in general.

Now I feel like true Data Engineer @ AWS ;)

Check your emails!

&#x200B;

PS. Took exam 1/12, in last day where beta was available, no preparations, just wanted to see what is the true ""assessment"" of my skill, to challenge myself. Hopefully, it worked out.",2024-02-13 17:31:13
1af8ts7,Modern data pipeline for on premise,"I would like to hear stories of data pipelines running on prem. A lot of the tools now are from cloud, but there are industries that are not comfortable using cloud. I have some clients in manufacturing that have stable electricity but not stable internet since they are located at the outskirts of the city. Also, had a gig working in healthcare that requires all data processing to be on prem. I managed to do them with simple python scripts running on local machines. Mostly processing large amounts of csv and text logs from a NAS drive into an OLAP database. Interested to know if there are better implementations.",2024-01-31 04:10:52
19cxuav,Am I too fussy?,"Hi guys! seeking some advice on my data engineering career.

Long story short: in 3 years I have had 4 different jobs. I left all of them. I don't know if I am asking too much to companies or I am the problem.

Long story:

I am in my mid 20s. I left all companies due to different factors (no pay raise, bad projects, bad management...). My longest job has been 9 months (actual job). Recruiters keep sending me offers but, would jumping so much affect me in the long run?

Another question I have: why do folks stay at a bad company? I have seen tons of tech employees working at a company they don't like for years. Obviously I am not saying just leave, but look for opportunities. It really amazes me.

Those are my main points because I am starting to think that I am the problem and I should stay at a company although it doesn't have all the requirements I need...

Thoughts on this?",2024-01-22 15:14:12
18mvojy,6 Proven Steps to Build a Data Platform Without Breaking The Bank,N/A,2023-12-20 14:20:11
180ysrr,What is the work life balance like as a data engineer?,"Thinking of switching to data engineering. I currently make $91k in clinical informatics with amazing WLB. 

Wondering, what is the WLB like as a data engineer? How does it compare to SWE WLB?",2023-11-22 02:42:04
17ow2ku,Shunning 'low code/no code' solutions in a world where analytics warehouses provide great performance.,"Pretty often while browsing this sub I see people put down 'low code/no code' data engineering solutions, and promoting ETL processes running on engines like Spark as opposed to transforming data in SQL. The reasons stated are usually for being easier to follow software engineering practices with 'real' code.

However, it's undeniable that there exist cloud-based analytics warehouses that significantly outperform the traditional 'real code' frameworks when it comes to data transformation. I have transformations running in BigQuery where my testing has shown the costs of running on BQ to be about four or five times cheaper than with PySpark. I've read articles that back up this performance gap and it seems to be well accepted that tools like BigQuery significantly outperform traditional batch processing frameworks.

On a personal level I would prefer to be writing these transformations in Python - it's easier, it's cleaner, it's more versatile. With functions written in BigQuery we're usually ending up with long queries featuring a mess of joins that are hard to troubleshoot when things start going wrong.

But if I go to my manager and say ""I would rather write these transformations in Spark - it will be significantly more expensive but I will find it easier"" then I don't think it's going to go down well.

So I'd like to put it to this sub - if we have transformation processes that perform extremely well in BigQuery and our evidence shows that it would be significantly more expensive to move to other technologies, how could we justify moving away from a 'low code/no code' stack when it comes to transformations?",2023-11-06 05:52:49
15bycwi,A List of Database Certifications Here,"I see there are posts every week about database certifications and such.  I compiled a list late last year which I'm sharing below.  Please let me know of any that I have missed.  Or any dead links and wrong information for that matter.  Certifications are in the eye of the beholder; some employers value them, and others don't.

Here is a link if you want to bookmark my most recent list.

 [Database Certification List - Advanced SQL Puzzles](https://advancedsqlpuzzles.com/2022/11/18/database-certification-list/) 

**And below is just a copy and paste from the above link.**

Enjoy!!

\-------------------------------------------------------------------------------------------

[**Microsoft**](https://learn.microsoft.com/en-us/certifications/)

Microsoft has unfortunately sunsetted its SQL Developer focused certifications ([70-761](https://learn.microsoft.com/en-us/certifications/exams/70-761) and [70-762](https://learn.microsoft.com/en-us/certifications/exams/70-762)) and is focusing on role-based cloud certifications. 

* [DP-900: Microsoft Azure Data Fundamentals](https://learn.microsoft.com/en-us/certifications/exams/dp-900)
* [Exam DP-203: Data Engineering on Microsoft Azure](https://learn.microsoft.com/en-us/certifications/exams/dp-203)
* [Exam DP-300: Administering Microsoft Azure SQL Solutions](https://learn.microsoft.com/en-us/certifications/exams/dp-300)

\--------------------------------------------------------------------------------------------------------------------

[**Amazon**](https://aws.amazon.com/certification/)

Amazon offers the following database certification.

* [AWS Certified Database – Specialty exam (DBS-C01)](https://aws.amazon.com/certification/certified-database-specialty/)

\--------------------------------------------------------------------------------------------------------------------

[**Google**](https://cloud.google.com/certification)

And let’s not forget about Google and their cloud platform.

* [Professional Cloud Database Engineer](https://cloud.google.com/certification/cloud-database-engineer)
* [Professional Data Engineer](https://cloud.google.com/certification/data-engineer)

\--------------------------------------------------------------------------------------------------------------------

[**Oracle**](https://education.oracle.com/certification)

Oracle has numerous certifications ranging from high availability to administration to development. Here are a couple that I recommend. The 1Z0-149 is absurdly difficult, btw.

* [Oracle Database SQL Certified Associate Certification (1Z0-071)](https://education.oracle.com/oracle-database-sql-certified-associate/trackp_457)
* [Oracle Database PL/SQL Developer Certified Professional (1Z0-149)](https://education.oracle.com/oracle-database-pl-sql-developer-certified-professional/trackp_OCPPLSQL19C)

\--------------------------------------------------------------------------------------------------------------------

[**MySQL**](https://education.oracle.com/certification)

Oracle also offers MySQL certifications as it purchased Sun Microsystems in 2010.

MySQL is free and open-source software under the terms of the GNU General Public License, and is also available under a variety of proprietary licenses. MySQL was owned and sponsored by the Swedish company MySQL AB, which was bought by Sun Microsystems (now Oracle Corporation).

* [MySQL 8.0 Database Developer Oracle Certified Professional (1Z0-909)](https://education.oracle.com/mysql-80-database-developer-oracle-certified-professional/trackp_MYSQLPRG80OCP)

\--------------------------------------------------------------------------------------------------------------------

[**MariaDB**](https://education.oracle.com/certification)

MariaDB is a popular open-source relational database management system (RDBMS) that was initially developed as a fork of MySQL by the original developers of MySQL. It was created in response to concerns over Oracle’s acquisition of MySQL in 2010 and its potential impact on the open-source nature of the MySQL project.

MariaDB offers a database administrator exam, but not a developer exam.

* [MariaDB Certification Exam](https://mariadb.com/wp-content/uploads/2019/02/mariadb-certification-exam_datasheet_1005.pdf)

\--------------------------------------------------------------------------------------------------------------------

[**PostgreSQL**](https://www.enterprisedb.com/)

PostgreSQL is a free and open-source relational database management system emphasizing extensibility and SQL compliance. Because it is open-source, there are no vendor certifications, but there is a company called [EDB ](https://www.enterprisedb.com/)that offers solutions, training, and certifications for PostgreSQL. Their [certifications](https://www.enterprisedb.com/training/postgres-certification) appear to be focused on the DBA side.

* [PostgreSQL 12 Associate Certification](https://www.enterprisedb.com/course/postgresql-12-associate-certification)
* [PostgreSQL 12 Professional Certification](https://www.enterprisedb.com/training/postgres-certification)

\--------------------------------------------------------------------------------------------------------------------

[**IBM**](https://www.ibm.com/training/credentials/)

DB2 is a set of relational database products offered by IBM that traces its root all the way back to the 1970s. Currently IBM appears to be withdrawing many of its DB2 certifications and issuing new certification exams. The following appears to be the only DB2 exam currently offered, which is more DBA focused.

* [Exam C1000-122: Db2 12 for z/OS DBA Fundamentals](https://www.ibm.com/training/certification/C8003803)

\--------------------------------------------------------------------------------------------------------------------

[**Databricks**](https://www.databricks.com/learn/certification#certifications)

Databricks offers an associate and professional level data engineering certifications. These are great resources for understanding the product features.

* [Databricks Certified Data Engineer Associate](https://www.databricks.com/learn/certification/data-engineer-associate)
* [Databricks Certified Data Engineer Professional](https://www.databricks.com/learn/certification/data-engineer-professional)

\--------------------------------------------------------------------------------------------------------------------

[**Snowflake**](https://www.snowflake.com/certifications/)

Snowflake is a fully managed multi-cluster shared data architecture platform that capitalizes on the resources of the cloud. The SnowPro Core certification highlights the product features the best.

* [\[COF-C02\] SnowPro Core Certification](https://learn.snowflake.com/courses/course-v1:snowflake+CERT-SPC-GUIDE+B/about?_ga=2.109990149.1818508944.1668788648-1953636227.1655820550)

\--------------------------------------------------------------------------------------------------------------------

[**Teradata**](https://www.teradata.com/University/Certification)

Teradata (formed in 1979) provides cloud database and analytics-related software, products, and services.

* [Vantage Certified Associate Exam 2.3 (TDVAN1)](https://www.teradata.com/University/Certification/Vantage-Certifications/Associate-Exam-2-3)
* [Vantage Data Engineering Exam (TDVAN4)](https://www.teradata.com/University/Certification/Vantage-Certifications/Data-Engineering-Exam)

\--------------------------------------------------------------------------------------------------------------------

[**MongoDB**](https://university.mongodb.com/certification?_ga=2.155916475.143515463.1668790358-1726176162.1668790358)

MongoDB is a source-available cross-platform document-oriented database program. Classified as a NoSQL database program, MongoDB uses JSON-like documents with optional schemas. MongoDB is developed by MongoDB Inc. and licensed under the Server Side Public License which is deemed non-free by several distributions.

It appears MongoDB offers certifications tailored to various languages like C#, Java, Python and Node.js.

* [MongoDB Associate Developer Exam](https://learn.mongodb.com/pages/mongodb-associate-developer-exam?_ga=2.155876411.143515463.1668790358-1726176162.1668790358)

\--------------------------------------------------------------------------------------------------------------------

[**SAP HANA**](https://training.sap.com/certification/)

SAP HANA (High-performance ANalytic Appliance) is a multi-model database that stores data in its memory instead of keeping it on a disk. There are a number of HANA certifications that you can choose from. The following appears to be the most SQL focused.

* [SAP Certified Development Associate – SAP HANA 2.0 SPS05](https://training.sap.com/certification/c_hanadev_17-sap-certified-development-associate---sap-hana-20-sps05-g/)

\--------------------------------------------------------------------------------------------------------------------

**Below are a few vendor neutral certifications that you may be interest in.**

\--------------------------------------------------------------------------------------------------------------------

[**CIW Database Design Specialist**](https://ciwcertified.com/ciw-certifications)

CIW has vendor neutral IT certifications focusing on web professionals, but it does offer a Database Design Specialist certification. This certification focuses on concepts such as the relational model, relational algebra, design, modeling, and the SQL language. The [study guide for the exam](https://www.amazon.com/Study-Guide-1D0-541-Specialist-Certification/dp/1941404073) is a great encapsulation of many database concepts that we should all know.

* [1D0-541: CIW Database Design Specialist](https://ciwcertified.com/ciw-certifications/web-development-series/database-design-specialist)

[**ICCP**](https://iccp.org/index.html)

The Institute for the Certification of Computing Professionals (ICCP) is a non-profit (501(c)(6)) institution for professional certification in the Computer engineering and Information technology industry. It was founded in 1973 by 8 professional computer societies to promote certification and professionalism in the industry, lower the cost of development and administration of certification for all of the societies and act as the central resource for job standards and performance criteria.

Here are a couple of their certifications that may be of intetest

* [Certified Data Professional (CDP)](https://iccp.org/certified-data-professional-cdp.html)
* [Certified Big Data Professional (CBDP)](https://iccp.org/certified-big-data-professional.html)

[**DAMA International**](https://cdmp.info/)

Certified Data Management Professional (CDMP) is a globally recognized Data Management Certification program run by DAMA International.

This exam is centered around [DMBOK ](https://www.dama.org/cpages/body-of-knowledge)and is geared more towards Data Management and Data Governance.

* [About CDMP – Certified Data Management Professionals](https://cdmp.info/about/)

\--------------------------------------------------------------------------------------------------------------------

**Below are a few educational websites that advertise certifications, but these (most probably) do not meet the stricter guidelines of the above certifications. Regardless, they may be a good option for students beginning their learning path.**

\--------------------------------------------------------------------------------------------------------------------

[**W3 Schools**](https://campus.w3schools.com/collections/course-catalog)

W3Schools is a freemium educational website for learning coding online. Initially released in 1998, it derives its name from the World Wide Web but is not affiliated with the W3 Consortium. W3Schools offers courses covering all aspects of web development.

* [Certified SQL Developer](https://campus.w3schools.com/collections/course-catalog/products/sql-course)

[**Datacamp**](https://www.datacamp.com/certification)

DataCamp is an online learning platform that helps students build data skills at their own pace.

* [Data Analyst Certification](https://www.datacamp.com/certification/data-analyst)
* [Data Scientist Certification](https://www.datacamp.com/certification/data-scientist)

You have reached the end! Happy coding!",2023-07-28 14:03:04
14ql6jc,I created an interview preparation Trello template with technical & behavioural questions,"Link: https://trello.com/b/Rvw8Jygt/interview-preparation

I have been looking for entry level or junior data engineering roles, and have been using a Trello board to help with interview preparation.

It contains lists of common interview questions, technical questions, and behavioural questions.

I’ve tried tagging behavioural questions based on attribute it’s measuring (e.g., conflict management) so you can filter by these.

For the technical questions, these are a combination of questions I’ve been asked, or questions you could be asked if applying for a data engineering or analyst role.

For each card/question you can write potential answers in the description field.

You can obviously customise it however you like, but hopefully is useful for some of you",2023-07-04 17:19:37
14fxs4p,Software engineering practices that translate well to data ingestion?,"Not everything software engineers do translates well to data engineering, especially data ingestion, IMHO.  


But I particularly like the practice of **""keeping code changes small and simple, and getting them into production fast by testing & automating""** (if you can call this a practice) - that, in my experience works pretty well in **data ingestion**.

Here's how I'm going about it when working on data pipelines:

1. Ingest all columns for a new table, but don’t ingest additional tables unless necessary. Keep your ingestion code simple.
2. Prefer log-based ingestion > full table > key based. Log-based is by far the simplest duplication method—no need to hand code anything.
3. Always assume duplicates in all data tables. If you don’t want them, add a test to find problems before a dashboard user does!
4. Run complete tests on every change. Trigger your dbt models after the ingestion run to ensure they still work.
5. Have a prod-like environment for every developer, e.g., a {user\_prefix}\_data snowflake schema, to work fast.
6. Run everything through CI/CD. Why test by hand if a machine can do it for you?

&#x200B;

Do you have more ideas? Practices that translate well to data ingestion or data in general? Thoughts on those ideas?  
",2023-06-22 08:35:00
12wsatu,Delta Lake without Databricks?,"I understand that Delta Lake is 100% an OSS, but is it *really*? Is anyone using Delta Lake as their storage format, but not using Databricks? It almost seems that Delta Lake is coupled with Databricks (or at the very least, Spark). Is it even possible to leverage the benefits of using Delta Lake without using Databricks or Spark?",2023-04-23 22:15:31
12q5bi3,From PySpark to Scala,I’ve used PySpark for years. I’m just getting into Scala and I already understand functional programming. What are good resources for going deep into Scala and extending my Spark capabilities via Scala?,2023-04-18 01:32:29
11oi5p9,better career prospects of AWS vs GCP vs Azure cloud engineering?,"Hi folks!
Recently, I have been seeing more and more comments that AWS is not the best cloud career.  At my work the infrastructure team said it's better to consider azure, but I think it's because of our connection to the Windows environment. 

What are your thoughts  is the era of aws really passing away?",2023-03-11 11:45:21
1ah8nki,How can I achieve 5NF?,"Im trying to achieve 5NF, I believe it’s at 2NF at the moment, I’m getting different answers from google and chatgpt, but it’s saying since source_name is dependent on source_id it’s in 2Nf ,  and it talks about how their can’t be any join dependencies for it to be in 5NF I don’t understand can someone help ?",2024-02-02 17:08:33
194q4yk,How clean is your code?,"I’ve recently joined a data engineering team after a few years as a web engineer. I am not and expert but know the basic design patterns and principles. The team I joined writes really bad code.. but to be honest I’m not sure it matters? Most of these scripts never need to be adjusted and will continue to move data. 

I’m curious what coding standards you implement in your code. Are you using object oriented designs?",2024-01-12 08:22:45
18uote4,Is modern data stack relevant for small data orgs?,"I am looking to build the data/analytics function in a non-tech/data organization. We are not a startup, I have an on-prem Oracle ERP database that has about 25 years of activity in it (\~ 500GB). We have a few cloud-based enterprise apps (CRM for instance) that are cloud-based that we use but up to now we've not tried to extract data or do any integration. I've been tasked to change that. Operational data is mostly done via Access queries on the OLTP ERP database and shared Excel files. I just started using PowerBI to build out a couple dashboards for leadership but data governance/quality is a real issue. We have one DBA and myself to work on this.

My question is ... since we haven't moved to the cloud yet (although I see that coming at some point), and I see the need for data governance and something like a data warehouse/mart (i.e. modeled data, cleaned up snapshots over time, etc.) in order for us to make headway on providing data that is actionable, is it worth looking at modern data stack tools or should I really be looking at 90's era tools and techniques? Is something like Snowflake or Databricks reasonable for small data (never going to be more than 1 TB total)? Or should I just focus on building a local Postgres warehouse with Kimball/Inmon and ignore things like data lakes, Fivetran, DBT, Snowflake, and Airflow? Is there an in-between/hybrid approach that would work?",2023-12-30 20:25:11
18migud,Do you read emails?,"I've worked at a company for 4 years after 6 months I stopped opening outlook.

Simply put if it doesn't happen in jira it never happened, I've been so hard on this the rest of my team followed. PM SM and PO were all but hurt now they look at my tickets and are like oh ok I'll check tomorrow.

I put everything in my tickets from git commits, call recordings, results, error codes. I've done this to the point I'm smooth brained when asked questions on the ticket my answer is always what do the comments say?

Anyone else make their communication funnelled through on channel?",2023-12-20 01:36:26
18fusjx,What new to learn in DE now?,"I am currently unemployed for last 6 months. I am having 8 years of experience in ETL, data warehouse building, snowflake, spark and tableau. 

I also have bit of hands on experience in low code tools like fivetran, data modelling tools like dbt.

Due to the market conditions I am not receiving any job offers, so planning to upskill in the time being. I already have certifications in aws solution architect associate, snowflake snowpro core and Apache spark developer by Databricks. 

My question is, what new technology can I learn and get certified in to get an edge when the market is up? Should I try my hands on any NoSQL databases? Will learning kafka will open up my job chances? 

PS: my priority right now is landing up a job immediately once the market is up. I am continuously applying jobs, but not getting any calls. Seeing that there are not much calls, I am panicked and wants to learn something related to my field but can land me a job.",2023-12-11 13:41:09
17cckie,Platform engineers driving me nutz,"Some data scientists can be annoying (haha) but man, a crazy platform engineer really shortens your lifespan.",2023-10-20 14:27:20
17au1n7,Make SQL easy to understand with sqlvisual,"As an experienced data developer with 7 years of experience, I often find myself dealing with lengthy SQL queries. When I review my previous SQL code or take over projects from colleagues, understanding the SQL can be quite challenging. I believe you may have experienced the same frustrations.

&#x200B;

I frequently pondered how beneficial it would be to visualize the information within SQL queries. After all, a digram is worth a thousand words. With visual representations, we can easily identify joins, WHERE conditions, and GROUP BY clauses.

&#x200B;

After conducting extensive research, I developed SqlVisual, a tool that generates visual representations from SQL queries. As you hover over nodes in the generated graphics, the corresponding SQL code will be highlighted.

&#x200B;

You can try it out at [http://sqlvisual.net](http://sqlvisual.net) without providing schema information. Simply copy and paste your SQL code. I would greatly appreciate any feedback you may have.

&#x200B;

Thank you!",2023-10-18 15:53:10
16j1djb,Understanding Data modeling,"Hi 👋,
In 3-4 years of experience as data engineer I hardly came across data modeling at projects I worked. I want to understand - 
1. how data modeling is done?
2. Will there be many data models for each of the business requirements/stake holders? Or there would be a centralized one at company level.
3. How hard is it to implement data model, what is your experience on it

Thanks for answering in advance 🙏🏼",2023-09-15 02:34:55
15u5j2r,"First project, feel free to criticize hard haha.","This is the first project I have attempted. I have created an ETL pipeline, written in python, that pulls data from CoinMarketCap API and places this into a CSV, followed by loading it into PostgreSQL. I have attached this data to Power BI and put the script on a task scheduler to update prices every 5min. If you have the time, please let me know where I can improve my code or better avenues I can take. If this is not the right sub for this kind of post, please point me to the right one as I don't want to be a bother.  [Here is the link to my full code](https://github.com/bfraz33/CryptoETLLoad2.0/blob/master/CSV_Extract.py)

https://preview.redd.it/w7kr88rdorib1.png?width=977&format=png&auto=webp&s=108ad539340b5183e5052cb7b69964d9e9f74619

https://preview.redd.it/boul2vqdorib1.png?width=1481&format=png&auto=webp&s=e83e2f8e4ad05bea43935782f176646af9f6a398",2023-08-18 00:56:52
139ma7j,Airflow with Pandas,"Im new to Airflow, been studying for over a month now.
I know this is probably a very simple question but im kinda confused.

I have read that XCom is only made for sharing some metadata or flags between tasks and shouldnt be used for passing real data like for example a pandas dataframe.

Now what would be the best way to run a simple python ETL pipeline with pandas.

Should I let the tasks read/write from/to external csv files for example? Is this an efficient way? 

Another question where is the best place to put my python script? Because I also read that the DAG python file should only be used for the DAG configuration and not for real code, However, all the tutorials I found just define their python functions within the DAG file. Is this acceptable? Whats the best practice?

Thanks 😊",2023-05-06 12:59:09
12pwc42,Recommendations for a small DWH on Azure,"My new org is entirely MS/Azure based, and I am pretty much a data team of one. They have no real data warehouse to speak of, just a few MS applications in the cloud and PowerBI. 

Data volume is quite small (we have around 10k orders per year) and unlikely to grow significantly for at least a couple of years. Beyond Dynamics we have the normal suspects, socials, Google analytics and third party APIs.

Any suggestions for a good solution for a DWH? Budget is tight so I would like to use meltano to dump raw data into the DWH and DBT core to build some gold level tables to serve into PowerBI.

Would a simple MSSQL database do it? Or would a entry tier Synapse instance be a better place to start? Or perhaps even Snowflake?

Any experiences very welcome.

EDIT: amazed by the quality responses here, thanks so much folks.",2023-04-17 20:51:39
12gilc3,AWS Lambda — Local Debugging with VSCode,"Just published an article how to debug your AWS Lambda functions locally with VS Code ✨ Take a look and let me know if it was helpful!

[Article Link](https://erwinschleier.medium.com/aws-lambda-local-debugging-with-vscode-7e9da24a4dca)",2023-04-09 12:51:15
12fbvd5,Data Mesh,"Hello folks,

I know there would have been many discussions on this before, but being new to the field and going to work as a Data engineer now soon, I wanted to know is there any value in the concept of Data Mesh? Is it the next big thing? Or is this paradigm shift currently being used in the industry already?",2023-04-08 04:49:06
1293ctg,PySpark Interview Questions,"Hey everyone, I have my final interview for a company I’m in a loop for and it’s a PySpark coding interview.  I’ve never used PySpark before and I let the director know that and he said it’s fine.  It’s a 2 part interview (one part take home 2nd part is this week on zoom) for the take home part I’ve been asked to join a few .csv files together in a Jupyter notebook with pyspark, which wasn’t too bad with the help of google, and I achieved everything they asked for in terms of formatting etc.  the instructions say that the 2nd part will be related to my final table I made in the take home part.  I’m curious if anyone has any insight on what I might expect this week in my 2nd part. I’m familiar with pandas but the instructions specifically said to use Pyspark.  I would go through a PySpark book but I’m limited in time as the interview is so soon.  Any suggestions on what I could cram to study would be really appreciated.",2023-04-01 23:54:49
123ie4i,What exactly is the 'online' in OLAP and OLTP?,"> It is simply a remnant of olden times, when it was used in contrast to batch processing. ""Online"" here means ""interactive"", that is, requests to the database are processed as they come and responses are given more or less immediately, or at least as soon as they are available. Batch processing would collect requests into, well, batches, and execute them on schedule; responses would be given after the entire batch execution (e.g. next morning).
>
>Abbreviations OLAP and OLTP hint at another historical artifact: [""on-line"" used to be the more common spelling](https://english.stackexchange.com/questions/42044/which-is-correct-on-line-or-online) until mid-1980s.

Question by user Zeruno answered by user mustaccio on StackExchange: https://dba.stackexchange.com/questions/240914/what-exactly-is-the-online-in-olap-and-oltp",2023-03-27 10:11:33
11sintx,dbt - what are the alternatives?,"I love using dbt, I'm not going to stop using it any time soon.  I can easily explain where it lives in the Modern Data Stack / ELT style of doing data engineering AND I can explain the features and benefits.

BUT, in a meeting earlier today I was asked what are the alternatives to dbt? 

&#x200B;

As in ... for E & L you can use FiveTran or Hevo or Stitch, for database you can use AWS RedShift or GBQ or Snowflake, but for T ..... I can only think of dbt - is there any alternative?   (and to define that a bit better - any software that can transform data in SQL, cloud-based)",2023-03-16 03:18:30
1alwy45,Daily life and work of a data engineer,"Hello folks,

To all people who work as Data Engineers,

What is your daily life or daily work like? Is the job like ? What is exactly your work? Are you part of a Agile based software product team and responsible for the data for that software? Or is the work open ended, more research based than release/feature based.

Asking as I'm curious about it and want to start upskilling towards a Data Engineer job.

(Bonus doubt: Do you feel GenAI will affect Data engineering tasks too?)",2024-02-08 14:28:48
19533tp,Projects that would impress you,"As a recruiter or a hiring manager for a data engineering entry level position what projects would impress you more or make a student stand out, I know quite a bit of SQL and Python (I can solve Leetcode medium to hard in both) what would you recommend as project that would help me get into DE, thanks",2024-01-12 19:13:18
18svyhx,Who would emerge as the winner between Databricks and Snowflake in the race of all things Data and AI?,what are the differentiators when both of them are encroaching into each other's territories ,2023-12-28 15:04:28
17km8af,"Do you give the business what they asked for, or what they want?","The great philosophical question. I know the ""right"" answer to this question is to give users exactly what they asked for and if they asked for a dumb product, then that's on them and they should have given better requirements... but in my experience, the business doesn't know what to ask for and if you do exactly what they ask, you end up re-inventing excel.

On the other hand, Henry Ford once said ""If I asked people what they wanted, they would have told me they wanted faster horses.""  ",2023-10-31 14:23:22
17istl5,"One recommendation for whoever is applying for a DE role, especially if you want to take a technical path","Most likely it is a position that writes a lot of Airflow DAGs. Ask the team (during the panel interview or hiring manager interview) how they write DAGs. Specifically, ask them this question: ""Do you write your pipelines in a programming language (Python, C#, Java, Scala, whatever) or in YAML?"".

If the answer is ""in a programming language"", ask them whether they have a data platform team, and how does it work.

Basically, be very aware of any DE job that got invaded/abstracted away by upstream data platform teams. It is good for the data platform team as they can bag nice projects for CV, and it is sometimes good for the company because a replaceable screw is a lot better than a non-replaceable one. I'm 100% fine about platform team taking over deployment or whatever, but taking over the fun of programming is absolutely No No. If it is the way it works, join the data platform team instead.

Think about this: you don't want to work a couple of years and write ""I wrote a lot of YAML pipelines"" on CV. Actually, at a mid-senior level, I would never accept any position that PURELY writes SQL while the DAGs are simply a wrapper around those queries. Very bad for your career path.

In one sentence, avoid any programming jobs that doesn't actually write/read a lot of code (aside from SQL, which for sure you will write a lot).",2023-10-29 02:01:30
172x3p9,Are OLAP Cubes irrelevant in the present day?,"
There is this post a few years back summarising what they are. With the conclusion being they aren't really needed anymore if you rely on a modern cloud OLAP data warehouse. Especially when coupled with a metric semantic layer. 

https://analyticsengineers.club/whats-an-olap-cube/

But I was curious if this is indeed the case from other people's experiences? Or was this article an artifact of the ZIRP modern data tech stack hype era. 

I believe certain concepts are still relevant to understand since they can be applied outside of the actual OLAP cube(slice, dice, drill down, pivot the cube, cube design for handling non additive facts).

But are we in a place in time with technology where the computation and cost savings of storing preaggregated results in OLAP cubes are trivial and not worth the effort?",2023-10-08 11:41:29
16wl0dz,Airflow performance issues,We're using AWS Managed Airflow and experiencing high resource utilization issues but have only 50 DAGs on a medium instance. DAGs are not dynamically created and they're all of the same type/using similar code to run. There's no clear culprit because nothing we're doing seems particularly resource-intensive. It doesn't seem to be related to DAG parse time based on relevant metrics. It might be related to memory usage but I haven't been able to find out what's taking up a lot of memory. What steps can we take to narrow down possible causes? Nothing stands out apart from the two areas I've just mentioned (DAG parsing or memory usage) so I'm looking for general guidance on areas to investigate further and how.,2023-09-30 23:13:22
16t5d83,What is the best choice of ETL tool for Snowflake?,My work is using Matillion on Azure VM with Snowflake but using VM is not quite effective way to work. So I'm thinking of Matillion on cloud or using another solution. Any thoughts?,2023-09-27 00:31:17
16l4y6e,Is data normalization needed in 2023? How much normalization is actually required and when should it be done?,"In this example, say that you're ingesting some highly denormalized data files from some source. These files break 1NF, 2NF and 3NF and as a data engineer your goal is to bring this data into an analytical database for querying by data scientists.

My first question **is normalization even needed in 2023?** Maybe this is a hot take, but in my mind, one of the main reasons to normalize is to reduce database size. But nowadays storage is so cheap, you the tradeoff in development time may be worth the extra storage!?

Secondly, but after that, assuming normalization is required, **how much normalization is required?** Maybe only 1NF is required? Maybe 2NF as well? What questions would I need to ask myself to know the extent of what's needed for the use case?

Finally, **when should you normalize?** In a traditional ELT pipeline, should it be somewhere between E and L to prevent denormalized data from getting into the DB at the tradeoff of time? Or is this better done as a later transform step?

The reason I'm asking all of this is because I came into the industry from application development where I worked with highly normalized transactional DBs. At first I assumed that a database is a database and I should treat analytical DBs and related ELT pipelines the same way, but I want to check those assumptions :)",2023-09-17 16:21:28
168newt,Automate ourselves out of a job.,"I see a lot of discussions about tooling here. I also see a huge interest from business in no-code solutions. 

Is data engineering really that easy that it's just make a SQL database then use Azure Data Factory to handle literally everything?

Have we made enough tools now that data engineers don't need to write code anymore, because there's already a GUI that has that feature/connector built in? If so, won't the business just eliminate our positions when they figure out how to navigate their own GUIs? 

I find my company between ""Microsoft already spent billions of dollars developing this solution, nothing you make will be able to compete remotely so don't try"" and ""the best code is the code you don't have to write"". 

Sorry for the rant.",2023-09-03 04:28:39
15iq5bk,Currently building a local data warehouse with dbt/DuckDB using real data from the danish parliament,"Hi everyone, 

I read about DuckDB from this subreddit and decided to give it a spin together with dbt. I think it is a blast and I am amazed at the speed of DuckDB. Currently, I am building a local data warehouse that is grabbing data from the open Danish parliament API, landing it in a folder, and then creating views in DuckDB to query. This could easily be shifted to the cloud but I love the simplicity of running it just in time when I would like to look at the data.

I have so far designed one fact that tracks the process of voting, with dimensions on actors, cases, dates, meetings, and votes.

I have yet to decide on an EL tool, and I would like to implement some delta loading and further build out the dimensional model. Furthermore, I am in doubt about a visualization tool as I use Power BI in my daily job, which is the go-to tool in Denmark for data.

It is still a work in progress, but I think it's great fun to build something on real-world data that is not company based. The project is open source and available here: [https://github.com/bgarcevic/danish-democracy-data](https://github.com/bgarcevic/danish-democracy-data)

If I ever go back to work as an analyst instead of data engineering I would start using DuckDB in my daily work. If anyone has feedback on how to improve the project, please feel free to chip in.",2023-08-05 08:32:00
14vicy3,How to break into FAANG?,"I know that a lot of people say data engineering career at FAANG is not a full experience of data engineering, but it’s been my dream to have a career at FAANG. I just started my career as a data engineer not long ago, but the tech stack isn’t very modern and I don’t use Spark and not even Python. We use Informatica to do all the work, so I imagine I would have to build up my experience in other technologies to fulfill the Data Engineer job requirement at FAANG. With that said, how should I prepare for a data engineer interview at FAANG from ground up? I am thinking of applying possibly end of next year, so I will have around ~2YOE or less if I apply earlier. I know that grinding leetcode and statascratch is a must to prepare for the interview, what are some other stuffs? Also what company among the FAANG has the best data engineering experience?",2023-07-10 03:21:46
14f846e,Anybody want to join my book club?,"It'll be so fun. Every week we'll invite a stakeholder to join us and make an absurd request for some view or something, and we'll role-play how we respond to keep the stakeholder happy to our own detriment.

https://preview.redd.it/977fk80hkd7b1.png?width=910&format=png&auto=webp&s=cb19ec7af9a82b444fe4fa7b4d22d28fba081ea0",2023-06-21 13:42:58
14bn5fw,Pandas was faster and less memory intensive then crealytics pyspark. How is it possible?," 

I was trying to read excel files residing on **AWS S3**. As I already had pyspark pipelines setup, I attempted to use **com.crealytics.spark.excel** for excel. It worked fine for files **<10MB** however, with large files (50 to 150 MB excel files) I started getting job failure as follows:

>""java.lang.OutOfMemoryError: Java heap space""

I referred to AWS Glue's docs and found the following troubleshooting guide: [AWS Glue OOM Heap Space](https://repost.aws/knowledge-center/glue-oom-java-heap-space-error)

This, however, only dealt with **large number of small file problems**, or other **driver intensive operations**, and the only suggestion it had for my situation is to **scale up**.

For **50 MB files**, I scaled up to **20-30 workers** and the job was successful, however, the **150 MB file** still could not be read.

I approached the problem with a different toolset i.e. **boto3 & pandas** or **awswrangler**. That did the job with just 4 workers in under 10 mins. I bet not even 3 are required.

I wanted to know if I had done something incorrectly with crealytics, considering pyspark is supposed to be much more powerful, compute wise, considering its distributed nature. Also, if the above result is conclusive, could anyone guide me as to why this happened, based on how both work? Would be grateful for your responses.  
I had posted a question on [stackoverflow](https://stackoverflow.com/questions/76464770/com-crealytics-spark-excel-vs-pandas-awswrangler/76465167#76465167), however, there was no comprehensive response.",2023-06-17 10:03:30
13wsahj,Things you wish your fellow data coworkers knew?,"What are some of the things that drive you bonkers that your fellow coworkers don't know or are grossly incompetent at? And I don't mean outside of your department (the things listed for non-IT management would never end). I mean your fellow IT, especially Data related coworkers.  


Background: I'm fairly new to data management and have largely been working solo. I'm doing A LOT of learning, watching videos posted here, developing what I can, but I am also looking to transition to a new job in a much more team based environment, which will be new for me. So I am curious what it is that either may hold me back or might drive my co-workers crazy (especially since I'm mostly self-taught) that you guys have experienced. What is it about your coworkers that just makes you want to strangle them and force feed some sort of knowledge or practice into them?",2023-05-31 17:20:12
13wc7fl,Best tutorial/ course on Databricks,"Hey guys, i am in a DE role where I only work with Snowflake and SQL. As I am looking for other jobs in the market, very few jobs have only these 2 as part of their required skillset. I want to upskill starting by databricks which is one of the most common skill that I see for my target jobs. Can anyone help me with the best place to start learning?",2023-05-31 04:28:23
1199zm1,What are your best pandas tips&tricks,"What tips & tricks would you recommend to the beginner pandas users? What to definitely avoid?

(I would like to provide a pandas workshop to our business users so collecting ideas)",2023-02-22 19:52:52
18ydlv0,"What do you guys think of ""Fundamentals of Data Engineering"" book?","I'm a DE with over three years of experience. Due to the heavy promotion/recommendation of the book by DE influencers, I decided to read it myself. So far, I've read over 50% of the book, and I have mixed feelings about it. As a DE with some experience, I find the book overly bloated with unnecessary technical details that are difficult to remember and discourage me to read sometimes, rather than providing an overview of specific technologies and possibly more useful use cases. Sometimes after reading a chapter, I can easily remember the general topic but not the specific details. The only time I find the book valuable is when I have a reference to a service I've used (usually aws services) and can visualize a use case for it. However, I can imagine that this wouldn't be the case for everyone, especially beginners. I also miss a more engaging narrative style in the writing. The writing style comes across more like a technical leaflet with bullet points and defined terms.

I understand that my feedback may seem a bit harsh, but my intention isn't to roast the book. I'm glad that a book on this topic was created and I encourage to try it yourself, even if the writing style wasn't appealing to me. I wanted to ask you what's your opinion on that? If you have some tips on how to approach/read this book to get the most out of it? ",2024-01-04 13:49:22
18qumit,Recently accepted an offer as a DE for the first time! Any tips?,"Any tips career wise for someone in a DE position for the first time? It seems to be a bit analytics heavy but still has engineering components like cloud, infrastructure, pipelining etc. 

I also was in the camp of being a DA then transitioning to DE. It’s definitely possible and I did a lot of projects and made them public on my GitHub.",2023-12-26 00:03:53
17pfhzy,Is anyone using duckDB at work?,"I'm a big fan and have been exploring adding it as our data warehouse given we're at a small scale. Does anyone use it at work/production environment? If so, how's your experience been with it?",2023-11-06 22:46:25
17h6w9v,What are your duties as a Data Engineer?,"Please elaborate on this. 

Including your role at the company, your day to day tasks, tools and languages you’re using. 

Thank you in advance!",2023-10-26 21:15:54
17etfeh,Do you always feel rushed with your deliverables?,"Do you feel rushed with your work?

Something I’m pretty terrible at is consistently keeping a clean commit history and maintaining good code quality.

I work for a consulting firms, and this is especially difficult for me since I jump across clients, projects, and technologies.

I could be in a dbt repo one hour, another client’s dbt repo in the next hour (with totally different styling and organization), a custom ingestion repo in Python the hour after, and then wrap up my day pouring through the last client’s airflow repo.  And then 2-4 weeks later the clients cycle and I start all over.

I get given a ticket to jump into a monster, super messy repo that I’m not familiar with and am expected to have a PR up by EOD.

Even before consulting, the one year I did in industry was the same with the only difference being I got know (and contribute to) the 1-3 monster repo’s instead of learning a new on each week.  

But still, it was always just push stuff out as fast as you can since the backlog is so big and the team so small.  And I see this mentality with the data teams I work with at the clients I’m assigned.

I’m not saying that we should drag our feet to ensure absolute perfection, but I feel like this is harming my growth as I learn poor practices with disincentive to take time to learn good practices.",2023-10-23 19:55:54
16yx6f1,Is Polaris worth learning over pyspark? Or is it just hype while the industry has no intention of moving away from a Apache/Cloud framework,"I’ve been reading a lot about polars but one thing about the industry I’ve seen is that new technologies come and have a a lot of hype but do not cause any real change to the current accepted stack. 

I’m wondering if this subreddit feels the same way about it as me. Obviously it’s good to keep learning but between getting better at pyspark vs Polars I’m wondering which should be the focus",2023-10-03 16:55:49
16taezn,Fear of Missing Out from Big Data Technologies,"Hi,

**Background**

I have CS Background, and currently, I am in my second full-time data engineering job. In my first full-time job, we were using Hive, Spark, Flink, Kafka, and Scala. I stayed there for around 7 months and then I changed to my current job to move to Germany.

In my current job, we are mostly using, Bigquery SQL, Airflow, Python, and Terraform. We do not have any product that needs read change data capture so that's why we only run daily jobs on Bigquery SQL. I learned some new stuff like infra as code, and cost optimization on GCP but I feel like I am missing out on the up-to-date challanges in the Big Data world and the technologies used to solve these challanges. I am also feeling that I am missing the work with more JVM-based languages and being close to more software engineering practices.

**Background Summary**

* First job (7 months) - Scala, Spark, Flink, Kafka
* Second and current job (13 months) - Python, Bigquery SQL, Airflow, Terraform

**Career Questions**

1. I have been in this position for over a year now, do you think it is time to move to another team or company where I can work with more challenging tasks?
2. If so, what do you recommend to get better at the technologies that I am not using at the moment like Scala, Spark, and Kafka because I feel like I have forgotten them now?

Thanks in advance.",2023-09-27 04:12:28
16lsx5u,Day 1 of a new job starting remotely - what do you do to make it to smoothly?,"Going to be starting as a Sr. Data Analytics Engineer. 

I have never started a job remotely. I have experienced onboarding new hires remotely and tbh(coming from the onboarding end) this is the one thing of WFH that I have found especially intimidating.

What advice do y'll have?",2023-09-18 11:27:42
15wiqd5,"Data Engineers working in Government or Big Business, how do you feel when you hear people say stuff like ""They have our data! Who knows what they could be doing with it! ""?","I imagine the reality is...not quite so romantic.

Also, if I had to guess, I'd imagine that one of those is not quite the player people make it out to be.",2023-08-20 18:21:09
13s6ugn,Docker Compose vs. Kubernetes: Understanding the Differences and Choosing the Right Tool,"As a DE, I test many of pipelines locally with Docker Compose and then deploy them on K8s. Here, I tried to explain their differences.   
[https://medium.com/gitconnected/docker-compose-vs-kubernetes-understanding-the-differences-and-choosing-the-right-tool-32f3e16fdb43](https://medium.com/gitconnected/docker-compose-vs-kubernetes-understanding-the-differences-and-choosing-the-right-tool-32f3e16fdb43)",2023-05-26 07:58:35
1232nsl,Is it smart to specialize in a cloud environment or be diverse?,"Aspiring DE here, would love to get advice from seniors.

I feel like I’d love to become specialized in Azure platforms like Data Lake, Synapse Analytics, SSIS, Analysis Services.. etc but there’s also a lot of demand for AWS and GCP out there. Would it be career smart to specialize in Azure and not learn much about the others? I don’t wanna limit myself, but also I wanna become a sharp knife in an avenue.",2023-03-26 22:50:15
117tm6i,Doing DE side projects or getting Cloud certification?,"Hi DE fellows,

I'm a Data Engineer for 1.5 years with a business background, I do ETLs mostly, query data from PostgreSQL server, do some calculation then load to another PostgreSQL table, automate using Airflow. I want to improve my skills so I can add to CVs and impress new employers, which one should I stay focus on for the next 6 months? Doing many DE side projects or get a Cloud certificate? If the answer is Cloud cert then which one is better, GCP or AWS?

Thank you for your advice.",2023-02-21 03:50:07
196mzxv,What's the cheapest way to host Airflow for personal projects?,"I'm looking to build up some DE personal projects in my spare time. Mostly just in the interest of learning and upskilling with no plans to profit off of anything.

I'd like to orchestrate some DAGs through Airflow, and would like to host in on AWS. At work we use MWAA which would be way overkill, and I'm curious what the cheapest hosting strategy would be for hobbyists? I'm planning to play around with the idea of running all tasks as ECS/EKS tasks, so the core infra doesn't need to support anything too intensive.",2024-01-14 18:49:58
17w0ml8,A Junior's perspective on centralised tools like DataBricks and Azure,"Hey everyone! 👋

As a junior data engineer still finding my footing in this vast field, I've been pondering over the trajectory of our tools and platforms. I'm eager to hear from the more seasoned data engineers, leads, and others about your take on this.

I've observed a growing trend towards centralized solutions, particularly with platforms like DataBricks and Microsoft Azure. They are continually unveiling new features and tools, which is undoubtedly a sign of progress. However, this leads me to question if we're heading towards an overly centralized future in data engineering. 

The heart of my concern is whether these platforms might be turning into a sort of never-ending learning treadmill and we data professionals end up 'trapped' inside the ecosystems of Azure, DataBricks, etc. On one hand, it's fantastic to have constant innovations, but on the other, it feels overwhelming at times. As we delve deeper into ecosystems like DataBricks and Azure, are we risking losing sight of the broader landscape? Is there a danger of becoming too specialised in these environments to the detriment of our versatility? Doesn't it give DataBricks and Azure more control over your decision-making and data?

Being relatively new to this field, I'm completely open to the idea that my concerns might be misplaced, and I'm really keen on understanding different perspectives. I'm all for specialisation, but I also value adaptability and a diverse skill set.

I'm looking forward to hearing from the veterans and experts in this community. Do you think these concerns are valid, or is this simply the nature of our evolving field? How do you balance the need to stay current with specific platforms while maintaining a broad skill set?

Thanks for taking the time to share your wisdom and insights! Really looking forward to this discussion!

Edit: sorry, had a duplicate paragraph in there somehow...",2023-11-15 18:49:26
17oljhd,Best ETL tool?,"I've been researching different ETL tools to get an idea or when to use each but thought I can drop in here to see what others think.

1. Talend - I hear is open source and easy to use. It's made as a low code/no code solution for ETL.

2. Pyspark - I'm kinda learning this one already on my own already as I already know python/pandas and my tech stack kinda aligns with learning this eventually anyways.

3. Informatica - I heard this one is ancient, should I just avoid?

4. Fivetran - I heard is relatively new but don't know much about it, pros and cons?

Any others you would consider and for what use case?",2023-11-05 20:58:51
17m94wu,Things you learned that were of no use. [mainly juniors-mediors],"What are things that you dedicated a lot of hours to learn (through books, bootcamps...) thinking that they are going to boost your knowledge/skills in DE but that are of no use in a DE job?

(learning by avoiding other people's mistakes)",2023-11-02 17:43:43
16gyw84,Hybrid Execution in GlareDB: Scale your workflow with GlareDB Cloud,"(Disclosure: founder of GlareDB)

Hi everyone,

We've recently released GlareDB 0.5.0, and our biggest feature for this release is Hybrid Execution.

First, what is GlareDB? GlareDB is SQL database that can read data from a variety of sources, including Postgres, Snowflake, S3, and more. Our goal is enabling running analytics across data sources without having to move data around.

And with Hybrid Execute, we're taking that a step further. Hybrid Execution lets you connect to a GlareDB Cloud deployment from the GlareDB CLI or Python library, which  allows queries to use the combined resources of both the local and remote machines. GlareDB splits up the query during planning, and executes parts of the query locally or remotely depending on the data being referenced.

This also enables being able to join local data (parquet, csv, and json files) onto tables that exist in your cloud deployment. For example, if we have a `user_metrics` table in our cloud deployment, and a `company_users.csv` file that's just sitting on our laptop, we're able to write a single query that joins data from both:

    SELECT
      m.user_id,
      max(m.output_rows),
      avg(m.output_rows)::int
    FROM
      user_metrics m
    INNER JOIN './company_users.csv' u on m.user_id = u.id
    GROUP BY m.user_id
    LIMIT 5;

We don't have to manually upload csv files, we just write a single sql query and glaredb takes care of the rest. This also works for dataframes when using our Python library.

We can go even crazier. Since GlareDB has integrations for connecting to databases like Postgres, Snowflake, and more, we're able to join local data onto remote data sources all in a single query. Here's what that could look like in Python:

    import glaredb
    import pandas as pd
    
    con = glaredb.connect(""glaredb://<user>:<pass>@glaredb-better.remote.glaredb.com:6443/dogfood"")
    
    users = pd.DataFrame({""email"": [""sean@glaredb.com""]})
    
    con.sql(""""""
    select count(*) as query_count,
           max(e.elapsed_compute_ns) as max_elapsed
      from snowflake_segment.glaredb_prod.execution_metric e
      inner join pg_prod.public.users u on e.user_id = u.id
      inner join users u2 on u.email = u2.email
      where e.timestamp::timestamp > now() - interval '3 day'
    """""").show()

And as before, we're not having to do anything special with the `users` dataframe, we just reference it in the query, and glaredb will work its magic.

If you want to learn more about Hybrid Execution, check out our blog post: [https://glaredb.com/blog/hybrid-execution](https://glaredb.com/blog/hybrid-execution)

And take a peek at our open source repo: [https://github.com/GlareDB/glaredb](https://github.com/GlareDB/glaredb)

Let us know what you think!",2023-09-12 18:24:42
15pvfty,“Fun” domains to work as DE?,"Currently work for a tech/ecommerce company in marketing domain and while I like doing DE tasks, I feel that the data we are working on and the aim of all the pipelines is very boring. My job is basically to unify customer/order data, classify them by industries, products, etc, and then create ad campaigns to send them targeted ads. What’s worst is to have to setup fancy pipelines and dashboards for measuring the lift tests and a/b tests of the ads only for marketing departments not to use them or for them to run the full campaign even without reaching a good clear consensus of the experiment. Why experiment then lol, spending thousands of dollars in experimentation and tools for it for them to not even consider it. Waste of time imo. 

The engineering part is “fun”, specially using all the fancy tools of a modern data stack, but the business use case I find it extremely boring. 

Anyone working on some better business use case as a DE? Maybe financial data and financial modeling? Maybe some data around health or pharma? Don’t really know.",2023-08-13 10:28:55
159v9e8,How do I become a better data engineer?,"I just started my career as a data engineer. I have been learning a lot at work, but I think it’s slowing down and not fast enough. I really want to get good at data engineering, I aspire to become a better data engineer. What can I do during my spare time to work on that? I am planning to start work on Leetcode, to practice my Python and build up DSA knowledge. What else can I do to learn faster?

I use mainly SQL and no code GUI for ETL (which is bad I believe)",2023-07-26 04:19:24
14zzmfq,Jobs to just coast,"I legit don't care for the hustle and bustle of promotions and improving shit anymore. I'm super content just being a worker bee.

Any industries or employers where DEs can just coast that pay well?",2023-07-15 02:32:48
14n1xjw,"Using SQL inside Python pipelines with Duckdb, Glaredb (and others?)","Most of day to day is working with python scripts (with a smattering of spark, pandas, etc) with pipelines moving data to/from Postgres/SQL server/Snowflake. The team I'm on is very comfortable with python, but we're exploring using duckdb or glaredb in some spots for data transformation, both for performance and how well sql maps to these transformations.

We're still hammering out what exactly this would look like, but I thought we could get some outside opinions on using either of these projects in our pipelines. Concretely, has anyone introduced either of these projects into their pipelines, and how did that go? Any pitfalls?

For reference:

Duckdb: [https://github.com/duckdb/duckdb](https://github.com/duckdb/duckdb) \- seems pretty popular, been keeping an eye on this for close to a year now.

Glaredb: [https://github.com/GlareDB/glaredb](https://github.com/GlareDB/glaredb) \- just heard about this last week. We played around with hooking directly into snowflake, so that was cool, but I haven't heard of anyone else using it.

Any other projects like this that I'm missing?",2023-06-30 14:27:14
13lrov0,Easy to learn ETL solutions for a one man data team?,"Context: I will be moving to a new role at work in a few weeks where I will essentially be a one man data team. I have a good background in Python, SQL, Power BI and I just got my Azure Data Fundamentals certificate. In the meantime before I start the new role I’d like to pick up some data engineering / ETL knowledge so I can be a bit more effective and cover my blind spots.I will not know what type or quantity of data is available until I start.  In the meantime I’m looking for something relatively easy to pick up, perhaps equivalent to a ~10 hour udemy course. Right now I’m completely overwhelmed by the amount of options available to me. Options I’m looking at are:

- Azure Data Factory or Synapse Analytics

- Spark / Hadoop / Python implementation

- Spark with Databricks

- Airflow

- DBT

- Others?

Thoughts?",2023-05-19 10:35:20
138o36n,r/dataengineering + freddie mercury,N/A,2023-05-05 14:12:41
12sswj8,Databricks Certified Data Engineer Professional - experience,"Has anybody taken the Databricks Certified Data Engineer Professional certification exam and could share experiences?

How does it compare to Databricks Certified Data Engineer Associate?

Are there free sample tests available?

In return, I will be happy to share experience from the Associate Developer for Apache Spark (Python) exam if you're interested. I've passed it lately.",2023-04-20 09:44:17
11wj33t,Seeking career advice and guidance. I'm making a career switch from construction to being a data engineer,"Hi, I’m in my late 30’s and I’ve made the decision to change career in the tech industry and become a Data Engineer. I’ve gained my Data Engineer Fundamental and I’m currently studying for my DP-203. However, I don’t know anyone within the industry that works as a Data Engineer. I’m looking for a mentor for advice and guidance to help me successfully make my career switch.

If you’re a Data Engineer or know someone who is, I would love to hear from you. I’m looking for someone who can provide me with guidance, advice, and support as I make this transition. I’m eager to learn and grow in this field, and I believe that a mentor can help me achieve my goals.

Thank you for your time, and I look forward to hearing from you soon.",2023-03-20 13:37:05
119a3vs,What pandas can do and polars can’t?,"Polars is getting pretty popular recently but I would like to know what pandas can do and polars still can not.

E. g. I found polars cannot work efficiently with json (missing e.g. json_normalize function).",2023-02-22 19:55:36
1am4i56,What happens when DataBricks/ SnowFlake shuts down (hypothetical),"So, a hypothetical scenario. 

&#x200B;

A company utilizes Databricks/ SnowFlake for their needs. Lots of Notebooks/ workflows are built overtime.

&#x200B;

Now company wants to part ways with / the vendor decides to go kaput one day. (Hypothetical)

&#x200B;

What happens? I believe data still resides on our cloud platform choice. What about codebase? 

PS: Not a DataBricks/ SnowFlake user. Neither is my company. I usually tend to think about these edge scenarios so Me/ company is not left hanging should something go wrong with any vendor in future

&#x200B;

thanks in advance.",2024-02-08 19:52:24
1agxov7,Do you think Leetcode type questions is a good metric for data engineering skills?,"So I recently caught up with a friend of mine who is a experienced data engineer and works for a marketing startup. While we were chatting up, he told me that he has taken 100s of interviews and his main filter is always leetcode. Only candidates who are able to solve different medium level leetcode problems across multiple rounds are considered for hiring.

I was a bit surprised by that because wouldn't it be easy to lose out on a lot of good candidates but he said that leetcode type problems help him understand how smart a candidate is and how well he can come up with ideas and tackle DE problems. What are your thoughts on this? How would you choose to interview potential candidates and does leetcode type competitive coding questions have any bearing?",2024-02-02 06:56:57
19eio7f,Allowing report access to 35000 people,"Hello!

I work for a retail firm and we are running an incentive over 6 months. I produce the incentive reports using python code and we usually share it with a few important salespeople.

However this year, management is looking at creating something like a powerbi report and allowing access to all salespeople. The number of salespeople will be about 35,000. 

I'm experienced with python and powerbi but I've never done something that needs to be accessed by such a wide audience.

Does anyone have any experience with such a scenario where a lot of people need access to a report and how it was achieved?

Edit: I forgot to mention that the salespeople are in a hierarchical structure and one should not see details of another hierarchy ",2024-01-24 14:43:51
184xh8c,How do you avoid DWH mess?,"We are using dbt, but still, it's getting messy pretty fast with tons of models, sometimes overlapping logic. Is there a clear convention you're using to write models, architecture wise?",2023-11-27 07:34:59
183hgs2,Oldcoderguy - The World’s Most Secure Data Warehouse,N/A,2023-11-25 11:05:20
17o83kd,Tool to get rid of excel sheets?,"I was tasked with finding a solution to the following problem:

The company uses excel for everything and each department has its own set of excel files with data that might overlap. I need to put everything ""into a database"" and the end users are of course supposed to still be able to input data, but none of them know sql. There also needs to be some data validation before the data is persisted.

Later only I need to be able to get that data out into a datalake.

I am considering using airtables for this, but I wanted to hear whether somebody here has a different idea, be that another tool that does the same or a completely different approach.

Thanks!",2023-11-05 09:12:30
17d8xv3,Is Apache Beam still popular for Data Engineering?,"It's been at least 5 years since I played around with Apache Beam... Back then, I really liked it. 😍

But now it looks like Spark and Airflow have gotten to a point where Beam really isn't adding all that much value.  Is that an true?

If no, could you please share a couple use cases where Beam outshines these other tools? Thanks,",2023-10-21 18:24:09
17754gk,What python skills I should focus on for a Senior Data Engineer technical interview round?,"I have 5+ years of Data Analysis experience. I am pretty good with SQL/PLSQL, BI tools, in python - pandas, numpy.

It's  a one hour interview with a senior data scientist and a senior manager.  They will evaluate my SQL skills, Python and System Design.

Since  python is so vast and me having sub par skills, can you all recommend  any resources/ topics I should focus on most? I bought leetcode and  stratascratch monthly subscriptions, but the problems are overwhelming  me.

The employer is on GCP platform. Their main data engineering tools are Dataflow, Cloud Composer, Pub/Sub and Datafusion.

All responses are appreciated!",2023-10-13 18:20:38
16jefur,Do you think a proper CS or Data degree is becoming more necessary in today's competitive job market?,"I'm considering doing masters in computer science. In the past few years, I've been an advocate of the ""you can learn your way to success"" mentality in the tech industry. But it's becoming evident that unlike the last decade, the supply of software/data engineers is catching up to the demand - at least for the foreseeable future. 

As background, I live in the US and have a quantitative non-CS degree from a top 30 college. 3 years experience as a data analyst and 2 years as a data engineer. My current title is senior data engineer and slightly inflated compared to industry. But I do know my stuff, and lead a small team of data engineers. We have proper programming practices, CICD, testing, devops, cloud infrastructure, workflow orchestration, etc. 

In the past 6 months, I've applied for more than +400 jobs to see if get a gauge on the job market and see if there are better opportunities out there. I've gotten zero offers and am barely even getting interview requests these days. 

Getting a $40k piece of paper from a masters program is sounding more and more appealing. Jokes aside, it'll probably help me solidify some of my CS knowledge and add some proper credibility to my resume. 

Also - it's likely not a resume thing. I'm fairly well versed in that stuff and have had little trouble in the past landing jobs. ",2023-09-15 14:09:45
15so5vl,Apache Doris 2.0.0 is Production-Ready,"With the new version of this open-source analytic data warehouse, we bring to you:

1. Auto-synchronization from MySQL / Oracle to Doris
2. Elastic scaling of computation resources
3. Native support for semi-structured data
4. Tiered storage for hot and cold data
5. Storage-compute separation
6. Support for Kubernetes deployment
7. Support for cross-cluster replication (CCR)
8. Optimizations in concurrency to achieve 30,000 QPS per node 
9. Inverted index to speed up log analysis, fuzzy keyword search, and equivalence/range queries
10. A smarter query optimizer that is 10 times more effective and frees you from tedious fine-tuning
11. Enhanced data lakehousing capabilities (e.g. 3\~5 times faster than Presto/Trino in queries on Hive tables)
12. A self-adaptive parallel execution model for higher efficiency and stability in hybrid workload scenarios
13. Efficient data update mechanisms (faster data writing, partial column update, conditional update and deletion)
14. A flexible multi-tenant resource isolation solution (avoid preemption but make full use of CPU & memory resources)",2023-08-16 12:11:19
15dutwi,Data Engineer interview experiences,"Greetings everyone,

I am a Data Engineer with approximately three to four years of experience in this domain. Currently, I am exploring job opportunities, particularly within product-based companies in Europe.

I would greatly appreciate it if you could share your recent interview experiences for Data Engineering roles ( any level ). I'm particularly interested in understanding the various stages and types of interviews you encountered during your job application process.

With few interviews which I gave, it looked something like below
1. Screening round - call with recruiters, briefing for what role is about
2. Hiring manager round - interview round with hiring manager, discussing depth about your previous experiences
3. Technical round or take home assignments - not much aware of this round, since I have just started interviewing and few are lined up in upcoming days
4. Designing data pipeline
5. Culture fit / Behavior round 
6. HR and release of offer after negotiations.

Thank you for your insights in advance.",2023-07-30 20:00:59
15a45gt,Great Expectations is bloaty. What are the alternatives?,I set up GE on databricks but I am not happy with it. To the point where I am considering to just keep the very plain selfmade validation modules. Are there other options worth exploring? What I particulary dislike is the creation of custom expectations and all this data context bloat.,2023-07-26 12:12:53
1594xvv,How do you and your team write good documentation?,"Hi fellow engineers,

I’ve been recently creating documentation for a new process that we’ve built and was wondering how other engineers approach writing documentation? what tools do you use? how'd you keep documentation up to date? Etc

I think the main challenge we are running into is the keeping docs up to date part. 

We’re using confluence to write docs and we’re also using Comala Document Management to help provide a review process to docs as well as auto expiring pages after a year so we can re-review but it still requires people to be checking for expired docs etc.",2023-07-25 10:48:13
150korq,"Why use ""GROUP BY 1""?","I'm going through some of the dbt training courses on their website. Across multiple videos and presenters, they seem to use the syntax ""GROUP BY 1"" in their SQL code. I honestly had to Google wtf that meant lol.

Please correct me if I'm overgeneralizing, but it seems like in almost every case, you should just use the column name in the group by clause. 

I'm very new to dbt, so please let me know if there's a good reason to use GROUP BY 1 rather than the column name.

Edit: Appreciate everyone's responses! As I suspected, there's a lot of reasons one would do it that I hadn't thought of. Really interesting to get everyone's thoughts. Great subreddit!!",2023-07-15 19:27:27
14z3dvp,Data Engineering Headaches,"As a Data Engineer what’s your biggest headache, frustration, time suck?",2023-07-14 02:05:44
14qiuds,DE Interview question for handling ETL pipeline errors.,"I have DE interview coming up  and I am thinking to prepare few questions based on handling ETL errors.

A data pipeline should address these issues:

1· Partial loads (A scenarios where Partial processing of the files or records or any failures of ETL Jobs occurred; to clean up a few records and re-run the job)

&#x200B;

2 · Restart-ability (You have to re-run from a previous successful run because a downstream dependent job failed or reprocess process some data from history. for e.g. We need to run since last Monday or a random date)

&#x200B;

3· Re-processing the same files (A source issue where they sent multiple files; We need to pick the right records)

&#x200B;

4 · Catch-up loads (In case you missed executing jobs for specific runs and playing catch up; Batch Processing) .

Any Answers on these would be super helpful. Thanks. 🙏 ",2023-07-04 15:50:36
14lu0dk,How can I schedule python ETL code?,So I am still a newbie. I have a python project where I am scraping data from some log files on a daily basis and cleaning it then pushing it to db. I have scheduled this project to run daily on windows task scheduler. Is there any other(better) way it can be scheduled on a windows system?,2023-06-29 03:36:48
14e8nuu,What is the best way to optimize 5000 dashboards ?,"I know i might be asking something very general, but if you are thrown 5000 dashboards and asked to do a BI Modernization program, what 's the approach ?

Collect the max hit dashboards

Check the datasources

Check the consumption pattern

Check the performance

Check if there are blending in the transformation 

&#x200B;

I wonder if there is a way to export the tableau metadata information to collect all of this, i am sure, there should be API's but i don't think i will get access to those

&#x200B;

&#x200B;",2023-06-20 11:32:58
13xl68u,Anybody avoiding data warehouse vendor lock-in?,"Feels like DEs can get stuck in this box of having to use Snowflake, Databricks, or whatever your cloud offers. I know it's a big boost to have one of the ""Big 5"" as a skill on your resume, but that seems like a classic vendor lock-in strategy. Is anybody trying to break out of the box and do something unique/different with Data Warehousing? Just wanna hear your stories...",2023-06-01 15:52:35
13otv07,Anyone using Snowflake + Databricks?,"I often come across discussions, articles, and videos comparing Snowflake and Databricks individually, but what about considering the combination of Snowflake and Databricks? From my perspective, I could extract and load all source data from many relational databases into Snowflake for analytical data storage, rather than into Data Lake or Data Lakehouse. Then, within Snowflake, DBT could be utilized to construct data models and marts specifically for traditional reporting and data warehouse workloads. Simultaneously, the data stored in Snowflake could be accessed and analyzed through Databricks for advanced analytics, data science, and machine learning use-cases. I find that this approach would offer the benefits of centralized data management within the warehouse while enabling both BI and ML on the same data, which could be a superior alternative to a Lakehouse architecture.

I guess alternatively, you could ingest/stream data into landing -> bronze -> silver Lakehouse layers and then ETL from silver into a kimball-style warehouse in Snowflake. Not sure which method would be preferred?

Sure, I'm familiar with the fact that Databricks, with its Lakehouse architecture, is capable of supporting both data warehousing and machine learning workloads. However, if an organization heavily relies on both of these workloads, I question whether Databricks performs as effectively in the realm of data warehousing compared to Snowflake. Instead of solely relying on one platform, why not leverage both platforms to get the benefits from each and achieve the best possible outcomes?

&#x200B;

Edit: Rephrasing",2023-05-22 15:20:05
12ndwmo,Dagster Documentation Hurts my Brain,"Thats all, thats the post. Idk why but it is just simply not intuitive. I feel like Dagster has so many great concepts, but trying to create my simple pipeline into a reality has been a real pain. Does anyone else feel this way? Do I need to keep sticking with it? 

If anyone has any repositories or things/concepts that helped it click for you let me know.",2023-04-15 18:33:00
12inq6z,How to use dbt source freshness tests to detect stale data,N/A,2023-04-11 15:38:19
126i5bs,Unions,"Hi folks,

Just wondering if anyone working in the UK is member of a union or if one even exists that we can join ? And do people think there’s any merit in doing so ? 

Cheers",2023-03-30 09:57:58
124385k,Productivity advice,"This has been bugging me for a while and I wanted to discuss it with you all. I have been working with spark for around 3 years

Whenever I'm working on long spark jobs or running multiple iterations of shorter ones, I get super unproductive just waiting around for them to finish. I understand its not like app development where we get instant feedback. 

But switching to another story/task sometimes feels like an unnecessary context switch, plus if I start browsing or learning something new, it's tough to snap back into the job at hand. 

Have you been on similar boat? What do you all do to stay productive or what would you do during these slow moments?",2023-03-27 22:32:42
11yxoz3,Scrape Thousands of Housing Records in Minutes!,"[RedfinScraper](https://github.com/ryansherby/RedfinScraper) is a scalable Python library that leverages Redfin's unofficial Stringray API to quickly scrape thousands of housing records.

 It is super easy to download into any Python environment using `pip install redfin-scraper`.

Let me know what improvements you'd like to see!",2023-03-22 21:40:21
11spizb,Moving Petabytes of Data from DBs to a DWH,"Wondering what the community has used for moving petabytes (say 100+) of data from a clustered of databases to another (lets say from Mysql & Postgres to snowflake)?

&#x200B;

in my old life I'd use standard Microsoft tech using BCP wrapped in Powershell, honestly I was quite divorced from the whole process since it was cookie cutter templates that my old team developed.

What I've seen so far -

&#x200B;

* JDBC connectors to incrementally load data - works well if you only have a few TBs of data and can switch to a CDC approach to avoid reloading in future.
* BCP/ Bulk flat file exports to Blob / S3 then stage that into Snowflake / Synapse / BQ / Redshift.
* off-the shelf solutions for cloud owned databases (Aurora RDS -> Redshift, Datafactory(SQL Server) -> Synapse / Databricks).
* EL tools like Airbyte, Fivetran, Stitch?

&#x200B;

with regards to EL tools - are they using some magic to lift and shift massive amounts of data - or is it all JDBC with throttling as you don't want to kill the databases? Looking at the source code for Airbyte I know it's a incremental batched process to persist data into memory and move it to an external location to stage.

happy to hear some stories from the more experience folk.",2023-03-16 09:51:28
11rxy7l,Should you use CTEs in Snowflake?,N/A,2023-03-15 14:25:13
11pb78i,Full time data engineering freelancer,"Is any of you guys working as a freelancer full time?  
 I am looking into doing that, I quit my well-paid bank job because of the bureaucracy that was really bothering me (plus the stack was getting old and there was no more room for improvement after 3 years there), I am looking for something a lot more freeing and figured taking on projects would be the best way to go.

If any of you have done/is doing this, it would be great if you can share some tips and tricks on how to get projects and maybe share websites that mostly worked for you.

I joined Andela and searching on Upwork and Freelancer as of now.",2023-03-12 10:20:06
11a3ecv,A day in the life of centralized IT...,N/A,2023-02-23 17:30:20
114hb0e,Turn Jupyter Notebook to Web App with open-source Mercury framework and Python only,"
Sharing results from Jupyter Notebook might be a pain:

- you can't share notebook to stakeholders because they don't speak Python, cant run notebook by their own,

- you need to copy-paste charts into PowerPoint presentation,

- you need to rewrite your results to some web framework to make your results interactive.


Jupyter project offers a Voila as a solution for running notebooks as web apps. I found Voila a difficult to use, especially ipywidgets.

## Mercury 

I started to work on my own open-source framework for running notebooks as web apps. Mercury allows you to add interactive widgets in Python notebooks, so you can share notebooks as web applications. It offers a set of widgets with simple re-execution of cells.

GitHub repository: [github.com/mljar/mercury](https://github.com/mljar/mercury)

Documentation: [RunMercury.com](https://RunMercury.com)

## What you can build

You can use Mercury Widgets to create:

- [web apps](https://runmercury.com/tutorials/web-app-python-jupyter-notebook/), reports, dashboards,

- [presentations](https://runmercury.com/tutorials/presentation-python-jupyter-notebook/) - use widgets to interact with slides, slides are recomputed during the slideshow

- [websites](https://runmercury.com/tutorials/website-python-jupyter-notebook/) - share notebook as static HTML (with code hidden), 

## Features

Unique features that make Mercury different than Voila or Streamlit

- simple re-execution of cells after widget update,

- all input widgets are displayed in the sidebar, your app layout is ready,

- authentication to control access to notebooks (coming soon),

- easy file upload and download in notebook,

- export result to PDF or HTML with button click.


## Future plans

I would like to provide a cloud service for notebook deployment. You will upload notebook and it will be available as web app/website.



Looking for your feedback and comments! Thank you!",2023-02-17 10:50:55
1aqmnx0,"My company just let me open source our orchestration tool 'Houston', an API based alternative to Airflow/Google Cloud Composer that we've been using internally for the last 4 years! It's great for low-cost, high-speed data pipelines",N/A,2024-02-14 13:17:38
19121xo,Data engineering cases from real companies,"Hello,

I am looking to sharpen my skills as a Data Engineer to work in sectors I am interested in. Is there such thing as a website where Data Engineering problems from real companies are posted to be solved and/or used for practice? I am partial to the finance and gaming sectors. Thanks.",2024-01-07 20:46:33
18k208t,What do you look for on a resume when hiring for a Data Engineer (Junior/fresh out of college) role?,"Hi all! I'm fresh out of college with an MSIT degree. I've been applying for DA/DE jobs in vain. If you have experience as a recruiter or have supported someone in recruiting a junior Data Analyst/Data Engineer/Analytics Engineer, what are a few must haves?  (I'm aware that they are intrinsically different, but I'm confident that I can do a great job at either of them.)  
I've had experience with ETL (mostly OLAP data), SQL & Python, Databricks and AWS amongst few other things, but with each recruiter having their own infrastructure, I'm finding it hard to make focus on one specific skill. A post from the other day ([How I Interview Data Engineers - by Yordan Ivanov (substack.com)](https://datagibberish.substack.com/p/how-i-interview-data-engineers?r=odlo3&utm_campaign=post&utm_medium=web)) gave me one person's view on how to give an interview. I'm trying get to the interview, which is proving to be difficult. How do I land an interview? Any pointers will be helpful. Thanks in advance.",2023-12-16 22:11:47
17tedzn,Learning Pyspark,"Hi Guys, I want to start my journey to learn pyspark, but I haven’t found any option to do handsOn like SQL in hackers rank with questions and solutions. i would like to know if similar kind of practice website available for pyspark as well.
Thanks in advance for any insight!!",2023-11-12 06:30:58
17paeyc,What are useful data engineering certifications in 2023/24?,"I'm curious to learn about the landscape of data engineering certifications in 2023/24. 

If you don't mind sharing your insights, I have some questions:

1. In your opinion, which DE/ML certifications are highly regarded or considered valuable in the industry right now or will be in near future?
2. What is your perspective on certifications from major cloud service providers like AWS, GCP, and Azure, in terms of their relevance and impact on a DE career? Which one would you recommend?
3. Do niche-specific certifications (e.g., CDMP, Airflow, Snowflake, Databricks, etc) hold significant weight?
4. Are there any other certifications or training programs you think are worth mentioning for those looking to advance in the field?

Thanks in advance for sharing your knowledge and experience!",2023-11-06 19:11:50
16xbiar,dbt vs sqlmesh vs ?,"Most are pretty aware of dbt, but technology moves fast. Has any serious competitor come up? Curious if anyone has tried sqlmesh - what's your first impression?

Contex:
Just generally curious",2023-10-01 20:14:24
16gzu8p,How Does Anybody Make Good Use of Redshift?,"I used to work with BigQuery as my org's data warehouse solution and I miss it...

Below are some of my qualms with Redshift:

* It isnt magically scaleable like BigQuery
* Doesn't have sharding
* Doesn't have DB replication
* Cannot query data catalogs in glue if with IAM authentication (even with the expensive nodes + extremely poor documentation around this)
* Cannot run cross-DB queries on external schemas (extremely poor documentation around this) and to get around it you need to make an external schema in every DB if you want the functionality
* Pay per node vs data scanned (like in BigQuery, so you are pissing money away during downtime)
* Spectrum is slower than the resources it is querying from (slower than it would be to just use athena)

Are there any other downsides besides those? Or even better am I missing any plus sides?",2023-09-12 19:01:30
16gxz8v,If you had to start over..,"If you had to start over, what would be your plan to go from zero to hero in 6 months",2023-09-12 17:48:55
16a6xhs,It feels impossible to get into data engineering no matter how good you are,"I was top of my class in college at a state university, one of the very few people who is not only extremely passionate about data and technology, but actually follows through with it myself studying outside of school, cross training, working on projects to try and explore technology, seeing what tech stack companies are using, studying it on my free time. I've built my way up in my career from intern to project Management office, to business analyst, senior data analyst/data engineer. But it's like it's not good enough. Every company wants staff data engineer that comes from FAANG and has at least 7 years of experience, with a computer science background, at least seven different technology stacks, including SSRS, Hadoop, you name it they want it....

Like, I thought we were trying to establish that experience is not the most important thing as a data engineer, passion and ability to learn and grasp what you're doing and perform was? In terms of talent I wouldn't say that I'm one of the best Like obviously not because I don't have formal experience working as an engineer, I have a muddled role like many other people do where I did a lot of data engineering and pipeline building, tons of SQL and Python.... 


But these employers want someone who is a veteran. They want someone who has at least 7 years of experience, has seen everything there is to see, can do the job of three different people. It's insane. The requirements are absurd. It's like they don't want someone who's competent and able to do the job, they want someone who is good on paper and checks all the boxes, and that's it",2023-09-04 22:55:58
1695hxl,checkout my first complete data-engineering project," 

Hello guys, i need you to score my side project (give a mark :p )... do you think it's worth mentioning in my cv.

[https://github.com/kaoutaar/end-to-end-etl-pipeline-jcdecaux-API](https://github.com/kaoutaar/end-to-end-etl-pipeline-jcdecaux-API)

 ",2023-09-03 19:13:44
150e59x,What should a DE really know in SQL to succeed in an entry level job?,"I used SQL my whole life and I don't have issues with data modeling or querying in general. But when I see jobs asking for a good level in SQL, I wonder what does good mean ? What are the items that I should really know to qualify as ready for an SQL DE job ?",2023-07-15 14:58:22
14sh6f0,Do you think it's valuable to maintain a personal portfolio even though you have a job?,"











Hi guys, in my current job I only have the possibility to use excel, but I think of learning sql, python and others as a way to have better jobs. The point is that for that I would need a daily practice, which I could only achieve with personal projects. Do you think this is a valid way to keep the knowledge with me and show that I master the tool? or would a recruiter/employer not care too much about this? Honestly, I'm a little discouraged about learning only through courses and not having the opportunity to put anything into a project, I think that way I would understand better. Anyway, thanks for advice.",2023-07-06 18:13:13
14khz2i,Snowflake and NVIDIA partner to let businesses use their own data to build custom AIs,N/A,2023-06-27 16:15:30
145ig4o,A-ha moments,"Had a bit of an analytical SQL a-ha moment today while working out a BI calculation, something I would previously have leaned on looping in Python for. It felt good and someone might get something out of it so I thought I'd share.

The puzzle: Calculate how much time a service desk ticket has been worked during business hours (09:00 - 17:00 Mon-Fri, no holidays). Note: Ticket duration can be multiple days.

fact_service_desk_tickets
--------------------------
ticket_id | start_timestamp | end_timestamp | date_key | time_key
.

dimension_date
---------------
date_key | full_date | year | month | calendar_week | day_of_week | weekend_indicator | holiday_indicator
.

My initial thought was that I need to generate a 1-minute time series for each ticket, which would have taken forever to run. The a-ha moment was realizing I can use least(), greatest(), and a where clause on a table with 1 day granularity.

SQL (Postgres)
-----
-- Break the tickets into 1-day rows and add the _date column to represent the day.

with

  day_series as (

    select 

      ticket_id

      ,start_timestamp

      ,end_timestamp

      ,generate_series(date_trunc('day', start_timestamp), date_trunc('day', end_timestamp), '1 day'::interval)::date as _date

    from fact_service_desk_tickets

),

-- Add start and end times for each day the ticket was worked on (total, not business hours), as well as columns with business hours start and end time.

  main as (

    select
      ticket_id

      ,start_timestamp

      ,end_timestamp

      ,_date

      ,case 

        when start_timestamp::date = _date then start_timestamp::date

        else '00:00:00'::time

      end as start_time

      ,case 

        when end_timestamp::date = _date then end_timestamp::date

        else '23:59:59'::time

      end as end_time

      ,'09:30'::time as start_business_time

      ,'15:30'::time as end_business_time

    from day_series ds

    join dimension_date dd on ds._date = dd.full_date

    where

      dd.weekday_indicator = 'Weekday'

      and dd.holiday_indicator = 'Non-holiday'

select

    ticket_id

    ,sum(extract(epoch from 

      least(end_time, end_business_time) - greatest(start_time, start_business_time) as business_hours_duration_seconds

from main

where

  least(end_time, end_business_time) > greatest(start_time, start_business_time)

group by ticket_id


Using least() and greatest() in the final select clause lets us pick the correct start and end times for that day (for start time: either start of business hours or when the ticket was submitted, whichever is later. vice versa for end time).

And then in the where clause, we filter out days where the ticket was not worked during business hours (resulting in a negative number).


Congrats if you made it here and thanks for reading. Feel free to share some of your data a-ha moments in comments!


p.s. I have never done leetcode or interview prep, are the problems ever this tough?
      
 p.p.s. Sorry about this cursed formatting",2023-06-09 22:11:00
139wcpq,What can I do about redshift slowness?,"Hi Reddit DE - I'm a data analyst that changed jobs to join a dinosaur working with Redshift. I was previously working with Bigquery for SQL scripts, where just looking at table samples (e.g. SELECT \* FROM table LIMIT 5) took microseconds. Under the AWS Redshift architecture, these same table sampling jobs now take 3+ minutes and I'm going crazy. 

The admins have set up resources dedicated under a user cluster, so things could be worse, but is there anything small you suggest I push for to make life more bearable? I think I need to start by asking for more 2x, 3x more resource slots, but please stop me if this sounds stupid.",2023-05-06 17:28:39
12zsn6p,Crappy day,"My boss, who is universally liked in the company, resigned for reasons (I don’t want to divulge too much… she made the right decision for herself, but she never should’ve been put in that position). She was universally liked and added a ton of value to the company. Everyone on our team is upset right now and reeling. 

And on top of that, I’m behind on work, feeling demotivated, and my polars code fucking broke. Thanks life.",2023-04-26 18:24:49
12p1uoq,Mono-repo or multi-repo for collection of ETL pipelines in Databricks?,"We are building a collection of 20-30 ETL pipelines in Databricks foillowing the medallion architecture. These are each run in a Test and Prod environment.

The question we face now is whether to build a mono-repo with a strong folder structure, or build multi-repo with a repo per ETL pipeline.

As I see it

Mono-repo advantages:

* Easier to manage all code in one place instead of separately configuring/cloning individual repos
* Potential for code sharing/reuse

Multi-repo advantages:

* Easier to release individual pipelines to prod without having to release the entire repo
* Cleaner separation of dependencies

What is the conventional wisdom here?",2023-04-17 04:23:40
11mj5cw,Databricks: Programming in Python vs PySpark,"Alright so Databricks allows for Python and PySpark programming. 

If i code in only Python, does databricks distribute the computation across nodes using Spark? Or would that only occur if I use PySpark?

I’ve been using databricks for a month now and didn’t think to program in PySpark and have been programming in Python but now I’m afraid I’m not getting the full benefit of Databricks.",2023-03-09 04:53:36
11hngbs,azure datafactory.....ouch...help,"So, I work or a large pharma company and we have been asked to start exploring azure as a cloud service. We currently do all our work on prem with sql servers and SSIS as our ETL tool. We just finished up a one week training on adf and I don't think this is the right tool for my team. We write lots of code and ADF is very low code and in the low code set up, simple tasks feel painful to accomplish and very clunky.  My question can azure databricks be set up where all our pipelines run in an a databricks notebook? My goal would be to have notebooks set up and then use adf to kick off those notebooks on a schedule. I want to stay as far away from all the tools and widgets within adf. I simply want to use it as a means to kick off a notebook with a pipeline in it. Is this something that a lot of you do or have seen people do?

&#x200B;

thanks,",2023-03-04 02:55:24
11bw23f,Thoughts on analytical dashboard insanity,"
TL;DR Do you think dashboards are really as useful as the amount of work that goes into them?

I've been thinking about this a lot. Sorry for the slightly clickbait title. This may be selection bias but I frequently hear from other data engineers that they build dashboards that don't get used a lot. Or get stuck in infinite scope creep. 

Which is an odd relationship to have because there is definitely a large chunk of analytics focused data engineering work that follows a high level pattern of 'integrate disparate data sources, model data views and metric calculations, make it available to a dashboard' 

I wanted to get this community's thoughts about how realistically useful dashboards are, or how often they think the dashboards justify themselves in value.

My two cents is that one fundamental problem of dashboards is that they are very often built around an assumption of what questions are worth answering. But the universality of that is in flux. The answer's value ebb and flow with time and accumulate nuances and caveats as the business evolves and the markets change. And so what happens then (which causes the infinite scope creep) is that someone with subject matter expertise will look at the dashboard and say 'can you also show, from this subset of whatever, what the breakdown of whatever is?'

I think operations dashboards are necessary. Even just for us to have a simple place to check and see if sh*t is breaking. Operational dashboards simply provide the first order answers of what the hell is going on in very literal terms. Nothing beyond basic aggregations and generally are just a print out of transactions almost. 

Would be interested in hearing more people's perspective on this",2023-02-25 21:19:33
1aqx9sx,Why do some companies want Data Analysts to also perform the role of a Data Engineer?,"I've seen a lot of postings, where the role is for an analyst and they are expected to prepare, maintain and improve data pipelines. preparing the data so teams can collaborate and extract business value. Is this the norm, or is it part of the job of an analyst in general?",2024-02-14 20:43:20
1aqcvov,DBT Column Level Lineage only for DBT Cloud customers,"DBT announced a preview of a much requested feature - Column Level Lineage. 

The only catch is you have to be a Cloud customer. 
Does anyone know anymore about this and if it is coming to core?",2024-02-14 03:17:09
18ym35o,Are you using Iceberg in production?,"I’m a big believer in the data lake, even during the Snowflake hype. But it’s always been difficult to setup and use. Iceberg (as well as Hudi and Delta) made data lakes easier with built-in support for upserts and optimizations.

Databricks had been pushing Delta and I’ve seen it used in production at varying scales. But I think Iceberg is a better, more open alternative. Aside from Netflix I haven’t seen much about companies using it in production.

Are you using Iceberg in production? 
Are you considering migrating existing data lake to Iceberg?",2024-01-04 19:50:03
18kfn28,How is Databricks fundamentally/architecturally different from other hyper scalers when all them have launched their own Lakehouse replicas? For e.g. BigLake under GCP,Title,2023-12-17 11:43:47
16ycoj4,"New job. Minor shit show rant, and advice needed.","So like the title says I started a new job today. Figure out why this report written in R broke 2 months ago. I open up a single file with 6952 lines in it. All that's left of me is a 2 page word dock explaining it the main file that call 3 other files, each with 2-3k lines of R code, to create a single 7 page PDF report from a single data source.

WHAT THE ABSOLUTE HELL AM I LOOKING AT LOL. I don't know R, but it's similar enough to python that I can follow along to get the gist of what it's doing. BUT STILL! WHO NEED 10K+ LINES OF CODE FOR THIS?

Should I tell them it's all bunk and make my case to start from scratch, or just buckle down and learn myself some R to make it work?

Edit:
So after further investigation there are 3 major issues.

1: He would comment out old code and leave it in the file. Shout out @Strider_A for the git joke. After deleting all the obvious old code I have taken it down to a total of 5,679 lines of code among 4 files.

1.5: he has multiple SQL queries as strings in file. I could probably just copy paste those to a different file and figure out how R opens and reads files pretty easy.

2: He seems to be allergic to loops or reusable functions. 300 lines of calling the exact same function with the same inputs, but typing out every column name rather than looping through the columns. Or 1000+ lines of the same function copy/pasted with a different name for each column name.

3: assigning variables with hard coded paths to dozens of files rather than walking through the directory.

Bonus round: I don't know enough about R to know if this is bad, but like 400 lines of creating 3 tables. Running a few data validation scripts on each column matching regex. Idk this part is probably fineISH.

I might have overreacted to the 10k+ lines. This probably won't be too difficult to figure out. Thanks for all the advice and for joining me in this day 1 freak out.",2023-10-02 23:52:46
15z7eva,Data engineering at Bloomberg,"Curious if anyone's had data engineering experience at bloomberg or similar. I'm debating taking an offer from bloomberg (UK) for a data engineer position, while currently working in fintech also as a data engineer.

My main concerns from having asked questions in the interview process is that the tech is very abstracted, high level, config based tools to build workflows and pipelines. I.e. I would not learn or touch any infra, cloud, spark. So basically just SQL, Python and bloomberg tools. Some tools (airflow, jenkins, kafka) are basically open source equivalents or actual open source tools with a bloomberg wrapper built by their software engineers, which is not as bad I guess.

My current position is kind of the opposite, a modern open source tech stack with Scala, spark, airflow, GCP, k8s, jenkins, terraform.

Companies these days seem so fixated on engineers having specific experience in X,Y,Z. I've even been rejected before because I didn't have serverless AWS experience. Are my concerns of using prop tech and lack of spark/cloud/infra valid or would it not matter when I eventually try move from bloomberg elsewhere? Is it a good name to have on the CV?",2023-08-23 15:39:38
15loaaj,Anyone Freelance on the Side?,"I have a full time job I’m happy with. But because I’m a capitalist I want to make more money. I also don’t want to deal with the constrains another job or 1099 gig has, I want to be my own boss. 

I’m considering doing freelance via upwork or similar. I have about 5 years experience with python and sql and various tools like snowflake and airflow. I think lower effort clients like webscraping or maybe even no code tools could be a good start. 

Anyone else do this? Is it super hard to start?",2023-08-08 17:18:00
15etn2w,Advice on improving the data architecture at a company I just joined?,"A few weeks ago I started a new job at a small company, and due to several people leaving, I'm about to become a data team of one. Before you tell me to leave immediately, I took the job because I'm going back to school soon and the work life balance afforded at this company should allow me to balance studying with a full time job. If it doesn't, I'll quit because school is the higher priority for me at the moment. And I was fully prepared to work part-time before I had this job offer.

But that being said, I would like to give an honest attempt at improving the state of the company's data pipelines. Currently there are a ton of pipelines all running via airflow, and the airflow site itself is consistently going down, dags will fail and have to be manually re-run, some dags seems to run in an order such that a downstream table will build prior to one it relies on, resulting in missing data in some columns.

I'm coming from a very large company that builds data analytics software, so I'm used to using tools built in-house and within one cohesive ecosystem. I realize this post is fairly vague, but I suppose if you could give a pointer or two to someone entering the deep end, I'd really appreciate that!",2023-07-31 22:06:38
13rkcjz,The State of Data 2023,N/A,2023-05-25 15:11:49
125tqsk,"Software engineer vs Data engineer, does job title matter?","Recently my manager approached me about switching my job title from data engineer II to software engineer II, and I’m not sure what to do.

For some background I’m three years into my career: two years in a post-grad rotational program, and one year as a data engineer. During those two years I was always working to become a data engineer and hope to grow in the data space to a data architect (my company only has architects at the leadership level). I work on an agile squad that is 1 of 6 squads working on an overarching master data management solution. On those squads there are two managers that developers report to- all data engineers report to one manager and all software engineers report to the other. Previously all the squads were a mix of data and software developers, but now all squads are made up of one or the other. I am the only person on the 6 teams misaligned, a data engineer on a team of software engineers.

This is why they want me to change titles. The managers want to be able to “own” squads, so they can be better aligned to the products they support. I totally understand the reasoning, but it really seems like I’m just an inconvenience to them for HR purposes. My day to day won’t be changing but I’m hesitant to be called a software engineer when I don’t have the technical skills to match that role. I’m also worried about career prospects if I want to leave my current company because I’m not inclined to lie about my job title and all my interests are in data roles. 

Sorry for the long post I’m early in my career and just don’t know how much things like this matter. Does anyone have any advice? I’m usually a team player, but when it comes to things like this I feel like I need to think about myself first.",2023-03-29 16:55:33
11lid59,How would you explain a data pipeline to a non techie?,"We all talk about data pipelines when we talk about DE or DE related positions. What how would you explain a data pipeline with examples to a layman(maybe). How many different examples can we think of?

Just a random question that popped up.",2023-03-08 01:38:01
1anua9t,How do you unit test your SQL pipelines?,"We want to start doing unit test at my company and I wanted to gather some best practices from other folks. How do you approach unit testing for SQL pipelines at your current roles? Which technologies do you use? Do you execute them on a daily basis, scheduled, only once? Incrementally or for full tables? Thanks!",2024-02-11 00:01:20
1algpln,What more do companies want? [Rant/RealityCheck],"If I can help a large company save $30k dollars per month, by moving a data pipeline from pure S3 batch processing, to near real time, S3 event driven processing.

And also save the company 100s of TB of data per month, by interacting with both Data Producers and consumers and finding the right columns and schema/data model to reduce external API calls and reduce data size at data lake.

What more do companies want when they see my resume or take an initial round of interview? Whatever interviews I have given so far, don't seem to focus too much on my achievements, rather pin point shortcomings in my understanding of certain terms/concepts they want me to know.

I am honestly asking you guys, why won't Engg Managers or Team Leads look past shortcomings to see that a person csn actively identify gaps and be really productive, while in the age of ChatGPT learn and understand the concepts or terms in Data engg that he/she may not be aware of?

Please give me detailed answers no matter how you see me question here",2024-02-07 23:12:36
19en4gl,Switch career from Data Engineer to Data Architect,"What should i focus on developing my skills to move to Data Architect from a Data Engineer? Is it worth it career wise? 
Also if i could get a good resource or links to projects to learn data architecture that would be much appreciated.",2024-01-24 18:19:05
18e6vcg,I passed the databricks data engineer associate exam!,"Coming from AWS SAA-C03, it was surprisingly easy. Or maybe i overprepped because the AWS one was tricky. Hooray! Derar's course in udemy was super helpful. Exam Topics websites too!",2023-12-09 05:56:17
181axwe,Dealing with big JSON objects - flatten into tabular or find a way to query JSON efficiently?,"Source data is big json objects. We don't use all fields but some of the ones we do use are heavily nested.

My instinct is to land the json into a raw stage, then extract the pertinent fields to yield flat tables that can feed data marts. 

Obviously that's a very high lever overview. But wondering if anyone has opinions on that direction vs somehow keeping JSON intact.

Thanks for any pointers, links, thoughts.",2023-11-22 14:56:07
17rd3su,What are your thoughts about Tests in Data engineering?,"Hi all, I'm in the DE since 5 year and I've been working always within the python stack (spark, pandas, polars, duckDB etc.), and i never ever worked/written with tests or unit tests in data engineering field (Data quality, integrity, testing pipelines etc..).

What are your thoughts about this? Did you ever developed or used these kind of tests?",2023-11-09 13:37:29
16dgqgl,"What is the difference between a data engineer, a data platform engineer, and a software engineer - data platform?","I've been looking into the differences between these 3 job titles, and am having trouble wrapping my head around the differences

* Data Engineer
* Data Platform Engineer
* Software Engineer - Data Platform

I guess for the purpose of the question, the DE title can exclude analytics engineer and focus more on the infrastructure side rather than analytics

In addition, what even is a 'data platform' :P",2023-09-08 17:31:46
15flpsj,Honest admittance,"I am a very experienced DE, I have built very complex data models for huge commercial entities, and nobody has been able to explain to me what a webhook is in non-technical terms. I work with students but I am still in the dark.",2023-08-01 19:09:41
14v1nq9,Merging Data Too Big for Pandas + Moving to Cloud for More Compute Power,"I'm analyzing large datasets of real estate transactions (CSV) and parcels (xlsx) for public policy research (I'm a CS student). The files are separate for each year and type (1.5 GB total).

The goal is to merge sales data (left join) to its associated parcel data for the year of the sale.

Then, identity ownership of properties using fuzzy matching on the owner name + address columns to find the largest owners.

I have 2 main challenges I'd appreciate advice on:

\- I've appended sales data for all years. However, the parcel data is too large to append easily using Pandas on my laptop (memory issues). I think it also makes sense to put these files into a more efficient database or at least file format (parquet perhaps). I might take the approach of merging each year to sales first, that way I don't have to append. But the latter point still stands.

\- Later I will also want to run fuzzy matching on the dataset, which I've already optimized a bit by comparing owner zip code first and creating smaller buckets of names.

I'm thinking I want to create a database (AWS preferable), then use a lambda function for fuzzy matching. But I'm struggling a bit with the details and which service might be best, especially for the merge... I have slight experience with RDS and Lambda. Thanks!

&#x200B;

EDIT: Thanks for the suggestions! For this instance, it seems cloud is probably overkill, and there were some great suggestions for how to avoid it right now. However, in the future, I will have much larger data, so that was part of the motivation for using this as a first example.",2023-07-09 15:39:31
14lhzv9,What is it you do with Python in your Job?,"I want to switch jobs and trying to determine what skills I would need to learn to try and get some interviews. My current job skills are heavy database, and coding in c#.  Therefore I think one of the skills I apparently need is Python, but I am not sure in want context I would need to learn/use it.

So my question is how are you using it? Is it simply to import files, run transform calc and export file? Are you exporting results to Databases/ pulling from databases? What editor do you use for your programs/scripts? I just feel lost at the idea of the generic ""learn python"", like I do with most other skills I am going to have to learn.",2023-06-28 19:02:20
12kppyf,"Data Pipelines - how do you build data pipelines for sources not available in today’s ELT tools (Fivetran, Talend, Airbyte)? Old fashioned scripts and YOLO?","While the “modern data stack” marketing has made the higher execs believe that the ETL/ELT tools solve for all data ingestion problems, but in reality all the platforms offer only handful of connectors that they maintain themselves - rest is outsourced to community which might/might not be very active, depending on the data source. In such scenarios, how do you handle data pipelines?",2023-04-13 13:44:27
12kd66n,SWE --> Data Analyst --> SWE again,"Worked as a full stack dev (front end) for four years and pivoted to DA. I enjoy exploring data but found it difficult to produce useful analyses. After being in the role for 4 years, I realized my communication skills do not get better, and I kinda hated sitting in a lot of meetings. A lot of my analyses weren't used, and I want to produce something useful and tangible. I enjoyed the first three years because I mostly built dashboards, wrote SQL and didn't have to attend too many meetings. It's definitely less stressful that SWE.

My last role as a product analyst in a startup and I hated it. I'm getting good at coming up with metrics but so many meetings and I am expected to work as consultant (i.e. provide recommendations about the product that I didn't know much in the first place. I interviewed around, and it seems really hard to get a role that pays > 150k as DA (unless maybe in big tech). The analysis that took me weeks to produce is also not used. And, there are endless questions to explore. Started to think that I'm not good at this role. 

Because I hate dealing with people, I don't think I'd be a good candidate to be an analytic manager either.  What do you do if you enjoy exploring the data but you are not good at it nor good at presenting the results? I am suck at presentation btw. I don't enjoy SWE as much as DA, but observing how fast I am troubleshooting bugs/researching something, I feel like I'm more natural as SWE. I love the investigation part of data analysis, but don't feel like I'm good at it. I don't find building as enjoyable as exploring, although I don't hate it. 

With someone in my background, is it easier to be front-end dev again, even though it was four/five years ago, or is it easier to pivot to Data engineer? And will the hiring manager think it's strange that I want to be SWE again?",2023-04-13 05:23:30
126qavy,"In terms of efficiency, what are DOs and DON'Ts when working with Pyspark?","I'm newish to Databricks and Pyspark but not new to analytics and Python + Pandas. Now I need to build some workflows. 

Are there certain DOs and DON'T when handling Pyspark dataframes and views? 

At a high level, I'm doing something like this:

1. Read tables from Snowflake to df -> df.createOrReplaceTempView('df\_view')
2. Get data from S3 with spark.read.format(""csv"") etc. -> create 's3\_view'
3. Do a bunch of joins and transformations using spark.sql(""select \* from df\_view join s3\_view..."")

What I'd like to know is, for example: 

Are there performance differences between df.join vs. spark.sql(""select....join"")? Or it just goes through the API and is handled the same in the end? 

Is there unnecessary overhead with createOrReplaceTempView if I can process the df directly?

It seemed to work fine for the first two joins but now a third one is taking too long. 

TIA!",2023-03-30 15:27:47
11tqzbe,What’s the best AWS service to run a Jupyter-like Python script calling an API to download data?,"I have a working prototype of Python code in a Jupyter notebook that downloads data onto my local machine.

How do I set this script up such that it automatically runs on a schedule and downloads data to AWS S3? I’ve read AWS Glue, Lambda and maybe an EC2 instance but not sure which is the “best” (cheapest + reliable) approach. Any thoughts?

We are using Snowflake as our data warehouse.",2023-03-17 13:22:35
11lra9t,"Introduction to Kestra, the open source data orchestration and scheduling platform",[https://www.loicmathieu.fr/wordpress/en/informatique/introduction-a-kestra/](https://www.loicmathieu.fr/wordpress/en/informatique/introduction-a-kestra/),2023-03-08 08:56:43
1aphlml,"Using pandas in a data-intensive application - what the best, RAM-efficient, alternative?","One part of my app acts as a pseudo data-pipeline, where I ETL a single player's data at a time (I have to due to the nature of the API I'm pulling data from).

I am currently using pandas for all of this, where I pull the data from the API, throw it in a dataframe, transform it, and run \`pandas.to\_sql()\` to store it.

The problem is that this is memory intensive, and I'm running into RAM issues when running this in a docker container.

What would be a good memory efficient tool to work with moving datasets through an application?

To clarify: I am using pandas as a tool to efficiently pass around this dataset that I add data to, transform it, and store it in the database. I would need this replacement tool to do the same thing in python. I also have only 1 server, so a distributed system like Spark isn't an option for me",2024-02-13 01:52:35
19ap4ga,Data Analyst > Analytics Engineer,"Hi everyone, I’ve been a data analyst for about 4 years, primarily using SQL on a day to day basis. I’ve been cleaning raw data and creating data tables to use for analytics and dashboards. I feel like I’ve been doing both an analytics engineer and data analyst job. 

I’ve recently been laid off and I want to pivot into the analytics engineering route. Does anyone have any recommendations on how I can get started? Any additional skills I need to learn? 

Thanks in advance!",2024-01-19 17:53:52
18tx3o5,Analytics Engineering in 2024,N/A,2023-12-29 20:50:52
17uc34w,People that transitioned for no-code/low code to databricks/spark/python sw development: how did you do it?,"Basically the title.

Currently I’m an Azure ETL developer and want to break into real time streaming jobs. I need all your advices to transition from no code to software development jobs.",2023-11-13 14:41:25
17po009,Personal Project of End-End ETL,"Hello everyone,

I recently completed a personal project, and I am eager to receive feedback. Any suggestions for improvement would be greatly appreciated. Additionally, as a recent graduate, I'm thinking whether this project would be a good fit to include on my resume. Your insights on this matter would be very helpful.

&#x200B;

The architecture is:

https://preview.redd.it/h1j2kx65bvyb1.png?width=3582&format=png&auto=webp&s=a7ecb570b0a95dad3f25d8fef196068ab53a6d72

The dashboard for the project is: [https://lookerstudio.google.com/u/0/reporting/89878867-f944-4ab8-b842-9d3690781fba/page/CxAgD](https://lookerstudio.google.com/u/0/reporting/89878867-f944-4ab8-b842-9d3690781fba/page/CxAgD)

&#x200B;

https://preview.redd.it/3w4gqbn8bvyb1.png?width=2482&format=png&auto=webp&s=9d3e65c593dcb3c2155af6b46c058911d1117b1a

Github repo: [https://github.com/Zzdragon66/ucla-reddit-dahsboard-public](https://github.com/Zzdragon66/ucla-reddit-dahsboard-public)",2023-11-07 06:16:26
17lfb8v,how many analytics engineers here work on ad-hoc SQL requests?,"hey folks, I'm on a team of ""full-stack"" analytics folks. Our scope is the entire modern data stack: ETL, warehouse, data modelling, BI, reverse ETL, reporting & analysis.

We don't have budgets this year or next to hire data or BI analysts so just wondering how many other folks here just get rammed with ad-hoc requests? Many of us just want to focus on data modelling and not having to deal with stakeholders. We've had to constantly  push our infrastructure and modelling projects month after month because there's a constant influx of request from stakeholders.

Is this a problem for you guys? What are you doing to address it?",2023-11-01 15:56:59
16zjdbu,From nurse to data engineer ...send help.,"Hey there,

I'm a nurse, so I have a batchelors albeit incredibly unrelated. I'm approaching 30 and I'm so very burnt out.

I want to get into data engineering, it's always interested me and it has a much broader field than pidgeon-hole nursing. (Actually I'm quite downplaying my love of data, design and solutions...), however, I need to find a qualification that fits around my current job.

What would you suggest?

I've been looking at online academies and Microsoft certifications etc. Doing a masters isn't financially viable right now. But there's so many pros and cons and all the pathways are a little overwhelming for someone new to the industry. I've been dwelling on this for over a year and I just need to take the plunge.

Also, my current role is quite well paid so ideally I'd be looking for my first data job to be £40K+ (about $50K+), to cover for mortgage payments etc. Is there a qualification I can do that would help me get there?

Please help 😅",2023-10-04 10:17:11
16xyt26,"To those that work in manufacturing, do you believe IoT (and heavy collection of manufacturing data) is a fad?","I'm working at a company that is trying to make a push for I4.0. Whether or not you think thats a buzzword, there are tons of companies making the push towards smart manufacturing. My question, is it worth it?

Maybe it is only because we are new to it and I'm not seeing the full benefits but cost of collection (sensors, PLCs, etc) is often expensive. If you want production data to be captured constantly (good counts, scrap counts, etc.) you need redundant networking/power.

Moving data to the cloud is unbelievably expensive at this volume and SCADAs/Historians are equally expensive. The hardware requires highly skilled employees to set up and maintain the system.

All this for managerial metrics and the ability to root cause faster? Maybe if you're good enough get some predictive maintenance?

To the others in the industry, is this all worth it? Have you seen the value add at your own job?",2023-10-02 14:55:06
16jqwut,Data Quality: Horror Stories (And How You Fixed It),"Wanted to hear data quality-related horror stories and also how people fixed them (like [https://www.reddit.com/r/dataengineering/comments/130rfc2/whats\_your\_favorite\_data\_quality\_horror\_story/](https://www.reddit.com/r/dataengineering/comments/130rfc2/whats_your_favorite_data_quality_horror_story/) but also with solutions that people can learn from)

&#x200B;

My story: Last year, I was handed a dataset to analyze and forecast our Q4 sales. I didn't double-check the data source and ran my models. Predictions were off the charts, and the team was ecstatic. But a week before the big presentation, I realized the dataset had duplicate rows, and some dates were formatted MM/DD and others DD/MM. The ""huge"" sales spike? Just a data mirage. I had to pull several all-nighters to clean the data and re-run the models. The team was not too happy",2023-09-15 22:27:59
15tqf1i,"How much ""data"" experience did you have before getting your first data engineering job?","Tools experience also works too.

Basically what level of familiarity did you have with the topic in general?",2023-08-17 15:14:57
158fbot,Why learn terraform & docker for data eng? Am I missing something?,"I don’t understand what the need for docker is, having every pipeline set up as a docker image, even if it’s from scratch would still take up significantly more space than just a simple script with the correct environment already setup.

As for terraform, what need is there if the infrastructure is already set up? Do I really need to know how to deploy 5 ec2 instances? Wouldn’t they already be running in the background?

I’m at a point in my career where I’m picking up devops and cloud, including terraform but I’m not entirely sure if it’s particularly useful.

Hoping someone can help me understand",2023-07-24 16:22:59
151edrn,I made a Stock Market Dashboard,"Coming from a finance background, I've always been interested in trading & investing.As I switch to tech and data for my career, I wanted to create my very first DE project that combines these two interests of mine:[https://github.com/hieuimba/stock-mkt-dashboard](https://github.com/hieuimba/stock-mkt-dashboard)

I'm proud of how it turned out and I would appreciate any feedback & improvement ideas!Also, where do I go from here? I want to get my hands on larger datasets and work with more complex tools so how do I expand given my existing stack?",2023-07-16 19:00:39
14s0d0h,No entry level jobs for data engineer or machine learning engineer??,"Job requirements are ridiculous, On LinkedIn Data Engineer roles require 3+ years of experience and Machine learning Roles require around 7+ years of experience. I was preparing for a data engineer role but perhaps there's no point in doing that anymore, Maybe I should start focusing on being a data analyst and then switch jobs?? What advice would you guys give to a guy in college interested in the field of data??",2023-07-06 06:29:38
13rqkz8,Rust is coming to Databricks?,"I was listening to the D3L2 podcast (yes, I'm a nerd but so are you) and it was mentioned that Databricks will be making some sort of announcement about Rust in the near future.  Any clues to what it is or do any databricks employees want to go ahead and leak it here (seem to be quite a few hanging out in this subreddit)?

My guess is that the JVM is going to get its last nail in the coffin and Rust will be the native language for all the distributed data processing going forward.

[Clip from podcast](https://www.youtube.com/live/NEL6DluUxgw?feature=share&t=2401)",2023-05-25 19:17:19
13e939e,ODD Platform - An open-source data discovery and observability service - v0.12 release,N/A,2023-05-11 01:13:29
136kv4w,"I am lost. ""DE"" for 2+ years, but lost. Advice?","I joined a healthcare tech right after college. So that is a total of 2.5 years in the field -  be it data, IT, corporate, you name it. 2.5 years. Not a fresher. Not a senior either.

First project was just a bunch of DML SQL queries fired up in Snowflake as we got tickets. I took time to learn Snowflake better, gain a bit of healthcare knowledge and so on. Current team is Data Governance and Data Quality in Informatica tech stack. I worked on tiny part of Python, APIs, PBI dashboards, small ETL setups via data bricks, ADLS and so on.  I know a couple of things about a lot of things, nothing in depth!

It is always maintaining something some senior has built. Then manager says that I ""lack initiative"". I have tried to create views that will help the business, delivered on everything to the best of my capacity. I am very active outside of my DE role in office. So what he means by ""lacking initiative"" I am not sure. He suggested that I could upskill, because I am not a ""fresher anymore"". I want to switch jobs, but I have NO CONFIDENCE in my tech skills.

I thought of upskilling, did couple of projects end-to-end from EDA to building reports/dashboards using Tableau and Power BI.  I have this aim to utilize the company benefits policy and gain a certification in Azure Data Engineer Associate track, in the hope that I have SOME leverage in the hiring market. Completed Azure fundamentals and Data Fundamentals \[mentioning here to show that I have already started prepping\]

I had even asked for project ideas in this subreddit in the past.

But nothing is giving me confidence, I am not sure why I am this lost in my mindset.

Is it because we use loads of no-code ETL options? Maybe I am terrified to code? I have to google syntaxes a lot and that makes me scared, I cannot do that in an interview!!

Do I spend more time doing projects? Do I stick to Azure because we are ALL migrating to Azure like crazy here. How much can you actually learn outside of work?

I have started reading DE books too. To get a proper structure to my upskilling/gaining more knowledge process.

&#x200B;

I love this field - because it makes sense, if you know what I mean? I hate typical SDE roles. ETL and Data Governance make sense! And I want to get better. I really want to look at a problem and come up with Data Architecture solutions, I want to do be able to do proper analysis and know what tools work together, which component goes where and build things from scratch!

But is there someone who has taken this path, faced similar struggles?

I am okay getting called out too :D",2023-05-03 13:40:54
13292ov,Developer Deletes Entire Production Database (r/videos xpost),N/A,2023-04-28 20:36:52
12er9ut,How do I sort a massive (300 GB) csv with Spark?,"I've got large file, about 20GB as a parquet and 300GB as an unsorted csv. I'd like to sort by a field and save it as a single csv file. I'm using an ml.r5.12xlarge sagemaker instance to read the parquet to a dataframe, sort, coalesce to a single partition, and save as csv. I thought I might hit problems when I coalesced, but it's actually choking during the sort. I've tried it filtering the dataset to a quarter of the size, but it still chokes.

Suggestions?",2023-04-07 16:42:51
128dwr6,Seeking Recommendations for a Master Data Management Tool,"Hello Data Enthusiasts,

I am currently in search of a reliable and effective master data management (MDM) tool. We previously used SQL Master Data Services, but it has since been deprecated. We were considering partnering with Profisee, but their customer service hasn't been impressive enough for us.

We need this MDM tool for one of the largest shipping companies in the UK. I would greatly appreciate any recommendations based on your personal experiences.

Thank you all in advance! Cheers!",2023-04-01 06:47:39
12874cr,Moving from Analytics Engineer → Data Engineer. Does my reasoning make sense?,"Context: I’ve been in the same company for 5 years and moved from data analyst → business analyst → data engineer. Based on what I know about Data Engineering, I feel my work is closer to Analytics Engineering since our team is limited to SQL + Python & task scheduler (no airflow). I tried out dbt-core recently and found it amazing for complex data curation.

I’ve been applying for new roles recently and was recently offered a role as a cloud engineer to optimize cloud spend (tools look to be python/grafana/GCP/Azure/tableau). I’m trying to justify if I’m making the right career decision.

Based on my experience with the current team:

1. I enjoy ingesting new data into our data warehouse (understanding APIs, translating business requirements to data requirements, ingesting data with python & transforming data into production datasets with dbt)
2. I like building datasets using dbt + implementing git on my scripts
3. I can build business/executive level dashboards

That said, these are my reasons for why I’m considering a change:

1. Been at the same company for 5 years and looking to explore something different + pay bump
2. I want to try the Export and Load part of ELT before transitioning into a people leader role. I believe that by doing all roles in ELT would help me stand out should I become an analytics or data engineering manager in the next couple of years.",2023-04-01 01:31:57
11npctm,How much raise and bonus are you getting this year ?,"Trying to get a sense of what the market is looking to like.

I got 3% bonus and 3% bonus, imo, it's not bad not good, so okay !!",2023-03-10 13:33:59
11grme8,Geospatial DuckDB,N/A,2023-03-03 04:49:29
11awcne,4 ways to build dbt Python models,N/A,2023-02-24 17:06:57
1ad1xqt,"What's the purpose of using Kafka, when the same can be processed though an Event Driven Architecuture?","Some context : I'm working on a predictive maintenance prototype on Azure. Essentially, the sensors send in readings periodically every 30s (Temperature, Vibrations, Pressure, Noice, etc.). The data is added into an event hub. The data is then processed and dumped into ADLS V2. The readings are passed into an ML model and run against some basic checks(If temp exceeds, send an email notification to the asset owner, etc...) The notifications (for now) are processed via logic apps(When a blob is created within the datalake)

Can these events directly be processed via an event driven architecture instead of using Kafka? Or processing the data through serverless functions? 

Also, what are some good visualization tools that can let me monotor this data in near real time?

I've just started learning to use Kafka, and would appreciate any answers. ",2024-01-28 12:47:47
1acqprq,Data Warehouse approaches on GCP,"Hello!  


First of all, all my apologies if that question has been asked before.

I need to design a data platform on GCP.     
I gave some thoughts about the possible ingestion and aggregation approaches.    
In the past I used a full BigQuery approach (Option 2 below).    
I was wondering if keeping the raw and staging data in Cloud Storage and using Spark for transformation could make sense.    
I think it can be great because of cost and being cloud agnostic. In that scenario the Data Warehouse will be split between Cloud Storage and BigQuery.

Do you guys have any experience with using external tables and Cloud Storage + Spark compared to a full Big Query Data Warehouse approach?  
How about the cost?

## 1) First Option

* The csv/json files will be ingested in a Cloud Storage Bucket.
* The csv/json files can be read and written in parquet format in another bucket (staging area).
* Data transformations are done with Spark.
* The resulting transformations (usable by data users & BI tools) are accessible through BigQuery external (or internal) tables.

## 2) Second Option

* The csv/json files will be ingested in a Cloud Storage Bucket.
* The data are ingested into BigQuery as raw tables (staging area).
* Data transformations are done with BigQuery (SQL) directly.
* The resulting transformations (usable by data users & BI tools) are accessible through BigQuery tables.

## 3) Third Option

* The csv/json files will be ingested in a Cloud Storage Bucket.
* The data are ingested into BigQuery as raw external tables (staging area).
* Data transformations are done with BigQuery (SQL) directly.
* The resulting transformations (usable by data users & BI tools) are accessible through BigQuery external tables.

&#x200B;

https://preview.redd.it/db1jzh7l23fc1.png?width=1357&format=png&auto=webp&s=9d404ee6da16310d3f7b05a62b9b6429f5bbaf79",2024-01-28 01:27:12
19bvq2q,Is there interested in a Data streaming 101 course based on Rust and WebAssembly?,"A number of data folks I respect has recently nudged me with an idea to create a data streaming 101 course since I have been managing an open source data streaming project and a managed cloud service for over a year now.

I have thought about it a few times in 2023, and I'd like to ask the community, if you folks would like a data streaming 101 course.

A bunch of good ones already exist. Here is how this one would be different.

The implementation and hands on labs of this data streaming course would be based on Rust and Web Assembly. It would be entirely self hosted. There would be a bit of complexity to grasp, but I would work to make it as simple as possible.

I am thinking 7 emails with the course content delivered with text, video, and supporting code in a GitHub repo.

The only investment for the course would be time. And that too not a lot. Say 2 - 3 hours to consume the content and 2 - 3 hours to implement the labs.

Does this sound interesting? Let me know in the comments.",2024-01-21 05:10:08
18t52ro,DDIA,Isn’t there any more consumable book than Designing Data Intensive Applications That will help me move to data engineering from data analysis ? Everytime I try to absorb it I feel like I’m stupid 😂,2023-12-28 21:28:10
18pwyrs,Scraping tools," 

I'm just getting started with web scraping, and I could really use some advice on good web scraping tools. Any recommendations?

 ",2023-12-24 15:08:37
18p68zx,Learn DE by reviewing public projects?,"I'm curious to know how or even if you can learn DE concepts by reviewing project code bases?

If I were to find some project on GitHub and read through their documentation and (hopefully) well-commented code, would that help in the process of learning DE concepts?

I'd like to create some projects myself, but I only know the basics of programming, and although I'll likely use GPT to help me through the whole process (please don't shoot me), my problem is finding real world use cases that can be ""projectized"" by simulating the scenario and building my own solution.

So, my intention is to find projects on GH so I can get a better grasp of how others are approaching similar situations. 

TL;DR: What's your views on learning DE by reviewing GH projects, and what are recommended best practices?",2023-12-23 13:59:26
17rzyiy,"How would you deal with a tech lead that doesn't communicate, doesn't comment, but expects you to understand their code?"," I am no beginner, but I am not fully independent yet either. On my project, we have a tech lead who writes powerful, but confusing code. There are no comments. It's like a labyrinth. He sometimes changes things without communicating it. He communicates some things long before they become relevant, and then he gets angry, when I don't remember that particular sentence from weeks ago, which explains that one specific problem I ran into today.     ",2023-11-10 08:52:54
1771qcz,Introducing Dagster Pipes,N/A,2023-10-13 15:45:18
171h49k,Why is staff higher than senior?,"I’m currently a senior DE at a major media streaming company, and am being put up for promotion to become a staff DE.

Started my career as an accountant and in accounting, the progression is staff accountant -> senior accountant -> accounting manager. There are several other professions that follow that path as well. 

Whenever I tell my family “I might get promoted to staff engineer!” They act confused and say it sounds like a demotion 😂

Does anyone have any idea how/why staff is higher than senior for engineering roles??",2023-10-06 16:39:37
16vreyv,Does focusing on leetcode and job hopping make more sense than understanding the business?,I see dueling pieces of advice whenever I read career advice online. I keep reading that if you want to make as much money as possible you need to job hop and just keep grinding leetcode. But then other people say your main contribution is solving problems and code is just one way to do that. But if you're constantly leaving jobs then there is no time to really understand the industry you're in. Can someone explain the job path for someone who really understands the business? Is it moving into upper management and becoming a tech lead?,2023-09-29 23:45:47
16gscf7,Big company to solo engineer at a small one,"So I've been a DE for around 14 months now. I've learnt a lot and done some really cool projects. The main thing I've gotten here has been experience and mentoring getting to grips with working in cloud platforms. I've been promoted in the last year, done well etc. Though I'm currently the only engineer for the older systems.

Thing is I currently work for a huge multinational company. I just got an offer from a smaller company (still not tiny, but they've basically exploded in size over 5 years) who want to improve their data warehousing, automation etc and transition to being properly ""data driven"". I have no idea how i got it over 2 other candidates who likely had more experience than me. But ill be the first and for some while, only engineer. There wasn't even a real technical interview because they have nobody who could actually conduct one. I'm coming in to a fresh MS sql server and a lot of excel processes, and a team of one IT person and one BI analyst. The longer term plan is that they want to move to Azure and get some advanced analytics off the ground, but they have realistic expectations of where they are and just how far away that is.

They're also offering me a pretty massive increase over my current salary. Obviously I want to take it. I have lots of ideas, being able to do everything right myself from the start and not dealing with bad legacy systems is appealing. Though part of me is wary. I'll have no back up, nobody to ask for help if things go wrong. I'm technically a senior where I am now due to an early promotion, I run workshops for the non DE guys and have done some good work but given how fresh I am this feels like a massive step up, or taking two steps at once.

Would you guys take something like that on when you're still relatively early career? It feels like a long term project that I'm committed to, and it means I'm not gonna be touching databricks or ADF for a while. Its gonna be all sql server and airflow/dagster at first.

Edit: Thanks for everyone's advice. Lots to think about it, but I've taken the offer.",2023-09-12 14:07:39
16fkudt,Elegant way of writing SQL inside of ETL .py files,"Hey everyone! We currently have a series of .py files with ELT logic (load and write functions). However, we find that maintaining SQL code within these files is not elegant / best solution. What's a more elegant / best way to approach handling Spark SQL inside our elt .py files? Should we keep embedding SQL logic (say select * from table where date) inside the .py files or have .sql files in our repo and execute such .sql files inside our python elt files? Any advice/comments would be helpful!",2023-09-11 03:36:57
16cagf7,Worthwhile managed EL (Extract-Load) tools in 2023?,"Hello r/dataengineering!

I'm on a quest to find the best managed EL tools out there. Our home-grown Python scripts have been a significant source of headaches, and with a small team, self-hosting just isn't viable for us. We are keenly interested in cloud-based solutions to make our life easier.

So far, here's what's on our radar:

* **Fivetran:** It appears fairly production-ready and robust, but I have reservations about it being a proprietary system. Additionally, the costs seem to rise significantly given the relatively high active row count (IoT business).
* **Airbyte:** While it seems promising, I've observed numerous issues on their GitHub. Moreover, they're in the midst of rolling out a major update with their V2 destinations.
* **Meltano:** I recently discovered they have a ""Meltano Cloud"" offering currently in its open beta. This could be a potential game-changer, but I would love to hear experiences from anyone who has used it.

Given how rapidly the tech landscape changes, I'm sure there might be some gems out there I'm unaware of in 2023. Any insights, recommendations, or experiences with the aforementioned tools (or others) would be hugely appreciated!

Thanks in advance!",2023-09-07 08:34:14
15kanjk,How do you get to know all these *stuff* concerning the internals of various tools?,"I'm a rising senior in university; I know that DE jobs are relatively harder to come by for new grads so looking for backend-ish software engineering jobs. But I hope to eventually transition into this field.

I was reading [this AMA](https://www.reddit.com/r/dataengineering/comments/udboyq/comment/i6gj0iy/) and one of the commenters asked how the Netflix DE interview goes, and further into the thread, OP is asked how they would deal with an out of memory issue with Spark and answers with several options - each showing that they understand not just how to use this tool, but why certain things work the way they do.

My question - how would one come to know this? Is it experience? Book / blogs / podcasts? Someone just telling you over the course of time (e.g. me reading this AMA would prolly count)? Feels a but overwhelming. Maybe what I'm doing - reading stuff made by knowledgeable people is part of what gives you this knowledge; but how the hell do you remember it? I can probably bet that if I ran into an OOM error in Spark a year later, I would probably **not** go ""Oh this was mentioned in a random reddit AMA that one can remove duplicates and/or process outliers separately let's try that!"". I'd probably go whine to a mentor and/or hopelessly google and/or if I'm lucky, remember this reddit thread and come back to it.

Even more worrying - a commenter asked ""How much understanding do you need for OOM you either  use more memory or less memory"". Which worried me because that reeks of the Dunning Kruger effect / not knowing what I know. if I do not even know what I don't know, how do I learn? How do I know that a source I'm looking it is a credible one to learn from? How would one get past this at all?",2023-08-07 04:49:22
15f127h,Can anyone elaborate me about the 'DevOps' side of DE or how should I learn them to fit for an interview or working as a data engineer,"Been working at a large domestic company before as a DE, but used mostly inner self-made tooling to write SQL/Spark code to do ETL. No so-called 'deployment' or CI&CD concept. Most of the time It actually is just filling out a form with parameters and configuration and you submit the code and workflow. Most DE just focus on the business logic and a little bit of parameters tuning. 

&#x200B;

So when I first realized that many DE jobs on the job market required experience like DevOps, gitlab, CI&CD, Jenkins and Agile, I got painfully nervous. I think these are really important and wish I have these knowledges because I can be more prepared for the work in the future. However I don't really possess the 'Software developer' side knowledge since I came out as a data-guy from the beginning. 

Can anyone recommend some materials or share some learning experience about these 'DevOps' side tooling/concepts/learning paths? Much appreciated.

&#x200B;",2023-08-01 03:35:37
14uytfe,Hitting plateau in tech learning at work,"I'm currently working in a team that requires a lot of functional expertise, but there aren't many opportunities for technical learning or addressing tech-related problems that requires learning. As a result, I'm feeling demotivated because I thrive on technical challenges. Additionally, the high functional complexity of my work leaves me with a heavy workload, making it difficult to find time for side projects. For those who have experienced a similar situation, how did you handle it? Have you considered switching jobs or teams? I've only been with this company for less than a year.",2023-07-09 13:38:40
14uwsuk,What exactly is the lakehouse? Is it a technique or a process? How is a lake house different from a warehouse?,"Does anyone here already know and implement lakehouse? For a company now running a data warehouse in a controlled manner, is it necessary to convert to a lakehouse? What are the key use cases for a lakehouse? What does a lakehouse have that a data warehouse  does not?",2023-07-09 11:59:25
14jm3gi,"Building a Modern Data Pipeline: A Deep Dive into Terraform, AWS Lambda and S3, Snowflake, DBT, Mage AI, and Dash"," 

https://preview.redd.it/1zgo3vnl3e8b1.png?width=2000&format=png&auto=webp&s=cb912643a1faef8dbf412eb58e6caddc9fb95376

This blog post will provide a detailed walkthrough of creating a modern data pipeline using a combination of Terraform, AWS Lambda and S3, Snowflake, DBT, Mage AI, and Dash.

[https://medium.com/@stefentaime\_10958/building-a-modern-data-pipeline-a-deep-dive-into-terraform-aws-lambda-and-s3-snowflake-dbt-cac6816f2100](https://medium.com/@stefentaime_10958/building-a-modern-data-pipeline-a-deep-dive-into-terraform-aws-lambda-and-s3-snowflake-dbt-cac6816f2100)",2023-06-26 16:33:57
14imbg7,Draw Data Model Schema and generate code,"Hello everyone! I am excited to share with you a tool I've been developing.

[https://modelfirst.codegeniux.com/](https://modelfirst.codegeniux.com/)

I'd love to hear your thoughts on the code generator and answer any 
questions you may have. Is a tool that allows you design the Data Model
visually and generate the code for different technologies.

* Screenshots
* SQL 
  * 29 dialects and 76 code generators
* NoSQL
  * MongoDb V4.x
* CSharp
  * EF Core v6
* Java
  * Hibernate v5
  * JPA 2.1
* JavaScript
  * Sequelize V6
  * Knex
  * Bookshelf
  * Mongoose V5x
* JSON
  * Draft 07
  * Ajv schema
* PHP
  * Doctrine v2.8
  * Laravel v8
* Python
  * Django v3.2
  * SQLAlchemy v1.4
  * Pony
* Ruby
  * Rails 6
* TypeScript
  * MikroOrm v4.5
  * TypeORM MySQL
  * TypeORM Postgres
  * TypeORM Sqlite
  * TypeORM SqlServer
  * TypeORM CockroachDB
  * TypeORM Oracle
* Prisma schema
  * Sqlite
  * PostgreSQL
  * MySQL
  * SQL Server
* Visual Basic Script
  * MS Access MDB
  * MS Access ACCDB
* GraphQl
  * GraphQl Schema
* Swagger
  * Schema V2
  * Schema V3
* JHipster
  * Domain Language
* Graphics
  * GraphViz

I'm still working in some technologies to support Audit and Soft-Deletes.
It's only available for Desktop now, Sorry

Thanks for your time.
Greetings",2023-06-25 13:28:24
14ifxjz,How to become a data engineer,"Hello everyone,

I’m trying to start my journey in data engineering, but I’m worried I’m starting wrong or may have the wrong path forward. I created a roadmap for myself and I want to know from those who are data engineers is it realistic? Or not? Should I be doing a bootcamp like Lambda school or something of the sorts? Or is the road map I created realistic to becoming a data engineer, would love some guidance. 

Thanks in advance",2023-06-25 07:31:06
14ieh8u,Any feedback on Zach Wilson’s Data Engineering bootcamp?,"Did anyone from here attend his first bootcamp? Can you please share your experience?
He is starting enrollment for second cohort next week. I am thinking of enrolling but wanted to know if anyone has any feedback about the bootcamp?",2023-06-25 06:05:18
1493oql,Data Engineer with Scala?,"Hi, I'm a Data Engineer and I work mainly using Python and pySpark on Databricks. I noticed that 6 out of 10 most paid jobs in Data Engineering field are ""BigData Engineer with Scala"" and simmilar, often related with Azure and Databricks.

So to meet market expectations I want to learn Scala in context of Data Engineering. If there is a someone with job like I mentioned, I will take any advice on what to learn and how to learn Scala for Data Engineering.

I'm asking for help because I dont want to be a Scala Developer, so maybe some experts can point me some directions what should I learn, and what shouldn't  :)",2023-06-14 09:51:11
145f8f7,Just DBT and Snowflake,"I just use DBT and snowflake to transform data for a BI layer at my job.  No python, aws, or ETL.

&#x200B;

How common is this, and does my role fit into data engineering?  I'm trying to figure out how my current knowledge, Snowflake and DBT, fits into the wider world of data engineering and where I can branch out next to learn more.  I'm currently snowflake core certified and was considering getting AWS certified, but it all feels like rote memorization, and I want to figure out my connection to this field first.  


I've been reading articles, medium blogs and am hoping to read  

# The Data Warehouse Toolkit: The Definitive Guide to Dimensional Modeling

 

# Fundamentals of Data Engineering: Plan and Build Robust Data Systems 

 Designing Data-Intensive Applications: The Big Ideas",2023-06-09 20:03:22
13i5a8h,Data Platform - Discussion,"Hey Guys and Girls,

Wanted to setup datawarehouse for my startup,
We do not have lots of data so we are not going with Data lake

Earlier we tried postgres database as datawarehouse with airflow for pipelines.
But our business demanded few data to be more real time, our system databases are pretty distributed, we need to take data from multiple databases as well as tables merge them and dump them into our datawarehouse and we want to do it as real time as possible.

Being a early phase startup, we do-not have much budget as well as engineering bandwidth, so as cheap and less overhead as possible! 😅😅

Any help and advice would be appreciated!",2023-05-15 11:17:25
12yw1ul,"Hey guys, so I work for an organization and we're thinking about using SharePoint as a ""database."" Do you guys have any tips or recommendations on how to make this work? We're kind of new to SharePoint so any advice would be greatly appreciated! Thanks in advance.","Hey guys, my organization has been using O365 with the power platform for a minute now. We've got some real MVPs on our team who are treating SharePoint like a database for their power apps (""front-end"") and power bi reports with power automate work around. But, here's the kicker - the company doesn't want to invest in a proper database like SQL server or full Datavers. 

Personally, I think this is a recipe for disaster. I mean, SharePoint is great and all, but using it as a ""database"" just doesn't seem sustainable. What are your thoughts? Should we continue to use SharePoint or should we explore other options? Let me know in the comments below.",2023-04-25 21:05:31
12y8yu2,Curious if anyone has adopted a stack to do raw data ingestion in Databricks?,"I’m building out our Databricks deployment and related DE infrastructure (new start up, greenfield). As the only DE, I’m using Airbyte for raw extraction and load into our S3 data lake. 

I like the idea of only having to use one tool for all our DE needs. The only thing that comes to mind would be manually building out extractors to our data sources (CRMs, DBs, Tools, etc) or running  python based ETL libraries like Meltano in our notebooks. 

With Databricks workflows and orchestrators, this could consolidate tooling. 

I will keep using airbyte as time is of the essence and the libraries help with the lift. 

However, I’d love to have a discussion around projects or ideas with this type of infrastructure. 
Thoughts?",2023-04-25 05:11:27
12lyhbw,Premier League Project Infrastructure Update,"For anyone that has any interest, I've updated the backend of my Premier League Visualization (Football Data Pipeline) project with the following:

* Implemented code formatting with [Black](https://github.com/psf/black) and linting with [Pylint](https://github.com/pylint-dev/pylint) in my CI pipeline.
   * Here is my updated GitHub Actions Workflow file: [ci.yml](https://github.com/digitalghost-dev/premier-league/blob/main/.github/workflows/ci.yml) 
* Split up the data endpoints into their own Docker images to achieve more of a ""micro-services"" architecture. Previously, I had one Docker image for all endpoints and made troubleshooting a bit tougher.
   * The files are under the `/data` folder in my [repo](https://github.com/digitalghost-dev/premier-league/tree/main/data).
   * I run the containers twice a day now. I'm thinking of upgrading my subscription to allow more calls for more frequent updates.
   * I also plan to bring in a ""fixtures"" tab to show game scores and history.
* I also updated the [Streamlit dashboard](https://premierleague.streamlit.app) to include the rest of the teams in the league with their form for their 5 previous games (only games played in the Premier League) in the ""Top Teams Tab"".

I've been learning a lot about code quality and whatnot so I wanted to share how I implemented some of my learnings.

Flowchart has been updated:  


[Flowchart](https://preview.redd.it/j35bye26wuta1.png?width=1796&format=png&auto=webp&s=6ccf1befce168916915fe62e1ea7e95ce9c35497)

Thanks 🫡",2023-04-14 14:07:55
12k1o5r,Blog Post on how DoorDash used the metrics layer to scale and standardize Metrics for Experimentation,"Hey folks, we just published a new blog post about our in-house Metrics Layer for Experimentation. At DoorDash, we faced numerous challenges with our experimentation analysis platform, Curie, due to our ad-hoc approach to metrics. In our latest post, we delve into these challenges and share how a Metrics Layer helped standardize and scale our metrics for Experimentation. We also discuss the design and implementation of our data models, metrics authorship, metrics governance, and our highly scalable metrics computation engine, while documenting our key learnings.

As the trend of Metrics Layer adoption gains traction in the data space, we're thrilled to share a practical application of its value. We would love for you to give it a read and share your thoughts! [https://doordash.engineering/2023/04/12/using-metrics-layer-to-standardize-and-scale-experimentation-at-doordash/](https://doordash.engineering/2023/04/12/using-metrics-layer-to-standardize-and-scale-experimentation-at-doordash/)",2023-04-12 22:21:27
1apolbz,The only two books you need to read about CI/CD and Data,"I am not usually reading about my craft, but recently came across two books that captured my imagination in CI/CD and data.

Reading these two helped me to understand the different perspectives coming from the technology side and from the business side.

As a result, I can better understand how the business sees problems which is not always easy for me as a dev. I think I can also have better conversations with the business side as an engineer.

* The Phoenix Project: A Novel about IT, DevOps, and Helping Your Business Win
* The Unicorn Project: A Novel about Developers, Digital Disruption, and Thriving in the Age of Data

Reading these two helped me to understand the different perspectives coming from the technology side and the business side. ave better conversations with the business side as an engineer. ineer.

The title is obviously a bit of an overstatement, I'd love to hear your recommendations for the books you think are best.",2024-02-13 08:20:39
1aff8gu,Python and Spark Scala,"Edit : Thank you guys, you're all awesome and helpful, this was absolutely my first post here and you guys provided valuable and curated tips and tricks. Thank you. 

My manager at work told me to focus on these two, I have some low level experience with Python but never heard about Spark Scala, do they work together, I will be working on big data shortly with no experience.

Sorry for this stupid question, I am still flabbergasted by this stack that got thrown onto me, did some internet search and all I see is advanced and complex guides.

&#x200B;

Thank you so much.",2024-01-31 10:54:34
1ad7pcc,Does anyone else struggle in DE zoomcamp?,"This is by far the hardest course I ever took.

Most of the things don't work I am constantly on Slack and running around on the net and harrasing gpt.

I can't even figure out how to do the homework.

Some questions are trivial, but there are others that require knowledge of data ingestion and docker compose.

I have such a headache these last few days as I haven't moved an inch. The project for certification is copy paste, but the homework is basically knowing advanced knowledge for the thing you are supposed to be learning.  
EDIT: I was wrong, I must have been somewhere else.  
Fortunately there exists a template for creating personal projects.  
So if I manage to get there that is a big plus.

It can't just be me who has the issue with the structure?

&#x200B;",2024-01-28 17:21:50
18il8i8,How would you populate 600 billion rows in a structured database where the values are generated from Excel?,"I have a proprietary Excel .VBA that uses a highly complex mathematical function using 6 values to generate a number.  E.g.,:

=PropietaryFormula(A1,B1,C1,D1,E1)*F1

I don't have access to the VBA source code and a can't reverse engineer the math function.  I want to get away from using Excel and be able to fetch the value with an HTTP call (Azure function) by sending the 6 inputs in the HTTP request.   To generate all possible values using these inputs, the end result is around 600 billion unique combinations.

I'm able to use Power Automate Desktop to open Excel, populate the inputs, and generate the needed value using the function.  I think I can do this for about 100,000 rows for each Excel file to stay within the memory limits on my desktop.  From there is where I'm wondering what would be the easiest way to get this into a data warehouse.  I'm thinking I could upload these 100s of thousands of Excel files to Azure ADL2 storage and use Synapse Analytics or Databricks to push them into a database, but I'm hoping someone out there may have a much better, faster, and cheaper idea.

Thanks!

** UPDATE:  After some further analysis, I think I can get the number of rows required down to 6 billion, which may make things more palatable.  I appreciate all of the comments so far!",2023-12-14 22:58:12
18ekmn6,Delayed promotion,"I'm currently a data engineer with 7YoE, in line for promotion to Senior DE.
This is how promotions work in my company. I should start working like a senior would, make a case with management that I'm already operating on a senior level and need a promotion. Management evaluates the case and gets it done.

I was told I'm ready in December '22, I made the case after critical deliverable in March '23 (Fool to have delayed so much) manager stalled until September to make me look better for upper management until another critical deliverable. All I hear after it is ""It's happening"". Yesterday I heard it might not even happen until April '24.

I feel stupid to be working as a senior without raise for 1.5 Years now. Is it a fair game? Any insights that can help me navigate this are appreciated",2023-12-09 19:10:16
18e7qo9,Data engineering at home?,"Hi! I'm a grad student who's interested in pursuing a career in data engineering. I've had experience working with data on multiple projects. What I've come to learn is that data engineering is often done on large scale data and requires paid services. As a broke student what are a few things I could do to actually get experience with data engineering that is similar to a workplace use case and put on my resume?
I've tried using databricks SQL but it needed me to pay to use warehouse features.",2023-12-09 06:55:05
18al3gf,Just took the GCP Professional Data Engineer Exam...AMA,"For those considering it you likely know that the exam guidelines changed on November 13th, meaning all the courses that are geared to the 'old' version are practically useless. Yet, they are also the only courses available.

I used up all 2 hours, felt like I guessed all of the first 25 questions which seemed to all be based on the newly added topics (which is pretty fucked up of Google but I digress...)

But somehow, I passed! So if anyone has questions on the new format, let me know. But I would say the main things they asked about which were not on the A Cloud Guru / Linux Academy course nor the Pluralsight course, was:

* Memory store
* Alloydb
* Biglake
* Datamesh/Dataplex
* Analytics Hub

The questions were all extremely detailed, so make sure you know not only what product to use, but how best to optimize your usage of the product.",2023-12-04 14:15:27
16vetkl,Look what you made me do...,N/A,2023-09-29 15:33:35
16h0kpv,"Ok, who here actually prefers working in a contract position versus being fulltime?","I guess I never fully understood the love for being a contractor versus fulltime when it comes to data engineering careers. I, like many of you, are getting hit with a lot of LinkedIn recruiters right now and some of these roles they're trying to fill are ""long term contract"". I guess I see contract work as great when you want something that might pay more but is probably shorter term so you can move onto the next job and get a variety of experiences over the long term. I have a family I support so I've always prioritized fulltime roles so that I don't end up stranded at the end of a contract that may/may not get renewed. A recruiter reached out to me recently with a really great paying ""long term contract"" role but since it is contract I'm very hesitant to seriously consider it. For those of you who prefer contract roles, why exactly?",2023-09-12 19:29:18
16dx5fj,"Boss is the opposite of a buffer for constant requests and scope creep, advice?","I'm a data engineer at a mid-size, non-tech company, with a relatively small data team. I build pipelines, reporting all the usual stuff. 

I report up to a department head who is not technical, and seemingly super disconnected from all development. Really I don't think he would be able to speak to whats actually being worked on, or the commitment requirements apart from the very high level (like literally just project names he hears in scrum). 

We have a packed annual corporate project plan which keeps us super busy on top of constant ad hoc requests from the business. I keep reading about leaders who support their teams by buffering out constant requests and scope creep from the business, allowing the technical team to focus on executing. Our leader does the exact opposite. 

They are constantly sending 'high priority' requests on behalf of other departments etc. with really no context or understanding of whats being asked for, or the time required. As usually every business group thinks their request is *the most urgent,* and they dont temper expectations or anything. They will hop into a project meeting that they haven't been to in ages and start throwing out random ideas to the business users, saying ""we can do x, y, and z"" which are all way outside of scope and don't really make sense.   

Basically, we're too damn busy to deal with this lol. Any advice on what I should do here or how I should handle this overall would be greatly appreciated. 

 I 100% want to succeed and excel, and can definitely deal with a bit of suck it up and grind it out when needed, but this just seems so poorly planned, poorly organized, and so duuumb.

Love you all 

&#x200B;",2023-09-09 05:17:11
16do38w,Matillion Opinion,"My customer wants to move from a AirFlow-Python process that has a lot of flexibility to Matillion and I am helping them make that transition.  There are \~500 excel files (\~500 lines/5MB each) that need to be configured, transformed and there are numerous rules that need to be applied to make the data usable. My worry is that these rules will be hard to configure in Matlillion, is it better to these rules reside outside Matillion so it is easier to support and change or does Matillion provide a custom script module. Any idea on the costs of deploying Matillion.  They have quoted a flat $40K/year license fee and something like $4.80/hour for executing the data flows.  My worry is that this $4.80/hour is not very clear and transparent.  Any insights would be helpful",2023-09-08 22:17:23
15byxaz,"First technical interview with another company, not sure what to expect. Advice?","Hello folks, here's the situation:

4 years ago I started as an intern in a small company, and then just leveled up there to senior DE.

Since I was an intern obviously there wasn't a technical interview, just a couple ""let's know each other"" talks with HR and the hiring manager.

Recently I interviewed with another company, another small one, which is looking for a senior DE to move forward their data endeavors (they don't have a dedicated data team yet).

The first interview was with their tech lead, who just today confirmed we're moving forward, and the next interview will be a technical one, with the tech lead + another SWE at their company.

I really have no idea WTF to expect. I am confident in my skills, but I also know I don't really perform well in an ""exam setting"", so I'm afraid my brain will freeze.

Any advice you have is more than welcome",2023-07-28 14:24:34
156sgv3,"To those who are self-taught coders/data engineers, how did you study?","In college a lot of time I spent taking handwritten notes and then doing practice exams or problem sets for economics, but I feel like I need a better way to study since I’m self teaching when not working.

My questions is

- is it even worth taking handwritten notes when coding or should I just read and try to do a problem right away 
- is there any time I should taken handwritten notes? Like maybe definitions for OOP, or data structures?

I’m just feeling pretty dumb because I’m going through a Python book and I feel like it’s taking me forever to get through using my old study habits.",2023-07-22 19:10:01
14c68nt,When do you “know” a technology enough to mention it when you apply to a job,"This post was mistakenly removed as it was thought to be a res ume review but actually it’s a question of when is it considered reasonable to include a technology on your resume? 

Background(last paragraph is main point):

I’ve been working on a data pipeline project for a little over 2 months now and I’ve been learning and using tools like Spark, Nifi, and Airflow for it. 

I typically read up on the fundamentals of each too before figuring out how and why to use them in the project. Each tool plays a pretty major role in the project itself. Looking to include Kafka too but doing Kafka with python isn’t as easy

What I wonder is if once you’ve gotten to this point, using the tool in your project, if you are okay to put said tool on your res ume, even if you aren’t necessarily an expert, but rather you have experience using it in your own work. How do you typically get it on your res ume in a way that doesn’t imply you’re an expert?",2023-06-18 00:21:44
13zuhvl,How to integrate data quality test in ETL pipeline?," 

# How to integrate data quality test in Python ETL pipeline | Test Data Pipelines | Data Quality

📷[**Blog**](https://www.reddit.com/r/dataengineering/search?q=flair_name%3A%22Blog%22&restrict_sr=1)

**Integrate test cases in ETL pipelines**

[**https://www.youtube.com/watch?v=7FPksG-LYOA&t**](https://www.youtube.com/watch?v=7FPksG-LYOA&t=2s)

Topics covered:

* Data Pipeline
* Data Quality Tests
* ETL

Tech Stack: **Python, PyTest, Postgres, SQL Server**",2023-06-04 00:04:07
12cbfkr,5 Helpful extract and load practices for high-quality raw data,N/A,2023-04-05 06:55:57
11t1e0u,do you think Zhamak has any idea how much time has now been wasted on orgs discussing if we should 'do a data mesh',title,2023-03-16 18:00:00
11pxhjg,What is data mesh? Architecture and best practice guide,N/A,2023-03-13 02:25:59
11kyceb,I created an open source project to help you transfer files between various remote locations,"Hello fellow data engineers! After having struggled at work with frequently having to transfer files, or even entire directories, between different cloud storage services, I decided to try and make this task easy for me, so I created Fluke, a Python package that offers a simple API which hides away all this complexity regarding file transfer. If your day-to-day work is similar to mine, I think that this project can help you in a big way!

As of yet, you can use Fluke to transfer files to/from the following locations, but I'll try adding even more in the future:

* Local file system
* Remote file system (through SSH/SFTP)
* Amazon S3 (through HTTP)
* ADLSv2 (through HTTP)

Give it a go and tell me what you think!

* Github: [https://github.com/manoss96/fluke](https://github.com/manoss96/fluke)
* Docs: [fluke.rtfd.io](https://fluke.rtfd.io/)
* Example: [https://github.com/manoss96/fluke#usage-example](https://github.com/manoss96/fluke#usage-example)",2023-03-07 12:42:11
1aikorj,Databricks/Apache Spark preference over other warehouses like Snowflake/BigQuery,"Hi everyone,

The organization where I work recently planning to move data infrastructure to BigQuery/Snowflake instead of Databricks. We have been using Databricks/Apache Spark rigorously from ETL tasks to warehousing the data in lakehouse platform. The team is small, but what's more shocking is it's like going backwards (Managed DWH Era). 

Sure Snowflake/BigQuery provides the managed infrastructure for Processing and Storage of data, but its mostly expensive and there's a less chance on the fine-tuning the system to manage costs. 

I feel it's more of a competence issue since none of the team members are comfortable dealing with writing complex transformation in PySpark, forget about dealing with Delta Tables, Delta Live Table and multiple storage formats! Even the Data Architects are of old era stuck up with modelling concepts specific to older data warehouse platforms. 

I want to know the opinions if this is a right step forward or it's just going backwards and not embracing the new data platform architecture? Would you invest more time learning Apache Spark/Databricks/Lakehouse etc over learning a platform that causes more of a vendor lock-in issue?

NOTE: Snowflake/BigQuery, even other managed platforms are moving towards embracing Open Table Formats and other stuff, since they know that it's revolutionizing to invest in something that is way forward in time. Look at DataPlex! Its some what similar to Databricks' Unity Catalogue and prefers external storage assets like GCS. Snowflake is also trying to get Apache Iceberg integration from a long time.",2024-02-04 10:08:24
1ae4wnv,Why is it so hard to land a DE role.?,"I have worked as a BI Developer/SQL DBA/ SQL Developer for like 5 years.For the last 2 years I have been working as a Big data analyst working on Databricks, Python,Pyspark and some AWS services.
Also been working on Looker.

I have been trying to switch to a pure DE role where I get to create pipelines and work on more advanced AWS services etc.But most do the DE roles out there requires lots of skills set and I don’t even see many entry level DE roles.

Any one in similar situation?",2024-01-29 20:12:13
198lq59,What is next after Lakehouse architecture?,"Lakehouse architecture improved the traditional RDBMS-OLAP based data warehousing architecture. It has been around for a while. 
Is there something new in this space ready to replace lakeshouses?",2024-01-17 02:40:48
195k5tz,Data Fatigue?,"I work at a smallish company but we don’t spend a lot on data team resourcing. So by default I’m the all inclusive data engineer, architect, analyst and requests come from all departments. 

Anyone here find it really challenging to source, ingest,  model, shape AND then do analysis?

I used to be analyst but had a much smaller slice of the pie and did no engineering, I was good at doing the analysis and making recommendations.  But now I get to the end of the whole process and I really struggle to analyse the data, anyone else been here or have any tips?",2024-01-13 09:20:32
18ynoig,I recorded a PySpark Big Data Course (1+ Hour) and uploaded it on YouTube,"Hello everyone, I uploaded a PySpark course to my YouTube channel. I tried to cover wide range of topics including SparkContext and SparkSession, Resilient Distributed Datasets (RDDs), DataFrame and Dataset APIs, Data Cleaning and Preprocessing, Exploratory Data Analysis, Data Transformation and Manipulation, Group By and Window ,User Defined Functions and Machine Learning with Spark MLlib. I am adding the link below, have a great day!

[https://www.youtube.com/watch?v=jWZ9K1agm5Y&list=PLTsu3dft3CWiow7L7WrCd27ohlra\_5PGH&index=8&t=74s](https://www.youtube.com/watch?v=jWZ9K1agm5Y&list=PLTsu3dft3CWiow7L7WrCd27ohlra_5PGH&index=8&t=74s)",2024-01-04 20:55:35
187kxxk,UK Data Engineers : have you noticed the job market drying up and advertised salaries going down?,"I want to see if its just me or if other people have noticed the same thing

I've been looking for appealing options to change jobs , and 6-12 months ago, the caliber of jobs available seemed much higher and the salaries on offer were also higher (80-110k for seniors, but now seems to be 65-85k roles)

At that point i wasn't looking to change, but now I've actually started looking the market seems to be a bit of a turd

Only exception seems to be a lot of jobs wanting expertise in Azure products",2023-11-30 15:18:21
17ebg8x,How useful do you find AI coding tools for DE?,"Curious if people are using copilot or ChatGPT in their everyday work? What do you use them for? Where do you find them lacking?

Personally, I use ChatGPT a lot, but don’t find copilot nearly as useful. I wish there were more DE workflow specific AI tooling.",2023-10-23 03:48:11
173vk4f,Introducing Asset Checks,N/A,2023-10-09 16:05:00
16tnvsu,Scope of Data Engineering in future,"Most of us would have observed recently that the companies are moving to cloud for data engineering. Be it Azure,  gcp AWS or databricks cloud, they provide managed cluster platforms.Since they are making the data engineers life easier by integrating job optimisation tools,  managed airflow, orchestrationservices, CI CD and what not.  But when it comes to on-premise, currently most of these are being manage by data engineers and some intermediate level coding is required.
I would like to understand from the experienced data engineers in this group is --  in a few years down the lene, say 5 years, the dependency on data engineers will decrease for the type tasks mentioned above ?
Of course, data engineers would be doing ETL by writing python sql scripts and that is not going to go away but if we keep this part side, what is the future of the rest of the tasks?
 I think there are some big organisations trying to move to the cloud and in the near future, they would need tremendous support from the data engineers and this theerw would be a high demand in the short run.

Thanks.",2023-09-27 15:39:59
16j53vw,What do we think is a fair way to interview data engineers; by level,"I made a post earlier , in some ways venting , about the variety of questions asked in data engineering interviews and questioning how effective the process is in measuring a candidates ability to do the job. 

Instead of just complaining about a problem , I’d like to adress it and come to a solution that can help others who may encounter this problem in the future. As a result, I’d like to open a conversation on what we think is the most effective method to interview a data engineer in 2023, and hey maybe it will catch on and help candidates and employers find the right match and make our job market more efficient. 

I can propose an idea if this doesn’t get traction as a jumping off point and/or consolidate feedbac if we get good traction  but ideally I  wanted to capture everyone’s feedback first so I will hold off on sharing an approach I think is sound.",2023-09-15 05:54:51
16ew9af,Data architecture proposal feedback,"Hey fellow DE's. I've recently started a position as a senior DE and I'm tasked with designing and building the data infrastructure of my company. The benefit is that it's almost a greenfield project, there is very little in the way of DWH and data modeling and I have a lot of decision power. I have a proposal ready for my team, but I was hoping to get some feedback from other professionals first. I'm the only actual DE on the team, the others are Data Scientists who had to take up engineering tasks for their work, so they're not going to be the best soundboard I'm afraid.

Another reason I ask is because I don't have that much experience with the companies' chosen cloud provider (Azure). I have 5 YoE with most of my focus in AWS, Snowflake, dbt and python/PySpark. It's not like I feel I can't handle Azure or the task, I just know that I might be overlooking some intricacies that come with experience with the platform.

Some background on the project: my company is an industrial producer of certain materials. The data flows I will be handling will mostly be IoT data from factory lines. They currently have a setup that drops the raw data in Azure Storage in hourly batches and one process that transforms the binary data into parquet. The scripts that move the data into storage are maintained by another team and not my responsibility. I've had a look at it already and it all looks clean and stable, so I'm happy with the current setup. Beyond this they have very little (read: nothing) in terms of data infrastructure. Scientists pick up their input data directly from storage and do ad-hoc transformations when they need them. I am currently unaware on how business operates their dashboards.

The goal of my infrastructure would be to create a DWH where consumers (business and the scientists) can pick up their transformed datasets and do their thing, without having to worry about spending their time wrangling data.

My proposal consists of using Azure Synapse Analytics (ASA) as the datalake/DWH. The raw parquet files would be loaded into ASA in a 'RAW' schema and picked up by transformation processes and dropped in a data mart schema to be consumed. For my transformation process I would like to use dbt. I would containerize the project and use Github Actions to push it to Azure Container Storage. I'm thinking of setting it up such that deployment would happen when a merge happens on the test/prod branch, given that the source branch has a certain naming convention (e.g. starts with 'feature-{project-code}-'). I would then use Azure Data Factory as a scheduler to run the code in either Azure Kubernetes Service or in Azure Container Instance (I'm leaning towards AKS though).

----------------------------------------------------

Some feedback I can foresee and get ahead of:

- *'Don't use dbt, it's bad because {insert-hater-reasons}!'.* I know there's some people on here that don't like it. I've used it plenty and I've seen it used well. I know the pitfalls of model bloat, etc, so I will keep an eye out on that stuff. I like the tech, it's powerful and it does what it does very well.

- *'Why not use Databricks?'* The only benefit I can see for Databricks would be that it integrates  more easily into the Azure ecosystem. Currently the volumes we're talking about do not require PySpark. Factory lines are fully independent and have separate resource groups. The hourly batches are in the order of 10-100MBs. Nothing a well organized incremental dbt model can't handle. Another problem is that I don't know of a good way to create a true/proper CI/CD pipeline with Databricks. Notebooks in production is an 'over my dead body' type of thing. I've once had to clean up a production 'pipeline' like that and I'll fight a motherfucker before I introduce that nonsense in my project.

------------------------------------------------------
TL;DR:

I want to make a data platform using:

- Containerized dbt code for my transformations

- Github Actions for my CI/CD pipeline

- Azure Data Factory as orchestrator

- Azure Kubernetes Service as executors (alternatively maybe Azure Container Instance)

- Azure Synapse Analytics as datalake/DWH

I'm happy to hear your thoughts!",2023-09-10 09:50:48
162de5b,Feeling Stressed about Junior Data Engineering position,"So, I just started my first junior data engineering position and I am already feeling stressed out on the first day and for the first week. As soon as I joined, I was already put on a project that is due in a couple of days. My manager does not have any knowledge of data engineering tools or concepts, so they have put me under a Senior Data Engineer. However, this Senior Data Engineer has not answered my questions or answered any emails of mine with questions that I have had about the project. When I finally met the Senior Engineer, they questioned why I was so slow in transferring data from one source to another. I am watching youtube tutorials like crazy because I want to keep this job and I want to produce results to keep the job. Is this normal for a Data Engineering team? Is there any advice that someone can give me? Is there a mentor within the Data Engineering community here that I can DM for more advice and guidance? Especially with the project that I am working on? Thank you. ",2023-08-27 01:52:51
15vgnrd,How hard is it to get into jobs with tech stack different from your experience,"I have been an associate data engineer for about 3 years now. The company I work for has gotten me stuck in just designing SQL queries in Impala. I am pretty good at SQL but my job is basically writing queries that transform and move data from one table to the other.

My managers kept promising me that they'll get me better coding projects but seems like a fluke. They just want to working on the same thing since I am cheap labour and already have the context. 

I want to switch and try outside but whenever I search for DE jobs, the job requirements are pretty intense with heavy cloud exposure and years of Spark experience. I never got to do any hands on coding. I can and did learn from courses online but I am not confident enough as it is not same as working on a proper project.

So what do I do at this point? Do I re-apply for entry level jobs with upto 20% lesser salary or will other employees be okay to employee me in spite my non-exposure. Any advice would be helpful for me. Thank you.",2023-08-19 13:43:42
15t5otn,What is the risk of using Redshift as the only database?,"Hi friends, I'm hearing this from a friend but didn't get the chance to get the details. Basically in his company the BIs are ONLY using Redshift, and he said it is a bad thing. I have no experience with Redshift but I work in a GCP shop where we mostly use Google Bigquery and I didn't see any issue.

What could the issue if Redshift is the only DB BI is using? I assume this is going to be data warehousing as BI is the user.",2023-08-16 23:22:58
15jxfd4,Do you need Azure Data Factory and Databricks? What about dbt?,"Just wondering if people see all three of these as being necessary. I’m in an organization that has an Azure data warehouse. This gives us access to ADF. But databricks salespeople have started reaching out to us recently. 

The ability to work within python and r in databricks sounds great. But how does it differ from a more sql-based tool like dbt?

For context, my background is more in data analytics/science. But our organization (from what I’ve seen after being here a couple months) is in desperate need of data engineering skills. They’ve gone out and hired data scientists, but their basic institutional data is a mess and in many cases not existent in the warehouse.",2023-08-06 18:56:55
15fyvw4,How to combine 100 JSON files with 100k rows each?,"I have 100 json files with 100k rows each that we need to combine in one single file for ingestion into a custom vendor process.


Although this is an ad-hoc task, I was wondering how do I make this performant if we had to do it daily. Certainly, if I use pandas, I'd be disappointed. 

Total data size is 15gb.",2023-08-02 04:35:26
15ehmrq,A simple way to estimate memory consumption of PySpark DataFrame,"A simple way to estimate the memory consumption of PySpark DataFrames by programmatically accessing the optimised plan information:

[https://medium.com/@miguel.otero.pedrido.1993/dataframe-memory-consumption-8687354263e2](https://medium.com/@miguel.otero.pedrido.1993/dataframe-memory-consumption-8687354263e2)

&#x200B;",2023-07-31 14:24:20
14u502l,Coming from Oracle on prem to Databricks - what to consider when writing SQL,"We have been using Oracle on prem and got introduced to Databricks now in our company. Is there a difference in writing the queries? I read somewhere a Group By  clause should not be used on Databricks as it is fundamentally against the concept of dividing work loads. 

I don’t know if it is true or I probably got it wrong. Is there anything at all to consider?

Would anybody have book recommendations for this question? Or a good blog to read?

Update, to be more specific:
I’m used to materialized views, cascading views, from broad data sets to specific ones for query run time optimization, and indexing

Is the materialized view the equivalent to a Delta table? Should we stick to the concept of cascading views? And what about indexing? Does this get handled automatically in the background by Databricks or do we have to set them as well? And if so, I assume indexing the same columns?",2023-07-08 13:58:39
14hhvtr,What is your favorite data catalog?,"What data catalog is your company currently using? What do you like about it and hate about it? What tool would you replace your current data catalog with if you could make the decision? 


Start from me:

ETL - Informatica power center

Data Catalog - Excel Spreadsheet (I know..)",2023-06-24 03:13:53
14dlx8v,Discussion: What are your biggest problems when building data pipelines?,"I am investigating building tools to help data engineers build data pipelines for machine learning.   


I was wondering what are the three biggest problems you encounter on a day-to-day basis.   


For example, is it extracting unstructured data, merging data streams, meeting throughput or latency requirements, keeping upstream and downstream schemas in sync, managing a large number of components in the pipeline, etc. or something else that gives you headaches? Curious to hear!",2023-06-19 18:07:14
13x79yg,How do you explain your job to laymen?,Title es self-explanatory. How do you explain your job to people that don’t know anything about data engineering?,2023-06-01 03:44:37
13r45sw,Data Architect Learning Paths,"Hi Guys. 

I've been in the data space for quite some time, and have struggled to find good resources to enhance my knowledge on data architecture patterns. Can someone suggest a good learning path, book, course etc. to fill in the gaps in my learning?

Thanks.",2023-05-25 01:29:54
13qijjq,"Getting bored, who to blame?","Context: I'm a data engineer working with the usual suspects Python, SQL (BigQuery), bit of Spark, Terraform, Power BI etc.

What's happening is that I'll get a task whether from my manager, or others within the business e.g. I want to see this report. I find where the data is at, get into data warehouse, build a very simple report in Power BI for business user (analysts tied up doing whatever). This is what my day to day looks like:

1. I get an ""urgent"" task.
2. I drop current task to do urgent task.
3. I get another ""urgent"" task.
4. Repeat step 2 and 3.
5. Get messages and/or email reminders and meeting requests from stakeholders, for updates, estimated completion dates, and the importance of said task (sometimes ignore or spend time formulating a response explaining the hold up).
6. Stakeholder complains to manager about their task.
7. Manager tells me to go back to task X, and complete as quickly as possible and return to ""urgent"" task X.
8. Meeting to discuss another task.
9. Go to step 1.

This leads to half-baked work and nagging from users.

I'm not overwhelmed or stressed or anything (I don't think/feel I am anyways), the work isn't overly complicated or difficult, help is always available (kind of), I'm happy with the pay...I do like data engineering (love is a strong word hence like), I enjoy it. I've just become **bored**, to the point where when it's time work, I don't feel like doing ANYTHING. I can't get myself to type code, message anyone, find the bug, etc etc

Anyone else been in this situation before? Am I doing something wrong? Am I missing something? If so, what?

Brash, savage, direct, punishing feedback and comments are all welcome.",2023-05-24 11:14:08
13if51s,Some workflow I really want to inject into our team if I'm the lead/manager,"1 - Every project should be classified as a major or minor one. Here are some examples of major ones:

* a pipeline that extracts data from unknown external API, joins with existing data from data warehouse and produces some output tables.
* a pipeline that consumes more than 1TB of data in the first try, regardless of complexity.
* a pipeline that joins existing data from data warehouse with some new data from a just purchased company's data warehouse in the same cloud.

For every major project, the team should follow the following steps:

* a brainstorm session, led by a senior/lead/manager (depending on the complexity), that produces a specification of requirements.
* The lead engineer then refines the specification so that it includes all permission requests and business requirements translated into technical terms.
* Permission requests are then given to lead/manager so that team can fast track them. There should be one such JIRA ticket for one project, unless more than one people are required to request permissions.
* Technical requirements are then divided by the team, preferably no more than 3 engineers, and written into JIRA tickets.

There are so many projects that I have worked on that no one bothers to do a thorough analysis so that junior engineers have to scramble to ask permissions left and right while managers/lead should do that. It wastes so much time and brings so much frustration.

&#x200B;

2 - Every major project should produce full documentations in ONE PLACE.

* The specification
* Each engineer should submit documentation (can be in the format of code comments) too
* Lead eventually gather them together and form a unified doc
* Most important, stakeholders should be tagged, and each engineer should be tagged too so that people can figure out the ownership quickly

OK I get it. No one wants to write documentation. But please, don't use 4 platforms for documentation! And don't let other people guess the owner by looking at Git Blame! Ownership should always go to the engineer who leads the project, or the manager if the said engineer leaves.

Every documentation should contain a WHY, a HOW and a WHAT and a WHAT IF section. Yes we will probably spend more time writing docs than developing stuffs but I think that's what things should be.

&#x200B;

3 - Full evaluation is needed for new tools/frameworks/language.

* Does the new toy satisfy all of our requirements? And if not how long it takes to develop?
* Does the new toy satisfy even the core requirements? And if not we should not migrate.
* Does the new toy add significant value? If not then it's going to be someone else's resume project and I don't like it.
* How much cost if we want to run the same thing on two tools for a while? For sure you cannot migrate in one shot.",2023-05-15 17:53:09
13fzayq,Need to refresh myself on some Spark (specifically PySpark) over the next week after not using it for 6 years - should I go with Spark 3 or is Spark 2 still common?,"I'm planning to find a course or some training material online to re-learn about Spark. I never was that proficient with it because the internals of it went over my head (RDDs), but I need to refresh myself with Spark for an upcoming interview. 

Is Spark 3 pretty dominant? It's not an issue of Python 2 vs 3 like when Python3 was initially released, right? If anyone has any good suggestions on learning materials, I'd love to hear them! I was hoping to find something like a Docker container online that I could re-use, but was going to spin up something on AWS to just play with.",2023-05-12 22:34:38
11ddhe5,Is analytics engineering a dead end career?,"Now that the dramatic title is out there - do you think it’s the case that analytics engineering, albeit still brand new, also risks being dead end?

Sure, this person can learn dbt core over dbt cloud and get further into CICD and devops/cloud type tasks (this is me right now), but where do you go from there?

Seems like the logical progression is to data engineer (meaning, a more python heavy role). By contrast, I could see how an analytics engineer would benefit from learning some ML to implement (mainly with the way dbt is going with python support).

What’s your opinion? What do you speculate will happen to the role?",2023-02-27 14:43:13
1aopq7t,"Pandas, high coupling and single responsibility principle in data pipelines","Hello. Everyone. I'm a data scientist who also does Data Engineering at work depending on the business needs. Pandas is the tool I use the most for data pipelines (if my task doesn't involve big data), and I always face the same problem in the functions I write for the different steps in the pipeline:

Huge functions with only 1 responsibility and too many steps which end up looking like huge blocks of pandas for accomplishing that one task. The thing is that if I start splitting that huge function into smaller parts, then those functions would only exist for that larger function to use.

So, I ultimately keep those larger functions because I'm not sure whether it is ok to have functions that are only used once for just one ""consumer""

How could I deal with this problem in a way that's easier to read, doesn't involve huge walls of pandas and having functions that are highly coupled to one another, and pretty much exist for a ""greater good"" (i dont know what else to call it lol)

Note: I'd like to know if there are common design patterns for this (maybe using OOP)",2024-02-12 02:50:42
1aonpuf,Isn’t DBT an unecessary layer if you use BigQuery ?,"I mean BigQuery can be called with the API, so I can just drop the config on a for and call the jobs when I need it orchestrated in Composer.

I know there is the lineage management but I’m not a fan of monolithic trees accumulated somewhere. BQ tables creation often occurs after transforming pipelines in Java or Python.

Finally, I had to write specific functions to « trick » DBT doing the optimization I need (which I could have done directly in SQL by BQ API calls).

Does someone has an experience with it ?
I’m not sure what I should think about it.

My main fear is that people tend to accumulate queries on a single point and make unecessary and dangerous dependencies between tables while not cleaning the mess. Yes a tool is as good as you use it but I feel like the tool itself encourages you to fell on the trap…",2024-02-12 01:02:42
1abifu5,What is your current biggest day to day pains in building pipelines?,"Is it a lack of connectors for a particular tool, is your data shuffling too much, what are the things causing you pain these days?",2024-01-26 13:33:12
19cyd9g,Where’s the boundary between “the added complexity doesnt outweigh the benefit” and “scalability?”,"I see arguments from time to time that it’s fine going straight into using spark, airflow, highly available rdbms, advanced git vc architectures, CI/CD, kubernetes, or whatever. The argument is typically that the designer expects the business to need to go this route eventually, and why design a system that you know you’ll have to redesign later on for scalability reasons?

Then there’s the other argument that these systems are completely unnecessary for small time guys, despite the fact that they offer many unique quality-of-life features that aren’t replicated elsewhere. The complexity of these systems alone seems to warrant a need for implementation, and if that need isn’t satisfied them it’s considered an unnecessary effort. Resume driven development.

So where’s the line between these perspectives? Does it depend on team size and knowledge? What else?",2024-01-22 15:37:36
192f47n,"Do you feel the tension between ""data democratization"" and the business need for high quality curated data?","If so, how do you deal with it?",2024-01-09 13:49:03
18n22z7,What's the most cost-effective way to learn DE in the upcoming 2024?,"I want to pursue a career in DE, I have 2 YOE as Data Analyst, my college major is Corporate Finance, what I use the most are Spreadsheets, SQL, Python for data manipulation, and some reporting tools such as Power BI. As you can see I have some technical knowledge but am not even close to asserting I can transition easily into a DE role.

Based on what I researched I need to learn topics such as Snowflake, Airflow, dbt, Spark, and Cloud services, just to mention a few.

I am here to ask for advice on how to learn DE by doing while on a budget, this means I would like to gain as much hands-on experience for free. I have read that running locally (I believe I have a capable pc) might be the most cost-effective option, but not sure if this is the right way to learn since Cloud appears to be the standard.

Thank you, I appreciate any guidance",2023-12-20 18:51:20
18bhf64,Data trends for 2024,"Every year, the data industry finds a new buzzword to latch onto. This year, I feel like everyone was focused on the idea of data ""activation.""

What are the new trends/buzzwords that the data industry will lean into this year? ",2023-12-05 17:35:55
184739o,Becoming data engineer,"What are the skills i will need to learn to become Data engineer
I have moderate knowledge on SQL, AZURE, POWERBI
Beginner on Python

Let’s assume i have mastered those skills, what are other challenges i might face to get a job?",2023-11-26 09:32:09
17zgbfg,Is Apache Hive still being used?,"I'm curious to know your thoughts on Apache Hive. Is it still actively used in your projects and overall relevant to the field, or it became obsolete and you have transitioned to other tools and technologies? Share your experiences and insights pls",2023-11-20 04:30:59
17ye6xm,Business Strategy vs. IT,"Seeking insights: 

I manage a data analytics department and a group of data engineers. We report up through Business Strategy. IT is proposing that the Data Engineers should reside in IT.

What are the key advantages or disadvantages of keeping data engineering within the business versus integrating it into the IT department?

Your thoughts and experiences are highly appreciated!",2023-11-18 19:40:35
16dktkh,SQL is trash,"Edit: I don't mean SQL is trash. But my SQL abilities are trash

So I'm applying for jobs and have been using Stratascratch to practice SQL questions and I am really struggling with window functions. Especially those that use CTEs. I'm reading articles and watching videos on it to gain understanding and improve. The problem is I haven't properly been able to recognise when to use window functions or how to put it into an explanatory form for myself that makes sense. 

My approach is typically try a group by and if that fails then I use a window function and determine what to aggregate by based on that. I'm not even getting into ranks and dense rank and all that. Wanna start with just basic window functions first and then get into those plus CTEs with window functions. 

If anyone could give me some tips, hints, or anything that allowed this to click into place for them I am very thankful. Currently feeling like I'm stupid af. I was able to understand advanced calculus but struggling with this. I found the Stratascratch articles on window functions that I'm going to go through and try with. I'd appreciate any other resources or how someone explains it for themselves to make sense.

Edit: Wanna say thanks in advance to those who've answered and will answer. About to not have phone access for a bit. But believe I'll be responding to them all with further questions. This community has truly been amazing and so informative with questions I have regarding this field. You're all absolutely awesome, thank you",2023-09-08 20:12:37
16bl4y2,It’s an n-dash!!!,"I’ve been banging my head against the wall for two days, trying to get formulas to pick up column names, which seemed to have the right data type.

They came with n dashes instead of hyphens!!!!",2023-09-06 14:02:40
164hlqq,A balanced reading guide on dbt,"I got inspired by [https://www.reddit.com/r/dataengineering/comments/13vrzlt/what\_does\_dbt\_labs\_get\_wrong\_about\_dbt\_best/](https://www.reddit.com/r/dataengineering/comments/13vrzlt/what_does_dbt_labs_get_wrong_about_dbt_best/) 

to collect a balanced set of resources on dbt, not just from dbt but rather from around the place so people can hopefully get an unbiased opinion and introduction to the tool. Unfortunately it got rather long :-D   
*P.S.: I'm sharing this as an exclusive gift for* [my data engineering newsletter ""Finish Slime""](https://www.finishslime.com/) *with some additional commentary.*  
\---

Dbt is just SQL + some templates, right?

Yes, and no. It’s become so much more, that we think you need a complete reading guide to get good at it!

Here are the **best articles on dbt I came across.**

 **1) Dbt 101**

* [Learn dbt the Easy Way](https://towardsdatascience.com/learn-dbt-the-easy-way-7d9f773d25ea) a quick intro by Madison covering the structure of a project and so on.
* [4 Things You Need to Know About dbt](https://medium.com/geekculture/4-things-you-need-to-know-about-dbt-e54c016f338c) Quick read 
* [How to design a dbt model from scratch](https://towardsdatascience.com/how-to-design-a-dbt-model-from-scratch-8c72c7684203)
* [What does dbt Labs get wrong about dbt best practices?](https://www.reddit.com/r/dataengineering/comments/13vrzlt/what_does_dbt_labs_get_wrong_about_dbt_best/)
* [Learn Analytics Engineering - The DbtLabs Courses](https://courses.getdbt.com/collections) (yes there are a lot!)

**2) dbt implemented**

* [Building dbt CI/CD at scale](https://medium.com/checkout-com-techblog/building-dbt-ci-cd-at-scale-365358f64b6f) 
* [dbt: How We Improved Our Data Quality by Cutting 80% of Our Tests](https://betterprogramming.pub/dbt-how-we-improved-our-data-quality-by-cutting-80-of-our-tests-78fc35621e4e)
* [dbt at Zendesk — Part I: Setting foundations for scalability](https://zendesk.engineering/dbt-at-zendesk-part-i-setting-foundations-for-scalability-34b55e6a6aa1)
* [dbt at Zendesk — Part 2: supercharging dbt with Dynamic Stage](https://zendesk.engineering/dbt-at-zendesk-part-2-supercharging-dbt-with-dynamic-stage-4703a49d1c30) 
* [Adopting dbt as the Data Transformation Tool at Instacart](https://tech.instacart.com/adopting-dbt-as-the-data-transformation-tool-at-instacart-36c74bc407df)

 **3) testing in dbt**

* [7 dbt Testing Best Practices](https://www.datafold.com/blog/7-dbt-testing-best-practices) 
* [The complete guide to building reliable data with dbt test](https://www.synq.io/blog/the-complete-guide-to-building-reliable-data-with-dbt-tests)s 
* [dbt tests: How to write fewer and better data tests?](https://www.elementary-data.com/post/dbt-tests)
* [Dbt test options](https://datacoves.com/post/dbt-test-options)

**4) deploying dbt (hint: you should consider non-dbt-cloud first!)** 

* [How to Build a Modular Data Stack — Data Platform with Prefect, dbt and Snowflake](https://annageller.medium.com/how-to-build-a-modular-data-stack-data-platform-with-prefect-dbt-and-snowflake-89f928974e85) 
* [How to Deploy dbt to Production using GitHub Actions](https://towardsdatascience.com/how-to-deploy-dbt-to-production-using-github-action-778bf6a1dff6) 
* [Why I moved my dbt workloads to GitHub and saved over $65,000](https://medium.com/@datajuls/why-i-moved-my-dbt-workloads-to-github-and-saved-over-65-000-759b37486001)

**5) potential alternatives (yes, there are now true dbt alternatives, and good ones!)**

* [Dbt vs SDF vs SQLMesh](https://datajargon.substack.com/p/dbt-vs-sdf-vs-sqlmesh) You should always at least know how your tool compares to others. Dbt is a great tool if you know its weaknesses, this comparison is a good one. 
* [Dbt alternatives - finish slime](https://www.finishslime.com/c/dbt-alternatives)

Do you have more good one? Would love to extend the list.",2023-08-29 12:49:08
164er2u,How old are you guys?,And when did you break into DE?,2023-08-29 10:31:22
160x30t,Azure Data Engineer Associate certification (Exam DP-203: Data Engineering on Microsoft Azure),N/A,2023-08-25 11:52:33
15vvudm,Underrated Open Source Data Tools,"I'm working on a talk on the state of open source tool across the stack, everything from ingest to storage to transform, orchetration and viz. I already know of the common players: duckdb, postgres, dbt, meltano/airbyte, airflow/dagster/prefect/mage, and lightdash/metabase. 

Anything obvious missing that you think is worth looking into? Pleae for the love of god don't mention the project you founded, let someone else speak for it. ",2023-08-20 00:01:46
15ui4jj,What jobs are out there for an experienced data engineer?,"I have been a data engineer for almost 5 years, with 2 of those years at a well known tech company.  Every now and then, I check the job postings in my city and I'm still seeing very few data engineering job postings compared to the last time I was looking for a job.




Right now, what jobs are out there for an experienced data engineer?  I work a lot in python, SQL, and I'm pretty experienced in using one of the major cloud providers.  I also am pretty interested in devops work, but I'm not seeing a lot of job postings.",2023-08-18 11:38:22
15ltzr9,Prove a data model is bad,"Consultants have forced on us a truly terrible data model for our marketing warehouse. Tables and columns are named with inscrutable acronyms, there’s no clarity on how entities relate to one another or map to business processes, and the idea of a meaningful table grain is but a dream (they like to make new tables by grouping by dozens of unrelated attributes).

It’s a mess and the most infuriating part as an ETL engineer is that I can’t get that across to the business side because whenever I say “proper modeling standards” they hear “you’re going to make me wait longer for new reports”. The only way I can think to prove how bad it is would be to redo the whole design myself to give them a side-by-side comparison, but that’s months of work. And of course when the data quality goes to hell my team is the one that’ll take the blame.

Any suggestions or stories of similar situations?",2023-08-08 20:50:32
158l3pt,"Users of delta/iceberg/hudi in production, how has your journey been so far?","I think the technology is old enough at this point to start aggregate some first-hand developer experiences in production.

Any particular pros or cons to signal? Do you wish you would've implemented a proper DWH instead? This is the thread where you can complain (or praise) about this framework!",2023-07-24 19:54:27
14qdocu,VulcanSQL: Create and Share Data APIs Fast!,"Hey Reddit!

I wanted to share an exciting new **open-source project: ""VulcanSQL""**! If you're interested in seamlessly transitioning your operational and analytical use cases from data warehouses and databases to the edge API server, this open-source data API framework might be just what you're looking for.

**VulcanSQL (**[**https://vulcansql.com/**](https://vulcansql.com/)**) offers a powerful solution for building embedded analytics and automation use cases**, and it leverages the impressive capabilities of DuckDB as a caching layer. This combination brings about cost reduction and a significant boost in performance, making it an excellent choice for those seeking to optimize their data processing architecture.

By utilizing VulcanSQL, you can move remote data computing in cloud data warehouses, such as Snowflake and BigQuery to the edge. This embedded approach ensures that your analytics and automation processes can be executed efficiently and seamlessly, even in resource-constrained environments.

GitHub: [https://github.com/Canner/vulcan-sql](https://github.com/Canner/vulcan-sql)

&#x200B;

https://preview.redd.it/x5mq6yz9xx9b1.jpg?width=1898&format=pjpg&auto=webp&s=7535c114eebed8618007b730ea41249976184a8e",2023-07-04 12:17:46
14icdki,Is a Masters Degree Worth It?,"Hey all!

I'm a Data Engineer, have been working on the DA/DE field for 5.5 years now, sometimes in leadership, sometimes as an individual contributor.

My main question is would a Masters Degree in a data related field be worth it at all?

I make a nice base salary and I don't think the additional training would really open my earning potential or career prospecta up that much, however, I could be wrong, so I wanted to solicit advice from the group. 

Please let me know your thoughts!",2023-06-25 04:07:57
14cdr4i,What's Your Data Quality Strategy," 

I'm currently working on developing a data quality strategy for my organization and would love to hear your opinions and insights on this topic.  


What data quality strategies have you implemented in your organization?

How do you identify and handle data anomalies or discrepancies?

How do you involve stakeholders in data quality initiatives?

How do you measure and monitor data quality over time?

 Please share your experiences, challenges, and successes related to data quality.  

Techstack , AWS , Glue , DBT.",2023-06-18 07:07:00
144ccuu,"""Data Engineer"" vs ""SQL Expert""","Over the course of 13+ years, I've become very proficient on SQL. On the technical side, I can do really complex queries, CTEs, window functions, understanding perfomance plans, indices, and I've also learned about DBA regarding file management, logging, and things like that. 

I can very well translate business requirements into a relational database model, and build complex tools using SQL + VB.NET or VBA on Excel. For ETL I can use SSIS, and orchestrate everything with VBA, PowerShell, MS Flow/Automate, and different Windows schedulers or jobs. On the report side I can build a PowerBI dashboard or a very complex tool based on Excel with VBA or a Windows application with .NET. I'm starting to learn Python but so far have been able to make do with the tools I know.

I thought I could call myself a Data Engineer.

But everytime I look at Data Enginer job postings, or even recommendations on this sub, all I see are things like Spark, Hadoop, Snowflake, Databricks, AWS and Azure Cloud. Things that not only I haven't learned yet, but I haven't been able to _see_ in my work environment.

So... am I not a Data Engineer? Or am I just a different type of DE from what the current trend needs?",2023-06-08 15:30:42
13qzhe9,Business Intelligence Engineer intern interview at Amazon,"I have a one hour phone screen interview for this role in two weeks time, they sent a document telling me what to practice and how to set up the live coding session but I’m curious what SQL,Python and technical questions they could ask. 

Has anyone here interviewed for this position before?",2023-05-24 22:11:56
13lpd6v,Evolution and Trends of Data Engineering 2022/23,"**2022:**

* A declarative approach is being adopted everywhere. From Kubernetes (where code is infrastructure), to [orchestration as code](https://airbyte.com/blog/data-orchestration-trends), to [integration as code](https://airbyte.com/tutorials/configure-airbyte-with-python-dagster) with low-code approaches, it's present across all disciplines.
* This same underlying approach has been observed with the [rise of the semantic layer](https://airbyte.com/blog/the-rise-of-the-semantic-layer-metrics-on-the-fly) (essentially a declarative approach to Metrics).
* Metadata trends are consistently growing, with tools focused on data cataloging, data lineage, and data discovery.
* [Rust](https://airbyte.com/blog/rust-for-data-engineering) is likely to be the future of performance-intensive applications in data, potentially taking the role that Spark occupies today.
* Vector databases such as [duckdb](https://glossary.airbyte.com/term/duckdb/) have been adopted for handling small data. Newer ones are supporting the AI wave, with tools like Pinecone and Qdrant. Remember, AI is fundamentally a data game.
* With regulations like GDPR and CCPA, privacy and governance are becoming more important in every company, regardless of size.

**2023:**

* [Data modeling](http://airbyte.com/blog/data-modeling-unsung-hero-data-engineering-introduction) is making a comeback with the unveiling of the MDS. Amid the chaos that can occur when people start to work with complex data, modeling is proving helpful on all levels.
* However, there's a challenge: many people in enterprises are struggling to [utilize the MDS effectively](https://airbyte.com/blog/modern-data-stack-struggle-of-enterprise-adoption).
* AI, and particularly generative AI like ChatGPT, are still finding their footing in the data landscape. There's a lot of hype, but there's also a lot of potential waiting to be unlocked.
* 2023 is shaping up to be the year of MDS bundling. There are [layoffs](https://www.reddit.com/r/dataengineering/comments/13l9ur0/dbt_lays_off_15_of_their_staff/) and consolidations happening (dbt [aquired](https://techcrunch.com/2023/02/08/dbt-acquires-transform/) Transform) across the MDS stack.

&#x200B;

Hi there,

As we continue to navigate through 2023, I wanted to take a moment to reflect on the trends we've seen in data engineering over the past year and discuss our predictions for the future.

I've compiled a list of trends and observations from 2022 and 2023 so far (see above). What do you agree or disagree with? What are some other trends you've noticed, and what predictions would you make for the future of data engineering?

I am looking forward to your thoughts as experts in the field.",2023-05-19 08:31:00
13d4wid,Data Modeling in the Lakehouse,"I have been studying a lot about data modeling, but much of the information is specifically tailored towards data warehousing, and not so much towards modeling in data lakes or data lakehouses. 

For those of you who manage a Data Lakehouse, I am interested in knowing how you approach data modeling in the various layers. Although a Lakehouse aims to merge Data Warehouse and Data Lake features by introducing ACID and CRUD functionalities on top of object storage, I feel that it is essential to prioritize appropriate data modeling practices, which are commonly utilized in data warehousing.

Lets say I have an ELT architecture that follows: Landing (ephemeral) -> Bronze -> Silver -> Gold 

My questions is: How would you (or do you) enforce proper data modelling in Bronze/Silver/Gold layers?

Based on my research, I believe that Inmon-style modeling is the most suitable approach for a Lakehouse. In this scenario, both the Bronze and Silver layers would be source-oriented and maintain the normalized ER model precisely as the source. The Bronze data would then be upserted into the Silver layer, which would resemble the Data Warehouse layer seen in the Inmon Data Warehouse. 

 Next, the Silver layer is utilized to generate or update the data marts in the Gold layer, in response to business requests. To achieve this, I would design Kimball-style star schemas, wherein the fact and dimension tables remain as Delta Lake tables. These star schemas would be unique to each project or use-case, and would not feature any conformed dimensions. Furthermore, Power BI or any other BI tool would perform queries on these star schemas using Serverless Compute. 

Is this a clear and standard way to approaching data modeling in the Lakehouse, or do you any of you do it differently?",2023-05-09 20:24:34
11fg6vq,2023 Raise Survey,"My company didn't give raises this year, so wanted to survey if that was common. 

Raise: 0%  
YOE: 4  
Industry: Healthcare",2023-03-01 20:07:46
11349lf,Is there anything like Kaggle for data engineering?,"I've no idea how it would work but a competition site where you could try things out, see different kinds of challenges, compete for prizes, awards and learning would be interesting.

Is there anything like this?",2023-02-15 18:01:49
1aj3fac,"People who use DBT (Data Build Tool) - what are you using it for? Do you use ALL the components like Tests, Semantic Models, Exposures, ...etc?","I've been using DBT working for a startup company for a few years & think its an amazing tool for anyone who works with SQL databases.  I'm starting to see how DBT could be used as a backend for a full headless BI system, if you leverages things like defining your data as a Semantic Model. 

I know DBT offers a lot, like [Source Yamls](https://docs.getdbt.com/docs/build/sources), [Model Property Yamls](https://docs.getdbt.com/reference/model-properties), [Semantic Models](https://docs.getdbt.com/docs/build/semantic-models), [Exposures](https://docs.getdbt.com/docs/build/exposures), etc.   


1. For those that you use, how do you maintain them at scale? Who manages the creation and editing of model data like descriptions and such?
2. Why don't you use the other ones? is it becuase its too tedious to maintain at scale? 
3. Anyone doing anything custom with Tags or Groups? User authentication or Row-Level-Security? 

Secretly here I'm trying to validate a SAAS idea. How many other people even use DBT? Let-alone the numerious use cases and special files you need to maintain. I've always thought  a UI where you could successfully CRUD (Create, Read, Update, Delete) all your DBT assets would be great. As I learn more about software development, I see this as an opportunity. Wondering if anyone else has pain points with DBT and if so what are they?",2024-02-05 00:38:10
1afxium,Most Hireable ETL Tools,"What ETL tools are the most hireable/popular in Canada/USA? I need to use a tool that is able to extract from various data sources and transform them in a staging SQL server before loading it into a PostgreSQL DWH. My coworker is suggesting low code solutions that have Python capabilities, so I can do all the transformations via Python. They suggested SSIS and Pentaho so far",2024-02-01 00:28:21
190vs11,Meta Round 1 Technical Interview,"Howdy compadres,

I have an upcoming first round technical Meta de interview. I'm curious if anyone have any info on the general difficulty of the questions? Would stratascratch mediums cover it or should I amp it up? A lot of info on the meta swe interviews out there but not a ton on the de ones (at least for this specific stage).

I'm fairly confident I can handle most joins/aggregations etc.. but you know the deal, interviews like this make you question your skillset.",2024-01-07 16:23:09
18wdkvp,"Accepted a 6 Month contract to hire job while I'm full-time, is this a mistake?","Ok, I haven't accepted a contract role for over a decade but this new role offered me a 13% raise plus better benefits and the ability to work with some tools I've been wanting to use. My current company is great but I've been feeling stagnant in my role and have really been wanting a raise. The recruiter told me this company is hiring a bunch of people currently and everyone is 6 month contract to hire but will be full time after the contract period (the contract period isn't performance based). Maybe I'm naive but that sounded fine to me. Am I crazy to leave my full-time role?",2024-01-02 02:05:26
18uesn8,I shared a Python Course (1.5 hours) on YouTube,"Hello, I shared a Python Course on YouTube. It is completely beginner friendly, I started with installation of Python and I finished the course with classes. I am leaving the link below. Happy new year!

[https://www.youtube.com/watch?v=VOdPQmm298o&list=PLTsu3dft3CWiow7L7WrCd27ohlra\_5PGH&index=1](https://www.youtube.com/watch?v=VOdPQmm298o&list=PLTsu3dft3CWiow7L7WrCd27ohlra_5PGH&index=1)

&#x200B;",2023-12-30 12:39:30
184kvgi,How to sell SWE best practices to team and management,"I've been tasked with implementing some code-based tools/frameworks that will be critical to our company's data platform.  I am essentially the solo driver of this project since neither management/PM's nor engineers have any experience with this type of work.  Team is primarily SQL + GUI ETL focused on data warehousing.  Management has my back and trusts me, but I want to avoid a situation where I get enough pushback from engineers that causes management to second guess.    
I've been slowly exposing the team to these tools/practices over the last few months, but it will become critical that devs are somewhat comfortable with:

* Containers
* Non-SQL programming (Python)
* Version Control
* Code quality/readability/reviews
* Unit testing
* CI/CD pipelines
* Prod/Test/Dev environments  


Some questions:

Regarding devs, most are more adaptable and willing to learn.  However, some will strongly push back in favor of keeping their old practices, and I'm worried some won't even be able to upskill at all.  Any tips to make the transition and training smoother?  


Do PM's/BA's need to be upskilled/informed as well? They're solely concerned with the business side of things, so I've been pretty much ignored, but I have a feeling they will come knocking once stuff starts really rolling out.  


I'm not a senior, but management pretty much told me to do it, and they've given me free reign with no oversight. I'm doing my best to set a good precedent for initial development by keeping code as high quality as possible (writing tests, consistency, modular, documented, future-proof, and as simple + readable as possible).  Any general advice for handling a large initiative like this?  


Bonus question: it is very likely I will receive negative heat from some senior engineers, and I will most likely have to deal with a lot of complaining and criticism. I mostly just tune it out, but I do not need this making my life more difficult. Any advice for dealing with this?",2023-11-26 21:07:17
183rcpc,Why Does Buying Software Suck So Much?,"The process of purchasing software sucks. It feels like the vendors do everything in their power to make it difficult to find honest opinions/information/benchmarks about their products, and once you do narrow down your options, dealing with sales teams is miserable. 

I want to do something about it. I'd like to create a product that simplifies the process for purchasing data/analytics software, and I'm in need of your help to determine where to start.

I think that the buying process breaks down into the following steps. I'd love to hear your thoughts on which are the most frustrating and why.

1. Pre-purchase: Identifying software needs
2. Pre-purchase: Narrowing options down from many to few (usually done via online research)
3. Pre-purchase: Narrowing options down from few to one (usually done via hands-on evaluation)
4. Pre-purchase: Negotiating favorable contracts
5. Post-purchase: Ensuring seamless integration
6. Post-purchase: Continuous monitoring and evaluation of software performance
7. Post-purchase: Vendor contract management (spending trends, renewals, etc)
8. Post-purchase: Vendor product management (new product announcements, new use cases)

If you'd like to contribute to a solution anonymously or just not on this thread, you can also submit your opinion [here](https://app.opinionx.co/9ce9b84f-8acf-4192-9970-fd0a395f1a5b).

Thank you for your help!",2023-11-25 19:25:54
17xbk3s,RDBS to Big data,"Hi Folks,  
We have Oracle database as our RDBS and we are facing issues on performance on some complex queries where number of rows are very high. Management wants us to implement big data and do all reporting/visualisation from big data and only transactional data in oracle rdbms. There is no big data expert in our team for now. I am planning to use Hadoop file structure and push the data from Oracle to HDFS using sqoop and then use Impala for querying. On top of we planning to use Qlik Sense ( we already have license for this )which already has impala connector. Do you have any other suggestions or opinion on this. Is there any case study/ documentation of this type of data orchestration",2023-11-17 09:45:36
17j39vc,What is the use of the Data Vault pattern actually?,"hey folks, hope you are all doing great!!!

As dbt Coalesce 2023 ended, I've finally got a chance to review all the content that interests me and one of them is [this 60 sources and counting: Unlocking microservice integration with dbt and Data Vault](https://attendees.bizzabo.com/433222/agenda/activity/1179760). I've heard the pattern of Data Vault from time to time but never heard of good successful experience of implementation.

Now, here goes my questions:

1. What is Data Vault really aiming to solve? focusing on the change capture with many different data sources??
2. From what I understand, Data Vault implementation not just requires a good level of competence of data engineers but also the analysts (i.e. the analysts have to be very clear about what they are doing). Is this level of expectation of analysts realistic actually?

Especially for 2, if the benefit of Data Vault is not strong enough I would properly just fall back to Kimbthe all model (maybe it is me being biased - I couldn't even find a good use case for Inmon the model).

Nevertheless, more than happy to be correct and learn more from the community here :) happy to be wrong so I could gain more knowledge.

&#x200B;",2023-10-29 13:46:18
178jc4k,Whats the first few questions you ask when you come across a Data Engineer?,"Maybe when you meet on a vacation, in a bus. 
(Spare the Tech Stack Question please)",2023-10-15 16:39:52
177xz4u,How do you schedule Spark jobs?,Looking to see what approaches others use. Do you use Airflow or equivalent? Use some cloud event triggers? Run bash scripts in cron jobs from a Hadoop cluster? Let’s discuss what DE’s have tried. Also indicate if it was for batch vs streaming.,2023-10-14 20:12:51
16lknys,What are the real use cases being solved using Apache Iceberg and how was it done before or what were the challenges?,"Apache Iceberg is a popular open-source table format for large data sets, but I'm curious to know more about how it's being used in the real world. What are some specific use cases where Iceberg is solving problems that were difficult or impossible to solve before? What were the challenges before Iceberg, and how has it made things easier?

I'm particularly interested in hearing from data engineers and scientists who have used Iceberg in production. What are your favorite features? What are some of the challenges you've faced, and how have you overcome them?",2023-09-18 03:33:58
16cuqt2,Starting my own analytics engineering team- give up or die trying?,"I'm currently a year into being a part of a small analytics team at a large and antiquated retail company. My manager and teammates are completely business-facing with minimal technical knowledge. Speaking to some higher ups and other functional teams, I realize there's a huge cap in data needs and people who can manage simple data workflow, such as getting the data, cleaning & exporting it, and creating dashboards.

Our leadership is not open to hiring additional senior level data folks, so I guess it's up to me to do the job if I choose to. My question is, is it worth the effort to start my own team and build an entire infrastructure from the ground up? (vs. simply find a new job with modern data culture) Currently, all of the raw data are in Excel/CSV. I've helped them automated all of the data transformation steps with Python and built some nifty dashboards for our stakeholders. 

What would be the tools & costs associated with building a completely new infra?

Stuff I can think of: (we're of course a MS Windows shop)

* A database (ex. MS SQL server on Azure)
* An orchestrating tool (Airflow/Prefect)
* Github
* Tableau (we already have)
* Additional headcount (I will be a one-man team to start. I don't have huge ambitions to be a people manager. I'd love to at least get a teammate so we have coverage) Eventually I'd like to get someone with data science knowledge, so we can provide more advanced analysis instead of purely reporting.
* What else am I missing?

Appreciate any advice!",2023-09-07 23:51:57
15wl1kn,Spark vs. Pandas Dataframes,"Hi everyone, I'm relatively new to the field of data engineering as well as the Azure platform. My team uses Azure Synapse and runs PySpark (Python) notebooks to transform the data. The current process loads the data tables as spark Dataframes, and keeps them as spark dataframes throughout the process.

I am very familiar with python and pandas and would love to use pandas when manipulating data tables but I suspect there's some benefit to keeping them in the spark framework. Is the benefit that spark can process the data faster and in parallel where pandas is slower? 

For context, the data we ingest and use is no bigger that 200K rows and 20 columns. Maybe there's a point where spark becomes much more efficient?

I would love any insight anyone could give me. Thanks!",2023-08-20 19:51:45
15olmmq,Why are u doing data engineering?,"Please tell me why you have chosen data engineering and not any other work like data analysis, dba, swe, devops, etc.",2023-08-11 21:41:32
15korgl,What is everyone using for a Portfolio Website,"So my github is a mess and more and more I'm seeing a ""Portfolio Website"" area on applications. For people that have a portfolio website what do you use? I started a github web page a while back, but feel like with the Jekyll (I think that is what it's called) it just felt ""clunky"" Anyone know a good website you can build a portfolio site on quickly and easily? Anyone running Django or something else on a VM or in Docker and using a domain that you purchased? Curious to see what everyone uses for making a Portfolio Website.",2023-08-07 15:59:43
15i500l,"Moving out to GCP after 8 years in SAP, is it a good decision ?"," I've been working in SAP (within SAP itself) for 8 years now, as a data and analytics consultant (Mainly Hana, BO 4, Dataservices and SAC BI - not planning but BI )  
As my pay didn't evolve much and as i'm getting bored and I don't see how i can evolve in SAP. I started to check other opportunities :  
I got certified as data analyst in Azure and GCP and i've been looking into going out from the SAP world as going freelance seems like a good idea for salary and new challenges, and going freelance in SAP is hard (no that many opportunities, except for SAC Planning that i've never worked on)  
So my plan is work with a company for one year on either Azure or GCP as data and analytics engineer then move to freelance.  
After a lot of work, certifications and after i got a positive answer from a good company to work on GCP i'm wondering whether or not i'm making a huge mistake. Leaving SAP behind for GCP, i'm in europe and the market is full of freelance offers in GCP, also unlike Azure or AWS, GCP is still relatively so i bet i can compete in a year or so..  
But at the same time leaving SAP after 8 years ain't that easy so... what do you think ? ",2023-08-04 16:40:00
15f83gv,"Are my external data engineers incompetent, or exploiting us?","Hey all!

I'm in a bit of a pickle. I've recently started a new job in a fortune 500 company. My role is primarily as a business analyst / marketing analyst. My team consists of around 7 analysts, whom come from domain specific backgrounds with zero or limited programming experience - they work with Excel.

One of our tasks is updating customer segments from sales reps: they hand in an excel spreadsheet, we see the corrections, and update them.

The update process, so far, is all done by using some old SQL queries, pulling data into multiple spreadsheets, doing a bunch of VLOOKUPS, then pulling historical action data out of our DWH, comparing and see if historic changes were met.

Now, in order to make the changes, we email a list to our data rep (xlsx), and they then load these changes.

I'm trying to understand: why do we, as analysts with zero knowledge of ETL processes, limited knowledge on DBs, work on manual joins and analysing historical changes ourselves? I totally understand the use of Excel, and automation takes time, but there is so much room for human error here.

The majority of my filtering / vlookup / categorising tasks could be cleaned up with some regex and automation server side + additional data validation checks. We consistently have errors due to the human condition: Tuesdays suck, we forgot to join certain IDs etc.

I would like to approach our external partner and ask why this is the case, but maybe this is a little too political?

so the question is: **as data engineers, is our external partner not delivering, or why might they want a workflow like this?**

Thank you!",2023-08-01 10:02:21
14lhlo6,Do you as an engineer care how much efficiency/cost reductions you're bringing in?,"Most people in their resume mention, they improved processing speeds by 20%, 30% etc, reduced costs. by x, y etc, . Do most developers know these kind of impacts in general? as an interviewer, do you care to ask them to elaborate? and people who mentioned stats like this, do you see improved calls for interviews? Also, at what years of experience people will care about this? ",2023-06-28 18:46:53
14kdb23,Do you study outside of work?,"




Hi guys, how is your study routine when you are already employed? do you study on the weekend or after working hours? or just during work? and if it's during work, do you try to research and implement the new concepts in some project at work or do you really study by taking a course, etc? thanks!",2023-06-27 13:07:05
14f73gz,PyCon DE & PyData Berlin 2023 Playlist,"[Playlist](https://www.youtube.com/watch?v=VtL6Y4x10O0&list=PLGVZCDnMOq0peDguAzds7kVmBr8avp46K)

Some of the talks I found most intersting:

* [ Use Spark from anywhere - A Spark client in Python powered by Spark Connect](https://www.youtube.com/watch?v=PzgPcvFDD4I&t=1339s)
* [Streamlit meets WebAssembly - stlite](https://www.youtube.com/watch?v=XivJYZUm1GY&t=1368s)
* [Pragmatic ways of using Rust in your data project](https://www.youtube.com/watch?v=Jk9NXfvgclU&t=1374s)
* [An Introduction to Apache Spark](https://www.youtube.com/watch?v=jOJceajwMGs&t=749s)
* [WebAssembly demystified](https://www.youtube.com/watch?v=VCkcv0ppYXs&t=5s)
* [Rusty Python - A Case Study](https://www.youtube.com/watch?v=Y5XQR0wUEyM&t=52s)
* [Towards Learned Database Systems](https://www.youtube.com/watch?v=VtL6Y4x10O0&t=13s)",2023-06-21 12:59:44
14f0msx,Best User Requirement,"Just received this requirement from our users.

https://preview.redd.it/dkvbl0ytob7b1.png?width=207&format=png&auto=webp&s=e2ce57b74736ef56cc8d74f8d4456a4cf33965c6",2023-06-21 07:22:03
13yywky,Do websites have separate (duplicate) databases for use with APIs?,"And if they don't, would it make sense to do so? I feel like it would allow them to increase rate limits and sell their data in greater quantity with less strain on the site itself.",2023-06-03 03:11:58
13uu7qh,Are Masters Degrees beneficial in this field?,"MS in Data Science or CS doesn’t matter, my department president is saying that those degrees can help get me more prepared for the job market, but I’m not sure if this is simply because it’s not easy to find a job right now. I’d be a masters student with YOE as a data analyst trying to be a DE. 

Not sure if there’s a big overall benefit to having a masters for Data Engineering

(I am getting a BS in Computer Science with a Computational Data Science specialization, as requested)

Edit: I feel it’s necessary to add that looking at a lot of data engineering jobs on sites like Indeed and LinkedIn directly state “Masters degree preferred” or reference that a masters is acceptable. It seems like many responses here aren’t aware of that.

Edit 2: Wow I’m happy to see so many people engaged with this post! I’m noticing a huge bias AGAINST a masters degree here. I can definitely understand that, but the job descriptions 99% of the time say they prefer a master’s degree in DS or CS. I understand that may not teach you everything but it seems to matter in these descriptions. Regardless, I think my best option is to continue on being a DA, and maybe getting masters part time….  

The most important aspect, though, is going to be a project or projects that showcase actual practical DE skills, according to many responses here. I have a data pipeline project started and hope to be in the “maintenance” stage of it by the end of the year, I’m hoping that will lead into a DE position, and avoid getting a masters.",2023-05-29 12:58:31
1211nsk,At what point is Python not answer for piping data?,"I have been working as a Data Engineer for a bit now, but most of my background is in full stack development utilizing Node, dotNet, and front-end frameworks, and both noSQL and SQL databases etc., but rarely Python. Python was always used as a scripting language or for hobbies. Now, in the world of ""data fill-in-the-blank"" Python is the bee's knees. I do like it..., and have decided to move forward with a sticker on my Mac but at what point an I forcing a round peg in an octagon hole? I have been using Python to write small pipelines moving a million rows or less from various flat-files or DB tables to some data warehouse. Everything seems simple: Read in a CSV with Pandas, do some cleaning, upload to the whatever database, etc. However, some of the projects I'm working on are requiring a bit more sophistication or heavy lifting. Pandas and Sqlalchemy seem chunky and not great (slow) for migrating greater than a million rows from one source to another. Are there options other than Sqlalchemy or Pandas in the Python world for moving data or have I been reading too many articles by Data Scientists?",2023-03-24 22:48:23
11yd7b9,DE interview - Spark,"I have 10+ years of experience in IT, but never worked on Spark. Most jobs these days expect you to know spark and interview you on your spark knowledge/experience.  

My current plan is to read the book [Learning Spark, 2nd Edition](https://www.oreilly.com/library/view/learning-spark-2nd/9781492050032/), and search internet for common spark interview questions and prepare the answers. 

I can dedicate 2 hours everyday. Do you think I can be ready for a spark interview in about a month's timeframe? 

Do you recommend any hands on project I try either on Databricks community edition server, or using   AWS Glue/Spark EMR on AWS?

ps: I am comfortable with SQL, Python, Data warehouse design.",2023-03-22 09:22:42
11ixh3k,Anyone else feel like they are using Pandas as a crutch?,"I am one of the very few people who like Pandas syntax more than SQL. So, I use it everywhere. Even places where I obviously shouldn't. 

If I am just reading in a CSV file doing very basic shuffling around and operation, I am busting out Pandas. I don't even need Python, I could do some bash+grep+awk hacking and get a highly performant solution.

Even if I should use a DB as an intermediary, I am running a Pandas operation that takes around 10 minutes that would have taken less than 1 minute through SQLite or any smaller db. 

Sometimes I would do intermediary processing with Pandas which will far faster if I just opted to run that operation on the DB side.

I constantly say to myself, let me write the ""pseudocode"" to this solution in Pandas, I will convert the solution to SQL later. But as you all know once you have something that works you just jump into the next thing. There is often no going back. If it takes less 10-20 minutes, I will just keep the Pandas solution there.",2023-03-05 13:43:50
1apqjnd,Pick 30% paybump for a small company or work in a big bank?,"My friend is struggling to make a choice - remote, but 30% less pay and for big corporation or working in small firm of about 200 employees as a solo data analyst

Big corporation - 50k employees +/-, 13 data analysts on her team, most are experienced, not much room to grow or manoeuvre (big bank in Poland). Remote as well.

Small firm - 30% paybump compared to big corporation, solo data analyst, about 20mins travel (which is OK for Poland) works underneath firm's director as solo analyst. Last one left because they wanted to move abroad. Company tripled in revenue in the last 2 years. Not a startup, been on the market 15 years. 

So obviously remote is big plus, but not much room for growth -  she's still young, only 25, and it'll be hard to gain promotions and grow given there's 13 people in her team, and the second youngest person is over 30 with lots of experience. 

Other company - room to grow and expand, possible manager within two-ish years. Works underneath the firm's director and CEO. Obviously if something goes wrong it's on her. And in office, no hybrid. 

What would you pick, I said I'll ask some people but we're all kinda like ""that's a tough one"".",2024-02-13 10:40:28
1ai5iha,DE Side projects,"Hi All

I’m curious to know, what data engineering side projects are you working on? I’m thinking to start one using Kafka and spark because I want to learn these technologies . Trying to find good real time data set or API?",2024-02-03 20:35:32
1af15s3,What is the ideal masters for data engineers?,"Comp Sci, Data Analytics, Information Systems, stats, math etc  


Your thoughts?",2024-01-30 22:18:48
193a5e1,Less technical + large salary increase or more technical + slight salary increase,"Hi all, first off I would like to thank this sub massively. I’m more of a lurker than a poster but it taught me everything I needed to know to transition to DE roughly 1.5 years ago.

To put it simply, I have two offers, A & B. I wondered if some of the engineers on this sub could offer some advice.

A) 1.5x salary, more seniority, outdated tech stack (think SSIS, Teradata). Role is more of an analytics engineer I’d say.

B) 1.1x salary, same seniority, great tech stack for learning (GCP, Scala, Spark). Role is much more Software Engineering with a data speciality.

Background) I’m early careers. Using the outdated stack in A), currently in a less senior analytics engineer type role. Probably overcompensated in my current role for my age/experience. Non Computer Science but engineering background/education so I pick up technical skills relatively quickly.

Goal) I’m very good with and leading people so I’d like to eventually lead a team of engineers once I’ve learnt the relevant skills to understand the difficulties and pain points engineers have. Interest lies more in the technical software side rather than analytics.

Any comments would be much appreciated!

TLDR - Should I be learning or earning in early stages of my career!",2024-01-10 14:57:17
18hs6ue,What role did you go into after Sr. Data Engineer?,"Curious to hear what your current role is now and how you made the decision that you were ready for a change. 

Currently looking to make a change but not sure if I should go into people management or an architect type role.

Cheers!",2023-12-13 21:50:17
1808qro,Should I use Airflow,"Working on a personal project where I’m pulling NBA data from a scraper and putting it into a database. This scraping obviously needs to be automated. Obviously going to need to automate my data cleaning as well. A little bit further down the line I’m going to want to automate the process of pulling data from my database to visualize it on a website. Not sure if this calls for Airflow, I have zero data engineer experience.

Thanks!",2023-11-21 04:40:39
17y0dyx,How is DE at Meta/Facebook?,"Hi Everyone,

I read a lot of bad things about DE at Meta as it's more like an analytics job where they are just writing SQL queries. Does anyone here has more idea or can elaborate on the role? Thank you",2023-11-18 06:32:21
17u2ova,Your Thoughts on OLAPs Clickhouse vs Apache Druid vs Starrocks in 2023/2024,"I've been conducting a research around OLAP dbs that are relevant in 2023 with my data science team for a new project. 

The ideal solution would be *a db that can make use of an S3 bucket as long term storage, while being easy to deploy and manage on Kubernetes.* They will be consuming *kafka topics.*

I have narrowed down the solutions between **Apache Druid**, **Starrocks** and of course **Clickhouse**.

While **Druid** shows some very neat tricks on real time data storing/indexing, its *deployment on kubernetes is really ugly*. 

**Starrocks** is not so far, also a bit complex to deploy and manage the *""front end and back end"" clusters*, while **Clickhouse** *makes the k8s deployment a bliss*. 

What got me in **Clikchouse** is the *kafka topics ingestion*, its a lot of work to create and keep managing sql materialized things everytime I need to get some new data. **Druid** otherwise makes it super cool by *almost automatically understanding the json received from the kafka topics*.

However I feel like **Druid** is getting a bit old and with not so much community or development resources or attention. ChatGPT is saving my ass I must admit. Tho, a lot of ""what corps are using this tech"" sites show that Twitter, Neflix and Reddit itself use Druid. I'm not sure if that is true or old data. 

**Clickhouse** otherwise feels like the industry baby, ultra-seeded and megacorp friendly, but with its own problems like painful cross table joins. **Starrocks** have a nice fanbase, but falls a bit in the same place as Druid, even being some years ahead of Apache Doris, plus the fact it is mantained by the Linux Foundation.

So, what are your thoughts on these three OLAP databases today? 

&#x200B;",2023-11-13 04:19:07
17f8xjx,"Should, a data engineer, uses Pandas in his production code?","Pandas is a fantastic library for reading datasets on the go and performing daily data analysis tasks. However, is it advisable to use it in our Python production code?",2023-10-24 09:43:51
172usqc,Big tech companies with “analytics engineer” roles,"Currently work for a fully remote and very good low tier tech company. Tech is great, people are great, culture is not bad, BUT I can’t manage to get a promotion. My company usually hires ex-FAANG/bigtech for senior roles and this is my first real FTjob so my chances to get promoted are very slim. 

Since my company has a good reputation in data engineering, I don’t think it should be that hard to get a job at a big tech company and that seems to be the only way to get to the holy grail the “senior” role. 

My question is, which big tech companies have data engineer teams that act as analytics engineers. I know for a fact Meta data engineers are actually analytics engineers working mainly with sql, which is what I like and want to do. 

I’m not a fan of working with scala and managing spark clusters. I have done this before in a part time job i had for 10 months before my current role and I definitely don’t like it. I prefer using dbt+snowflake/DWH and doing some slight devops (docker, terraform, cicd, etc) and maybe work on dashboards here and there (but that’s mainly the job of analysts). Like helping analysts build data products for whatever the business need is. This feels more fun to me since I’m a bit closer to a “product”. 

My goal would be to do a lateral move as a mid-level data engineer to a big tech company, work there for some years and if I like it, push for a senior promo in the same company or go back to a low tier tech company like the one I’m in rn for a senior role. Only thing missing is which companies I should apply to. Obviously Meta is going to be one but that’s definitely a reach, need many more companies to improve my odds",2023-10-08 09:20:02
16erfno,System Design,"I’m new to the data field, and I was looking for resources/books that could help tie in how different tools and concepts can work together in one data project. I hear a lot about dbt, airflow, apache spark, apache hadoop, kubernetes and other tools & technologies, but never how each can work hand-in-hand in one project to achieve different outcomes. 

Any recommendations on where to get started understanding how they’re each used and integrated together in the industry in one system.",2023-09-10 05:07:30
16b2ajv,New book - Cost-Effective Data Pipelines,"Hi folks,

I just published my book, Cost-Effective Data Pipelines, with OReilly. I wanted to offer this community a free 30 day trial to check it out:

[https://learning.oreilly.com/get-learning/?code=CEDP23](https://learning.oreilly.com/get-learning/?code=CEDP23)

I wrote CEDP to give people access to a lot of hard earned, experiential advice I would have really appreciated not having to learn the hard way. 😅 I hope you find it helpful in your work!

Examples for the coding portions of the book are available [on GitHub](https://github.com/gizm00/oreilly_dataeng_book)",2023-09-05 22:22:55
15x8wfk,Is my career stuck?,"Hey all, wanted some input on my situation. 

I’ve worked at my org for around 3 years now. Originally was hired on to do advanced analytics work, instead have been doing data engineering work for the entire time. Have been searching for a new role since January, with no luck. I really have enjoyed the problems data engineering has presented, so I want to stay in this lane rather than an analytic one.

While trying to remain anonymous, here’s a bit about my current job. 
 - 70% Apache Nifi
-20% SQL
-5% Python
-5% leadership/mentoring new teammate, educating on Tableau/data best practices
- no cloud/Hadoop
- Make around 100k

I’m still young so I don’t have a lot of experience outside of this job but every opportunity I apply to just seems very far out of reach. Thing is, taking an entry level job would be a large paycut that I can’t take do to some major life events. 

I feel like I’m not building any useful skills and am not learning anything. For reference this is my first job outside of internships. In my opinion, I have certain mid-level responsibilities, at mid-level pay, but only with entry-level skills, and that makes it hard to grow and move to any new position. 

So, how do I upskill in ways that are meaningful to employers, or should I suck it up and take a ~30k paycut as soon as I am able to?",2023-08-21 14:14:21
15qua72,What are the use cases of chat gpt you have come across in DE?,"Uses case that you can apply in DE as well as delivering anything to end users?

&#x200B;",2023-08-14 13:08:52
14leo16,Databricks releases official SDK for Python,N/A,2023-06-28 16:51:23
14cve1u,Best resource to learn Apache Airflow?,I usually learn from YouTube videos but I don't see a reputed playlist on Apache Airflow. Any recommendations from where I can learn the same?,2023-06-18 21:14:32
13u1oma,What are you currently solving for?,What are you currently solving for in your role? CICD issues? Refactoring pipelines or a data warehouse? Having to jump into analysis?,2023-05-28 14:23:58
13pb7dr,DE Title but Unqualified for Position,"My current title is Data Engineer, although I landed here in a non-tradition way. I started as an intern on a Data Management team, hired on as IT Application analyst, then the company re-titled all IT staff to either data engineer or software engineer. Since, I was in the data space, I received the data engineer title. I do not have coding background, my department purchased third party vendor tools for most tasks in our workflow (i.e. Informatica (ETL), MicroStrategy (BI), etc.). Recently, my company has transitioned to AWS with an emphasis on CI/CD and Infrastructure as Code. I realize that I am extremely unqualified for my job title. I have watched Python videos online in hopes of learning. I recently completed a free Data Analysis with Python course (more geared towards Data Analyst than DE). I currently work primarily with new engineers so the lift I provide the team is essentially my knowledge and experience in the data space (include data domain knowledge, ETL and data quality standards, etc.). I primarily play more of a consultation role -- I design and they build.

I am in search of Data Engineer work and willing to take entry level work because I want to work alongside other data engineers and learn. But, all roles require Python. I'm looking for advice, suggestions, or recommendations on what to do. Ideally, I would like to remain in a data engineer position, but without the coding background I cannot consciously embellish my python experience. Those familiar with my situation tells me ""to apply for the job I want not the job I qualify for"". But, I feel the lack of Python is significant. My team currently has an individual who potentially embellished their experience and I just don't want to be stuck in that same situation where I cannot pick up coding efforts...

I have considered instead a Data Analyst or Data Governance Analyst position. My SQL querying skills are fairly decent. But, I would lose out on the technical knowledge I've gained in the past few years (especially the AWS experience).",2023-05-23 02:23:30
13ig4tr,Data modelling for startups,"I've recently seen a few posts asking about working with data in the context of a startup, including a few about setting up a stack and doing data modelling.

As luck would have it, I've been writing some reflections on those subjects. I'm shortly going to be moving on from my current org (which I joined when it was very early stage) and wanted to record some thoughts before I leave and memory fades...

Anyway, I have finally published a series of three articles on our engineering blog! I've entitled it 'data modelling for startups', and you can find each article here:
- [Part I](https://medium.com/apolitical-engineering/data-modelling-for-startups-part-i-f566af2a88ca)
- [Part II](https://medium.com/apolitical-engineering/data-modelling-for-startups-part-ii-1b1fa58700e4)
- [Part III](https://medium.com/apolitical-engineering/data-modelling-for-startups-part-iii-aca31e0f4176)

I hope some of you find these interesting or useful. In any case I would really appreciate any thoughts/feedback from the hivemind!",2023-05-15 18:30:02
127vsrf,what tools do you use to pull data from APIs?,"I work in healthcare and on a data engineering team and when we need data from APIs, our team uses powershell. The code is documented well, but it just looks ugly. I find python so much easier to read, but nobody on my team wants to switch to python from powershell. Does anyone here use powershell to pull data for APIs?",2023-03-31 19:11:43
11kyewm,How Discord Stores Trillions of Messages,N/A,2023-03-07 12:45:18
11jzppg,What's your favorite end-to-end tech stack?,"E.g: database systems, workflow engines, ETL/ELT tools, analytical tools, cloud services, etc., basically anything that you feel is a convenient tool set to get data from source(s) to destination in the desired form.",2023-03-06 14:00:28
118ccej,What are your thoughts on MOOC CMU DB courses by Andy Pavlo?,"Primarily 15-445/645 - INTRO TO DATABASE SYSTEMS 

I came across it on Youtube and I guess it must be interesting for people who work with relational DBs a lot (esp. OLTP systems). The syllabus mostly focuses on internals of an abstract database like Buffer Pool, Query Optimizer, Concurrency Control, ACID Transactions etc.   


Aside from that they have an active community on Discord and all coding assignments are available to the general public for submission and auto grading.  
   
Has anyone taken this course recently? Would be great to hear your thoughts on it.",2023-02-21 18:27:52
1anekuq,What is missing from the current data engineering tool landscape?,"Data engineering has come a long way in the last 10 years. Especially the ""Modern data stack"" -hype brought a lot of interesting ideas and highlighted areas that have been previously overlooked, like data quality tests and bringing more software engineering practices to data engineering.

However, I still feel that the workflow for data engineers could be improved. I have been working a lot with Databricks and while I think it's a very great tool with great flexibility and great cost efficiency, the whole Spark ecosystem feels a bit like using a heavy sledgehammer for regular-sized nails.

I think my ideal platform would be something similar to Databricks, but instead of Spark, it would use just a basic Python environment and something like Polars where the actual processing of the data is done in a more performant language.

What are your thoughts and ideas on the current state of the data engineering tool landscape?",2024-02-10 11:47:24
1ak7g53,What DE Stack is best for beginners to early professionals?,"I have already posted here about data modelling and how am i not sure about it as my career. The worst thing i feared happened. i have 2 to 3 yrs exp in total. i was thrown out of project due to billability. my stack is data modeller,snowflake, azure. personally i know python and little bit of java. i am quite comfortable with data structure. As i am in bench, i tried to apply for many job openings. all of them ask for previous software experience for software jobs which i dont have and databricks experience in DE which i dont have. not a single interview went my way so my confidence is going low.

So i decided to give it a month to read and upgrade myself. what stack would you people suggest for me. (i  was confused with too many tools like scala,spark,hadoop, aws redshift,emr, adf, airflow, alfka,nifi)  Thank you in advance!",2024-02-06 11:17:55
197xr9y,Open-Source Observability for the Semantic Layer,N/A,2024-01-16 08:07:40
191m6ke,"People who transitioned to DE, how did you study?","I'm a junior Data Analyst/Scientist consultant, mostly doing analytics and BI, but I also work with machine learning.


I want to move into Data Engineering. I've started learning on my own and I'm curious about how others have done it and would recommend doing it.


My plan is to learn the basics on my own, replicate 3 full projects from the web, then do my own project on something I like. 


Does this sound good? What do you think? Do you know some end to end project that you would consider worth learning from?",2024-01-08 14:16:08
17np6mq,Vulnerability of Data Engineering to Outsourcing," 

Hey guys,

Data analyst with a masters in DS trying to transition into data engineering here. So appreciate this subreddit and your guys’ help and advice here.  

I had a previous role where I was developing Python pipelines as a data analyst, and my process was to be migrated into SQL and a database system to hold all the data. I was excited to do this work, but the company ended up outsourcing the work to a tech consultancy in India. I gave them the details of my project for a couple months while they did all the work, and I left the situation with little to no growth in data engineering.

This was a fairly large company, so maybe it had to do with company size. But that has got me worried that a lot of this work may be vulnerable to outsourcing, like what happened to me at that job. Any thoughts? Ways to make myself less vulnerable to this? Appreciate your help. Long-term thoughts on work deletion from automation and AI could be useful as well. Thanks so much in advance.",2023-11-04 16:21:22
17loju3,How do you deal with an insecure manager who throws a lot of buzz words and wants to implement his way of doing things because he found a new tech or process a few weeks back?,Working with an insecure bully who wants to appear smart and be seen as a savant . However he is a pain to work with because he wants us to implement solutions he thinks is best and is usually something he read up on or found out a few weeks back . I and the other senior engineer voice concerns and he then becomes nasty and belittling making comments like “have you ever done this before” or “do you even know anything about data architecture or xyz technology”,2023-11-01 22:45:33
176gql9,Do i take this job?,"I had an interview with the CEO of this startup that is obviously too obsessed with generative AI and wants to build a “centacorn” by doing stuff with AI. 
I will be coming in as a data engineer, working completely on AWS stack if everything goes well. Will be working on creating solutions to architect, model the data. 
 The CEO said he wants “billions of rows of data” so he can predict “the future.” The company has around 50 employees, product and service based and been stable for a few years. 
Graduated last year, right now i am working as a DE in an ETL department, mostly just working on query/process optimization and migration process (locally). I don’t think i am developing my skillset here that’s why looking at different options. 
This potential job offers a decent increment, good data stack and cloud of course but i am uncertain about the company after the meeting with the CEO. Market is horrible. Takes months to get a new job in chance of lay off and i cannot afford that. Is it worth the risk?",2023-10-12 20:35:01
16z4nwt,How much time do you study each day?,"I have been working as a big data engineer for 5 years, and every day I realize that the topics are too broad and complex. I often get lost and don't know what to study in depth.

Do you have any advice on how to become a good data engineer?
How many books do you read in a year?
How many hours a day do you study? (Outside of working hours).

Any advice is welcome, thanks in advance",2023-10-03 21:50:28
16wysjd,What is the best way to query JSON and parquet files on S3 for data verification and pipeline building?,"We have a bunch of files on S3 (minio) primarly in JSON and parquet format. We are searching for a simple solution to do some ad hoc sql queries against those files to verify data and see the structure. mainly for the engineers to build the data pipelines in dbt/dagster. we don't want a complex solution which involves many components and there's no need to scale computing beyond a single node.

it seems there are many solutions with a hive metastore like trino/presto. but this requires multiple tools and manual registration of the single files.

our preferred way would be something which we can point to a s3 bucket and it shows all the files as e.g. views in the database. it shouldn't require to manually specify the schema/structure of the files. just lookup the structure on read. the files are usually only around 1-20MB. so really small scale.

anyone have an idea which tool could help us here? And what are you using to engineer your pipeline queries from unstructured data?",2023-10-01 11:32:57
16q16lp,Leetcode SQL for FAANG,Is there any list Leetcode/Hackerrank questions list for SQL Data Engineer interviews at FAANG companies?,2023-09-23 10:37:33
16hpyf2,What to do with a 4 years gap in an IT resume,"I'm wondering if a good portfolio (I don't know yet how good) and certificates can cover the 4 years gap I have. Knowing that I didn't choose to chill for 4 years (and it was far away from that anyway). Employers don't seem to care about why I was hit hard by life to the point of discontinuing my career. Anyway, I'm not asking for their pitty but I'm determined to make my profile succeed in getting me good jobs despite of the gap. I've been autolearning for a year now and I built some projects from scratch. How can I show my motivation by exposing these projects ?",2023-09-13 15:25:42
152c49r,"What's the best ""free"" set-up to learn or try to build a data pipeline on my own?","I am new to data engineering, and I like to learn by trying/building. I am very unclear what's the best way to go about it?

My ultimate goal is to be able to build my own website with a data pipeline in the backend and front end with data analytics visualization etc. Should I go directly with free cloud resources (like AWS), or should I try to manage a local data pipeline such as postgresql, and once I hit the bottleneck, I migrate to better scaling solutions?",2023-07-17 19:54:44
1509lyr,Pyspark VS SSIS,"Hi,

I currently do my etl task with spark. It is simple ETL jobs. The amount of data is small to medium. I essentially work with on-prem data, but some may be stored in S3 buckets or Redshift.

I have good knowledge of SQL.

Since I am looking for a new job, I wonder if I should learn SSIS to do ETL, since a lot of companies use it, or is AWS Glue and Pyspark is sufficient to tackle ETL tasks.

Is it worth to learning SSIS ? What are the pros and cons ?

Thank you.",2023-07-15 11:28:48
14m7hqv,Event-driven architecture best practices for databases and files,N/A,2023-06-29 15:10:03
13ftlcp,Experiences with trino? What am I missing,"I've only been using trino for about a month now in a practical sense but I have really grown to love it. It's a really nice way of joining datasets across databases. 

I've been pulling data outta parquet files in s3 (hive/glue) then joining them with a postgres instance. Works wonderfully. It's been working so well that I even got the green light from my boss to start using it in production. 

I don't see many folks use it here, which makes me wonder what obvious flaw in missing. Would anyone care to help me understand what it's barriers to adoption have been?

Update: thanks for all the support guys",2023-05-12 18:48:56
12ie2fq,[ETL Project] Transformation with Python pandas too slow,"I've made a simple ETL project orchestrated by Airflow. The process is basically download >> transform >> load into Postgres.

    with DAG(...):
        task1 = PythonOperator(
            task_id = ""DownloadData"",
            python_callable = download_task
            )
        task2 = PythonOperator(
            task_id = ""TransformData"",
            python_callable = transform_task
            )
        ...
        task1 >> task2 >> task3

First task downloads more than 10k text-based files containing weather data. Its speed is okay (4 minutes w/ multithreading).

The problem is the second task. I used pandas to transform text-based files into clean data in .tsv format.

    def _read_file(filename):
        with open(filename, ""r"", encoding=""UTF-8"") as f:
            for line in f:           
                line_content = [float(i) for i in line.split()]
                yield {
                    ""station_id"": filename,
                    ""date"": f""line_content[0]-line_content[1]-line_content[2]""
                    ""air_temperature"": line_content[4],
                    ...
                }

I use `_read_file()` to read the content of the text-based file and make it a pandas dataframe to transform it.

[raw text-based file \(weather hourly data\)](https://preview.redd.it/vritpkald8ta1.png?width=867&format=png&auto=webp&s=207c8d5cdadc507c51a3aca65778a68f6cb1846b)

&#x200B;

    def transform(filename):        
        # Create a dataframe out of file data
        file_data = _read_file(filename)
        df = pd.DataFrame(file_data)
    
        # Get the summarization of data (min, mean, max)
        df = df.groupby(['station_id', 'date']).agg(
            air_temperature_avg = ('air_temperature', 'mean'),
            air_temperature_min = ('air_temperature', 'min'),
            air_temperature_max = ('air_temperature', 'max'),
            ...
        )
    
        df.to_csv(f""{filename}.tsv"", sep = ""\t"")

&#x200B;

[transformed data \(daily summary: mean, min, and max\)](https://preview.redd.it/06xqm23od8ta1.png?width=890&format=png&auto=webp&s=db601f8c6bb33262d40a094f578698e00cee7ab6)

Then i use the transform function in transform\_task to summarize 10k+ raw text-based files one by one

    def transform_task():
        for filename in glob.glob(""raw_directory/*""):         
            transform(filename)

&#x200B;

Note: I'm doing this locally in my laptop, not in cloud

Transformation of each file is quick, about 0.15-0.20 seconds each. However, there are more than 10k files to transform so it takes about 40 minutes to accomplish the task. What do you think can be done to make this process faster?  Thanks in advance!",2023-04-11 09:15:09
11imzu0,Do you guys like the idea of having Mentors at Job?,"Hey, fellow data engineers!

I don't know if it just me or most of us have the same thought on mentors. I really feel like it is beneficial for all of us to have someone to help with answers or make us aware of the career trajectory.I recently got promoted to Senior Data Engineer at a large personal systems company. My Manager wants me to take up the Lead role as two of our leads had to leave the team.  It is exciting to take up new responsibilities but on the other hand it is overwhelming to even think about me making decisions without seeking for some approvals from now on.I requested my Manager to help me find a Mentor who is a Lead/Architect who could guide me to learn things much faster and also understand their way of approach. Although he liked the idea, he wasn't positive on pairing me up with someone as he thought that it would take up extra time from others.

My question is to all of you guys here is, Do you have Mentor Program at your company? How do you actually manage taking up new responsibilities that are being assigned without some familiarity? 

And also, Architects and Leads, I am seeking for a Mentor with whom I can discuss Once every two months on unique perspectives on the Advancing Data Engineering Technologies and also help me nurture to go beyond my current level. Please drop a comment and I can DM you.",2023-03-05 04:05:26
19d6f86,First time connecting to an API,"Backstory - First let me say I’m a newish BI analyst and in a department of me. We are starting to develop a data culture but I need to show value. I’m using PBI and made several reports from on prem databases. 

My goal is to connect to an API and put the acquired data into a MS Access database. The API returns the data in JSON format. 

I’ve written a very simple Python program that pulls the data from the API on a daily basis and places it in a JSON file to be consumed by PBI. 

I’m not a data engineer by any stretch of the imagination but I figured this would be the right sub to ask this question. 

Is it possible for Python to pull the data from the API and insert into an Access database while also performing some ETL? Am I in way over my head if I’m very new to Python?",2024-01-22 21:08:49
18lcjqp,Python for Data Engineering VSCode Extensions,"I am in charge of, or rather I volunteered to create the documentation that will be the foundation for the development of data pipelines and ETL processes using Python. Part of that documentation includes establishing the configuration/environment used for development. (We currently use SSIS and SQL Server for our pipelines)

I will be using Visual Studio Code and Python 3.

What extensions would you recommend that would help with Data Engineering with Python?

In addition, what extensions would help with other necessary steps in development, such as Version Control, syntax (PEP8), and Testing?",2023-12-18 16:17:31
18ca2sc,Forecasts for 2024 Job Market,I expect job openings to grow by mid-year and salaries to stagnate. What do you think?,2023-12-06 18:08:07
188eyqq,"What are the most valuable data engineering roles ? Ie if ai is a gold rush and data engineering are making shovels , how do we make sure we are making shovels used for gold minding and not just dirt moving ?","Lmk if that analogy makes sense. 

Also would be great to hear what tech is best used and also how to pivot , ensure to grow my career in that direction. 

Been in data for awhile but want to stay relevant (understand tho there are many relevant use cases for data that are valuable and varying levels of sexiness). 

Also, asking about data engineering roles not like some other tech role , ie people that focus on data problems ",2023-12-01 15:43:34
187mtw7,How does a Data Engineer prepare for FAANG technical rounds? Specifically Python?,"Hi r/dataengineering peers,

I have an Amazon technical interview coming up within a week, and I could use some advice on how to effectively prepare over the next week. The interview will focus on SQL and Python.

With SQL, I feel confident handling medium and hard-level questions. However, I'm a bit unsure about my Python preparation, especially when it comes to Data Engineering interviews.

My main dilemma is **whether to focus on DSA topics or concentrate on specific Python libraries relevant to data engineering, like PySpark, Pandas, and NumPy.**

Given the limited time, what would be the most effective way to prepare? Should I prioritize DSA or dive deeper into the specific Python libraries? 

&#x200B;

Any insights or suggestions would be greatly appreciated!",2023-11-30 16:38:35
1817rwr,what roles are close to data engineering?,"I lurked in the sub and saw posts talking about transitioning from data engineering to other roles like backend , dataops , data analysis, BI  , cloud engineering and devops 


I think there are no roles 100% data engineering and the fact i didn't find alot of jobs or interns so it's best to be more rounded? 

So what skills are mostly common between them and data engineering. i know it depends on the company and the role but what you guys think?",2023-11-22 12:15:02
17urs2s,Which role in IT has good salary and less compitition?,"I am confused 😕 in choosing the right role in IT. I did my bachelor in electrical 1.5 years back but want to switch to IT. I did nothing in between tried doing business but failed miserably. Preparing for MS now. But confused.
I know basic of SQL and Python and that's it.

First i wanted to go into data science but I found out that entry level job is hard to get there. 
Other options for me is either data engineer,cloud engineer or software developer.

I am looking For a role that has high salary and less competition, unlike data science.",2023-11-14 02:18:08
17tnjco,QUESTION: Does your company use (or plan to use) LLMs in production?,"I am just curious if the hype is real or not. Are companies actually using LLMs for real production use cases, or is it still an exploratory topic?

If you are using LLMs in your company, are you using OpenAI or open sourced models?",2023-11-12 16:17:31
17dh3xg,Does Databricks Spark offer any performance benefits compared to the fully open source Spark or managed Spark?,"Assuming same number and type of machines for driver and executors, with same configurations, running on same Spark version running on the same code, read/write API from same source, etc. --

Is Databricks optimized in some way to run ""faster"" or more optimally than fully self-managed Spark or services like EMR?

Team recently got pitched by Databricks and the platform and ease of use is really appealing. Things like DLT and ease of running structured streams was cool. But we got a lot of confusing, overly complex responses when we asked questions of this sort, leaving me really confused.

Like for example, when we asked this just casually, we got an additional 30 minutes of pitching on the Delta Lake layer and things like Z-order sorting, but is Databricks Z-order engineered  to be more efficient?

The only real part of the answer on our question on this that made sense was the Photon Engine, but we were also heavily pitched that Photon isn't really worth the price most the time by Snowflake so not quite sure.

We will be setting up a demo account for a PoC soon, but was wondering if I could get some casual insights.

Also adding -- Databricks is very new in my part of the world, so I think most the current Databricks sales people here are just as lost as we are lol. ",2023-10-22 00:46:43
1725sqt,Is learning this stack worth it?,"Hi All,  

I have recently received a scholarship for a 14-week BootCamp course designed for Data Engineering and I was wondering if you think the stack learned in the course would be enough for an entry data engineering role paired with a standard computer science degree?  

Stack:

* **AWS,** 
* **Apache Spark,** 
* **Kafka,** 
* **Apache Airflow,** 
* **Prometheus,** 
* **Grafana,** 
* **Docker,** 
* **Github,** 
* **Python,** 
* **SQL.**  

Would you guys recommend any other technologies on top of these?  

Thanks in advance.",2023-10-07 12:57:42
170us8m,Data engineer to data architect. What do I need to learn?,"I was a director of data engineering after a datascience background and was offered a job a sr architect 

As a director of data engineering I just pulled data from aws into databricks and built delta pipelines that fed tableau work books

I’m sure I have knowledge gaps. What should I read up on ?",2023-10-05 22:01:21
16tv2io,"Jr. Data Engineer who is in over his head, looking for advice from more senior members.","Good afternoon readers,

Want to keep this to the point as this is a longer post so I am going to jump right in to my situation. I recently signed at a large consulting firm as a Jr. DE (perfect for me, very exiting) but my current scope of responsibility seems way over my head, maybe I had a mistach in expectations I am unsure.

To get directly to the ask, our team has a Tableau enviroment that various users have eyes on. The data sources are populated from various parts of the Navy (my client). Currently, roughly 40 different excel files are used to refresh data sources and it is a completely manual process. In other words, user goes to website, user downloads more recent data, user transforms said data manually, user refreshes data source on our Tableau server. This process is repeated more or less from varoius stakeholders and many web portals.

So far, to me everything makes sense, the ask is to build a database and streamline the data management process (build data pipeline). I would say I am strong but no expert in SQL but so far within my means. Two major problems exists.

**Problem 1: Moving Data from SQL to Tableau Server**

Since this work is done in a secure space, Tableau does not support the functionality of connecting directly to our database (a pSQL enviroment). I have gotten Tableau techs on the phone and asked them directly, a direct connection between these two services simply cannot be done in a secure enviroment. I have also had discussions with our cloud dev team and managers and asked them how to streamline this data, they are not helpful.  More recently, I have been poking around with the Tableau Rest API. However the only data sources it can ingest are tableau files (.tds, .tde, .pbix). So I am unsure if it is possible to export data from SQL and manage that conversion.

So I am essentially at a stand still, I do not see a way to move data from my pSQL enviroment to Tableau server. This essentially makes the ask impossible, but maybe I am just young and naive. My intuition tells me surely there is a way to integrate these two programs, but I see none. Thoughts here are greatly appreicated.

**Problem 2: Data Ingestion**

This is a much smaller problem for me and advise on problem 1 is more impactful, but I figure since I am posting to put this out there. I am unsure how to ingest data to a SQL enviroment. Our cloud leads tell me a transition to Azure pSQL is happening soon, so I am guessing there is some type of UI tool that I can instruct users to 'drop' excel files that I can write code against to transform/write to our SQL enviroment. But this is a guess and ideas on data ingestion would be great.

I have previously tried tried to ask the owners of the raw data sources that our users are pulling data from to see if they have an API or if I can integrate with their backend, I am just left on read.

**Carreer**

I just do not know what to tell my manager, he wants updates and everything I explore to solve these problems leads nowhere. It feels like I am on the wrong project and that not much more optimization can be done. But I want to leave it to the experts that I am sure exists on this sub.

Am I perhaps in over my head as far as problem solving/skill level? I was hoping my first gig I would have an architecture already in place that I could develop/expand on, but perhaps that expecation is wrong.

Should I tell my managers I do not love this contract? This feels bad as a Jr. I want to prove myself, not fun away from the first gig they trusted me with.

&#x200B;

Much love to any and all readers and I appreicate your thoughts in advance.

Signed,A Jr. DE in need

&#x200B;

Edit: I appreicate all who engaged below.",2023-09-27 20:27:33
16qezis,Advice for a new Junior Data Engineer,"I'm about to start a new role as a Junior DE and I'm beyond excited. Previously, I worked as a DE Intern at a small start-up but will work with a much larger team and well-established company. I want to make sure I hit the ground running and would love to get your insights on what I should focus on before my first day.

### Software/Data Engineering Principles

I've been brushing up on my Python and SQL skills but I've been curious about the theoretical side of engineering, like the SOLID principles. Are there any similar or additional principles I should look into that can help me feel more confident as a DE?

### Data Modeling & ETL

While I'm comfortable with the basics, what should I understand about data modelling and ETL processes to excel in my role?

### Best Practices

As someone new to the field, I want to make sure I adopt best practices early on. What are some do's and don'ts in data engineering that you wish you had known when you started?

### Note-taking Methods

As I'll be learning a lot, I want to be able to retain as much information as possible. Do you have any note-taking methods that you swear by for technical work?

### Desk Setup/Accessories

Since I'll be spending a considerable amount of time at my desk, are there any must-have desk accessories or setups that have made your work more efficient or comfortable?

### Communication

One aspect I'm keen on improving is my communication within a DE team (Did feel like a headless chicken sometimes in my previous role). What tips can you share about effectively communicating in a data engineering environment? How do you convey technical complexities to non-technical stakeholders?

I'm eager to hear your thoughts and advice. This is a big step in my career, and I want to be as prepared as possible. Thank you in advance for your valuable insights!",2023-09-23 20:53:50
16phdqw,Coding standards and code quality,"I was working as a software developer for a mid size company for around 3 years as a PHP developer. The tech stack was pretty outdated and all I did for three years was code patches to an existing code base. Did not do a lot of new development and didn’t learn much. 
There was an opening for a Data Engineer in my company which I applied for. I have past BI experience, which helped me get this position. 
I’ve been at the Data Engineering position for around an year now. And my team lead had been constantly giving me negative feedback on my code quality and coding standards. I’ve tried to improve but she still seems unsatisfied with the work I’ve been doing. I would like to know what are the different ways that I can improve my code quality. Are there any websites I can use to learn to get better at programming? Learning better coding standards and improving my overall code quality? Mentorship or peer review learning options would also be helpful.",2023-09-22 18:18:12
16hlca1,Data infrastructure from scratch,"I am tasked with designing the data infrastructure for my company from scratch in Azure. They have data from various ERP systems (mostly Microsoft Dynamics). They want to create reports using Power BI. 

Previously I worked as a BI developer / data analyst, so I'm in a bit over my head. I'm currently going through the DP-203 certification material. Any other advice or interesting learning resources I can look at to better handle this project?",2023-09-13 12:09:57
165iuv3,Any examples of good documentation for typical data engineering work?,I've been a DE for about 2 years for a startup and I've enjoyed my time but documentation is all over the map. Just wanted to see documentation out side my environment to model my behaviors after,2023-08-30 16:06:35
15i5dul,Moving on to a managerial position,"So I have been working as a senior engineer for a while now.  Quite comfortable with streaming, batch processing, python, SQL, Snowflake, DE concepts etc. I have been doing well at my current place so they have offered me a managerial role. Now I really liked getting my hands dirty but I have decided to take it up.

So wanted to know from folks who made the transition. What were the pitfalls? And any advice in general would greatly help! ",2023-08-04 16:54:52
157lwnh,Scope creep in DE?,"I'm currently in the market for a DE position, couldn't sleep last night and was reflecting on interviews I've had so far. In 4+ cases, the hiring manager has specifically asked how I  would use data to increase sales. I completely understand that this role requires being a liaison to every department but how far does/should that go?

This isn't verbatim, but one conversation went like this:

Hiring Manager (HM): ""Say for instance, we had a new director of marketing. How would you build a dashboard to help their department?""

Me: ""I would meet with the director to understand what metrics are most important for their team.""

HM: ""What if the director didn't know what metrics they should be looking at.""

Me: ""Then I would ask what goals the department has and work backwards from there. Additionally,  I would prepare some visualizations for data exploration that we could review together. 

HM: ""What if the director didn't have any goals?""

At this point, I was thinking I wouldn't want to work at a company without any leadership from someone in a director role. I think I understand what he was getting at, but it was still awkward. Is this equivalent to asking a prospective hire in sales this:

HR: ""How would you design a data pipeline that incorporates salesforce and hubspot data?""

Sales Hire (SH): ""I would work with the analytics engineering team, outlining what I need the data for.""

HR: ""What if the analytics engineering team didn't know how to design a pipeline?""

SH: ...

Another example of a question I got verbatim for a DE position:

HR: ""How should we increase air conditioner sales?""

Me (Never having sold an air conditioner): ""I would assume that air-conditioner sales are driven by new home construction. I would identify areas that are experiencing growth/new development and focus marketing there.""

Maybe it's just this market, but I feel like I keep flubbing these questions because I don't have a degree in marketing. ",2023-07-23 18:15:21
157fm9c,How you switched from data engineering to software Backend development?,"As the title, has anyone made the switch from data engineering to software dev (backend)? How was the transition (outside of internal switch)? What steps did you take to achieve this (new programming language? side projects?  Leetcode?). Any pointers on where to find side projects to collaborate with experienced backend engineers? How has this switch affected progression in your career? What level did you apply for (junior? Mid?) How’s the learning curve on starting the first SWE job?

Background: I am a mid level data engineer who enjoys solving problems writing code but have found that DE domain offers less opportunities for me to improve my coding skills. I primarily use Python, SQL and spark in my day to day but feel like I could do more with Python, SQL and other languages. In my spare time, I practise by taking on leetcode challenges and reading on backend architecture and design and find it very interesting. I have also created some personal fullstack projects in the past and looking to learn better by collaborating with more experienced backend engineers on projects before applying for SWE roles.

Has anyone had this experience and how did they successfully make the switch? Any recommendations will be highly appreciated.",2023-07-23 14:01:45
14xkfjd,Joined a new company as a DE for a big project and I'm feeling a bit underwhelmed - Would like to hear a second opinion.,"Hi everyone - for context I'm a DE with \~3 YoE that I spent working in a small startup where I was, *for the most part*, the sole DE.  
I was responsible for not only implementing an end-to-end data pipeline that handles data from ingestion to analytics dashboards but also built the websockets and APIs needed to interact with different parts of the pipeline - which I really liked.

Around a month ago I got the opportunity to join a big company as a DE, both the job listing and the interview had big emphasis on experience with end-to-end data pipelines and good SWE practices.

I figured this would be a great opportunity to work with more experienced engineers on a project with significantly more data than what I'm used to.

The data stack is: Airflow - dbt (core) - AWS S3 (data lake) - AWS Redshift (dw) - Quicksight.

Most of the tasks I pick up involve building dbt models, solving merge conflicts, configuring Airflow DAGs or sometimes assisting the analytics team.

The good thing is that the company has a great data culture and big emphasis on CI/CD and good SWE practices - they also insist on having DEs handle the building of dbt models and query optimization which is a good thing imo.

My major concern however is that the role doesn't feel as technically challenging as I was hoping for it to be, so do you guys think this is a good position to progress my career long term *from a technical standpoint* or should I keep an eye on other opportunities ?",2023-07-12 10:18:20
14g58uf,How much Airflow do I learn as a student?,"So far I've made one airflow project: Airflow running on an EC2 instance pulls data from an API, transforms it and loads it to AWS S3. 

I am a CSE student learning DE, how much Airflow should be sufficient to be able to impress a recruiter or maybe a potential mentor? 

I already have some base built on the basics of DE and want to spend the next 1-2 weeks learning airflow. What projects should I make?",2023-06-22 14:36:44
146uka5,Manager with or without technical skills? What’s your preference?,"The manager of the data team where I work recently decided to switch to a different department so that he could be more hands on, they wanted him to be more managerial and it was too much for him. To replace him they found someone who has a history of managing teams in the tech space but has no technical experience herself and wouldn’t be able to do things like code reviews before deployment. 
On one hand, we loved him because he had the same background and he knew our capabilities so he took a hard line with the business if something was unreasonable. On the other hand, now we have someone purely dedicated to management who I feel will represent us well to the business even without the technical knowledge but won’t be able to answer the technical questions or take care of certain tasks. 
What’s everyone’s preferences with these types of scenarios? Is one inherently better than the other?",2023-06-11 13:55:44
146ifov,🤔❓How do you use SQL in daily tasks as a Data Engineer?,"After reading some recommendations on [How do you guys ace your SQL skills? : dataengineering (reddit.com)](https://www.reddit.com/r/dataengineering/comments/vj10xz/how_do_you_guys_ace_your_sql_skills/) .

I started reading the book T-SQL: Fundamentals. It’s organized and packed it with lots of topics, such as joins, table expressions, window functions and so on.

But I haven’t known exactly how will I apply those things in my upcoming Data Engineer position.

I often ask myself:

* What the point of learning CTEs?
* Do I even need to master all kinds of joins as a Data Engineer?
* How will I use window functions or stored procedures in my job?
* …

I had minimal working experience, so here are my guesses:

* Converting business rules, e.g. use mostly SELECT queries to calculate the total expense for each customer?
* Building ETL pipeline, e.g.
   * Extract: use SELECT to move data from Source tables to Staging Tables to store the query result temporarily.
   * Transfrom + Load: use SELECT combined with aggregate/windows functions to transform and load (using MERGE? I guess) to the destination tables.
* Probably implementing SCD type 1, 2

And you see, my guesses are mostly related to merely using SELECT queries and not advanced SQL concepts.

>So my question is: ***How do you use SQL in daily tasks as a Data Engineer?***

I did paste the entire question to chatGPT but I really want to know the actual use of SQL in real life and that's something only experienced Data Engineers out there can answer.

Thank you for reading. I hope you guys have a good day!",2023-06-11 02:43:42
139qju4,Gathered courage and created a Trial GCP Account,"Hey Redditors, I have gathered the courage and have entered payment details to get a free GCP Account 300$ worth of free credits for the next 90 days. I plan on learning the following asap -

1. Docker - can learn via Cloud Shell
2. Kubernetes - can learn via Cloud Shell
3. Apache Airflow - how do I learn this in free GCP? Will I have to spin up a new VM via Compute Engine?

Moreover, is the plan sufficient enough to learn these?",2023-05-06 14:15:58
134hjyy,Does anyone use a no-code data transformation tool?,"Hey everyone,

I made a post on Friday asking about DBT and got loads of insightful responses so thanks for everyone who replied to me on there.

I was wondering if anyone has used/or are using a no-code data transformation tool?

The two I am looking at currently are:

[https://www.osmos.io/](https://www.osmos.io/)

[https://www.boltic.io/](https://www.boltic.io/)

They are both pretty expensive, but seem to offer similar features to DBT cloud (scheduling, a simple interface, etc) albeit without the flexibility of DBT modelling.

Any info would be extremely helpful - thanks!",2023-05-01 09:24:46
1318f7s,Why pay for DBT cloud when Fivetran has built in DBT Core?,"Hey everyone, sorry if this is a stupid question. But I'd love to know why companies would pay for both DBT cloud and Fivetran now that Fivetran has built in DBT core with scheduling?

It seems like there's an ever growing number of data tools, each with their own expensive price tag. So I am trying to understand which tools are necessary and which aren't. Thanks!",2023-04-27 21:49:39
12zfj4u,What exists on the spectrum between a cron job and airflow?,"
I'm the sole 'data' person at a small company where my role spans a bucket load of titles, I know there are many in a similar position out there... As such any chance I can get to offload any work/mental capacity to a managed/streamlined platform I'm usually all for it.

The company has a bunch of legacy/niche industry software that require custom pipelines to be put together. Now I'm sure something like airflow would fill the shoes and there some, but I just can't justify the management nor the costs of a managed service for this.

Surely there has to be something between, like scheduled lambdas with monitoring/alerting/validation rolled in?

I've so far in my spare time cobbled together a series of scripts and a rudimentary frontend to streamline and achieve the above, but was hoping for something pre-existing I could drop in...


Really to sum it up, my wishlist is basically bring code, aka serverless function style but have the monitoring ease of something like airflow or prefect.

Thanks for any comments or thoughts",2023-04-26 12:09:03
12z1s36,Who sets the pipeline/warehouse strategy and roadmap in your organization?,Essentially title. Is it left up to the data architects? Principal/staff engineers?,2023-04-26 00:59:03
12r38jq,Other Subs?,"Looking for something thought provoking/educational...lately this sub is filled with either a) posts from people trying to break into DE, or b) people who have googled/heard something about DE and want to sound relevant. 

Other than Software/Tool specific subs, what other subreddits are you all subscribed to that keep you interested?",2023-04-18 20:29:47
12qi81h,Hot Takes on the Modern Data Stack,N/A,2023-04-18 10:07:14
11lm6v8,How We Deploy 5X Faster with Warm Docker Containers,N/A,2023-03-08 04:27:13
11kpw2w,Tesla data engineer,Hey everyone! Anyone recently cleared a Tesla data engineer 30 min Python coding interview? Would love some pointers as to what to prepare. Would love some advice please. Thanks a lot!,2023-03-07 05:06:35
19cld0z,My Thoughts on EcZachly/Zach Wilson's Bootcamp V.3,"I was already hovering over [Zach's Bootcamp](https://dataexpert.io/) but was a bit insecure since the price was huge and a few not many positive comments on two posts from this subreddit [here](https://www.reddit.com/r/dataengineering/comments/14ieh8u/any_feedback_on_zach_wilsons_data_engineering/) and [here](https://www.reddit.com/r/dataengineering/comments/12otxy6/interview_prep_anyone_in_zach_wilsons_data/). So I have seen the posting about a PPP discount based on the country that you live in, since Brazil's economy is kinda crap, I decided to try, if I got I would buy, otherwise maybe think a bit more and try on another opportunity. To my surprise, I was selected! Now I will give you guys my feedback for all the weeks since I got the both tracks course.

It's important to notice that I have been a Data Engineer for almost 2 years, but never worked on big tech, FAANG, etc. meh experiences, not complete garbage but nothing mind-blowing, know a bit of Scala, worked with Airflow, PySpark, Cloud, Pandas... The classic stuff.

TLDR: Was worth it? Yes. Further, I will point out a few things that made it worth it. Just the knowledge may not be worth it for you.

Week 1 - Dimensional Data Modeling

This first week, I believe Zach was extremely motivated to teach, the classes were insightful and focused on data engineering basis, there I fixated on the differences between OLTP and OLAP, and also learned about the existence of Master Data.

There he points out a lot about **how** you are delivering your data, and the importance of noticing that for each kind of consumer, you may want to prepare the data in different ways.

Learned about additivity on the dimensions, a term that I had never heard before the boot camp, and also about SCD tables, I don't know why, but never heard about this one before too.

Week 2 - Fact Data Modeling

This second week Zach was also extremely motivated, I believe these two topics are his favorite, not that he wasn't motivated on the others, but the difference between Week 1 and 2, and the rest was clear. There I also fixated on the difference between a fact and a dimension.

During this week Zach taught about techniques for fact tables deduplication, and ways to aggregate fact data into lists or binaries format to get fast analytics.

It's good to point out that Zach brings a lot of his experience to FAANG-like companies, so some cases will not apply to you probably, but it is nice to know how happens there, this extends to the whole boot camp.

Week 3 - Analytics Track - Analytical Patterns

Here Zach taught about what kind of patterns to aggregate data would suit better for each type of requirement, for example, what to use when we are looking for root causes, what to use when looking for rankings, etc.

One insightful class from this week was related to the data engineering interview process (usually on big techs), he told me about what to expect in terms of technical tests, what to pay attention to during the coding interview, tips and tricks for window functions, and there I learned also a new thing that never seen before GROUPING SETS, GROUP BY CUBE and GROUP BY ROLLUP.

Week 3 - Infrastructure Track - Flink Streaming

I hated this week, not by Zach's fault, but I didn't like streaming, I think it was good knowledge, but certainly not enough time for someone who has never seen that before. I believe that for people like me that never used or seen Flink before, I was only able to digest and understand the theoretical part, like Kappa and Lambda architecture, or the concepts of micro-batch and near real-time, etc.

During the labs, we used Flink with Kafka, I have never used both of them, but tbh, I was warned, he says on the requirements sections that for infrastructure track: ""Basic understanding of Docker, Flink, and Kafka."" So if you want to do the boot camp, try to look just a bit to understand, it will make your life easier.

I discovered that maybe I don't want to work on Uber lol

Week 4 - Analytics Track - KPIs and Experimentation

This week Zach taught about leading and lagging metrics, another concept that I have never heard before, and also [Timothy Chan](https://www.linkedin.com/in/trchan/) taught about A/B tests, experimentation, etc. Tim is a nice guy, but the content for me, was boring.

Week 4 - Infrastructure Track - Spark Batch

Here was one of the most awaited weeks, here Zach covered topics from the basics of Spark theory, so what is a plan, driver, and executor, to JOIN optimizations and tuning. We have seen differences from the caching and broadcasting, as well as Notebooks x Spark Submit. It was nice but maybe expecting something different.

Week 5 - Analytics Track - Data Quality

Here I can summarize that it was related to the importance of trust in data, and what kind of data quality checks we can use for different cases and each type of table. I used my notion annotations from this class as a cheat sheet to check if I am not missing any type of QA check. Interesting to point out to you guys that he mentioned an Airbnb framework called MIDAS, google it when you have time.

The second class was presented by a Brazilian fellow that is specialized in dbt, it was interesting, of course, have heard about dbt but never had the opportunity to try it.

Also here we learned about data design document building, and I liked it.

Week 5 - Infrastructure Track - Also Data Quality

This week wasn't anything mind-blowing, but was important, here we discussed about differences between SE testing and DE testing, why they have higher quality standards, why most organizations miss the mark

In the second part of this week, the Airflow God [Marc Lamberti](https://www.linkedin.com/in/marclamberti/) caught the reins and gave us a presentation on the theory of data contracts, best practices on data validation, and ways to enhance the data quality, followed by the technical part using Airflow.

Week 6 - Analytics Track - Visual Impact

Here we had a class where the knowledge there was insightful but not useful for me yet, he discussed challenges and what separates the senior data engineer from the staff data engineer, as a few career insights more related to professionals in higher places of the hierarchy, so not absorbed much in my POV, since I am still kinda a minion.

The theory behind Dataviz was taught here, it would be like maybe the Week 3 classes being used in real life, very insightful, for those who are looking for analytics engineering, this week is a must.

Week 6 - Infrastructure Track - Pipeline Maintenance

This one was maybe even harder to digest than the Flink one for a reason, I never had to schedule maintenance on pipelines, reduce costs, or optimize computing on pipelines yet. This kind of stuff is out of my decision power, so great content, but not applicable to me. He taught about the impact of ownership on projects, the significance of domain knowledge, and effective communication. Another example that he talks about is related to tech debt and data migration, so yeah, I have never had to deal with that, so kind of abstract for me.

I have to point out a few things about this boot camp:

1. I thought the weekly homework would be easy peasy, Udemy quiz-like. I couldn't be more wrong. They are hard and require a lot of time. If you don't mind about the certification and the mentorship program, you don't need to worry about that.
2. Zach has a discord community for those who are in his boot camp, there you can chat with your peers, Zach, and people from other boot camps, it's nice and helps keep the engagement.
3. With the boot camp, you gain access to past classes and talks from people who have been there, so you can watch for example the [Joe Reis](https://www.linkedin.com/in/josephreis/) talk that happened during V3 boot camp.
4. Weekly is a Career Development Q&A with [Sarah Floris](https://www.linkedin.com/in/sarah-floris/), we can ask questions, tips for LinkedIn, etc.
5. For those who do the homework on time, we have access to a weekly coffee chat with Zach, where we can ask questions for him. Extremely worth it, that was what motivated me the most for doing homework, I could participate in all of them, and it was nice to be on the last one, because the first one had like 80 people, and the last maybe 8, so only the warriors were there.
6. Access to other classes like LLM-related or 30-minute classes to prepare for technical interviews, like data architecture, data modeling, SQL, and DSA.
7. In the end, we have a capstone project that we developed by ourselves with a few requirements, fetching all the knowledge, it is a good idea, but this one was too much for me, the due date is on Jan 26th, but I will not be able to finish it, marriage ceremony preparation, masters and other life things are draining too much time for me to dive into that, but I would recommend doing that.

With those points above I feel that was worth it, it was intense, but I feel grateful for the knowledge. As I said before if you are already a data engineer master, that is the data modeling king, and all the topics that I mentioned you are comfortable with, or at least with most of them, maybe it will not be worth it for you, this boot camp is more suited for someone that already know something, but still need to climb the ladder, so maybe an end junior\~end mid-level range.

For the V.4 boot camp, Zach removed from the curricula the pipeline maintenance and dataviz week, but it will be available from my cohort and will be adding a dbt week and an end-to-end Machine Learning week though, to be honest, I am not a big fan of ML and didn't fall in love with dbt, so I would prefer doing my version lol, but I am sure that it will be cool too.

I am sure that on many points Zach is improving the UX of his boot camp, so things that were bad from the V.2 were better on V.3 and the V.4 will be better than mine. I conclude with if you can, do it, but be prepared to dedicate 6 weeks to that, just watching the recorded classes is a waste of an opportunity.

If you guys have any other questions about the boot camp I am glad to answer them, I know that it is not cheap and you may feel insecure, you can ask here or reach me on DM.",2024-01-22 02:51:26
193dqrc,Are DE’s less prone to first wave layoffs?,"I’ve been at companies before in a DS role and observed round after round analysts and DS impacted. But sole DE responsible for critical infra related to billing etc was the last holdout. This, in addition to fun of learning something new, inspired me to transition to DE years ago. I can see an impending storm this year based off many corporate red flags and hints.

I’m wondering if I should go to greener pastures like city or utility type DE work. I have made myself the sole DE responsible for critical billing calculations and infra at current company but my comfort level when layoffs start isn’t very high and I maybe have 4-5 months of savings not the 1 year that is vaunted. 

The last thing I want is to be impacted and forced to get a job that I’m not excited about or find out the wait to get an offer is 9+ months for an experienced DE",2024-01-10 17:28:21
18jnsql,Functional DE: how to deal with changing facts,"I've found a lot of inspiration from Maxime Beauchemin's [article](https://maximebeauchemin.medium.com/functional-data-engineering-a-modern-paradigm-for-batch-data-processing-2327ec32c42a) on Functional Data Engineering.The concepts of immutable partitions and dimension snapshots, in particular, have caught my attention. However, I'm trying to figure out how to handle updates in fact events, which may change over their lifecycle.

Take, for example, a payment transaction. It might be AUTHORISED on DAY 1, then SETTLED on DAY 2, and possibly REFUNDED on DAY 3.According to the principles laid out in the article, I would avoid applying UPDATE operations. But how should I manage these kinds of status changes? Taking a daily snapshot of all the facts is unbearable. And if I decide to update the rows I lose most of the benefits tracked down in the article.And if the creation date is still the same for all the statuses, appending three rows would break the idea of single unit of work, that is, every job is responsible for inserting overwriting only one partition. (the table would be ideally partitioned by creation date)

Thank you :)",2023-12-16 09:37:56
18cxamc,Living Book on Data Engineering Design Patterns - Feedback Welcome,"Hi there

I've embarked on a unique trajectory to write a book in the open. The book lives on [dedp.online](https://www.dedp.online/), you can also find it on [dataengineeringdesignpatterns.com](https://www.dataengineeringdesignpatterns.com), but I like shorter :).

To be clear, this is just the beginning. But as I'd like to make it a great book, I'd like to strive for feedback early on, so I decided to share it with you. The latest changes are documented in the changelog.

The online book is entirely free to read.

## What does the book contain as of now?

You might wonder if it's worth delving into the book in its current, unfinished form. Well, it depends.There's already a wealth of content. However, the much-anticipated design patterns are still in the works. Suppose that's what you're most excited about. In that case, you'll find the beginnings of exploring the patterns of `Caching` and `Ad-hoc Querying` in the first Convergent Evolution chapter, covering ""BI vs. Semantic Layer vs. Modern OLAP vs. Data Virtualization"".

## The Content

As of now, the book offers:

* The outline of the book: What it is all about (to keep you excited 😉)
* Introduction to the Field of Data Engineering with the history and state of DE and challenges along the data engineering lifecycle.
* Introduction to Data Engineering Design Patterns (DEDP) with the critical starting point with Convergent Evolution and what it is.
* An overview of some of the patterns and design patterns that forked from the Convergent Evolutions
* The first is an analysis of four terms that form a Convergent Evolution and their patterns.

## Bonus

In addition, I've enriched my second brain with sixty new terms, creating a valuable resource for anyone in data engineering. Explore this at [Second Brain](https://brain.ssp.sh/).

## Seeking your early feedback

Your critiques, suggestions, and questions are very welcome. This book has just started, but it may spark some thoughts, ideas, and terms you heard repeatedly. This feedback I'd super appreciate featuring it in my book.

I am looking forward to your honest and constructive feedback. Things will change and hopefully improve.

Thanks.",2023-12-07 14:53:55
17rm18a,Iceberg vs Hudi beef over ACID guarantees,"Apache Iceberg co-founder Ryan Blue recently published [a post](https://tabular.io/blog/iceberg-hudi-acid-guarantees/) in which he made a case that ""Iceberg is reliable and Apache Hudi is not"". 

Vinoth Chandar responded with his [own take](https://www.onehouse.ai/blog/on-iceberg-and-hudi-acid-guarantees) bringing receipts and demonstrated Iceberg's own set of problems.

There are a lot of technicalities in both posts, but the biggest focus is on ACID guarantees in both systems and the difference in approaches to fulfill them.

Personally I think that transaction guarantees aren't as important in modern data lakes (will be happy to see examples of the opposite). 

Who is more convincing in your opinion?  ",2023-11-09 20:20:06
17jh22q,Cracking the Data Modeling Interview,"If you are prepping for the data engineering interview, you should include the data modeling loop as a key section of your prep. Here is a two part series that includes an overview of the signal that they are looking for, practice problems, and a deeb dive into some of the technical aspects you will need to know.  


[https://medium.com/@seancoyne/cracking-the-data-modeling-interview-part-1-an-overview-b09e7d5a7938](https://medium.com/@seancoyne/cracking-the-data-modeling-interview-part-1-an-overview-b09e7d5a7938)  


[https://medium.com/@seancoyne/cracking-the-data-modeling-interview-part-2-normalization-indexes-and-partitioning-fac334d767ca](https://medium.com/@seancoyne/cracking-the-data-modeling-interview-part-2-normalization-indexes-and-partitioning-fac334d767ca)",2023-10-30 00:34:02
17gqx0w,Is there a Leetcode for Pyspark to practice coding tests ?,Is there a website like leetcode to practice pyspark ? Or is there any website with good amount of question and solutions related to pyspark ?,2023-10-26 07:23:28
16x6mf6,"Any DEs here have homelabs to upskill , make a dedicated dev env, and do other server side stuff ?","Planning to buy a Nuc 11 Enthusiast instead of a Steam Deck to upskill myself as well as not screwing my Gaming PC . Are there any ideas where i can start this journey . My aim to setup homelab is :  

1. Networking side with OpenWrt , pfSense 
2. Containers/Virtualization  with Dockers and Vmware 
3. Server Side with Proxmox

Share if its sufficient to run multiple VMs and Containers with this thing as well as share how do you use homelab for upskilling your DE skills .",2023-10-01 17:08:28
16mmpuh,Are DataBricks cost savings over Snowflake to good to be true?,"We are using Snowflake, and I have recently engaged with DataBricks as they claim they can reduce our warehouse spending. They analysed our Snowflake usage and have claimed they can reduce our Snowflake costs by a factor of 5. I am sceptical about this, and I am hoping some of you can provide some feedback on how realistic this claim is  


Have any of you implemented Databricks in your organisation to optimise costs? 

If so, did you witness a significant reduction in costs, as claimed by Databricks? 

Was it worth the Engineering effort to switch over?",2023-09-19 09:52:57
16lnp6o,Transition to develop event-driven architecture?,"Hello,

I found a job position that I really like and decided to apply. They are looking for a data engineer, but instead of (only) doing ETL moving data from A to B, they also want the DE to work on developing an event-driven microservices architecture.

Do you see it like a career progression or regression?
Personally speaking I really enjoy the idea of being in a DE/SWE hybrid role. It could also be an opportunity to play with beam, stream processing engines, and real time analytics tools like druid or Pinot",2023-09-18 06:21:19
157tf13,What is your unit testing implementation?,"Curious to see what others use to unit test in the DE space. I work primarily on the transformation side, where we’ve picked up DBT. Our stack is GitHub Actions for CI/CD, AWS/Airflow for orchestration, logging, observability, restartability, and a light runtime to create the initial DBT compilation, that’s then pushed down to SnowFlake and utilizes that run time to populate the objects and run data quality checks in the SnowFlake runtime.

I’ve been addressing unit testing in this stack. My team’s skill set more so pertains to SQL developers rather than DE, and even then their SQL is half baked at best. 

Unit testing within DBT is spotty at best, flat out doesn’t exist at worst. I’ve written a Python script that extends DBT to dynamically generate a test to every node, and allows for additional tests to be written and compiled/ran, within the CI/CD pipeline. This enables the team to continue using SQL, and the script will do the rest. If the written or generated test fails, the deployment fails.

Data is loaded to and read from the warehouse. I hoped to abstract from the warehouse, but SnowFlake having a closed source engine wasn’t conducive to that approach. My thought was to ingest the node sql files into duckdb and have an in memory database. Not the case given SnowFlake, but oh well.

What are your stacks and unit testing solutions?",2023-07-23 23:12:45
14m8ze6,pyspark-ai: English SDK for Apache Spark,N/A,2023-06-29 16:08:46
14igqpe,Book recommendations,"Hello fellow Data Engineers. Im looking for some good book recommendations. It can be on a specific topic, technology or more broadly scoped than that. If you ever read a really good book id like to hear about it, can be non-technical too. Id prefer material available in a physical medium. Go!",2023-06-25 08:18:27
14ialnx,Question about using Databricks as an all-in-one solution for everything,"Hi, so I have a question about using Databricks as the one stop shop for everything developer/data engineer/analyst related.

The company I work for currently uses AWS (mostly lambdas/S3) + Snowflake + Postgres (RDS) with a few large Spark jobs running on Databricks, and wants to transition to basically using Databricks for everything.

They want to use Databricks for everything under the sun - ETL orchestration, running ETL on S3 files, invoking lambdas, running ECS tasks, ingesting events, publishing events, SFTPing stuff, sending emails, reading emails, sending Slack alerts, cleaning logs, generating analytics, parsing Excel sheets, monitoring things - you name it, and my company wants it done with Databricks.

Thing is, I know that Databricks can _do_ all of this stuff, since well, it just runs Python and is hooked up to AWS and any other 3rd party service. 

I'm also not trying to say that Databricks isn't good for anything - it seems like we're replacing what we used to do with lambdas and other AWS/Snowflake services and rewriting everything as a notebook or script or Spark job in lieu of anything else.

I'm having trouble articulating why this feels wrong from a technological standpoint (or maybe I'm the one who's wrong and behind the times) and would like to ask for some help understanding if and/or why this seems like a bad idea, or if instead I'm the one who's misunderstanding how this all plays together.

---

I have a mostly SWE cloud background with a slight slant to some DE stuff (like I've built ETL on AWS/Snowflake etc) but I've never really worked on a production ready Databricks envionronment, and doing _everything_ in Databricks feels ... wrong to me (?) if that makes sense. Like, is it normal to move everything over to Databricks and basically treat it like a layer on top of AWS?

I hope this makes sense, thanks for reading!",2023-06-25 02:33:55
13u90j8,"What is your failure story, that others can avoid?","Mention anything - big or small! Looking for technical stories not career issues, but not mind reading them.",2023-05-28 19:35:59
13j4dch,Thoughts About Snowpark?,"To those using Snowpark in addition or as a replacement to pySpark, how has your experience been? 
My organization is looking at Snowpark and Snowflake is pushing hard for us to move away from Databricks.",2023-05-16 12:52:12
13g08hr,Best IDE for data engineering,"Hi all,

Trying to maybe switch to data engineering from data science. I use vs code. Would you guys can recommend me an IDE and some extensions I can use for data engineering.

I apologize if the question is dumb or doesn’t make deme so any input is appreciated.",2023-05-12 23:12:30
13bpeqx,"How Leading Data Organizations Achieve Success: Prioritize People, Process, and Product",N/A,2023-05-08 13:39:08
1393kdj,Why would you ever not use CDC for ELT?,"As the title says... I can't think of a reason why CDC would ever not be the ""gold standard"" for any ELT data integration processes? I can understand that in some scenarios, CDC may not be possible, how would you have a proper EL process without CDC?

The only other patterns I can think of would be:

1. You can schedule a full source database scan during off-hours and simply replace all of the tables in the destination, but this would be far too inefficient. Even worse, for global companies, there aren't really any ""off-hours"" that a job like this could run during. Even more, you would lose any ability to analyze the history of something with a changing state, for example: open orders on July 1st in 2019, or customers who had an address change. I don't see how you would handle SCDs with this full ""flush as replace"" pattern. Lastly, you can forget about low-latency analytics, I think that is obviously a true statement.
2. Something better would be to ingest records that have a ""modified\_at"" column where I could create an extractor to just extract records that have been modified since the last extraction, and then upsert those changes into my destination tables. I think it would be very wishful to think that every table has a column like this to begin with. I could also handle SCD's to flag an updates to a record as ""current"". Also, what if there are multiple state changes between extraction jobs? This pattern would only pick up the most recent state of a record, which could be bad. Finally, I suppose you can have more frequent ""micro-batches"" that issues a SELECT query to all tables every 5 minutes to get the newly modified records, but this seems quite inefficient.
3. The only other option (I can think of) would be CDC. Every data mutation to a source table is considered an event that would create a message in a queue. Subscribers would get notified to persist the mutation into the target location. Every mutation is captured in the order that it occurred, so analysts could then analyze any data at any state at a given point of time.",2023-05-05 22:28:18
136bog1,Semantic layer,"In my organization, we are looking for some tool that will help us easily create semantic layer. We have a lot of traffic tables and dashboard, and each one can calculate the KPI in a different way.
We are planning to create some layer and all the dashboard will take the metrics from this layer.
Because we have a lot of traffic tables, it's become pretty tough because in faxx the revenue will be faxx_revenue and in fayy it will be fayy_revenue. This requires a lot of semantic layers. There is some tool that can help us?",2023-05-03 05:43:39
12pd2hk,Resources for Current DE Interested in Learning Data Science,"I'm currently a DE and interested in building a basic working knowledge of machine learning/data science. I don't plan to make a switch to a DS role but see value in adding a basic working knowledge to my skill set.

What are some good free/paid resources for this?

Thanks!",2023-04-17 11:48:18
121jccb,How are you using DuckDB?,"As per title. How are you using DuckDB, either privately for passion/side projects, hobbies, or professionally in production environments?

Would love to just hear the innovative ways in which people are using it.",2023-03-25 11:43:21
11l0m2w,Did current data engineers start their software journey as data engineer or something else,"Hi I am not sure whether this question has been asked earlier or not. Just needed to know how most of data engineers started their journey

1. Directly joined a data related field, say data analytics or data engineering
2. Or transitioned from some other related backend role, such as server side or something similar
3. Or some other domain

I ask this because I wanted to know whether directly getting into data side without much grounding in other backend related technologies would be good choice or not (maybe it could be learned alongside)

Edit: would like to add a bit about myself, came out of college and employed straight into de role. Do not have much footing in other software domains",2023-03-07 14:20:57
11fyyi5,"Group by 1,2,3,4,5,6,7,8,9,10,11","Hello fellow Data Engineers,

I sometimes find Analytics Engineers in my company create select statements with group by 1,2,3,4,5,6,7,8,9,10,11

I'd like to help my team avoid this bad practice but I feel I don't have strong-enough arguments yet.
Besides being hard to read and maintain, I'm sure there are other drawbacks.

What are the drawbacks of creating such long group bys? And what is the appropriate way to avoid them?

Thanks a lot in advance for your help 🙏",2023-03-02 09:57:40
115akgk,Your opinion on testing in data engineering,"Hi fellow data manglers 

  
I currently work for a large railway company in Europe and provide internal consulting on our data engineering tech stack. Which is Snowflake, Snowpark for Python, Openshift, ADF and Argo Workflows.   
Over the next two years the companies analytical applications should transition from the previous two techstack generations, which are PowerCenter or Hadoop/Spark, to the above-mentioned.   
Due to the extreme shortage of data engineering profiles in the company, our goal is to enable full stack Java developers to create, migrate and maintain the plethora of analytical applications. (Teach a bear to dance, but don't wonder if your head gets bitten off)   
To achieve this, we provide sample applications, pilot migrations and a lot of documentation and tutorials on (best) practice with the formerly described stack. 

  
Since half a year, I maintain a hefty discussion concerning the test concept. Since we're using Python for implementing our pipelines and some developers in our teams are former fullstack developers/application architects, a conflicting position around unit testing arose. According to my view and experience, the classical test pyramid, where unit testing forms the foundation, is not applicable, when implementing data pipelines. Automated measuring of data quality dimensions\[1\] and conducting user acceptance tests with personas who have the required insight, always deemed me much more significant. It's not even true unit testing, because the Snowpark API requires a Snowflake connection in order to generate the SQL statements, so it is violating isolation anyway. 

  
I'd like to have your opinions and experience on testing in data engineering context, maybe I'm a little short-sighted/biased concerning the matter. Thanks!

  
\[1\][https://www.sciencedirect.com/topics/computer-science/data-quality-dimension](https://www.sciencedirect.com/topics/computer-science/data-quality-dimension)",2023-02-18 08:23:26
1aqhsg8,Interview question," To process the 100 Gb of a file what is the bare minimum resources requirement for the spark job?
How many partitions will it create?
What will be number of executors, cores, executor size?",2024-02-14 08:03:57
1al96or,“Lakehouse” - Realtime data,"So we use S3 for ingesting raw source data. We save it in unchanged format from source. Then in the next stage we transform them using python apps and save them as parquet files to S3 again. After that we take parquet files and load them into Postgres as final stage for consumers. 

How do you handle realtime events data in such architecture to make them available to consumers within lets say 1-5s? I think dumping data from Kafka/RabbitMQ to go through the entire pipeline (raw, parquet, postgres) would take longer and saving them directly into postgres is probably not a good idea. Maybe dumping into S3 AND into Postgres? Or having another database for “realtime” events? Or is there any best practice for such case?",2024-02-07 18:00:39
1aepsld,Processing high amount of data with SQL,"Hi! I’m a Jr data engineer which is facing a challenge and I would love to know your opinions and expertise in this topic:

I’m currently handling allots of data in SQL, we receive at a high frequency JSONs with raw data in it (in a single json there could be more than 10k raws) 

The thing is that we need to make some statistics with this JSONS 
We need to concatenate several Jsons and then apply the statistics (calculate outliers, calculate avgs, calculate percentages, stds, frequency, etc…) 

And after calculating it we need to insert it in a new table which handles summarizes data. 
All of this in a SQL stored procedure, the hole process lasts more than 3hours to complete, is there any advice for this kind of stuff, some literature I can read, videos or something to optimize the solution? 
I’m also open to other robust pipelines besides only using SQL!",2024-01-30 14:34:17
196erg9,A Guide to Data Lake Interview Questions,N/A,2024-01-14 12:24:33
18y56lj,How hard is it to switch from pyspark to scala?,"Hi,

I've worked for a while with pyspark, did some courses, read a few books etc. I also did some courses and now feel quite comfortable with OOP in python. I haven't worked with any languages other than python/sql.

Now it is possible that I'll be moved to a project with spark+scala (batch programming, so no streaming there). How hard is it to pick it up for someone with no Java background?

Can you recommend some courses/books? I think of picking up 'Scala for the impatient'. 

If it were up to me, I'd stick with python for now and become more advanced with it, but the market in my location sucks (and so do I with my skills), so I won't have much of a choice.

Thanks

&#x200B;",2024-01-04 05:19:41
18xldbk,One Billion Row Challenge—using SQL and DuckDB 1️⃣🐝🏎️🦆,"u/gunnarmorling [launched a fun challenge](https://www.morling.dev/blog/one-billion-row-challenge/) this week: how fast can you aggregate and summarise a billion rows of data?

I'm not a Java coder (which is what the challenge is set in) but thought it'd be fun to do it in SQL with DuckDB nonetheless.

Loading the CSV in is simple enough:

    CREATE OR REPLACE TABLE measurements AS
            SELECT * FROM READ_CSV('measurements.txt', header=false, columns= {'station_name':'VARCHAR','measurement':'double'}, delim=';') LIMIT 2048;

as are the calculations:

    SELECT station_name, 
               MIN(measurement),
               AVG(measurement),
               MAX(measurement)
        FROM measurements 
        GROUP BY station_name

The funky bit comes in trying to reproduce the specified output format:

    SELECT '{' || 
                ARRAY_TO_STRING(LIST_SORT(LIST(station_name || '=' || CONCAT_WS('/',min_measurement, mean_measurement, max_measurement))),', ') ||
                '}' AS ""1BRC""
        FROM src;

The final script looks like this, and takes about 26 seconds to run:

    ❯ /usr/bin/time -p duckdb -no-stdin -init 1brc.opt2.sql
    -- Loading resources from 1brc.opt2.sql
    
    WITH src AS (SELECT station_name,
                        MIN(measurement) AS min_measurement,
                        CAST(AVG(measurement) AS DECIMAL(8,1)) AS mean_measurement,
                        MAX(measurement) AS max_measurement
                FROM READ_CSV('measurements.txt', header=false, columns= {'station_name':'VARCHAR','measurement':'double'}, delim=';')
                GROUP BY station_name)
        SELECT '{' ||
                ARRAY_TO_STRING(LIST_SORT(LIST(station_name || '=' || CONCAT_WS('/',min_measurement, mean_measurement, max_measurement))),', ') ||
                '}' AS ""1BRC""
        FROM src;
    100% ▕████████████████████████████████████████████████████████████▏
    1BRC{Abha=-33.0/18.0/69.2, Abidjan=-24.4/26.0/75.4, Abéché=-21.1/29.4/77.1, Accra=-25.1/26.4/79.0, […]Zanzibar City=-23.9/26.0/77.2, Zürich=-39.0/9.3/56.0, Ürümqi=-39.6/7.4/58.1, İzmir=-32.8/17.9/67.9}Run Time (s): real 25.539 user 203.968621 sys 2.572107
    
    .quit
    real 25.58
    user 203.98
    sys 2.57

**👉 Full writeup:** [**1️⃣🐝🏎️🦆 (1BRC in SQL with DuckDB)**](https://rmoff.net/2024/01/03/1%EF%B8%8F%E2%83%A3%EF%B8%8F-1brc-in-sql-with-duckdb/)",2024-01-03 15:01:04
18tho6r,Next-Gen ETL Tools,"Hello everyone

I really would appreciate some guidance. From what I've read tools like Fivetran, Airflow, and Airbyte and on paper they sound interesting because you can code and more flexibility customize your pipeline. btw I've experience with Talend, Informatica PowerCenter, and SSIS .

But i be honest the prospect of coding in Python within an ETL context really piques my interest but i have concerns Do these modern ETL tools (or as what they call it the new wave) support CDC and Incremental load ?

Our DWH is SingleStoreDB On-Premises. which ETL tool would you recommend for me to push my skills and keep up with those new tools

Edit: we have very large data like like one table has 25M records so are the modern tool can handle those large records faster then the traditional ETL tools like power center.. etc",2023-12-29 07:44:16
18s5uuo,"How do you combine Agile dev work, with unpredictable support tasks?","My company is going to try Agile.  We're also a company that is doing pretty constant, severe transitions in technology and structure, and we're very much a support group as well.

I like the concept of Agile, but I wonder how the planning out workloads etc. meshes with one-off, urgent support requests (which we deal with frequently). 

One of my old places had a department that practiced agile.  In retrospect, I think their solution to project vs support was *to drop support entirely*. 
 
  * They used a seperate IM app from the rest of the company - no pinging them
  * Other depts weren't allowd in their office area uninvited - no pinging them
  * Emails were mostly ignored, up to and including from the CEO. 

Naturally, they weren't real popular. 

So like, how does one do Agile when saying ""well our velocity is 15 points a week...unless a large emergency came up, then it's 2, or if somehow everything was quiet, then it's 25...so to answer how long this 50 point project will take...""

**EDIT** case and point - I'm on vacation, but just got a direct email about the report we use to bill our largest client is ""being weird'.  They didn't use the ticketing system even, as they know it'd be me anyway. ",2023-12-27 17:21:37
18evqnt,"Downleveled for Data Governance, How to Study","10 yoe, senior data engineer at a tech startup, I just finished an interview process with a big tech company for a lead data engineer role and got to the final round which included a coding round, case round, and behavioral round. I got the feedback at the end that I crushed the coding and behavioral rounds but performed poorly on the case round due to my lack of data governance knowledge. Questions they asked were around things like cloud storage retention policies and I communicated that I knew the tradeoffs were generally around storage cost vs access cost so stuff that was only available for audit purposes or other logs unlikely to be used should be in lower storage tiers, but was very honest that I hadn't made those decisions, that's tended to be a cto-level decision at companies I've worked at, even de managers aren't making those calls. The other similar line was around how what the interviewer called the ""operational layer"" and ""analytics layer"" should be stored. Same story I've never been involved in those decisions but I communicated that obviously we'd want some sort of layer of more raw data in a more raw storage and then a layer more optimized for analytics, this company uses GCP so it would be bigquery in that example. I also discussed personalized data and the potential need for multiple layers some including no pii and others that only few could access for audit purposes including that pii.  


I don't think they were lying to me about the feedback because I did get an offer but it was for a senior role which would have been a pay cut, and I don't even fully disagree with them on the feedback because I knew this role was a stretch when applying for it in the first place I think I'm a strong senior but a lead role is still a bit of a stretch. But at my current org I have no opportunities to be involved in data governance that's literally a C-suite decision, and anything I did for a personal project would be just me guessing at stuff and probably getting more wrong than right. Are there any maybe textbooks or articles I could read or courses I could take that could help me learn and improve my data governance knowledge? My current company has a learning budget so it could definitely include paid courses. Thanks in advance!",2023-12-10 04:45:47
18dichi,Best practice for API data integration,"Hello,

I work as a mix of being a data engineer/data analyst and I recently got more into API integration. I was just wondering, if there are any best practices on how.to access data from an API and integrate it into a dwh.

From what I found so far on the web, it seems to be either:
Authentifcate, Store the json object raw, flatten in the dwh VS. Authentifcate, flatten the json and then store it.
On this part I am quite clear and I think it depends on the usecase? However, I am am unsure about the actual way of implementing it, for example in python for a given endpoint:

Py Request (or similar), filter json, write to df, df to SQL/CSV, repeat for pagination.
Also where would on put unit tests? During the actual part of accessing the data?

Thank you in advance, any tips are welcome.",2023-12-08 08:25:01
185yh0m,WTF date formats.,"How does one,  effectively clean dates and timestamps of different format to one format?? There are so many combinations and feels like the type of formats are endless.",2023-11-28 15:20:55
182fuyq,Is Databricks becoming redundant?,"My workplace makes queries to data in Snowflake, from Databricks, but it seems like Snowflake is working hard to develop their own ETL, ML pipelines using NVIDIA Rapids/GPU computing even.

I have been signing up for online seminars on Snowflake data science/data engineering because I have a problem with Databricks.

I want to use OpenZiti on Databricks so only client machines with a Ziti identity can connect to the Databricks.  The Databricks itself needs an OpenZiti identity so it can connect to an Ziti Edge Router which in turn would connect to Ziti Edge Router on Azure that connects to my web app on Azure.  From here I want to use Azure private link to connect to OpenAI services (natural language -> SQL query) and to my Snowflake on Azure.

So the Snowflake on Azure can be totally cut off from the internet as well as my web app on Azure.  But Databricks does not have this capability, ie. close of all incoming ports, only outbound to Ziti edge router and have a Ziti identity.

&#x200B;",2023-11-24 01:08:00
17jvmmf,Haunted data warehouse pentagram schema architecture,"**Pentagram Schema for Evaluating Haunted Data Warehouses**

In the spirit of Halloween, I present a unique and playful approach to evaluating data warehouses in the world of the supernatural. I call it the ""Pentagram Schema."" This schema is not just about data; it's about the eerie, the paranormal, and the mystical.

**Pentagram Schema Overview:**

At the heart of the Pentagram Schema lies the **""Haunted Data WareHouse Access"" fact table.** This central table embodies the core interactions within the supernatural realm. Visitors are at the center of the paranormal activity, and the Pentagram Schema seeks to understand their experiences in haunted houses.

**The Five Satanic Dimensions:**

1. **Visitor (Dimension 1):** In the eerie world of haunted data, understanding the visitors is paramount. This dimension focuses on identifying and categorizing the entities or users who dare to explore the spectral data. It delves into user roles, permissions, and their interactions with the data, ensuring a comprehensive understanding of the spectral visitors.
2. **Query (Dimension 2):** Queries are the incantations of the data world, and this dimension classifies the different types of queries used to summon insights from the haunted data warehouse. Whether they're regular SQL queries, cryptic data requests, or magical spells to extract information, Dimension 2 captures and analyzes these spectral interactions.
3. **Outcome (Dimension 3):** The outcome is the ultimate manifestation of data sorcery. Dimension 3 seeks to answer two fundamental questions: Did the spectral ritual work, and how quickly did the requested insights materialize? By scrutinizing the outcomes, data conjurers can refine their data spells for more efficient spectral results.
4. **Tables Hit (Dimension 4):** Just like chambers in a haunted mansion, data in a warehouse is stored in different tables, each with its unique properties. This dimension uncovers which spectral data chambers were accessed, how frequently, and to what extent, allowing for a comprehensive understanding of the spectral data landscape.
5. **Maintainer (Dimension 5):** In any haunted attraction, there's a shadowy team behind the scenes keeping the paranormal elements in order. Dimension 5 profiles these custodians of the arcane, revealing their roles and contributions. These unsung heroes ensure the spectral operations run smoothly, shining a light on the often-hidden aspects of haunted data warehouse management.

&#x200B;

**Why the Pentagram Schema:**

These five satanic dimensions combine to create a holistic evaluation framework for haunted data warehouse usage. By analyzing visitors, queries, outcomes, tables hit, and maintainers, data sorcerers can uncover the mysteries within the spectral data, optimize their data spells, and ensure a seamless and enchanting data experience in the world of the supernatural. As you navigate this unique schema, remember to approach it with the spirit of Halloween—fearless, curious, and ready for some unearthly revelations.

Happy Haunting!

  
(edit, just having fun with how i evaluate usage + halloween + gpt for words :) )",2023-10-30 15:11:30
1709g07,Data engineer vs Devops?,"Hey everyone,

I’m at a crossroads in my career and could really use some insight. I’ve previously worked as a Data Analyst, and have now been presented with an opportunity to train in either Data Engineering or DevOps from scratch, courtesy of a company I’m considering joining. I’m intrigued by both roles but also drawn by the attractive pay scales I’ve seen in the DevOps field.

Should I stick closer to my roots with Data Engineering, or explore the dynamic world of DevOps? I’d love to hear your experiences and perspectives.

Thanks in advance!",2023-10-05 05:11:23
16kurdj,Any feedback on Chat-GPT 4 vs 3.5 for data engineering?,"I saw the other post which reduced my guilt a lot about using Chat-GPT! I’m currently working as a the only data engineer in a startup and building everything from scratch so it’s certainly been useful!

I wonder if a lot of you are using Chat-GPT 4 or if you are using the free (3.5) version. 

My company could pay the premium price but I would need good arguments and I’m currently unsure if Chat-GPT 4 would be a significant improvement, although I feel like 3.5 suffers from significant hallucinations and I often have to write several prompts to get a working answer. 

Does Chat-GPT 4 make significantly less of these mistakes?",2023-09-17 07:57:47
1663we7,Is data engineering job boring?,If not what makes it fun?,2023-08-31 07:10:28
160ehi1,Why have a Data Lake?,"Hello,

Our team has a practical need for a Data Warehouse. So we've achieved a CDC-based, near-real-time system to build one from source data coming from PostgreSQL. That pipeline takes the data directly from PostgreSQL. It does not rely on a Data Lake. The pipeline handles schema changes, etc and so we enjoy it. 

Traditionally, Data Warehouses are built from a Data Lake. But in our position, we wonder if we're missing something by not building our Data Warehouse from a Data Lake.

Note, our CDC-based solution can simultaneously send what it gets to S3 (for example) to give us a Data Lake. But the questions remain:

1. Why even have a Data Lake, under these circumstances?
2. Why build the DW from the DL when we can read directly from the source?
   1. Wouldn't we incur much latency that way?
   2. Wouldn't we incur more complexity (e.g. schema management, data cleaning) that way?

Please and thank you.",2023-08-24 21:17:00
15kw2gj,Junior Data Engineer: technical interview but was told no coding or anything to prep for?,"Hey all,

I have a 1 hour interview in a few weeks with a data lead and a senior data engineer for a junior data engineer role that did not have a lot of essential/desirable skills.

I asked about any specifics I should prep for and was told to be ready for the following:
1. Talk through my work experience and CV.
2. specific questions to better understand what I know about data engineering. 
3. It wont be a test

For a normal technical interview I usually anticipate some sort of test/task to do but this is different and so would like to ask if anyone could have any idea on what I should prep for in terms of data engineering. I use Python and SQL in my current role and have a good foundational sense of how pipelines work but only in the context of my company, I don’t really have much exposure to different systems, architecture, etc.

Also some additional context, I had an initial phone call and was then offered this interview. I was told after this interview there is only one more which is more of a behavioural I believe?",2023-08-07 20:28:48
151yrep,What are the best no-code platforms you have come across?,"My company is interested in having a no code platform so that we can democratize data access and mitigate shitty practices like shared drive hoarding. We are looking at Palantir but it's absurdly expensive. I've never really dabbled with ""tools"" like that because I've never come across something truly no code that worked well.

Curious if anyone here has used Palantir's Foundry or other tools that give non technical users a place to search and see data exists, do their data manipulation, exploration and then visualization on both tabular and time series data.",2023-07-17 11:02:12
143wn2y,Interviewing for lead data engineer position.,"So I just finished a technical interview for a lead data engineer position. It is an hour long interview and spent the first half of it  going through SQL leetcode with complex window functions.

At around 40 mins mark I realised that they are just looking for a SQL guru and ignoring the facts that I have more to offers eg knowledge about AWS services, Terraforming infrastructure, data architecture, etc.

Is this data engineering all about (being great with SQL) or did i make a good decision and asked to stop the interview at minute 45? What are your thoughts?",2023-06-08 02:25:04
140enbn,Is CDMP Certification Worth it?,"Hey everyone!

I'm thinking about going for the CDMP (Certified Data Management Professional) certification. My goal is to move up at work (from staff engineer to a higher role like principal or architect) and also to just get better at what I do.

In my job, we mostly use open-source tools, so AWS, GCP, or Azure certifications don't really help me. But I heard CDMP is vendor agnostic, so it seems like a good fit.

Has anyone here done it? Was it tough? Did it help you in your job?

Thanks for any advice!",2023-06-04 14:24:50
13z9byl,Medallion/lakehouse architecture data modelling,"How is data modelling solved in medallion/lakehouse data architecture? Bronze + silver layers are just plain tables, no relationships and gold is “data mart-ish” with all the relationships? How about the normalization? Bronze + silver denormalized and gold normalized? Or? 

Also, how do you actually make a normalization considering you are working with e.g. parquet files? In the database it is simple but with files?",2023-06-03 11:02:04
13ppolj,How to progress after landing a junior data engineering job?,"I recently landed my first job, after studying by myself for like 8 months. I also studied and passed GCP and Azure Data Engineering certificates just to bolster my chances since I have no software engineering background/education.

My tasks at work are mostly the equivalent of centering the div. I play more TFT (some game) on the job than I actually work and I do not like that, I actually feel less skilled than when I was actively working on my personal projects.

  
So I want to use my spare time to improve, I don't want to cram ever again, I don't want to be 2-3 years into a data job, then when applying to a senior position find out that I need X skill, so i start cramming, nah, I want to take things slow and gradual, let them sink, and learn them well without pressure.

TLDR **what are the most impactful skills and knowledge that I could work on right now as a junior data engineer to prepare myself for things like a senior data engineer/architect/lead in the future?** 

P.S I come from civil engineering background and have worked as project manager and team manager in engineering and non engineering fields, so I was thinking maybe studying for a PMP would be advantageous for some lead position in the field, but I could be wrong, or maybe there are more impactful things that I should prioritize.",2023-05-23 14:10:23
12dii09,"Data engineers, what has been your experience applying for jobs in this economy?","I'm currently a data engineer at a unicorn tech company, and I started applying a couple of months ago to see if I could get a job offer in this economy.  I have 5 years of experience, and before my current job, I was a data engineer at a smaller tech company and a non-tech company in the fortune 500.






I only applied to about 20-25 jobs in the past couple of months, since there really isn't that many job openings in my area.  Which is strange since I live in one of the big cities for tech jobs.  LinkedIn in my area is the same promoted jobs, so there appears to be more jobs available than there really are.




Out of about 25 applications, I got one interview at a very small tech startup with a decent amount of funding.  I did really well in the recruiter and programming rounds, but I couldn't get a final round interview since there is a lot of competition for jobs.




It seems that even smaller tech companies and startups don't really care if you worked at a big tech company for several years and if you interviewed well, since there are so many engineers on the job market.",2023-04-06 12:18:28
126js1x,For those who have worked with Airflow and Dagster. Is Airflow in any aspect better?,"I have little experience with Airflow, but I am looking for new orchestration tool for personal project.

My initial thought is to set up managed airflow server on GCP because I have worked with airflow for few months in one of my client engagements.

However reading more about orchestration tools, it seems that Dagster might be a better choice in terms of usability and that it tackles the shortcomings of Airflow (e.g. passing data between DAGs, etc.)

For those who have worked with both, I would like to hear your subjective opinions on which one you prefer, and if Airflow is in any aspect better than Dagster.

Any opinions are welcome",2023-03-30 11:09:42
11wgmnf,Best airflow tutorials,"Does anybody know any airflow tutorials that are nice to watch. Mostly I find tutorials too shallow and too slow, so preferably something that is to the point and in depth. Thanks!",2023-03-20 11:47:52
11nwo53,What's the best way to learn dbt,"I'm interested in possibly using dbt for a personal project that I'm working on, and to learn it just because it seems to be a valuable skills to have.

If you were starting from scratch, how would you recommend someone go about learning dbt? the docs, youtube, udemy, etc...",2023-03-10 18:23:42
11drsam,Looking for resources for building unit testing for boto3 code and mocking AWS services in pytest,"I’ve got a monolith AWS lambda function that needs to reach out to and query Athena and also do some writes to various s3 buckets after processing what it got from Athena. 

Trying to set up pytest and moto but getting stuck because I’m also using awswrangler. 

Not even sure where to start finding info about building unit tests here and want to avoid deploying to AWS just to test minor code changes. 

Does anyone have any good resources in where to start?",2023-02-28 00:29:55
11dgjv2,Data engineer job hunt is a mess!,"I'm trying to break into data engineering roles. I have experience in dot net and data analysis and a MS in Data Science and worked on dot net, Python, SQL, Tableau, SSIS/SSRS, VBA etc.

However, what I'm finding is that there is literally no consistency among what skills companies are asking for DE roles. The data engineer has become a catch-all term for anything from simple data analysis, database dev, BI dev to ML/stats to actual pipelines development to a tools ninja.

There seems to be a flood of tools in the DE space and each job posting is asking hands-on experience in a different combination of tools.

I'm scratching my head as to how should I spend my time learning what tools and skills?

It's impossible to have hands-on experience on all/most of these tools, even in a regular ACTUAL DE job. For example, below is the list of frequently asked tools I've curated from job postings -----------------------------------------------------------------

**Programming Languages and Tools:** Python, SQL, C#, YAML, Unix Shell Scripting, CLI, DBT, REST APIs

**Data Formats:** Relational, Unstructured, Semi-structured (XML, JSON, CSV), Parquet, time-series

**Cloud Computing:**  Snowflake, Databricks, Amazon S3, EC2, AWS CloudFormation, Python Boto3 SDK, Amazon DMS (Database Migration Service), AWS Glue, Amazon Redshift, AWS Athena, Amazon QuickSight, SNS, KMS, CDK, Azure Storage, Azure Data Factory, Azure Synapse, Azure SQL DB, Azure DevOps, Google BigQuery (GCP), Google Cloud Dataflow (GCP), Terraform

**Tools:** Apache/Confluent Kafka, PySpark, Apache Airflow, (DevOps and CI/CD) Docker, Kubernetes, Jenkins, Github Actions, SQL Server, Oracle, MySQL, PostgreSQL, MongoDb, Azure CosmosDB, AWS Dynamo, Tableau, SSIS

**Big Data:** Hadoop, Hive, Pig, HBase, Cassandra Amazon EMR, Spark, PySpark, Metastore, Presto, Flume, Kafka, ClickHouse, Flink

\----------------------------------------------------------------

Also, **recruiters won't bother to contact you unless you tell them that you have X years of experience in Y technology**. So I have had to watch some tutorials about the tools and make up stories about having worked on them. This does not fill me confidence.

So how do I go about navigating through this mess? I'm literally overwhelmed right now. Anyone facing similar issue? Any suggestions are appreciated. Thanks.",2023-02-27 16:52:56
1apkimj,"What are instances where data storage, ETL, analytics, etc don't make sense on the cloud?",These days it seems like every solution is cloud-based for data engineering. What are some instances in which moving to the cloud does not make sense.,2024-02-13 04:14:27
1agijyw,Open Source Portable SQL Linter,"Hey everyone, I have created an open source SQL formatter and linter(SQLFLUFF replacement) which runs in the browser, is over 20x faster (exact performance improvements pending) and is built using rust.

It’s early days but I should be done with the first version of it next week - feel free to star it for updates and I can’t wait to give back to the community 🙌

The behaviour will be exactly the same as SQLFLUFF but the improvement is that it doesn’t need python to run :) it will literally run natively on my phone which is crazy 🤯

Im calling it SQRUFF :)",2024-02-01 18:57:37
19cfrl8,What resources did you use to learn Spark?,"I’ve been looking to compile some different spark/pyspark learning links into a repo for reference. What sites/courses/etc have you found helpful? Ideally free, open source resources. Thanks!",2024-01-21 22:28:11
193vm5s,"Moved from a small company, which uses simple cloud services to a big corporation, that uses robust frameworks for ETL, processing and validations.","Data changed from few thousand records to millions of records. Everything is well organized, tracked and validated. 

I’m struggling to learn so many frameworks, and understanding huge data with a lot of attributes for data modeling. 

Not able to handle the pressure and feeling of not belonging here, as everyone seems to know what they are doing and deploying lot of changes to production, every week. 

I have been working here for 2 months and I got a feedback that I should be faster. I’m not able to progress due to fear of getting fired. People who joined few months before seem to be performing better or meeting expectations.

Did any face these challenges while changing organisation? How did you handle it? Any suggestions?",2024-01-11 07:02:56
193dhko,Do you think that most job posts that ask for distributed computing actually require distributed computing?,"I've seen a major uptick in job posting asking for Spark and Kafka. 

Kafka I understand why it's asked for, but aside from setting up a consumer every now and then, from what I've seen, it isn't a significant part of a lot of warehouse data pipelines. 

For Spark, most workloads for most data products can be handled without a Spark setup, or is abstracted with current database/datawarehousing solutions. 

While I understand that there are a growing amount of products and applications that require or would largely benefit from these two technologies, I imagine that there also an overwhelming amount of data pipelines that don't really need or use it. 

However it feels like 30-40% of recent job listings are asking for experience with those. 

Do you think it's because companies are trying to setup to leverage real time data AI solutions? or do you think it's mostly employers that ask for skills for the sake of putting skills?",2024-01-10 17:17:52
18yy280,Having a very hard time as a Data Engineer,"how did you land your first proper paying data engineering role, i have worked in shadow for 2 senior data engineers in a MNC doing all their work all by myself for 700 USD/month. Despite having skill set and experience ,because i am in a third world country(Nepal) i cannot find a decent data engineering job.

Would you guys care to join in and share your experience and guide DEs like me?",2024-01-05 04:34:36
1bnps6k,My parquet file takes up MORE space than original csv. Why?,"Hi, so Parquet is famouse for being more efficient than csv in terms of storage sapce and query times. I wanted to check that, so I created a dummy csv file with 1 million records. It takes up 4.34 MB. I then converted it to Parquet using pandas and the newly cerated file takes up  35.9 MB! Almost 10 times more! Why? I expected the opposite.

&#x200B;

Code for reference:

    import csv
    import random
    from faker import Faker
    
    # Initialize Faker to generate fake data
    fake = Faker()
    
    # Define the number of rows to generate
    num_rows = 1000000
    
    # Define field names for the CSV
    field_names = ['ID', 'Name', 'Age', 'Email', 'Address']
    
    # Function to generate random data for a row
    def generate_row():
        return {
            'ID': random.randint(1, 1000000),
            'Name': fake.name(),
            'Age': random.randint(18, 80),
            'Email': fake.email(),
            'Address': fake.address()
        }
    
    # Generate and write data to CSV file
    with open('dummy_data.csv', 'w', newline='') as csvfile:
        writer = csv.DictWriter(csvfile, fieldnames=field_names)
        writer.writeheader()
        for _ in range(num_rows):
            writer.writerow(generate_row())
    
    print(""CSV file generation complete."")
    
    ##########################################
    
    import pandas as pd
    
    # Read CSV file into pandas DataFrame
    df = pd.read_csv('dummy_data.csv')
    
    # Write DataFrame to Parquet file
    df.to_parquet('dummy_data.parquet', index=False)
    
    print(""Parquet file generation complete."")
    

&#x200B;

&#x200B;",2024-03-25 21:34:58
1bc85h9,I hope your pipelines are atomic?,N/A,2024-03-11 17:01:44
1bbe7ab,What should I learn to call myself a data engineer?,"My background ~11 years of expert level ETL/Datawarehousing exp with Informatica Powercenter, Informatica Cloud, Matillion (1Year)

Some basic snowflake, basic AWS, training level exposure to python (I remember defining functions - could not recollect syntax of objects, methods in objects etc. no realtime project exp with python) I have been trying to learn Pandas and Pyspark over the last few days. ",2024-03-10 16:21:56
1brp3kj,Game plan to become a data architect,"I want to become a Data architect someday. My goal is to get to a place where I can pretty easily beat out my competition because of my experience and capabilities to setup a data infrastructure for any business. I’m not the smartest guy in the room, so I am not shooting to become a Data architect for Netflix or other similar top-tier companies. I will be happy to become a Data architect for a stable company where I can see myself retiring eventually.

I am very far away from that currently. I have been in IT industry for ~6 years now, with ~5 years in data engineering and ~1 year in software engineering. I have expert skills in SQL and intermediate skills in data analysis, python, and some AWS cloud technologies used in building data pipelines (lambda, S3, Glue, Postgres, SFTP, API Gateway, etc). I am a data engineer, and I am “okay” at my job. I can build you a working pipeline with the technologies you give me, but it might be crap to an experienced data architect/engineer. I don’t know much about data modeling, data design patterns, automated unit testing and integration testing for data, building dashboards, etc. 

It’s not possible to learn all hundreds of technologies, concepts, and practices in data engineering. I have to focus on learning the fundamentals, separate technologies into categories and learn a few technologies from each category, and work on challenging data engineering projects applying what I learned.

It’s easier said than done though. I would love to hear some feedback from the data engineering community. How do you suggest I plan my learning and growth for the next 5-10-20-30-40 years so I can become a competent data architect that businesses can trust?",2024-03-30 18:46:20
1bhutin,Key Insights from NBA Data Modeling Challenge,"I recently hosted the ""NBA Data Modeling Challenge,"" where over 100 participants modeled—yes, you guessed it—historical NBA data!

Leveraging SQL and dbt, participants went above and beyond to uncover NBA insights and compete for a big prize: $1,500!

In this blog post, I've compiled my favorite insights generated by the participants, such as:

* The dramatic impact of the 3-pointer on the NBA over the last decade
* The most consistent playoff performers of all time
* The players who should have been awarded MVP in each season
* The most clutch NBA players of all time
* After adjusting for inflation, the highest-paid NBA players ever
* The most overvalued players in the 2022-23 season

It's a must-read if you're an NBA fan or just love high-quality SQL, dbt, data analysis, and data visualization!

[Check out the blog here!](https://www.paradime.io/blog/basketball-by-the-numbers-insights-from-paradimes-nba-data-modeling-challenge)",2024-03-18 16:23:05
1bpgbpu,"Subqueries vs CTE, what’s your preference?","I heard people constantly decry subqueries for making queries unreadable or decry CTEs for making queries slow (depending on the DB).

How do you feel about this?",2024-03-27 23:20:21
1bohwi7,Workplace has 0 procedures. Time to look for a new job?,"I am a data engineer with 2 years of experience. I’ve joined a fairly well established company but found it to be completely different to my last (and only other) workplace. Is this normal?

- Single Terraformed GCP project hosting
  - Airbyte Open Source
  - Airflow
  - DBT
- Single Snowflake Account with DBT

There is no permanent Sandbox / Dev GCP project or testing procedure. Anything to do with Airbyte and Airflow gets done directly in production which is making me age twice as fast. If you need to configure a new VM you are free to create a new GCP project to give it a go but as only a production GCP account is Terraformed there is no environment parity. 

The infrastructure supports analytics so it’s not business critical, but it still feels like an uncommon way to do things. The Head of Data is onboard with my suggestion to deploy our IaC to a second GCP project but the Senior Data Engineer is less enthusiastic. The whole situation has gotten me wondering whether I’m right, or whether it should even be my job as a mid to do what a senior data engineer doesn’t feel the need to do.",2024-03-26 20:18:42
1bdhw74,Free Microsoft Exam Vouchers By Completing a Challenge,"As the title says, Microsoft is offering a free exam voucher for those who complete any of the Microsoft Learn AI Skills Challenges below:

[Build next generation apps with Azure OpenAI](https://learn.microsoft.com/training/challenges?id=da09d3ca-a2bb-47dc-ba42-bea77b386a3d&WT.mc_id=cloudskillschallenge_da09d3ca-a2bb-47dc-ba42-bea77b386a3d) 

[Azure AI Fundamentals](https://learn.microsoft.com/training/challenges?id=3ef5d197-cdef-49bc-a8bc-954bcd9e88cc&WT.mc_id=cloudskillschallenge_3ef5d197-cdef-49bc-a8bc-954bcd9e88cc)

[Fabric Analytics Engineer](https://learn.microsoft.com/training/challenges?id=b696c18d-7201-4aff-9c7d-d33014d93b25&WT.mc_id=cloudskillschallenge_b696c18d-7201-4aff-9c7d-d33014d93b25)

[Azure Machine Learning](https://learn.microsoft.com/training/challenges?id=764de7ca-8b6d-4b3a-a491-1942af389d8c&WT.mc_id=cloudskillschallenge_764de7ca-8b6d-4b3a-a491-1942af389d8c)

Once you complete the challenge, you'll be given a free exam voucher which can be used for any of the exams listed below:

[AI-102 Designing and Implementing a Microsoft Azure AI Solution](https://learn.microsoft.com/en-us/credentials/certifications/azure-ai-engineer/#certification-exams?ocid=aisc24_cloudskillschallenge_webpage_cnl)

[AI-900 Microsoft Azure AI Fundamentals](https://learn.microsoft.com/en-us/credentials/certifications/exams/ai-900/?ocid=aisc24_cloudskillschallenge_webpage_cnl)

[DP-600 Implementing Analytics Solutions Using Microsoft Fabric](https://learn.microsoft.com/en-us/credentials/certifications/exams/dp-600/?ocid=aisc24_cloudskillschallenge_webpage_cnl)

[DP-100 Designing and Implementing a Data Science Soluition on Azure](https://learn.microsoft.com/en-us/credentials/certifications/exams/dp-100/?ocid=aisc24_cloudskillschallenge_webpage_cnl)

&#x200B;

You can read the links for all other details, you got a month to complete any of the challenges and you will get a voucher for any from the 4 exams listed. It would make more sense if you do the challenge that relates to the exam you plan on sitting for but it is not mandatory. You have to complete the challenge between March 19th and April 19th.

Which one are you choosing? I am thinking between AI-900 and DP-600.

Cheers!",2024-03-13 03:49:41
1b7fz0q,Company is converting to Databricks!,"We are currently going through a massive reorg and conversion. The existing ETL is built with Ab Initio and the DB is Teradata. We are now moving everything to Databricks / azure. Any advice on learning databricks? I have never utilized python, and now apparently pyspark is what will be used to build the ETL in Databricks. How different is pyspark to python and any advices on learning this as well? I was new to DE when I got here and inherited the legacy systems, so this is going to be the first tools / coding I do from the ground up. Thanks for any advice! ",2024-03-05 20:42:27
1bfzpjr,"What is the point, where you add an API Wrapper around a database?","At what point do you decide that you need an API Wrapper around your managed database or regular database?

I would love to get insights into the different factors you are considering, like write / read throughput, security, ease of access for non-data people,...  


Also: what are the hacks and tricks you have picked up around efficient writing and reading beyond throttling and buffering and what tools and services are you using for it?  

Thanks for any hints.",2024-03-16 06:23:07
1bapzzr,Saving $70k a month in DWH,"Learn the simple yet powerful optimization techniques that helped me reduce BigQuery spend by $70,000 a month.

I think lot of folks can take help from this one:
https://www.junaideffendi.com/p/how-i-saved-70k-a-month-in-bigquery

These techniques can be applied to most of the data warehouses in the market today.

Let me know what else have you done to save $$$.

Thanks for reading :)
",2024-03-09 19:32:52
1bkcret,If you have the option to switch to MacBook for work would you?,"Hey guys,

I was wondering if anyone started out with a new DE job with a MacBook when they’ve used windows most of their life? If so, was it easy to adjust and did it impact your work at all as a DE?

I’ve always wanted to code and work on a MacBook Pro but I’m worried I might hit roadblocks I’m not used to vs working in windows. 

One thing that worries me is I know MacBook isn’t compatible with SSMS which is one of my bread and butter. But I read online that you can access a container with windows installed and use SSMS that way? Something to do with docker?

Did anyone else face these concerns or issues converting to MacBook? And if so, did you overcome it and adjust quickly?",2024-03-21 17:53:47
1bc0n1z,How did you become a DE?,"I’m a DA right now trying to break into data engineering and I was curious how others got into this position? It’s my dream to work as a DE so I’ve learned the below:

* SQL - intermediate. Built scripts that do data quality checks, modularized tasks in stored procedures, transform data, and create import CSV files for my workflow. Learned how to use cursors to rebuild indexes for tables. I know all the fundamentals of SQL. 
* Python - intermediate. Built all kinds of apps (GUIs and using OOP principles) and scripts to automate ETL tasks like data cleaning. Also web scraping. I made some of my tools reusable/portable for my team when it comes to data cleaning. 
* Git/github - basic. I have repos on my GitHub to demonstrate my skills.
* API - how to get authenticated and extract data and feed it into a reporting/visualization tool through a free API for practice. 
* Scripting and automation
* continuously enhancing and automating a pipeline I created from the ground up in my current job
* Currently building a pipeline from the ground up from DB2 to Oracle Database as a recent project that came up at work which should be fun!
* Read 4-5 Python books and 1 fundamental SQL book. Currently reading fundamentals of DE and an advanced SQL book. 

Is there anything else I could learn to be marketable as an entry level DE? I know cloud computing is a good one to learn and probably an orchestration tool. But at my current job I don’t have the option to work with cloud computing and have yet to touch a tool like Airflow.",2024-03-11 11:07:35
1bmxjhe,Mid level and stuck,"Hey all!
I feel kinda stuck and lost career wise. I broke into DE and related fields by accident, taking an internship assembling the DW of a large company right when I was finishing my bachelor in Economics. 4.5y later, I'm very proficient with SQL and Python, I've built a few ETLs and done a fair share of KPIs, dashboards and fact tables and the like. I'm good with dbt but no senior level (failed a recent technical) and don't know much about PySpark, Hadoop, etc. I'm very insecure as to what to do, which technologies to learn (and how to prove I know them in my CV). I'm occasionally called an AE, DA or DE because the positions have interchangable titles here in my country. What do I try next?
",2024-03-24 22:31:19
1bmhcx9,Best Practices & Production-like Coding for Apache Spark,"Hi engineers,

I have been using Apache Spark for more than 2 years in a company. I know Spark and perform any task with it by proper searching. However, in my company there is no senior engineer who can show me the best practices and better production like spark code. So, I just learned it myself from courses and practice.

Last month, I applied for a job and they sent me a simple assignment ( collect streaming data, manipulate, and store). Although I have done the assignment correctly, I have got a feedback saying that ""your code is working and doing what we need however it is not production-like, you use monolithic way, no exception handling, no good test cases covered, no proper logging etc."".

To be honest, I also agree with them. In our company, we also don't write good code ( I knew it).

I really want to learn how to write production like Spark jobs, best practices and so on.

Do you know that kind of course? (not needed to be free). The courses that I find mostly covers Spark basics ( as I said above I know Spark, just I do not know how to write better code with it). So, those courses are not covering what I want.

**TL,DR: please recommend me a course that covers Spark best practices, writing production code, performance tuning etc. (instead Spark basics and APIs)**

I",2024-03-24 10:08:36
1bl1j5k,"How to Not Run a $12,000 Query on Snowflake",N/A,2024-03-22 15:09:19
1bh0jww,Tables all the way down,"I recently joined a new company with almost zero up to date documentation, with loads and loads of tables in different schemas.

How should I go about documenting/understanding what each table means and each field?

And when should I start suggesting changes to some tables?

I'm currently just building an ERD using dbdiagram.io with comments as much as I can but not sure if it's the right way to go.",2024-03-17 15:39:20
1b72khy,Data Jobs that are less analytical more technical ,"I’ve been a data scientist for years and hate the analytical part of creating reports/dashboards/KPIs/analysis giving analytical insights and presentations to end users (mostly sales people) that will never use it, give me urgent adhoc requests that will again never use, since they dont know what they want. 

I love the technical backend part of creating the dashboards using Power BI, using Python to automate reports, create models, forecast, graphs, use SQL to acquire data etc. 

What jobs can I look for that are less about having to analyze/provide insights to end users and more on the backend, like creating all the technical processes/automation/etc for maybe other analysts?  dont want to be a software developer. Could it be data engineering? ",2024-03-05 10:59:40
1bnebbr,Orchestrating dbt Core with Apache Airflow: MWAA vs Self-Hosted EC2 Deployment,"My company recently launched their data engineering team, and I'm their newest hire (a self-taught fresh grad, <1 month in the company). I have two seniors, and we're the only data engineers here.

We use Snowflake as our central data warehouse, and we do most of the SQL transformations there. Currently planning to use dbt Core instead and then orchestrate it using Apache Airflow. We're weighing the pros and cons of using Amazon MWAA vs self-hosting in an EC2 server considering the price, ease of setting up, scalability, and potential bugs/issues. If we self-host, we're gonna use Astronomer's Astro CLI and Cosmos to set up dbt with Airflow.

Has anyone successfully deployed dbt + Airflow on EC2? What are some of the issues you've encountered? How's Cosmos for integrating dbt with Airflow? Should we just go with MWAA? I've heard many had success with it, and cost is the only concern. We're trying to minimize costs, so as much as possible we're opting for open-source solutions (I believe Astro CLI and Cosmos are free if you're not using their cloud/IDE version).

Any insights/advice would be highly appreciated. TYIA!

",2024-03-25 13:57:31
1bej704,Latitude: an open-source web framework to build data apps using SQL,"Hi everyone, founder at Latitude here.

We spent the last 2 years building software for data teams. After many iterations, we've decided to rebuild everything from scratch and open-source it for the entire community.

Latitude is an open-source framework to create high-quality data apps on top of your database or warehouse using SQL and simple frontend components.

You can check out the repo here: [https://github.com/latitude-dev/latitude](https://github.com/latitude-dev/latitude)

We're actively looking for feedback and contributors. Let me know your thoughts!",2024-03-14 11:41:25
1be165f,How often do your pipelines break?,"I'm fairly new so I'm trying to gather a consensus on how often your pipelines break and why?

Is it expected that they will eventually break and there's nothing you can do about it? I've heard people say be as defensive in your pipelines as possible?

How do you get alerted about the breakages?",2024-03-13 20:00:42
1bqouxq,amazed by new technology,"what was the last technology you heard about and you thought wow, I didn't even think about that and how cool it is?",2024-03-29 13:07:41
1bpcmcc,Is Meltano dead?,"I see a huge potential with Meltano. Having used it on a few projects my experience is:

* It's quite slow.
* important packages are barely maintained anymore

A lot of the packages I need(dagster, evidence) is owned by private repos that are outdated with readmes that no longer work.

What's your opinion? I feel like the project is on a trajectory to die out, but I would hate to see it happen",2024-03-27 20:49:41
1bod2zr,How do you handle NULLs when cleaning data?,?,2024-03-26 17:07:30
1be05tn,Positive Job Market Outlook,"Just for all the people out there that are applying for jobs and having difficulty landing anything, keep pushing! 

When the market picks back up you will be happy that you kept upskilling yourself and you didn’t give up ",2024-03-13 19:20:40
1bhnh1m,Azure Data Factory use,"I usually work with Databricks and I just started learning how Data Factory works. From my understanding, Data Factory can be used for data transformations, as well as for the Extract and Load parts of an ETL process. But I don’t see it used for transformations by my client.

Me and my colleagues use Data Factory for this client, but from what I can see (since this project started years before me arriving in the company) the pipelines 90% of the time run notebooks and send emails when the notebooks fail. Is this the norm?",2024-03-18 10:28:49
1b5mw89,How to cheaply build/host DE personal projects?,"I don't get much technical growth out of my day job; everything is very cookie cutter, and I've decided to build out some personal projects to upskill in some latest technologies and tools. However, the challenge I'm finding is that DE projects are pretty expensive to build and host compared to say web app development or something... 

One real-world example, I'm interested in both hosting and building ELT projects with Dagster. Ideally, I want to actually make use of Dagster as an orchestrator which means hosting it long-term so I can ingest a sufficiently large volume of data, however, my back-of-the-napkin math shows this to be almost $60 per month in AWS! And this is just for my orchestration layer. I know I can host and run all of these locally, but I feel like I miss out on the experience of building in the cloud (i.e. IaC) and the tools themselves (e.g. an orchestrator running locally that's off half the time is a lousy orchestrator).

How does everyone else approach personal projects? DO you just do everything locally? Do you use a company-owned cloud account? Am I overengineering personal projects?",2024-03-03 17:40:27
1bn843b,Question to ask senior data engineers during interviews?,"What are the behavioral and technical questions would be the best to identify if a candidate a good fit or not, and if he/she actually really good data engineers? ",2024-03-25 07:53:31
1begg9k,Python requests best practices for data engineers,N/A,2024-03-14 08:35:08
1be7kij,How should early stage startups approach data engineering?,"**Early stage startups can't fund a DE team** obviously - not enough data, time, or money! But startups have a ton of data (stripe, customer, app analytics, finance/payments etc..) generally a better understanding of data. 

**So... for you guys:** how can early stage startups make the most of their data in your experience? Is there a way to stitch together all the key data without a large engineering effort? What specific examples have you seen work in your experience? Is this even a data engineering problem or is ad-hoc the way to go? ",2024-03-14 00:19:47
1bah0v3,How to deal with challenges in implementing data lake projects in an organization where the structure of the data at the source itself is changing frequently?,"Let me give a brief context. So, we are implementing a data lake project for a client and they are into manufacturing. Their IT team and manufacturing teams are two entities by itself and we work closely with their IT team or Digital team. The issue we are facing is that the data that is coming to the system is changing its structure now and then. To give an example, one of the data sources is SAP now when we extract the data based on the T-code for a year say 2021 we see the set of columns, but for the next year i.e.2022, we see some of the columns are missing or additional columns are present!! Needless to say, this is making the life of our team into a nightmare as we need to keep on loading the data again and again, pipelines are breaking, rewriting the logic... the list goes on. People will be thinking why the data at the source is changed, the answer is that some of the data is entered manually by the people in the manufacturing unit and they might not be following proper standards. At the organization level their IT team has very little say as revenue is generated by the manufacturing team hence it's very difficult for the IT/Digital team to force a standard for these manual data, even though they are trying from their end many of the times manufacturing unit people seldom follows it.
Now what I want to know is has anyone faced a similar kind of challenge in implementing a datalake project. If so how did your team overcome this issue or how did your team handle this situation? Because I feel our team is highly exhausted due to the same repetitive work. Please put your thoughts it will be really helpful.


NB: Please don't ask why we committed to this project, as it's not under my control or my decision!!!",2024-03-09 12:49:41
1b68gbb,Could regular databases be used in data warehouses?,"Why do data warehouses usually use tools like snowflake, BigQuery, etc? Couldn't they just use regular SQL databases like PostgreSQL or MySQL?",2024-03-04 11:14:51
1bo81xh,42.parquet – A Zip Bomb for the Big Data Age,N/A,2024-03-26 13:35:04
1b8vfdv,Favorite Python library?,What is your favorite Python library and why?,2024-03-07 14:03:40
1b7fcgo,"Is everyone becoming a data engineer? And is it still worth embarking on this career journey?""","   
I'm currently working in the data aspect of engineering. The path I've embarked upon involves SQL, on-premises ETL tools, and reporting. However, I'm eager to transition into Cloud Data Engineering. I've begun analyzing the requirements companies post for such roles and the number of applicants for each position. It's overwhelming to observe that almost everyone is now identifying as a data engineer, regardless of their experience. I know individuals who transitioned from roles such as Database Administration or C/C++ programming to data engineering. Each job application I've seen attracts anywhere from 500 to 1200 applicants. Additionally, companies are requesting a minimum of 10 skills for data engineering roles, spanning from database management to developing streaming applications. With over 10 years of experience, I wonder if I can secure a job within a year, considering the multitude of skills I need to acquire and the intense competition. Is the effort truly worth it, especially given that I need to start from learning Python to mastering various cloud platforms?

   
To be honest, I'm more inclined to master a select few skills rather than trying to be a jack of all trades. I'm aiming to specialize in those areas and work towards achieving a decent pay, perhaps around $100k to $120k, instead of chasing after the salaries of data engineers who are earning approximately $250- 500k with over 10 years of experience. 

 I'd appreciate your thoughts on this matter. ",2024-03-05 20:17:40
1bjb4f1,"Passed DP-203 on 17th March 2024,without any prior cloud experience","Context : 

* Completed my bachelors in CSE (November 2023)
* No prior experience with cloud computing and Azure
* Did not write DP-900 or AZ-900 or any other certification exam before this one

Most of the people on this subreddit have cleared this exam with some prior experience on cloud computing,I was unemployed(LOL ,I still am but I will start working from April onwards) and couldn't get placed from my college,so one of my uncle told that they  had a vacancy for a 'Azure Data Engineer Associate' and so I decided to **clear DP - 203 and ended up scoring 900/1000** 

**I feel this post will help people like me who have no prior experience with Azure or Cloud or SQL**

I found the exam to be moderately difficult started studying on approximately 17th January and gave my exam on 17th March (If you study daily for around 2-3 hours daily,**consistently** it might take you around 40 - 45 days)

First things first : You must have some theoretical and practical on SQL because the exam will test your knowledge on Transact-SQL (if you understand SQL it will not take time) 

So I researched and many people suggested that I start preparing from Alan Rodrigues's course (Bought it for just 449 INR) [https://www.udemy.com/share/104Rwq3@bLDvpnwu7U80WdvU1d3esdKYQotX82fguZgUCnKLTqK1bcWrGF8DyKzLxo1R9tFBWQ==/](https://www.udemy.com/share/104Rwq3@bLDvpnwu7U80WdvU1d3esdKYQotX82fguZgUCnKLTqK1bcWrGF8DyKzLxo1R9tFBWQ==/) , **initially found the course overwhelming** (because I did not study for DP - 900) so I spent a lot of time on ChatGPT understanding the basics like *Batch Processing,ETL,ELT,Stream and Reference Data,Telemetry Data,Power BI,Polybase,HADOOP,Apache Spark,Azure Data Lake Storage,Parquet,JSON etc.* (while preparing for this exam I took a look at DP - 900's syllabus and found a youtuber that explained the basics clearly,**according to me you don't have to clear DP-900 to clear this exam but atleast understand the basics**,watched some videos from his playlist on 1.5X [https://www.youtube.com/playlist?list=PLhLKc18P9YODENOj4F2nHbNXeYwY1zYGb](https://www.youtube.com/playlist?list=PLhLKc18P9YODENOj4F2nHbNXeYwY1zYGb) ) 

So after I completed Alan's course,took a practice test and ended up scoring around 20% (I would still recommend his course to understand *T-SQL queries,Synpase,DataFactory,Databricks,Pipelines,Data Flows,Azure Monitor etc.*)the overall explanation was good **but the mistake I made was** : focused too much on how to execute the services and getting hands on experience with the platform rather than getting an overall understanding of the concepts,but a little bit hands on experience will be helpful. 

So I started to understand  the paper pattern and the type of questions that frequently come on the exam,around 60 - 70 % of the questions came from this playlist [https://www.youtube.com/watch?v=mbo43UgIkYc&list=PL0AYtrUw-NRQu89sbJNXPEo0dkBVTujY5&index=3](https://www.youtube.com/watch?v=mbo43UgIkYc&list=PL0AYtrUw-NRQu89sbJNXPEo0dkBVTujY5&index=3) 

I would also recommend this youtuber as he explains all the questions clearly [https://www.youtube.com/@studyingasyouwere/playlists](https://www.youtube.com/@studyingasyouwere/playlists) 

I would also recommend these 2 channels to revise your preparation 

[https://www.youtube.com/watch?v=RTlZZMDA7qw&list=PLG3ClUcNEYt5Fmrx1hnhquUpLStdItu-y](https://www.youtube.com/watch?v=RTlZZMDA7qw&list=PLG3ClUcNEYt5Fmrx1hnhquUpLStdItu-y) 

[https://www.youtube.com/watch?v=6deS7pKBEGM](https://www.youtube.com/watch?v=6deS7pKBEGM) 

And towards the end I bought a course that had 6 question papers for 449 INR,but some answers were wrong so just google the question or read about the question on Microsoft Documentation

[https://www.udemy.com/share/109Ko43@oymPlhQbt8fnnMSk\_8khpWLUGJ8D7mSrFUvTFPMwj7rOI8n9gYK3xBO434pV4z3dWg==/](https://www.udemy.com/share/109Ko43@oymPlhQbt8fnnMSk_8khpWLUGJ8D7mSrFUvTFPMwj7rOI8n9gYK3xBO434pV4z3dWg==/) 

**A tip I would recommend**,if you don't understand any topic refer to Microsoft Documentation and still if you don't understand use ChatGPT

**And finally my exam experience** : Had a total of 43 questions (I thought the total number of questions were 65,I guess it changed recently) , had to answer a Case Study intially,then had Multiple choice single answer,Multiple choice Multiple answer,Drop down menus , **not a single question consisted of rearranging the sequence ,** total exam time : 1h 40 minutes,completed mine in around 60 minutes 

I hope someone on this subreddit finds this information valuable,study well and all the best",2024-03-20 11:24:18
1bmdd6b,Should PowerBI be used to extensively transform incoming data ?,"Greetings,

We have a department which has been allowed to build reporting using PowerBI from raw SFDC and NetSuite data which does a significant amount of data transformation in order to create the analytics.  This was built as a quick/dirty solution to financial analytics reporting.  The data is not transformed in any sort of data warehouse infrastructure (ETL or ELT solution) before PowerBI.   Is it common for companies to use PowerBI to do significant transformations instead of using traditional ETL/ELT to prepare the data (like a DW) before being accessed by PowerBI?   The team doing the PowerBI are not analytics  or data engineering professionals and seem to have been doing this as a 'side of desk' solution to side-step a properly engineered DW/ETL solution.   Just wondering.

Thanks",2024-03-24 05:36:17
1b7xuw3,"End-End Stock Streaming Project(K8S, Airflow, Kafka, Spark, Pytorch, Docker, Cassandra, Grafna)","Hello everyone, recently I completed another personal project. Any suggestions are welcome.

***Update 1: Add AWS EKS to the project.***

***Update 2: switch from python multi-threading to airflow multiple k8s pods***

&#x200B;

[Github Repo](https://github.com/Zzdragon66/stock-streaming-project)

## Project Description

* This project leverages Python, Kafka, and Spark to process real-time streaming data from both stock markets and Reddit. It employs a Long Short-Term Memory (LSTM) deep learning model to conduct real-time predictions on SPY (S&P 500 ETF) stock data. Additionally, the project utilizes Grafana for the real-time visualization of stock data, predictive analytics, and reddit data, providing a comprehensive and dynamic overview of market trends and sentiments.

## Demo

&#x200B;

https://i.redd.it/t85j4210dpmc1.gif

## Project Structure

&#x200B;

https://preview.redd.it/5pbi713ogdnc1.png?width=4084&format=png&auto=webp&s=ecf236611fb21ae68eb5872d6b4d9abefcc4509d

## Tools

1. Apache Airflow: Data pipeline orchestration
2. Apache Kafka: Stream data handling
3. Apache Spark: batch data processing
4. Apache Cassandra: NoSQL database to store time series data
5. Docker + Kubernets: Containerization and Docker Orchestration
6. AWS: Amazon Elastic Kubernetes Service(EKS) to run Kubernets on cloud
7. Pytorch: Deep learning model
8. Grafna: Stream Data visualization

## Project Design Choice

## Kafka

* Why Kafka?
   * Kafak serves a stream data handler to feed data into spark and deep learning model
* Design of kafka
   * I initialize multiple k8s operators in airflow, where each k8s operator corresponds to single stock, therefore system can simultaneously produce stock data, enhancing the throughput by exploiting parallelism. Consequently, I partition the topic according to the number of stocks, allowing each thread to direct its data into a distinct partition, thereby optimizing the data flow and maximizing efficiency

## Cassandra Database Design

* Stock data contains the data of `stock` symbol and `utc_timestamp`, which can be used to uniquely identify the single data point. Therefore I use those two features as the primary key
* Use `utc_timestamp` as the clustering key to store the time series data in ascending order for efficient read(sequantial read for a time series data) and high throughput write(real-time data only appends to the end of parition)

## Deep learning model Discussion

* Data
   * Train Data Dimension (N, T, D)
      * N is number of data in a batch
      * T=200 look back two hundred seconds data
      * D=5 the features in the data (price, number of transactions, high price, low price, volumes)
   * Prediction Data Dimension (1, 200, 5)
* Data Preprocessing:
   * Use MinMaxScaler to make sure each feature has similar scale
* Model Structure:
   * X->\[LSTM \* 5\]->Linear->Price-Prediction
* How the Model works:
   * At current timestamp t, get latest 200 time sereis data before $t$ in ascending `utc_timestamp` order. Feed the data into deep learning model which will predict the current SPY stock prie at time t.
* Due to the limited computational resources on my local machine, the ""real-time"" prediction lags behind actual time because of the long computation duration required.

## Future Directions

1. Use Terraform to initialize cloud infrastructure automatically
2. Use kubeflow to train deep learning model automatically
3. Train a better deep learning model to make prediction more accurate and faster",2024-03-06 11:54:16
1bgth7l,How Important is System Design for Data Engineers?,Any recommendations would be really helpful. Thanks!,2024-03-17 09:21:14
1bqgeay,If you had to setup a data engineering team from scratch for a startup what kind of people would you hire and what stack would you use,Obviously would differ based on use case,2024-03-29 04:25:48
1bp1pea,Data Engineer to LLM,"Has anyone moved away from DE to AI - more specific to learning about LLM’s and all the craze about AI. How was it? Share your experiences and path if possible.

",2024-03-27 13:20:20
1bmi65g,Data engineering future Europe ,"Heyo!

So I’m a European that recently graduated with a computer science bachelors, and currently standing between two job offers. One offer is as a junior “normal” developer for integration and support of applications, and the other one is as a junior developer for a data engineering team.

The question is mainly how secure and future proof the data engineering role is in Europe, as generally the market in the states (and most sources I find only discusses the American markets) is different from Europe. I have no real work experience in either field but I think both roles sound equally as interesting as per the research I have done about them. 

So I’m mainly interested in knowing what I can expect in terms of career path, I’m more of a coder and less of a “presenter”, and data analytics is not for me. However working with flows and data generally sounds fun, but I don’t want to stand in front of the board of directors and present the companies future direction for growth, for example.

The developer role sounds fun but the main product the company works on is kind of niched, and I’m a bit afraid that this niche will limit me quite heavily, as the product probably will only be used by a few companies for the whole country. However, the tech stack I’ll be working with is as broad as the DE role, which will provide me with a nice suite of skills.

Both companies are very large and I will be working in big teams for both roles, so there is no “risk” implied with having the company fail or so. Both companies also have mostly the same benefits and offers, so there is basically no favoritism involved.

Also, with the AI-situation, I honestly believe that both roles are secure, and that I can gather enough experience to still have a role by the time (if ever) companies decide to replace the junior roles in both career paths with some form of automation.

Other than that, the developer role pays a bit more as the offers stand, but as mentioned I’m more generally interested in the 2/5/10 years timeline. What kind of market can I expect for both roles, the range of roles, and benefits such as salaries and WFH opportunities?

Any and all tips (speculations welcome if you mention it) are welcome! Thanks for helping a newbie out. ",2024-03-24 11:03:02
1b506um,Is scala worth to learn as a spark data engineer?,In my company we are In a Databricks Azure environnement with most of the jobs are ETL jobs curently in python / pyspark,2024-03-02 22:07:04
1blx4lh,How do you organize and plan complex SQL transformations?,"What is your approach to this: you have a bunch of tables, need to do somewhat complex transformations to create new columns based on some business logic. To get from point A to point B will likely take several queries, a bunch of inner, left, and self joins, aggregation, possibly temp tables, etc.

Do you plan it and think about the steps, or do you just launch into the code? For example I paste table snippets of 50 rows into Excel, manually fill in the new columns I need (to help visualize it), think, start coding it, and write down the steps as I go (to keep it straight in my head, otherwise I get lost).

Is there a name for this process - how to plan and envision the steps? FYI, I have enough experience using the SQL other people might call ""advanced"", like window functions, CTEs, regex, etc. 

Where does one learn how to get better at this process?",2024-03-23 17:03:59
1bfy2sq,Dataset for family guy dialogues ,"Hello guys, I have created a dataset containing family guy dialogues from season 1 to 19. Anyone interested in text analysis can use this data on kaggle. https://www.kaggle.com/datasets/eswarreddy12/family-guy-dialogues-with-various-lexicon-ratings/data",2024-03-16 04:40:58
1bfh00d,What are some of the best entry level jobs to apply for if I want to be a data engineer?,"So for some context, I’m about to graduate with my degree in industrial engineering and I’ve known that I wanted to pursue data engineering for about a year and a half now. Since then I’ve been trying to build myself towards that. I have two internships:

Shell (not data related was more research engineering)

RMS (a smaller less known company that I worked as a database administrator for)

I’m about to take my associate cloud engineer exam for GCP and after I get the certificate for that I plan on working towards the professional data engineering certificate. 

So my question here is as a fresh graduate, what jobs should I be applying for if my end goal is data engineer and if I can’t find an entry level data engineering position?",2024-03-15 15:44:36
1bff92l,Ingesting around 10M events per minute from Kafka,"Hello!

I am currently going over streaming chapter in DE zoomcamp. I have an idea for a project, where I would like to do clickstream analitics. I would like to excersise a heavy load where my single Kafka topic would be bombarded with 10 million messages per minute.

As part of this process, I would to create a single consumer that would ingest that data and throw it to memory database, eg redis where i would run some queries on top of it.

My question is, is there an open source framework that has a python api, that would me allow to ingest this much data and throw it into memory db?
Thanks!",2024-03-15 14:29:17
1bf8d4j,Which is more future-proof and more secure in terms of work. Python Dev vs Data Engineer?,"Hey there,

I've been working as a Data Engineer for almost 5 years. I mean it was my title, but I think that most of this work was kind of just normal Python development work e.g. building api's with fast API, python library for data manipulation, dashboard app with flask/dash. Of course, 2nd part or even more was building pipelines with Airflow, Kafka, Spark, SQL, Python, and many others. But in the end, my title is Data Engineer, and now I am searching for a new role, and I am thinking about what's a better option. On one side there are fewer DE jobs, and I think these have a better salary, but on the other side: in one company DE = clicking AWS Glue, or writing SQL, whereas in other company is a heavy complex project where you join many other components, build your own, what involves a lot of coding.  
I see, that for me the most fun is where I have a lot of coding and that's why I am thinking maybe it's better to move into SWE in Python, as I already have a solid foundation. I also love data engineering, but it really depends on the project, cause some are kind of drag and drop, and some really challenging.

But I wonder if it's no step back, and in the end, it will be more boring. Do you think that DE is a better, future-proof career path? Or Python Developer in CV will give more more possibilities :)

",2024-03-15 07:31:08
1ba4g7v,How to diagram sql queries,"Hi! I am currently documenting the data flow of our project and I wanted the documentation to be easily understood by analysts in a visual manner. What is the appropriate way to diagram data flow between views, tables, left joins? ",2024-03-09 00:42:13
1b8kxr2,"Just created my first Data Engineering project, need the feedback!","Created a small data engineering project to test out and improve my skills, though it's not automated currently it's on my to-do list.

Tableau Dashboard- [https://public.tableau.com/app/profile/solomon8607/viz/Book1\_17097820994780/Story1](https://public.tableau.com/app/profile/solomon8607/viz/Book1_17097820994780/Story1)

Stack: Databricks - Data extraction- data extraction, cleaning and ingestion, Azure Blob storage, Azure SQL database and Tableau for visualizations.

[Architecture](https://preview.redd.it/zxo0v4e76umc1.png?width=649&format=png&auto=webp&s=c17adfff1ae82b19e82df4171e71f845bd3c83be)

Github - [https://github.com/solo11/Data-engineering-project-1](https://github.com/solo11/Data-engineering-project-1)

The project uses web-scraping to extract Buffalo, NY realty data for the last 600 days from Zillow, [Realtor.com](https://Realtor.com) and Redfin. The dashboard provides visualizations and insights into the data. 

Any feedback is much appreciated, thank you!",2024-03-07 04:04:49
1bq4y7q,What drives your interest in data engineering outside of work?,"Curious on what motivates many here. 

Senior DE with 10 years in the analytics space. Mostly evolved as a founding data engineer across many orgs working mostly across modelling, analytics, visualization. Last few years have been moving more upstream. 

Last year I completed several certifications around AI from zero to fully understanding different NNs models. I was curious whether I could do ml and I know now I can. 

Then switched to modern data stacks and focused on dbt and streaming systems for a while. 

Started a new role recently but the the org is severely old school re:stack, culture and data maturity as well as low performance levels across levels to be honest (I’m used to working with high performing teams). 

Job markets funky still. With everyone looking for you to know everything nowadays, I’ve been finding it hard to focus on personal projects as it may or may not matter. 

Curious what aspects others are motivated by and actively exploring. ",2024-03-28 19:55:31
1bpqf8l,DBMS vs Java,"So in my project, we have a PostgreSQL database that stores orders from the cafeteria. The task is to automatically create orders every morning for certain users in table *orders*. I view the solution as writing a procedure in SQL that would create orders for users and then schedule a job that would run the code every morning. However, my colleagues who are Java developers decided to write code in Java and schedule a job (I am not sure if the job is from PostgreSQL). I thought it was easier and more efficient to make the database itself complete this task rather than running Java code. My suggestion was kinda laughed at and I was told I should let developers do their job (I am just a data analyst; however, if the opportunity was given I would try to write the procedure by myself).

Could anyone please explain if my idea is bad and why? I am indeed inexperienced so my solution could be wrong as well, I just want to know why.",2024-03-28 08:24:45
1b6vwta,How important is your first role for the trajectory of your career?,"I was reading some older posts in this sub about working with legacy systems for your first role, and one thread was all in on the idea that your first job is the most important job for setting the stage of your career, and to switch if your work if all Excel files and legacy database management.

I thought, of course every role is important, but are you doomed to fail if your first job isn't exactly what you wanted? Especially in the current market where everyone would take whatever they can get?

It's a little discouraging for me because I turned down an offer to work with an AWS stack for my first job, to instead work for a huge corporate with amazing benefits but Access/Excel/SQL server stack. At the very least, the company is very well known and respected in my area, and one of my primary responsibilities is using Python and SQL to replace our ancient VBA and Access macros, so I think I'm getting relevant experience for marketable skills. 

Is your first job really all that important, especially if you plan on job hopping? Is there hope for data analysts/engineers working with legacy systems to make the jump into a modern cloud stack 1-2 yrs later?",2024-03-05 04:03:29
1bn1yl3,Justs Cleared DP-203 Exam - Some Real Talk and Advice,"Hey everyone!

So, I just smashed the DP-203 exam yesterday (24th Mar 2024), and I gotta get real about something. You know all those YouTubers claiming they aced it in just seven days? Yeah, total BS. There's no way you can prep for this exam in a week and come out unscathed.

Here's the deal: the syllabus is massive, like trying to eat a whole pizza in one sitting massive. Azure's got a bazillion products and concepts, and mastering them takes time. So, if you're thinking about cramming, think again.

But fear not, fellow Azure adventurers! I've got some advice that'll steer you in the right direction:

1. **MS Learn is Your Friend:** Don't waste precious time scribbling notes. Blast through the self-paced MS Learn syllabus to get a bird's-eye view of Azure cloud. Do all the exercises like your cloud career depends on it.

2. **YouTube Isn't Gospel:** Sure, YouTube is a treasure trove of exam Q&A guides, but don't take everything at face value. A good chunk of those answers is as reliable as a chocolate teapot. Crosscheck with your own logic and, hey, even bounce ideas off ChatGPT, Gemini, or Copilot.

And lastly, here's a pro tip: book your exam date before you even crack open the books. It's like setting a deadline for yourself, but with a bit of added pressure...and excitement!

So, there you have it, folks. Go forth, conquer the Azure skies, and remember: Rome wasn't built in a day, and neither is Azure expertise. 😁✨
",2024-03-25 01:47:35
1bg6wrr,"I Shared a Python Data Science Bootcamp (7+ Hours, 7 Courses and 3 Projects) on YouTube","Hello, I shared a Python Data Science Bootcamp on YouTube. Bootcamp is over 7 hours and there are 7 courses with 3 projects. Courses are Python, Pandas, Numpy, Matplotlib, Seaborn, Plotly and Scikit-learn. I am leaving the link below, have a great day!

https://www.youtube.com/watch?v=6gDLcTcePhM",2024-03-16 14:09:10
1bd1geu,Strong Data Engineering Profile,"Hello, I've been into data engineering ever since I graduate from my bachelors in CS in June 2023. So I've gained skills in SQL and experience with Azure. But since I graduated from CS, I was wondering... what would make me stand out from other data engineers having studied CS? Like I did projects related to web development and would say have a decent coding skills. So what would make me a stronger candidate than others? Having a strong knowledge in frontend dev. or backend dev?",2024-03-12 16:27:15
1baknx0,Data Analyst (SQL & Viz ONLY!) - Feeling Stuck. What's the Next Step?!,"Hey Reddit fam,

5-year Data Analyst here feeling like I've hit a wall. I'm a CS grad with proficiency in SQL, Tableau, Metabase, dbt, dimensional modeling, and a sprinkle of Python. My experience spans across Fintech, Health, and Telecom.

While I appreciate data analysis, I crave a more challenging and technical role. With my programming background, I feel underutilized. Is it too late to switch gears?

I'm torn between Data Engineering and Data Science for upskilling. My goal? Advance to a more technical position within data.

Here's the kicker: I don't just want courses. I crave practical learning that integrates with my current skillset and is job-market relevant.

Any advice on the best path forward? Open to all suggestions! Happy to share more details about my experience

Thanks in advance!",2024-03-09 15:47:06
1bgtc9f,Reviewer for Data Engineering Certification,"Hello, everyone! I've created a reviewer via Udemy for my Data Engineering Team, and I thought I'd share it here with all of you. Feel free to use it and provide any feedback or suggestions you may have. Happy learning!

**AWS Certified Data Engineer Associate DEA-C01 Practice Test**

[https://www.udemy.com/course/ultimate-practice-test-aws-certified-data-engineer-associate/?couponCode=FREEPRACTICETEST](https://www.udemy.com/course/ultimate-practice-test-aws-certified-data-engineer-associate/?couponCode=FREEPRACTICETEST)

**Databricks Certified Data Engineer Associate Practice Test 2024**

[https://www.udemy.com/course/practice-test-databricks-certified-data-engineer-associate/?couponCode=FREEPRACTICETEST](https://www.udemy.com/course/practice-test-databricks-certified-data-engineer-associate/?couponCode=FREEPRACTICETEST)

**Databricks Certified Data Engineer Professional Practice Test 2024**

[https://www.udemy.com/course/databricks-certified-data-engineer-professional-ultimate-practice-test/?couponCode=FREEPRACTICETEST](https://www.udemy.com/course/databricks-certified-data-engineer-professional-ultimate-practice-test/?couponCode=FREEPRACTICETEST)

edit. P.S. It's free, so please forgive the small question bank.  
edit2: Added the **Databricks Certified Data Engineer Professional Practice Test**",2024-03-17 09:11:44
1b9hnn0,Just launched my first data engineering project!,"Leveraging Schipol Dev API, I've built an interactive dashboard for flight data, while also fetching datasets from various sources stored in GCS Bucket. Using Google Cloud, Big Query, and MageAI for orchestration, the pipeline runs via Docker containers on a VM, scheduled as a cron job for market hours automation. Check out the dashboard [here](https://aeroatlas.streamlit.app/). I'd love your feedback, suggestions, and opinions to enhance this data-driven journey!",2024-03-08 06:46:54
1bnqqvl,where do you draw the line at the nebulous job title?,"I probably just need to vent. I have taken on so many analytic/ds type tasks (beginner to intermediate ml/stats type stuff) at this point that Im essentially the lead analyst of my small team... as well as the de, swe, infra guy etc (de is official role - those tasks involve managing our airflow/infra and ETL/db modeling, probably on a smaller scale than most des).

Let me get this out of the way: every job is like this to a degree, at the end of the day you do what youre told and get paid. And some of it can even be enjoyable. 

But at what point do you just nope out of there? At this point my team depends on me for everything - engineering, analysis, and any kind of soft skills task (presentations to stakeholders). Its exhausting and starting to feel ridiculous. Other analysts on the team were hired primarily for sql queries and dashboards, as we were told our team would be streamlining to mostly do reporting and the etl that drives it. Of course, that never happened. This leaves me with ""everything else"". I dont think mgmt realizes how screwed they could be re biz continuity, because they dont really understand any of the work involved. Not to brag that Im irreplacable, Im sure they could find some other sap to do the same.

Anyone else been or currently in this kinda situation? What did you do?",2024-03-25 22:11:25
1bcsjmf,Have you ever heard about a data visualization tool that can edit data?,"Hello everyone,

One of my colleagues mentioned she knew a data visualization tool that can edit data. By that, she didn’t mean editing calculated field but fields from the source, impacting the source.
Like you would plug a DB to Tableau and could modify data in Tableau and the data in the DB data would get updated.
A two way street between the source and the viz.

Sounds crazy to me but we live in a crazy world. ",2024-03-12 08:51:47
1bnda5x,How much of you guys have to deal with algorithms or ML algorithms?,"Someone I know who is into DE role >20yrs, he says there is absolutely no need to work on algorithms if you are a DE.  


In DE, to what extent are algorithms (ML algo or fine tuning related stuff) integrated into your day-to-day tasks or do you even deal with any sort of algorithm in your role? And if yes, are there particular ML algorithms or algorithmic concepts that you frequently encounter or utilize? ",2024-03-25 13:10:29
1bjfvir,Lightweight Airflow?,"Airflow is so darn heavy, has so much unnecessary over engineering and it makes it so necessary to adapt your scripts to it rather than the other way around — which in my opinion should be how it should work.

To be honest, maybe Im using Airflow wrong but no one on my team seems to be privy to more knowledge nor can I find much online. 

Is there a lightweight orchestrator that’s out there? Something simple, that does everything like Airflow minus the endless configuration. Something simple like CRON with a web ui for task status?",2024-03-20 15:15:15
1bs2vad,How much Python does a DE is supposed to know? Is Big Data tech knowledge must?,"Which python concepts should a DE is supposed to know to progress in Data Engineering.

Is proficiency in DSA and Algorithms is required, if so then how much?

Also is knowledge of big data tools like Hadoop, Spark, Kafka is required atleast in the beginning or mid of the career?
",2024-03-31 05:32:32
1bqszme,How would you learn data engineering today within a 6 month sprint period? ,"I’m a L1 Data Engineer with solid SQL fundamentals and beginner Python skills spanning BigQuery and Snowflake architectures but want to upskill into a L3/Senior DE role at a tech company, how would you do it over the next 6 months?

I really like the idea of just breaking ground in filling my GitHub repo out with a bunch of projects but don’t know which projects to begin or which certs to pursue. 

I currently have the following certs completed: 
- snowflake snowpro core
- GCP DCL

Wondering if I should pursue a coursera IBM DE cert. 

Any recommendations? ",2024-03-29 16:07:00
1bmqfny,What experiences do you all have with data governance at your data engineering jobs? ,I’m asking this as this question comes up a lot in tech lead data engineer positions or data architect positions. How do you use this in your day to day roles? How have you architected your etl pipelines using data governance? ,2024-03-24 17:37:40
1bjss49,What are the responsibilities of a ‘Head of Data’?,"Apologies if this is not the right sub, but I feel like this is relevant.

What, in your opinion, should be the responsibilities of a ‘Head of Data’ position? 

What does an ideal Head of Data do, and more importantly, what do they not do?

I understand that the responsibilities can vary between industries and organisations but still want to get an idea since it’s such a vague title. 

Thanks. ",2024-03-21 00:06:58
1bgbhwd,On prem data lake?! Help me prep for a debate with a laggard CTO.,"I’ve been pressing for more mature data management practices at work (managing analytics). Right now we have a single MS SQL server with a handful of shitty schema databases absorbing data from two transactional layer systems. I’ve tried getting companies like Fivetran to demo ETL systems for us but we’re just too far behind the curve. 

Current TTD for any new data source added to this “warehouse” conglomeration is like 1 year: convincing them we need the data stored, getting budget and project approval, getting resources dedicated, waiting to the start of that budget cycle under which it’s approved, pray no one quit that was allocated to manage the project, contract with a vendor to build any ETL with SSIS and whatever else, wait for their client project cycle to give us room, finally get shit hooked up. 

Needless to say, this is simply the wrong way to do things in 2024. 

I’ve got my boss convinced on just using cloud stuff (CFO and likes the idea of not having to engineer an entire modern data platform on prem with redundant backups offsite and whatnot - just had to pay the bill to have a second generator installed at main campus because we’re managing our own data center hosting literally everything so his wallet is stinging). 

BUT…

Our CTO and IT team are stuck in the 90s. They refuse to support open source, they refuse to use anything but Microsoft. They haven’t directly refused cloud, but have erected so much bureaucracy around getting any cloud vendor and service in use that it’s basically banned. 

We are really at a point of massive inefficiency though. No one can access data with waiting months. There rarely is data to even access. We partner with other companies and it always turns into years long sprint of IT using some niche vendor supplied scripting language to sftp files all over the place. They can’t even consume a REST API over there let alone expose one for vendors. If a vendor needs historic data to assess if they want to, say, spend $50M on buying assets from us (financial industry) we don’t have enough historic data to give them to model. We’re barely able to model ourselves. Our risk department signed on for some ERM tool and the vendor needed 4 years historic data, we didn’t have it so now we’re paying this vendor for nothing while we wait 4 years - and ironically have no plan or system in place to store that data along the way still be the same problem when it’s revisited. 

So, I broke down medallion architecture to my boss to give him some information tools to work with trying to sell this. All good there. But we were taking about data lakes and I realized, apart from rolling our own Hadoop cluster (not something we have the skills to do as an org - nor the desire) if the CTO insists on everything being on prem, I have no clue if anything exists for this. Especially not a packaged deal like we could get with any cloud vendor. 

Anyways, help me out for this impending stand off with a laggard CTO keeping us stuck in the late 1900s.",2024-03-16 17:36:19
1bbd3oi,Is it a good practice to mix spark with python parellel thread?,"For exemple, if I want to read and process multiples csv files or writes multiples csv files from coalesce(1) dataframes. Since for the latest, spark will rely on only 1 node ",2024-03-10 15:35:02
1bk1870,"Client replaced my solution, want opinions on what I could've changed","Would like to get opinions on what I could've done better here.

I am mid level data consultant. A little over a year ago a client approached us, I was the most senior person on this one and was kinda left on my own.

Basically his company was launching a new subsidiary (brand) and app to go with it. He wanted proper dashboards and reporting in place from day 1, so no exports and spreadsheets.

He had the app running on flutter, MS SQL DB, an ERP system and at some stage was going to get a CRM system.

With limited funds he needed a proof of concept, that would form the basis of Enterprise reporting in future.

We started with reporting for the app database. So new users, products, revenue, etc.

I proposed migrating the SQL DB to AWS S3 and using Athena for queries and connecting to Tableau for dashboards. Granted we could have just plugged Tableau into the DB but the idea was that in future we would migrate data from the other sources to S3 and then we could start running queries across data sets.

I recently found out he has decommissioned all of this and is getting the app dev team to build dashboards directly into the admin portal.

Was my solution bad?

In hindsight maybe I could've used a virtualization tool like Denodo or Azure Data fabric to achieve the same result. Or gone with a smaller POC and just plugged in Tableau.


",2024-03-21 07:53:36
1bes4bx,Solo DEs : How are you making sure your work is visible,"How are you showing visibility if you are part of a non tech management and org in general
Current team structure is of an analyst and data scientist.
DA and DS roles are front facing/stakeholder facing so they naturally get visibility. In a team structure like this, how do you make sure your work’s visible and adds value",2024-03-14 18:25:20
1bdjkaj,"""Seeking Advice: Convincing a Startup to Embrace SQL/Python Dataframes over Django Models""","Hey everyone,

I'm a data engineering consultant specializing in working with startups to develop robust data platforms. Currently, I'm collaborating with a Swedish company whose entire infrastructure is based on Postgres and Django. They've tasked me with either building upon or improving their existing platform to make it more manageable and scalable.

However, I've noticed that they heavily rely on Django objects for the ETL process rather than utilizing SQL. As a data engineer, I firmly believe in the effectiveness of SQL/Python Dataframes methodology over Django models for such tasks.

How do you suggest I approach convincing them of this? Or do you think I might be mistaken in my approach? Open to your insights and suggestions!",2024-03-13 05:18:46
1bd32bx,Gnarliest SQL queries you've ever seen?,"I've seen some pretty crazy SQL queries during my time working on data platforms. I've recently been dealing with SQL queries that consists of hundreds of thousands of union statements. 

I've also seen the abuse of dbt's ephemeral models which result in megabytes of massive SQL statements that are impossible to debug.

Recently, I've had to create automated functions to translate spark explode -> bigquery, it produces some pretty ugly code.

```SELECT POSEXPLODE(ARRAY(2, 3)), EXPLODE(ARRAY(4, 5, 6)) FROM tbl```

SELECT

  IF(_u.pos = _u_2.pos_2, _u_2.col) AS col,

  IF(_u.pos = _u_2.pos_2, _u_2.pos_2) AS pos_2,

  IF(_u.pos = _u_3.pos_3, _u_3.col_2) AS col_2

FROM tbl

CROSS JOIN UNNEST(SEQUENCE(1, GREATEST(CARDINALITY(ARRAY[2, 3]), CARDINALITY(ARRAY[4, 5, 6])))) AS _u(pos)

CROSS JOIN UNNEST(ARRAY[2, 3]) WITH ORDINALITY AS _u_2(col, pos_2)

CROSS JOIN UNNEST(ARRAY[4, 5, 6]) WITH ORDINALITY AS _u_3(col_2, pos_3)

WHERE

  (

    _u.pos = _u_2.pos_2

    OR (

      _u.pos > CARDINALITY(ARRAY[2, 3]) AND _u_2.pos_2 = CARDINALITY(ARRAY[2, 3])

    )

  )

  AND (

    _u.pos = _u_3.pos_3

    OR (

      _u.pos > CARDINALITY(ARRAY[4, 5, 6]) AND _u_3.pos_3 = CARDINALITY(ARRAY[4, 5, 6])

    )

  )


What are some of the worst SQL queries you've encountered?",2024-03-12 17:30:07
1bk5yyi,SWE vs Data Engineer,"Hey there, this question might be a bit out of context for this sub but I'm really confused and don't know what to do :

So im a 4th Year software engineering student(out of 5 years in total) and up until this point I've been focusing only on software development and all the projects internships I did consisted mainly of web, mobile development and recently I started to get introduced to data engineering and data analysis as they added some new subjects to the curriculum and I sort of liked them but I don't know if it would be a better career path If I shift my focus to data engineering instead of focusing solely on software develoment, 
Please help me I need advices and opinions.
",2024-03-21 13:00:58
1bixoc5,What are the biggest problems/painpoints in Analytics Engineering?,Considering a career pivot and discovered analytics engineering—which to my knowledge is pretty similar to DE but with more business context/less technical skills needed?). Was wondering to all analytics engineers out there what your job looks like and what problems you deal with on a day-to-day?,2024-03-19 22:39:23
1bgwc8l,How does your company distribute excel reports?,"Yes I know the overall goal of our work is to reduce excel reports. But everywhere I go there is some amount of ""essential"" reports.

I've seen so much - S3 buckets, onedrive, SharePoint, mounted disks, email, SFTP.

What does your company do? 

",2024-03-17 12:27:01
1bemv7l,Open-Source Data Quality Tools Abound,"I'm doing research on open source data quality tools, and I've found these so far:

1. dbt core
2. Apache Griffin
3. Soda Core
4. Deequ
5. Tensorflow Data Validation
6. Moby DQ
7. Great Expectatons

I've been trying each one out, so far Soda Core is my favorite. I have some questions: First of all, does Tensorflow Data Validation even count (do people use it in production)? Do any of these tools stand out to you (good or bad)? Are there any important players that I'm missing here? 

(I am specifically looking to make checks on a data warehouse in SQL Server if that helps).",2024-03-14 14:45:11
1b5ze8q,"If you could wave a magic wand and have a ""how-to"" guide for anything in DE, what would it be?","Open for interpretation.

Don't think, just answer",2024-03-04 02:23:24
1bky3mv,Share your Databricks war stories: What were your toughest use cases/projects?,I'd like to hear about the Databricks projects that pushed the limits. Enough with the medallion architecture and simple ETL/ELT demos...,2024-03-22 12:31:05
1b7f9f6,How do you detect source schema changes?,"If you have an un-versioned source which unexpectedly changes its schema, how do you deal with that situation? 

Do you maintain a manually entered schema and trigger alert on reads if schema validation fails?

Is it assumed the pipeline will eventually break, so you write your pipeline in a more 'defensive' way?

Is there a Sentry-like tool for ETL pipelines, meaning it can alert when things break?

Please excuse my lack of knowledge on this topic, I'm very new to DE.",2024-03-05 20:14:15
1bskrpf,is 1000 rows dataset enough for a data engineering project?,"Hello everyone,

I am a BI Developer and I was studying DE since top of this year, now is the time I build my first end to end DE project to showcase in my portofolio.

&#x200B;

I found this interesting dataset about ""mental health in tech"" from kaggle, but I am concerned that it's only about 1200 rows, some of which will be filtered out after data cleaning.

My project will include building a data pipeline and dashboarding.

Is it good number to go with or should I opt for something bigger? 

I am not sure if it's a good idea to randomly generate rows with python?

&#x200B;

Thank you in advance!

&#x200B;",2024-03-31 20:59:41
1box4an,What are some interesting problems in DE?,"I'm looking for a fun side project but want to be sure I'm solving a real problem.

What sort of stuff is getting in your way in DE and how do you go about fixing it now?",2024-03-27 08:45:21
1bktex1,Which Kafka solution best match my scenario?,"We are currently using a Kafka cluster primarily for receiving monitoring data from our other systems. The overall throughput of our cluster is around 20GB/s. As reading is usually lagging production by several minutes, most reads require disk access.  
Long-term usage of Kafka has exposed some serious issues that we hope to address through some technical upgrades. These problems include:

  
**Cost**: We are using a large number of high-spec SSDs to handle the reading of historical data, which are quite expensive.

  
**Elasticity**: Scaling Kafka clusters up and down is a risky operation. We have not been able to make adjustments to the cluster's capacity without affecting the business, which is quite bothersome. Moreover, each scaling operation is accompanied by a large amount of partition replication, which affects read and write operations.  
Since many of our applications are already running based on Kafka, we do not want to start from scratch. Therefore, we have researched some solutions compatible with the Kafka protocol. Could you recommend which one to use in our case, or suggest a better solution?  
[WarpStream](https://www.warpstream.com/): It seems like a decent solution. However, some of our Kafka clusters are quite latency-sensitive, and we have concerns that using S3 as the only storage for reading and writing might not meet our requirements in some scenarios. Also, we initially want to try an open-source solution, and WarpStream is not open-source.  
[Redpanda](https://github.com/redpanda-data/redpanda): It seems to match our needs quite well overall. It supports S3-based tiered storage to reduce costs and offer great performance. However, we are not sure whether it has solved the elasticity issues of Kafka.  
[AutoMQ](https://github.com/AutoMQ/automq-for-kafka):  This open-source project seems to match our needs the best at the moment. It extensively reuses Kafka's original code, only modifying Kafka's underlying storage to be S3-based. Its claimed features of replicating partitions in seconds and rebalancing network traffic automatically are the key challenges we have encountered while using Kafka. Another major reason we are currently leaning towards this solution is its high compatibility with Apache Kafka, as we do not want to require the entire business system to upgrade while we upgrade the messaging system's technical architecture.  


The above represents my personal learning and research results, which may not be entirely accurate. Can you help me validate the accuracy of my technical considerations? If there are better Kafka solutions, I would also appreciate your suggestions.  
",2024-03-22 07:17:46
1bjv3fk,Opportunity to switch to DevOps at my workplace. Should I take it?,"I work as a DE, and the Devops engineer supporting my team is leaving for a new job. My manager asked me if I would be interested in skilling up for that role. The Devops engineer would be supporting my team and 2 other teams.

This is my exp so far: 

* Previous job - BI work for 3 yrs 
* Current job - DE(70%) and BI (30%) for the last 5 yrs

My DE work is building and maintaining pipelines in AWS. Working on DB enhancements, ETL etc. We don't have any Big Data, I would call it small/medium data.

Ideally I would like to upskill next with Databricks, pyspark etc. But my company has no need of any Big Data tools. And the option to upskill in Devops is open now.

I'm somewhat familiar with the IAC and CICD tools my team uses, but there will be a lot to learn, especially to support other teams. I'm not sure if this is a good career move.

Other option I'm considering is to look for other jobs in the Big Data space, so I can continue to gain more deep DE expertise.",2024-03-21 01:52:52
1bhm35q,Seeking feedback on data engineering career course,"Hey reddit, I'm a data architect with 10 years of experience and for the past few years I've been [mentoring data engineers](https://mentorcruise.com/mentor/lewisgavin/) on a site called MentorCruise.

I've not long become a new dad so I can't take on as many mentees as I used to. But a lot of the advice I'm giving applies to most people looking to progress in their DE career. (I even saw a post here yesterday asking [about becoming a good engineer](https://www.reddit.com/r/dataengineering/comments/1bh2wha/how_to_become_a_good_engineer/))

I get an overwhelming number of applications but can't accept as many as I'd like due to just becoming a dad for the first time. So I've decided to create a course.

**This is where you come in.** I'd love to get your feedback on the course as a fellow data engineer to ensure I'm on the right track.

Here is the link to the course site: [https://next-level-data.mailchimpsites.com/](https://next-level-data.mailchimpsites.com/)

I'd love your feedback on:

* the course contents
* the course price
* the website/experience in general

This may seem like I'm farming for signups but I'm genuinely just getting started with this and all I'm looking for is feedback.

However, if you do look at the course and think it will be of interest to you then let me know. I can definitely setup some special referral discounts for Redditors who bring friends/colleagues along too.

Thanks in advance :)",2024-03-18 08:52:12
1b6iw84,How Important is SQL for Data Engineers?,"For context, I have implemented a large scale project for a hospital and the entire infrastructure was built on Azure. I set up the ELT pipelines using ADF and pyspark(for data manipulation and enrichment) and the company created API endpoints around their data sources. And so I used to extract data from the API and load it into a data lake. I then use this weekly generated data to create a dashboard which then auto refreshes weekly. I've never had to use SQL and even if I did have to use it, OpenAI's GPT-4-Turbo-preview model via the API has been absolutely great. 

Not to mention that I do know basics of SQL, doing transformations, Window functions, etc. But since OpenAI was able to write the queries exactly how I needed it, I wanted to know if it is worth investing significant amount of time to master SQL.  Yes, OpenAI may get expensive and I need to kno0w when to step in to get the correct O/P, but whenever I put in the schema, what I want, and an example of input and output, it gets the query right 95% of the time in the first go. So is it worth going into advanced SQL or to learn more about the different technologies involved in DE? Any advice would be great, thank you!",2024-03-04 18:54:03
1boh9qk,How big does big data get? in data lakes?,"It might be a silly question, but what volumes of data did you see in data lakes?

edit: and what monthly bills did you see for storage whether in S3 or other provider?

edit 2: how many sources?",2024-03-26 19:53:24
1bc1fyk,Best pipeline tool when using Python and R?,"Hi all, I am a bioinformatician and I've got a genomics pipeline that is a series of python, R and bash scripts. I'm looking to make this pipeline have more capacity to run at scale/repeatedly with ease, and also ideally have some interactive tracking element to watch the jobs run.

Originally I looked into airflow for this, but I've read that it's not directly compatible with R. Are there any other tools that would best suit my purpose? I've come across mage which says it works with Python and R as I need, but I'm not sure if there's something else that I'm missing.",2024-03-11 11:57:33
1b510m2,Time needed to build a data platform from scratch,"To all data enthusiasts here - a confession from a data engineer - my estimation of data platform setup time failed 100% of the time.

What's your experience in building a new data platform? Specifically interested in learning about the time spent in setting up & integrating the essential layers and making it production-ready - warehouse, ingestion, transformation, orchestration, and visualization.

This might be subjective given the requirements, love to hear about your experiences.",2024-03-02 22:42:29
1bq09ht,Project Lead is Dense,"Hey all, I'm just wondering if I'm not understanding something correctly. If I am, then can anyone else relate. 

TLDR: My project lead is not technical at all and I can't convince him that an API can only be accessed programmatically. 

Background:

I was brought onto a non technical team about a year ago to automate their processes and build data streams for data visualization. Since then, I have built multiple MVPs that were well received by leadership and just needed permission to get access provisioned. However each attempt was then shot down. Given the nature of the client (I'm a consultant), I was disappointed but not at all surprised. During the year, I've been told to just do non technical work. Fast forward to today and I built another pipeline after being asked to by a separate team for the same client.

Issue 1 (the main issue):

I need to be able to connect to ONE API and use a python script to do it. The team in charge of the database says it's fine to use the API if the API is accessed via your local machine, but if you have an automated pipeline in the cloud, then show them the architecture. EDIT: I'm fully aligned with database team here. This makes a lot of sense.

My lead is now demanding I not use the Python script on my local machine to run the pipeline because I don't have all the necessary approval. Instead I need to ""Access the API without code and just on your local machine"". I don't think he realizes that running Python to access the API is running it locally. I've had no success explaining this to him.

Now it may be the case that I'm just wrong and he's correct on his assessment. If so, please let me know and suggest what I can do.

Issue 2 (more of a rant than anything else):

This project was my project. I was in charge of the full architecture. And I created the full design. When he reviewed the design he made demands that didn't make sense.

1. Changing arrow flows. When I have to present the flows the arrows will make no sense

2. Removing options for stacks. I.e. I put Redshift or Azure Synapse because I was unaware which cloud environment (yes they have many) our client would give us access to. He demanded Synapse be picked (refusing to listen when I explained my reasoning). Fast forward with the meeting with the client and they said they can't provision is access to Synapse only Redshift.

3. Whenever I begin to present in a call about MY architecture design, I'm cut off after the client asks one question and I'm not allowed to say anything else while he proceeds to get the explanation of the project wrong (i.e. asking for a VM instead of using an existing cloud environment, then when corrected that a VM and a cloud instance isn't the same thing, he asked for a whole new cloud environment to be provisioned rather than using it clients and I had to correct him through ping)

4. Couldn't convince him that I didn't need to include Pycharm in my local architecture diagram because I just use that to run Python code. He refused to listen to the difference between the two.

Am I wrong to be frustrated and this is just how things are?

",2024-03-28 16:47:12
1bm0v2c,What are some under-discussed topics within the data engineering community that deserve more focus?,"Certain topics, like the shifting and competitive job market and how automation affects employment, often not talked about that much... Even though data engineering positions are expected to remain, the skills required for the roles are likely to change. As the job market grows more competitive, I'm interested in gathering tips to assist data engineers or data scientists. ",2024-03-23 19:40:09
1bil95m,Level of SQL for DE,"Currently working as a mix of DA/DE, and I believe my skills with SQL are not upto the mark to make a move to DE.
I understand there is just ""normal"" sql that data analysts, BI analysts use which is typically:

> Select xyz from abc where ysd = '123'

Yes this can get more complex with joins, ctes, window functions etc.

What is the difference between complex SQL that a DE would write, compared to the SQL a data analyst writes?",2024-03-19 14:13:42
1bfk3bf,A lament for Ab Initio,"Any old timers here remember this *ETL* tool called Ab Initio ? 

https://www.abinitio.com/en/

Back in 00s, it was a holy grail of ETL tool for F50 banks, retail, insurance etc. Heck even Netflix used it for etl !

What happened?

Today in Linkedin there is no developers jobs for this tool.

A few of the jobs are mostly outsourced maintenance.

I know old tools/Mpps like Informatica, Teradata adapted to change and offer cloud/spark/hdfs adapters etc.

Anybody has any idea what happened with Ab Initio and how is any DE with this skill doing ?",2024-03-15 17:56:47
1bbdm13,Joining a new company as a first data engineer with a data analyst/scientist background,"Hey guys

I am joining a new company as the first data guy. This company doesn't have basically anything set up right now. I'll have to set up a data lake and plan all the data architecture, etc. I've been working with data in data analyst/ scientist / BI roles for 3 years, so I know how to code and how to work with data, mostly from the analyst perspective.

Do you know any courses, documentations and resources that I could study in order to learn how to do stuff related to setting basic infrastructure, designing data architecture, and building the basic stuff so we could start a data team?",2024-03-10 15:57:17
1b50a0f,Best Spark course in 2024 for medium level,"Hello all, I'm looking for a great spark course for a data engineer with basic base knowledge who want to go deeper on spark concept, optimization, best practice,...",2024-03-02 22:10:50
1bdr3q3,What database should I use,"I am a non technical employee at a small business with mostly non tech employees. So any heavy lifting would be done by a consultant. We have an old Excel file that performs a critical function. The Excel file pulls data in from 5 CSV files (about 30mb total size) exported from one piece of software using Power Query, combines that with data pulled from 2 Salesforce Queries, and references other Excel files with Vlookups or Index equations. The query process used to be reliable and take about 5-10 seconds but has grown to about 30 seconds and fails frequently especially when there are lots of concurrent users(up to 12). I have a budget of $30,000 for a 50% chance at fixing the problem. I figure that having a database as the backend for the Excel file would make it run better. If the project is successful the follow up, with additional budget, would be:

Add additional data sources

Automate the data collection

Additional BI reporting

I am looking for software that can grow with our needs, attainable data connections to other cloud software or data sources, and enough outside consultants that I am not tied to one firm. It seems that whatever consultant I choose would be inclined toward the software they are familiar with. So when screening consultants I should have an idea of what would work best. 

Is my budget reasonable? What software should I consider?",2024-03-13 13:11:52
1b8c7m3,Reneging on a job offer...,"Who's done it? What stories do you have? Preface - I just made a professional faux pas by reneging on an offer I accepted about a week ago. Totally my bad, had to apologize and have a very awkward conversation with the client company who, for the record, desperately wanted me to join their team. But upon announcing the news to my current employer that I was planning on leaving, they made me an offer I couldn't refuse. I know I know I know... never accept a counter offer!!! Well... I believe that's true most of the time but this was a unique situation and I felt like accepting the counter was definitely in my favor. I had to explain that to the company who had presented me with the offer a week previous and they went down the whole ""this puts us in a really bad spot"" (which was technically true) and ""how much more would it take for you to reconsider and join our company?"". Sometimes it's nice to be in a bidding war but it can be stressful too. Ultimately I felt like my current role was a better fit all things considered. I feel really stupid now for having accepted the other offer without first having a conversation with my current company. Lesson learned... who else has a good story?",2024-03-06 21:44:56
1b7mhka,What is Hadoop and it's relation with Spark and BigData?,"Hi,

Coming from someone outside the data engineer field, I am just trying to understand what Hadoop is for?

I know, I've googled and read posts, articles and all and it doesn't really stick on me.

Is it really considered a file system or is it just an algorithm to distribute data across nodes in a cluster?

Docs says it's good to process datasets. What are datasets? Are these files, databases? CSVs?\\

How does Hadoop relates to these concepts, datasets, HDFS and ORC (Optimized Row Columnar)

I really think a simple example that I couldnt really find in internet could be the key for me to understand that once and for all.

And finally, how Spark and BigData relates to it?",2024-03-06 01:09:27
1b6cit9,DuckDB vs Polars - Thunderdome. 16GB on 4GB machine Challenge.,N/A,2024-03-04 14:41:36
1bsmfsq,Celebrating my first Data Engineering Project,"Hey everyone!

After dedicating over 6 years to software engineering, I've decided to pivot my career to data engineering. Recently, I took part in the Data Engineering Zoomcamp Cohort 2024, and I'm thrilled to share my first data engineering project with you all. I'd love to celebrate this milestone and hear your feedback!

[https://github.com/iamraphson/DE-2024-project-book-recommendation](https://github.com/iamraphson/DE-2024-project-book-recommendation)  
[https://github.com/iamraphson/DE-2024-project-spotify](https://github.com/iamraphson/DE-2024-project-spotify)

Feel free to star and contribute to the project.

The main goal of this project was to apply the various technologies I learned during the course and use them to create a comprehensive data engineering project for my personal growth and learning.

Here's a quick overview of the project:

* Implemented an end-to-end data pipeline using Python.
* Fetched dataset from Kaggle.
* Automated infrastructure setup with Terraform.
* Orchestrated workflow with Airflow
* Deployed on Google Cloud Platform (BigQuery and Cloud Storage).
* Created visualizations dashboard in Metabase.

Looking for  job opportunities in data engineering

Cheers to new beginnings! 🚀  


&#x200B;

https://preview.redd.it/3zu86f0qtqrc1.png?width=1043&format=png&auto=webp&s=5a882ad736755af0f388a14d7f8758e2db3e87f3

&#x200B;",2024-03-31 22:08:52
1bq83ej,What framework to use to process large JSONL?,"I receive via GCS, every day, once a day, 150 JSONL files with 1.5 million lines each file. Average size is 1GB. What would be the best framework and solution architecture to ingest them into BigQuery table? I am currently using Dataproc and submitting PySpark jobs. The job reads the files into Dataframes and export to BQ.

Thank you ",2024-03-28 22:02:50
1bnupia,Thoughts on scala? ,"I'm mostly a python guy but at one point I wrote c#. Lately I've been enjoying scala. Seems like a nice abstraction over java and has a much more declarative structure than python. 

My job is mostly writing backend APIs and automation in fastapi. I've done a few wildly basic akka endpoints over the years and really like that syntax. 

Thing is.. scala is pretty much not used by our industry. At prior shops they were either straight java / python or c# if they were Microsoft shop. I have yet to find a shop that embraced scala. I love the live execution debugger, such a nice feature. Feels like pdb. 

It's fair to say that I personally am not benifiting from any speed differential between python and scala Im preferential because I enjoy the syntax. My python is purely a wrapper over compiled binaries. 

Why don't YOU personally use scala?",2024-03-26 00:56:14
1bi6dus,Excel files sprinkled across organization with no documentation ,"I am sure many others have been in my shoes before. 

I have been with my org for about a year, and it’s approach to data management is extremely primitive. I am trying to do some dash boarding of various KPIs, but I’m finding that everywhere I look the approach has been that some random person enters data into an excel sheet somewhere without any documentation.

Again, there’s no documentation on where this data lives or how it comes to be. Over the course of my role, I’ve discovered that much of this data entry could just be automated by creating some views off our main production database. I’ve resolved about 70% of the necessary data processing by creating a view and loading it to a PowerBi data model, but I’m trying to figure out how to handle the missing ends. 

I’ve considered asking our operations manager if we could drive people to host their excel workbooks in share point. That way, I’d have access to the files and could take a periodic snapshot by just reading it to a pandas dataframe before loading it to a history table in a data warehouse. 

How have others managed this issue with undocumented excel workbooks floating all over the place? 

It’s tough because I get the sense that many are refusing to share information because they fear their job being automated. At the same time, it is simply not feasible for me to spend my whole day constantly chasing down spreadsheets everywhere.",2024-03-19 00:06:02
1bhzgyi,Going from DE to SWE,"I recently got my first offer as a Data Engineer coming out of school and wanted to understand the possibilities of moving out of DE into SWE at some point. Has any body made this jump before? Was it hard or easy? What kinds of skills did you have to pick up either on the job or by yourself?

Thanks!",2024-03-18 19:28:46
1bf8k8q,Explain like im 5: Databricks Photon,"I saw some reddit posts on this subreddit and read some articles but I have no clue what photon is. I currently use Apache Airflow, Spark3, and Scala. Python works with airflow to schedule DAG tasks which do very heavy dataframe computations and each of the tasks are ran on the Scala Jars.

I'm pretty new to scala and spark so im basically a noob, can someone explain how Photon will help accelerate my pipeline and dag tasks? I understand that somehow things get re-written in C++. From my understanding once a Scala code gets compiled it gets turned into byte code instead of object code which means scala will run slower compared to C/C++. But I also read on Databricks' website that there would be 0 code changes required so how on earth does that work.

I also read somewhere on this subreddit that it was mostly made for SQL and not for data frames. is this true? If so would this render it useless for my application?

Also are there other alternatives? I want to increase speed while reducing compute costs",2024-03-15 07:46:40
1bf5ij4,BI tools!,"I’m curious to find out whether you use the BI tool or rely on SQL to get insights from data. In my experience, the BI tools can be complex and also quite limited in their utility for detailed analytics tasks. What do you currently use today for your analytics workflow, and what do you love and hate about it?",2024-03-15 04:24:23
1b559ap,"How do you read .xlsx in general where the file’s shape is (120000,110)","Guys,

I am trying to read a .xlsx file in my local mac. I have tried pandas (takes 10 minutes), polars (take 5+ minutes and all the columns get “sting” dtype), pyspark won’t let me use pyspark.pandas.read_excel without giving me a type error of “squeeze”.

I also tried data bricks, but Microsoft data bricks has restricted my account to only use 100,000,000 bytes when my file requires 550,000,000 bytes.

Please give me your suggestion. 

The reason I’m doing this is,  I have 300+ files like this and want to convert all my .xlsx into a data lakehouse, so any senior engineers here please advice me!",2024-03-03 01:53:06
1brusu3,How do you manage your Docker images with Airflow as the orchestrator?,"At my current role we've got a managed Airflow instance with 300+ dags currently. Most of the those dags are just running sql on a data warehouse, but a growing number of them are using python to ingest from various APIs, do some data processing, and then pass it off to a data warehouse. This isn't probably isn't good practice at the scale we're at and has been causing some issues, so we're making the move to migrate most of those to containers running on EKS using the k8s pod operator. 

For anyone else doing something similar, how do you have it set up? Do you have your images for the python code doing data ingestion and processing in separate git repos from the airflow code and use a tagging method (semantic releases, commit sha, etc) to sync code between them?",2024-03-30 22:50:37
1bmnyjg,FANG Entry Data Engineer ,"I’m working as an analytics engineer currently on an extremely dysfunctional team, so I’ve been trying to work with other teams such as the MLE team and DE team within my company. I only have 1 year of experience since I graduated last year, I also have a CS degree from a pretty good university.

I’m looking to leave my current company if I can’t switch teams and I have stumbled on entry level FANG Data engineering roles (meta, Amazon). How is the experience working at one of these as a data engineer?",2024-03-24 15:52:18
1bmjk7e,Python project to advance career tips,"I am currently a data engineer with approx 2.5 years experience. However my experience has mainly been in low code tools such as SSIS, and SQL with some PowerBI. I want to break into a role that uses python. What would be the best way to do this? I am considering building a python ETL project on the side and then use that to showcase python skills in addition to my experience on my CV. Thoughts? Would be helpful to find out from someone who has done a similar thing with success too. Any advice would be much appreciated.",2024-03-24 12:26:35
1bl0iho,Extract data from API,"Hi everyone,

I’m starting a job where I have to extract json files from API and then put the data in azure SQL DB.

I was thinking on running the scripts on VM and setting the API key as environment variable, but would like best practices when it comes to similar scenarios?


",2024-03-22 14:25:19
1bglg31,Trying to expand my Reddit horizon: What other tech/programming Subreddits do you recommend?,"Hi all! I've been using Reddit more frequently lately, but I’m still active in just a few communities:

- this one, of course
- /r/cscareerquestions
- /r/apachespark
- /r/learnprogramming

what other tech/programming subreddits do you follow that compliment this community?",2024-03-17 01:17:31
1bfbbqt,How do I move pipelines away from Databricks?,"My team's tech stack is completely Azure based and we use ADF for orchestration with pipelines in Databricks and the data sitting in a delta lake in a Storage Account. 

This works well for the most part but for the majority of our pipelines, running Spark is overkill and it could be done using Python or Polars. Many other teams in the firm access the data using their own Databricks Workspaces so I want to keep the data in delta lake but move over some of the DE pipelines into something more suitable and cheaper. I also think certain things such as web scraping using Selenium is overly complex in Databricks and would prefer to develop something like that locally and probably execute elsewhere too.

How would you recommend going about doing something like this? I'm not too sure of the best architecture for this. My initial thoughts are that we can use VMs to execute our code that we don't want to use Databricks for but I have very little experience with this so some learning recommendations would be helpful too.",2024-03-15 11:03:13
1bevpcl,Can you recommend good enterprise job scheduling software for Windows?,"I am currently using Windows Task Scheduler to schedule scripts on a Windows Server but it is too simple.

I need software where I can easily manage and monitor 100+ jobs/scripts, including a good GUI.

What would you recommend? It does not have to be free.",2024-03-14 20:51:53
1bbzhez,Navigating the Transition: Landing a Data Engineering Job in 2024?,"Hey fellow Redditors,

I hope you're all doing well! I wanted to share my experience and seek some advice on transitioning into the evolving world of data engineering, specifically in 2024. Here's a bit about my journey so far:

I've been in the tech industry for around six years, initially starting as a Backend Developer (4 years) and gradually shifting my focus to DevOps and Data Engineering tasks the last two years in the role. Afterwards, I landed a full Data Engineering role for the past 2 years. Unfortunately, my team recently faced a layoff, and since then, I've been on the hunt for a new opportunity.

The challenge I'm facing is the industry's increasing demand for cloud experience and proficiency with modern tools, something I wasn't heavily exposed to in my previous role with on-premise solutions. I've been diligently searching for the past year, with moments of intense effort, followed by a need to recharge after facing rejection or reaching the final rounds only to lose out to someone with more cloud-centric experience.

I would love to hear your thoughts and experiences on how to successfully land a data engineering job in 2024, especially considering my background. I love learning new technologies and usually I'm fast to pick them up, so I'm a bit baffled nobody is willing to give me the opportunity to prove myself as so far I've heard only praise from my superiors and colleagues about the work I've done. What strategies have worked for you in bridging the gap between on-premise and cloud-based solutions? Are there specific tools or certifications you found particularly valuable in making this transition?

Here are a few specific questions to get the discussion started:

1. How crucial is cloud experience in today's data engineering landscape, and which platforms/tools should I prioritize learning?
2. Any success stories or tips from those who have transitioned from on-premise to cloud-focused roles?
3. Recommendations for certifications or online courses that can enhance my cloud skills and boost my marketability?

I'm eager to hear your insights and learn from your experiences. Let's support each other in navigating the challenges of the job market and adapting to the ever-changing tech landscape!

Thanks in advance for your valuable input!

  
Edit: How to approach the job market in the EU right now? Where should I look for jobs apart from Linkedin (lately they've been unresponsive)? ",2024-03-11 09:50:48
1b7zezu,A tool to quickly extract data from websites,N/A,2024-03-06 13:15:25
1b6ydx2,Intimidated and discouraged by Job Description,"I know this is common but wanted to hear any success stories from anyone who applied to role that were under qualified for but got the job and killed it.

Been looking at [Sr.Data](https://Sr.Data) Eng roles and I just get so intimated with the list of technologies they expect you to manage. All I really do is write up stored procs in BQ to create a dimensional model then throw it in a DAG for orchestration.",2024-03-05 06:16:31
1bpyaim,Does anyone have experience with airbyte / other open source warehouse data ingestion tools?,"Our team currently uses stitch for data ingestion. We are ingesting large-ish (\~10m rows per day) from a few key sources (S3, Klaviyo, Salesforce, Google Ads) and redshift is our only destination. We are interested in moving off of stitch due to its lack of maintenance and poor support.

One option we are considering is Airbyte, an open source tool for data ingestion. I was curious if people have experience using this tool or have strong feelings about alternatives.

We would consider fivetran but leadership doesn't have the appetite for that kind of spend.",2024-03-28 15:25:41
1bobzzj,Good solution for 100GiB-10TiB analytical DB,"Hey Everybody,

I have been in the field for some time and still unsure about optimal solution for analytical database of 100GiB-10TiB range. 

If you have less, you just go with PostgreSQL or some other conventional database with reasonable level of support of table scans +dbt. If you have more, you go with Spark/Athena. 

But that range in the middle… You cannot put it into a reasonably priced db server. A proper host would cost me around 10K/month. That’s roughly the same amount I pay for 50 servers Spark cluster. 

But that amount of data does not need massive parallelism for ETL processing and associated Spark complexities. I probably need 10 process running in parallel to convert json to parquet (oversimplification here). 

What technologies/products would you use for this sort of ETL/reporting tasks? 

Thank you
",2024-03-26 16:23:36
1blke3g,What do you when your organization gives you scripts or tasks from exemployees in a language you don’t really understand?,"Basically title. I’m a Jr Data Engineer in a big four company, I work mostly on the cloud with GCP and sometimes I work with Python to automate my daily schedule lol. Recently a manager quitted and he was the owner of some important processes that were used for some BI dashboards and all that, they gave everything to me, what it’s bothering me is that all the scripts are modeled in R, I don’t really care about it because they’re easy to run, but I live with the constant fear of what will happen when they need to change the logic of some column or need to merge some new fields, since I do not know shit about R, and his scripts are huge and pretty complex, so studying and understanding the logic of all of them have been a pain in the ass for me, I want to migrate the logic of the scripts to Python but I know it’s not gonna be easy and will probably take me more than what I want to spend doing it. Any suggestions or advices?",2024-03-23 05:11:19
1bl8vbz,Is anyone using PowerBI + Fabric at an Enterprise Scale?,"Atscale is not pulling any punches here about the limitations of Fabric when it comes to Power BI + big data.

  
Based on their report, their CTO and founder is saying that its a half-baked solution for large workloads. It might work with smaller data sets, but it falls flat (query performance & timeouts) after 100+ GBs of data if you are using the Direct Lake interface.

  
Is anyone else running into these types of scalability challenges?

  
[https://www.atscale.com/blog/power-bi-face-off-databricks-vs-microsoft-fabric/](https://www.atscale.com/blog/power-bi-face-off-databricks-vs-microsoft-fabric/)",2024-03-22 20:12:39
1ben94r,GitOps for Data - the Write-Audit-Publish (WAP) pattern,"[Link to blog post here](https://www.y42.com/blog/gitops-for-data-2) \- feedback welcome!  


Do you test all your changes in prod? 🤦‍♂️ Let's borrow some concepts from software engineering and make sure that bad data never enters production. One such way is the Write-Audit-Publish (WAP) pattern.

Just released a blog post explaining it and showing how to make sure you're:

* Always working on production data in an isolated environment (dev/staging/prod environments)
* Collaborating securely with custom approval flows (GitOps)
* Preventing faulty builds from going into production (CI/CD)

Check it out and share your thoughts :)",2024-03-14 15:02:02
1b8zeek,"Databricks SQL, Amazon Redshift, GBQ or Snowflake?","I am aware the answer will depend on many cases of what is the stack of the company, budget and familiarity but If you are working with one of these daily imagine you are selling/pitching each of these DW solutions here!",2024-03-07 16:53:31
1b82sdv,Help with Imposter Syndrome,"Context: I graduated June 2021 from a good UC, but took an opportunity as a contractor at MAANG to get some industry experience in Data Engineering.

I’ve been working as a contractor here for about 2 years, yet I feel like my imposter syndrome is still something I’m struggling with. The title as a contractor also creates this negative perception of myself as well. At this point, I think it’s hindering my progress.

Many of the FTE I’ve worked with have told me I have the skills to transfer to a full time position. I recently failed the full loop for transferring to a FTE. Consequently, this rejection adds to my imposter syndrome of not being enough.

How did you guys get over your imposter syndrome? How did you guys stop giving a fuck about what others thought and just do what you do to excel? Thanks!
",2024-03-06 15:40:13
1blwtyr,Gartner Data & Analytics Summit 2024 - My Experience & Key Takeaways,"Hey everyone,

I recently attended the Gartner Data & Analytics Summit held at the Disney Dolphin and Swan hotels in Orlando, FL, and I wanted to share my experience, especially since I run a solo data and analytics boutique focusing on data management, data strategy, data engineering, and data modeling. 

The summit was an excellent opportunity for networking with professionals in the field. It offered a variety of sessions, including roundtables (intimate discussions capped at around 30 participants), vendor sessions (focused on sales pitches for their data governance, mesh, or AI tools), and, most importantly for me, sessions led by Gartner analysts. 

Most of the sessions I attended dived into data governance and data product development within organizations, offering high-level frameworks and insights into the organizational change needed for these initiatives. They were incredibly informative, and great for me staying up to date on the latest trends and providing actionable takeaways. I definitely have some more reading to do on my part.

One of the highlights was chatting with other attendees. One highlight for me, was meeting a seasoned professional who shared his journey from the early days of loading data into tables with COBOL to attending one of Kimball's first seminars on dimensional modeling—I thought this was interesting because I learned dimensional modeling through Kimball's work. 

While the summit was broadly beneficial, offering insights into industry practices for managing and deriving value from data, it was also an invaluable networking event. However, I wish I could have gotten into one of the roundtable discussions. They can be particularly valuable because they are deep dive sessions into one area that is facilitated by Gartner analysts alongside industry leaders.

This year’s focus was on Generative AI, almost to the point of overshadowing other discussions. Despite this, I'm excited to attend next year's summit to stay updated on the latest trends and best practices.

If you have any questions or need more details about the conference, feel free to ask. I'd be happy to share more about my experience.",2024-03-23 16:51:46
1bddw39,"Learning Python for data engineering for finance. Is bonobo, PygramEtL and Petl still relevant?","I started to learn Python as a data engineering tool a few months ago. Since I come from a SQL background, I have gained good grasp of Pandas and Airflow pretty fast. Recently I heard about other ETL tools such as bonobo, PygramEtL and Petl. Are they still relevant? I tried to find youtube videos to learn more but most videos are quite old (more than 3 years old). I am not sure if I need to spend my energy learning these tools if they are not used often any more. If you are a data engineer, would you please tell me what python tools are you using? If you are still using bonobo, PygramEtL and Petl, would you please share some use cases? Thanks a lot!",2024-03-13 00:44:05
1bc1njq,"Need to setup a Petabyte scale geospatial analysis data platform. Need help to connect the dots, and choosing the right tech stack (OSS only)","We have telecom data with more than 3 trillion events per day for device location. The data scales way beyond 1PB per day.   
I want to create a platform where people can do geospatial analysis.   
right now the usecase that i'm having is selecting a area on hte map, and getting the list of current devices in that area, which fulfils the required condition like affluence price sensitifity, age etc.  
Right now we do all the processing using spark, convert coordinates to geohash, and do majority analysis at 7 precision, ohwever, we store most granular data at precision 9   
The setup needs to be open source,  
Right now we use kafka, spark/flink for processing, presto for query engine.

as per my current understanding, i'll select some area on map, and get geometry polygon. I'll need to get coordinates (or geohash) which lies within the geometry from data (based on time).   
the underlying data will be at different aggregation level, the lowest aggregation will be at a petabyte scale.  
I'm trying apache-sedona, and presto native geospatial query currently, has anyone worked on a similar setup.   
I want this platform to scale and be able to scater to any kind of reports interactively..",2024-03-11 12:09:15
1bb3wa2,What should an end to end personal project encompass for you?,"Hi! 
Given any hypothetical data (you may assume we have to scrape for the data ourselves, or have access to an API, or anything - up to you), what would you do with it to ensure it’s a project that teaches you the most crucial data engineering concepts, and one you can display on your portfolio?

Please help me ideate! 

P.S. The cheaper the better!",2024-03-10 06:47:39
1b71k0x,Is palantir framework experience transferable?,"

I am a recent graduate and I got a job offer for a data engineer position where I would have to work on a project with palantir. At the moment I am not familiar with this framework at all (the company would enroll me into a training program) so I wonder how useful is this going to be for my long term career of becoming a data scientist. Could someone shed some light on how niche exactly is the Palantir system and is it worth it to except this job offer if I don't yet know if I want to be working with it in the future. ",2024-03-05 09:51:41
1b6b2sa,Career Advice with next move,"Hello all,

I have 2 offers and don't know what to do?!

1. the first offer : lower salary , they are mostly using SAS DI for ETL (which is my concern as It's legacy and would limit where I could grow as a data engineer-at least this my understanding-), but the team is more organized and have good structure with team lead with experience more than 10 years and DE seniors where i can learn from.
2. the second offer has better salary and benefits, stack is spark and python and on-prem Hadoop cluster, using Nifi and python code for ETL pipelines, the team will only be one other than me with the same experience, the team is big but mostly focused on data analysis and data science.

my concern with the second offer is that there won't be someone with a lot more experience to learn from, is that critical or can I work on that myself even if it requires more effort.

&#x200B;

current experience 2 years in DE/data analysis using python, snowflake and Azure data factory.

It would be desirable to land a remote job in Europe or to be hired there in the future so which is better to pursue in terms of career and skills?  


sorry for the wordy post, I appreciate any advice on this.

thank you",2024-03-04 13:37:45
1bptwao,"Is it possible to use data from different database types in dbt? If not, why not?","I'm just getting started with dbt (I have an analyst background, but am new to analytics engineering). Due to the way the company is structured, our data is in a few different places - we have a Postgres database for sales transactions, we have an S3 bucket where the results of some analyses get added each week, and recently we've taken on Snowflake database (data lake?) to help add some consistency to things. In the short term, though, it won't be possible to simplify these data sources down further.

I'm trying to set up dbt to pull data from these different places so that I can join tables together for analysis and to put together some dashboards. I can't tell whether this is possible, and I don't quite understand why it wouldn't be. Is my only option to move all of the data around before running dbt?",2024-03-28 12:03:29
1boj2rh,Figuring out what tables are what in a database,"As part of my job I have to VPN into customer databases and extract the data that we need. Every customer has a custom table structure and there are often 300-400 tables. I need to find the 5 or so tables that have the specific data I am after.

Is there an easy way to figure out which tables reference other tables and what the keys are?

I’ve tried putting the table, column, and data type into a data frame then searching for words that might be what I am looking for, opening those tables and looking at the data but that’s rather time consuming and they do a lot of weird stuff, for example one of the things I look for is a column called ‘batch’ or ‘batch_number’ that has an integer data type but a few dbs I have looked at store batch numbers as strings for some reason",2024-03-26 21:04:46
1blnf39,Data engineering with inefficient practices: What is your perspective on this?,"As we all know, data engineering has a lot of shortcuts where when you start out that you can do inefficient data practices i.e. have your tables denormalised and do on top full table upsert that the ROI cost part is negligent to the business outcome values and less manpower need it in comparison to accomplish the same things efficiently which will take longer time and cost more at start.

Now I see a lot of data topics that “this is too expensive” and “how to reduce costs” popping up here and there meaning that many in our career did not put the homework or want to figure or understand how the cost will scale as they use these tool. Most treat them as black box and thank the AI lords that everything will be automated with a big magic button. Correct me if I am wrong, but won’t all this SaaS and cloud vendors go bankrupt or their stock will take a huge dive if they give all their black box services out of the box efficiently as possible that will charge customers less in the first place? I always feel those data cloud providers and vendors are like a free to play gacha game. If you know how to use these tools properly, you have to grind a lot to get most of the stuff to be almost free. Otherwise if you don’t have the time to do that, you have to pay a hefty amount. ",2024-03-23 08:35:38
1bkzmhm,Kafbat UI for Apache Kafka v1.0 is out!,N/A,2024-03-22 13:46:20
1bjpg6d,Appreciation Post - Thank you guys!,"I don't know if these kind of posts are allowed or not, but I truly want to admire how this community is very helpful and feels like home, whenever I am stuck in something I just come here and search for a special topic.

Many posts and comments have been helpful to my starting career in Data Engineering, at first I was lost and overwhelmed, but things got clearer once I joined r/dataengineering.

I am still learning day by day, and looking forward to becoming a data engineer, good luck to everybody who's chasing a dream.
",2024-03-20 21:47:28
1bcyqug,Making sure I understand the 3 tier architecture… Is this it?,"Say I’m developing a web app that will get shown on user devices. So the system would look something like this:

User Device <-> Streamlit Frontend -> FastAPI Logic -> SQLAlchemy Data Access <-> Postgres Data Store

Would this result in:

* Application Layer: Streamlit
* Business Logic Layer: FastAPI
* Data Access Layer: SQLAlchemy
* Data Layer: Postgres

Or am I misunderstanding anything?
",2024-03-12 14:35:37
1bcv2qo,Software QA in a Data Engineering Team,"Since Data Engineers are rare nowadays - relative to more common SE roles. I wonder if there exists a role of a Software QA Engineer for a DE team. If so, what do they do in your company? How do they define the needed test suites for a specific requirement?

",2024-03-12 11:35:15
1bchyko,we built an open source Salesforce CDP alternative,"Founder of Multiwoven ([https://github.com/Multiwoven/multiwoven](https://github.com/Multiwoven/multiwoven)) here. 

My co-founders and I have a background in product and engineering for Asia population-scale customer data infrastructure and applications - across Affle (3 Bn+ connected devices for consumer intelligence-based marketing), Truecaller (\~400 Mn. MAU) and Razorpay (India's largest payments platform).

We built Multiwoven as an open source reverse ETL tool for data activation. After our recent repo launch, Multiwoven started being used by data teams to eliminate the complexity and tediousness of building and maintaining data pipelines to third-party biz/marketing/sales tools. 

From our early users and discovery conversations, we quickly started seeing users who have a data warehouse use us like a CDP on top of their warehouse. Many of them were customers and prospects of Salesforce (CRM and/or marketing cloud). 

The simple solution that worked for them - 

1. Easily works with all their tools (not just Salesforce's ecosystem). We are adding roughly two new Connectors every week, and it's also easy to build or customize for someone's own needs (without going through long and expensive professional services)

2. Not being compelled to invest upwards of half a million dollars a year on Salesforce CDP. The Salesforce CDP Starter Pack is at $110K, and doesn't include Segmentation, Audiences and Ad integrations (imagine a CDP without these!)

3. Being able to easily self-host the software (that essentially has access to all their warehouse data) and not worry about jumping through Compliance/InfoSec's hoops. 

Gartner recently naming Salesforce CDP a leader in their first Magic Quadrant for CDPs is certainly surprising. We're not sure that users feel the same way. We recently released a connector to Salesforce CRM, and Salesforce Marketing Cloud is on the way. We are continuing to furiously build and working to meet users' needs.

As a young project, we'd love your feedback and support (pls star us on GitHub! [https://github.com/Multiwoven/multiwoven](https://github.com/Multiwoven/multiwoven))",2024-03-11 23:27:20
1bq0g7c,"Avro write time is worse than Parquet, but Avro is supposed to be better for writes ...","Hi, I read that Avro is better for write-heavy workloads and Parquet is better for read-hevy analitical workloads. But after conducting a test I found that writes are faster on Parquet:

**Avro Write Time: 1.7569458484649658 seconds**

**Parquet Write Time: 0.0912477970123291 seconds**

What am I missing? Why would anyone choose Avro over Parquet, if it sucks with writes too?

Code for refrence:

&#x200B;

>import time  
>  
>import random  
>  
>import string  
>  
>from avro import schema, datafile, io  
>  
>  
>  
>\# Example Avro schema definition  
>  
>avro\_schema = {  
>  
>""type"": ""record"",  
>  
>""name"": ""User"",  
>  
>""fields"": \[  
>  
>{""name"": ""id"", ""type"": ""int""},  
>  
>{""name"": ""name"", ""type"": ""string""},  
>  
>{""name"": ""email"", ""type"": ""string""}  
>  
>\]  
>  
>}  
>  
>  
>  
>\# Generate random data for writing  
>  
>def generate\_random\_user():  
>  
>return {  
>  
>""id"": random.randint(1, 1000),  
>  
>""name"": ''.join(random.choices(string.ascii\_letters, k=10)),  
>  
>""email"": ''.join(random.choices(string.ascii\_lowercase, k=5)) + [""@example.com](mailto:""@example.com)""  
>  
>}  
>  
>  
>  
>\# Write operation using Avro  
>  
>def write\_avro(file\_path, num\_records):  
>  
>with open(file\_path, 'wb') as out:  
>  
>avro\_writer = io.DatumWriter(schema.make\_avsc\_object(avro\_schema))  
>  
>writer = datafile.DataFileWriter(out, avro\_writer, schema.make\_avsc\_object(avro\_schema))  
>  
>start\_time = time.time()  
>  
>for \_ in range(num\_records):  
>  
>user = generate\_random\_user()  
>  
>writer.append(user)  
>  
>writer.close()  
>  
>end\_time = time.time()  
>  
>print(f""Avro Write Time: {end\_time - start\_time} seconds"")  
>  
>  
>  
>\# Parquet is not a native Python library, using third-party library: fastparquet  
>  
>import pandas as pd  
>  
>import fastparquet  
>  
>  
>  
>\# Write operation using Parquet  
>  
>def write\_parquet(file\_path, num\_records):  
>  
>users = \[generate\_random\_user() for \_ in range(num\_records)\]  
>  
>df = pd.DataFrame(users)  
>  
>start\_time = time.time()  
>  
>df.to\_parquet(file\_path)  
>  
>end\_time = time.time()  
>  
>print(f""Parquet Write Time: {end\_time - start\_time} seconds"")  
>  
>  
>  
>\# Test write performance for both Avro and Parquet  
>  
>num\_records = 100000  
>  
>write\_avro('users.avro', num\_records)  
>  
>write\_parquet('users.parquet', num\_records)",2024-03-28 16:54:55
1bliedv,Google sheets —-> ???? ——-> PowerBI,"So basically we have 50+ employees who use google sheets to update daily data. Like how many customers were seen, the why, and where. This data goes through a process from checking the data, posting it and then sending to be billed and finally billing it. All of this is done in google sheets by updating designated column. Multiple employees come to this sheet, do their job and leave it for the next employee.

On the other hand I use PowerBI to build weekly dashboards. First I started to copy paste data from 50 sheets, transform in excel, clean, and load in powerBi and build my report. This turned nightmare when my manager asked for an update of the same report ( coz not all customers are billed the same day/week and we want to see how many where billed vs not billed and why there were nt billed or whre in the process of billing are we for that particular customer). I had to copy paste values again from google sheet, clean and transform and then send report again. Now imagine having to do this every week. Workload only builds up. 

I started connecting google sheets directly to powerbI and refresh it before sending an update and to my surprise all the data was updated and accurate. But I still get overload error which has to do with api overload I believe. Hence, looking for another system to do most of the cleaning and transformation and storing. 

Another problem Im trying to solve is if my manager asks for report on quarterly basis, I need to spend a week gathering data and this cannot be done linking google sheets to powerBi coz im sure powerbi is simply gonna shut down which all those spreadsheets and individual sheets inside those soreadsheets 

We do not have a database system. And sometimes powerbi throws overload error when I have so many sheets connected. 

Im expecting a system that lies between google sheet and powerBI. This system should should be connected to google sheets, and constanly be refreshing data (like streaming data or batch processing atleast 1 refresh every hour) so that we can have a clear number of how many customers were billed in the start of the day vs end. And while this system should transform, clean and store data in a way I would do in powerBI. And then I want powerBI to link to this system to do my analytics without spending much time in transforming while also be able to do quaterly analysis

Ihv asked this question a few times here but because I have limited knowledge of databases, im confused. Ihv been researching alot abt big query, azure and snowflake. But still nt sure which one is perfect for this case. 
",2024-03-23 03:20:59
1bij7z5,Database choices,"I’m trying to work an enterprise workflow and am trying to think of what the right stack looks like.

Cloud at the moment is not a possibility - but the organization will go there one day. It’s just a money thing.

We have contracts with Microsoft and so my front end is forced to be power apps and power BI. We do have licenses for power automate etc. 

We have a very small allocation of dataverse and a decent sized share point. 

My organization has 0 APIs, but are going to build them.

We have a few large databases and a gateway.

My original plan was to do data collection through power apps, power automate into a json, build a rest API with fast API to receive post, route traffic to mongo, store into a Postgres staging area to run Python transforms and into another Postgres as long term structured data, put an API in front of that into an MDM, an API off the MDM into an analytical database, and then an API off of that into the various BI customers and generally run data services.

I’ve recently heard about Apache Cassandra and thought it was interesting. I’ve also heard of people building lakehouses and warehouses with duck DB.

Ultimately I’m trying to figure out what the best scalable databases are and the easiest to interact with. Also.. am I doing my flow right?",2024-03-19 12:38:01
1bfzuwr,I am experiencing burnout ..after 6+ years as DE...any suggestions how have others coped with this?,Writing this to learn from others experience,2024-03-16 06:33:25
1bfppjz,Data engineers...what's a great learning program?,"Okay I need some advice for choosing a training program that will get me job ready for data engineering. I prefer actual training over theory. Are there any good programs that have virtual training for real world examples? Such as building pipelines and using Python.


Background:

Worked as a business analyst, systems analyst and a data analyst for 6 years.

Been using T-SQL for over 6 years. Need to make the jump into data engineering. Have a little experience in Python.






",2024-03-15 21:57:59
1bf5csb,How Figma's Databases Team Lived to Tell the Scale,N/A,2024-03-15 04:15:29
1bbcrrl,Is this a good bundle for a Junior Data Engineer? I'm looking for good ways to spend the last $100 CAD of PD,N/A,2024-03-10 15:20:49
1b9mh05,Need Help: Optimizing MySQL for 100 Concurrent Users,"I can't get concurrent users to increase no matter the server's CPU power.

Hello, I'm working on a production web application that has a giant MySQL database at the backend. The database is constantly updated with new information from various sources at different timestamps every single day. The web application is report-generation-based, where the user 'generates reports' of data from a certain time range they specify, which is done by querying against the database. This querying of MySQL takes a lot of time and is CPU intensive (observed from htop). MySQL contains various types of data, especially large-string data. Now, to generate a complex report for a single user, it uses 1 CPU (thread or vCPU), not the whole number of CPUs available. Similarly, for 4 users, 4 CPUs, and the rest of the CPUs are idle. I simulate multiple concurrent users' report generation tests using the PostMan application. **Now, no matter how powerful the CPU I use, it is not being efficient and caps at around 30-40 concurrent users (powerful CPU results in higher caps) and also takes a lot of time.**

When multiple users are simultaneously querying the database, all logical cores of the server become preoccupied with handling MySQL queries, which in turn reduces the application's ability to manage concurrent users effectively. For example, a single user might generate a report for one month's worth of data in 5 minutes. However, if 20 to 30 users attempt to generate the same report simultaneously, the completion time can extend to as much as 30 minutes. Also, **when the volume of concurrent requests grows further, some users may experience failures in receiving their report outputs successfully.**

I am thinking of parallel computing and using all available CPUs for each report generation instead of using only 1 CPU, but it has its disadvantages. If a rogue user constantly keeps generating very complex reports, other users will not be able to get fruitful results. So I'm currently not considering this option.

Is there any other way I can improve this from a query perspective or any other perspective? Please can anyone help me find a solution to this problem? What type of architecture should be used to keep the same performance for all concurrent users and also increase the concurrent users cap (our requirement is about 100+ concurrent users)?

# Additional Information:

Backend: Dotnet Core 6 Web API (MVC)

# Database:

MySql Community Server (free version)  
**table 48, data length 3,368,960,000, indexes 81,920**  
But in my calculation, I mostly only need to query from 2 big tables:

# 1st table information:

Every 24 hours, 7,153 rows are inserted into our database, each identified by a timestamp range from start (timestamp) to finish (timestamp, which may be Null). When retrieving data from this table over a long date range—using both start and finish times—alongside an integer field representing a list of user IDs.  
For example, a user might request data spanning from January 1, 2024, to February 29, 2024. This duration could vary significantly, ranging from 6 months to 1 year. Additionally, the query includes a large list of user IDs (e.g., 112, 23, 45, 78, 45, 56, etc.), with each userID associated with multiple rows in the database.

|Type|
|:-|
|bigint(20) unassigned Auto Increment|
|int(11)|
|int(11)|
|timestamp \[current\_timestamp()\]|
|timestamp NULL|
|double(10,2) NULL|
|int(11) \[1\]|
|int(11) \[1\]|
|int(11) NULL|

# 2nd table information:

The second table in our database experiences an insertion of 2,000 rows every 24 hours. Similar to the first, this table records data within specific time ranges, set by a start and finish timestamp. Additionally, it stores variable character data (VARCHAR) as well.  
Queries on this table are executed over time ranges, similar to those for table one, with durations typically spanning 3 to 6 months. Along with time-based criteria like Table 1, these queries also filter for five extensive lists of string values, each list containing approximately 100 to 200 string values.

|Type|
|:-|
|int(11) Auto Increment|
|date|
|int(10)|
|varchar(200)|
|varchar(100)|
|varchar(100)|
|time|
|int(10)|
|timestamp \[current\_timestamp()\]|
|timestamp \[current\_timestamp()\]|
|varchar(200)|
|varchar(100)|
|varchar(100)|
|varchar(100)|
|varchar(100)|
|varchar(100)|
|varchar(200)|
|varchar(100)|
|int(10)|
|int(10)|
|varchar(200) NULL|
|int(100)|
|varchar(100) NULL|

# Test Results (Dedicated Bare Metal Servers):

SystemInfo: Intel Xeon E5-2696 v4 | 2 sockets x 22 cores/CPU x 2 thread/core = 88 threads | 448GB DDR4 RAM  
Single User Report Generation time: 3mins (for 1 week's data)  
20 Concurrent Users Report Generation time: 25 min (for 1 week's data) and 2 users report generation were unsuccessful.  
**Maximum concurrent users it can handle: 40**",2024-03-08 12:13:40
1b894aq,Would you make this job switch?," 

I have 2 years experience in the midwest.

Current company (small consulting firm remote):

* **Salary**: 83k (expecting small raise in may)
* **PTO**: 15 days
* **Bonus**: 3-8% but has always been the bare minimum
* **Raises**: bare minimum because of low budget
* **401k**: 1% match
* **Other info:** Company has had 3 rounds of layoffs in the last year. Seems to be slightly struggling but my manager says our team shouldnt be worried.

New company (big health insurance remote):

* **Salary**: 77k (could be negotiable)
* **PTO**: 23 DAYS!!!!
* **Bonus**: 10%
* **Raises**: yearly depending on performance
* **401k**: 4% match + employer also contributes 1.25% of annual salary
* **Other info**: only one small layoff in the last 6 years. happened a month ago

Obviously the pay cut is not acceptable but I will try to negotiate. But the PTO they have is amazing. And I am sick of feeling like im on a sinking ship at the current company. This is more of a lateral move because I will be the same level at the new company as I am at the current company. What amount of base salary at the new company would convince you to make the switch?",2024-03-06 19:43:18
1brc1jh,Switching from Win11 to MacOS/Linux as a Developer,"Hello guys,

&#x200B;

In 3 weeks I am starting a job in a new company. I need to choose a new laptop for work, there are only two possibilities Dell/Lenovo with Linux or Mac M1/M2 with MacOS.   
To be honest I have been a Windows guy for the last 20 years. As a developer, I just use Windows, configure bash as my main shell interface, and WSL for Docker/Minikube stuff. 

And now, you know starting a new project in a totally new technology stack + new OS can be a little bit tough at the beginning, so my productivity will drop for sure.  
And I wonder, what's the better option? You'd choose MacOS or Linux. And how hard is it to switch to a new OS from Win11?  I'm very fluent in UNIX stuff, but I kinda feel that this totally new interface, and new type of thinking, moving around MacOS/Linux will be frustrating for some time.  


  


  
",2024-03-30 07:25:30
1br0tfk,"What's more important for growth, the tools or business impact?","Especially as a new analyst, what is the main driver for career growth? Many say use the latest and popular tools, but not everyone can luck out with a role like that. How do you feel about data analysts who worked in non-tech teams with old tech (ane the lack of commitment to use modern data infra), but used Python and SQL to build the pipelines and infra that so many of the reports rely on. Of course there are transferrable skills and ability to learn, but would you take someone with cloud experience over someone similar to me for an engineer role? For an analyst role? My goal is to have an engineer title but I'm willing to grind it out as analyst first.",2024-03-29 22:04:18
1bj8kvh,Free Course On Analytics Product Management,"I'm working on an email course for Analytics team leads on product management. But since I'm super slow, I could release what I got so far for free.

The idea is this: I've seen that the most challenging part for analytics PMs is discovery, and not really the prototyping or the solution finding, but really digging into who the internal customers for their team really are, and what they really want. 

&#x200B;

So that's what those five short emails are about.   


1. Figuring out that users of BI systems are not the only internal customers, and likely not the most important ones
2. Finding key decision makers inside the company that your team doesn't serve yet  

3. Finding key decision makers for whom you've previously only had an indirect relation (e.g. through an analyst using the BI systems)  

4. Finding a good line of questioning to get to the bottom of the requirements.  


Figuring out that users of BI systems are not the only internal customers and likely not the most important ones. Here's my outline for questions I ask in discovery calls with internal customers:   
***Problem-Centric Questions***

* *Walk me through how you use this X.*
* *What do you do with it then? (Export?)*
* *How does this data help you make better decisions?*  

   * *A less aggressive version: How does this data help you area to make better decisions?*  
*Or: How does your area make decisions with data? Tell me all the steps.* 
* *If it is not you who makes better decisions, who is it? (also pretty aggressive, best used as follow-up with context)*
* *Can you give me an example?*
* *How does this help you move towards our company strategy?*
* *How do you get this data right now? Manually? CSV, some other tool?*
* *What does your workflow look like when preparing for X (sales meeting)?*

*I’ll follow up most of these questions by adapting them, extending them, letting them provide me with an example, or asking about one I have in mind that already exists. I’ll often make sure I cover, at the very least, half of the meeting with problem-centric questions.*

*Then, I move on to solution-centric questions.*

**Solution-centric Questions**

*I will always open the discussion of solutions by saying, “Nothing is set in stone; I’ll come back to you once I discussed everything with the team, but what I can say is we have different ways of implementing this, and I’m not the expert on it, the team is.”*

* *What would you do if we cannot build X? (literally) Both regarding the impact on your work, and what would you do to find a workaround?*
* *If not an X (your product feature), how would it best be delivered to you? Explain how you’d integrate X into your work to make/help make decisions.*
* *If we build this as X, how would your workflow change?*
* *Describe to me how this would change the impact on decision-making.*
* *If you could only get one of the features described, what’s the one that would have the most impact on decision-making?*

  
If that makes sense, please take a look at it here: [https://www.theanalyticspm.com/discovery-course](https://www.theanalyticspm.com/discovery-course) and don't forget to give me some feedback! ",2024-03-20 08:25:36
1bj2psw,Take less pay but more potential for a future in engineering?,"Hi all, in a bit of a very stressful choice to make here, hoping I can get some advice on how what would lead me into a data engineering or analytics engineering role the best

I’ve been a data analyst intern(mainly SQL and PowerBI) at my current company for 1.5 years, and they offered me a full time position for 67k. But I got a job offer for a more business enablement focused data analytics position for 75k.

 The issue is, the business enablement company doesn’t necessarily have any plans to get me involved in any ETL processes of the data  (they legit told me the position is far from any engineering in the interviews as there is 0 transformation of the data done). It’s strictly SQL and PowerBI and whatever sources PowerBI dataflows could use.

My current company knows my career goals, but couldn’t get me an actual engineering position said there will be projects in the future where I will be closer to the data source, and mentioned Python. Is this grooming? I’m honestly not sure, I’ve never done those projects before at this company, but going to full time typically gets our team members involved in a variety of projects, and if they know my goals maybe it would be worth staying?

TLDR: Company I interned at offers 67K and more POSSIBLE potential to expand skillset and transition into an engineering role down the line(there’s a chance it’s a smokescreen). Other offer, 75k, much more business enablement focused, doesn’t really want me going into the engineering path, would likely be much harder to leverage position in the future. Turn down 8k for a chance to get into engineering(and significantly higher pay) in the future?

",2024-03-20 02:23:17
1bi195y,Guidance: Snowflake first approach vs AWS first approach data ingestion and export jobs,"So i have a project where basically data needs to be ingested into a big snowflake table from multiple csv files that are sent to AWS SFTP (and forwarded to S3). The project itself is pretty simple tbh. The high level architecture and workflow is as follows: CSV gets send by customer applications into S3 --> loading into snowflake load table --> some small/basic transformations (joins across data of multiple files) --> database with clean data --> some customers will require export jobs like nested json or csv to their own s3 or whatever.

To be a little bit more specific: the data we are talking about here is very small like 30mb etc. the ingestion is very basic as well maybe some edge cases but overall just parsing CSV.

Our AWS architect made an architecture i will call ""aws first"": Basically whenever anything has to be done he wants me to do it with AWS Lambda and python3. Meaning: Ingestion made with s3 events that triggers lambda that will do simple ingestion. Then the copy process and joins also in a lambda and the export jobs as well.

The second approach i would call snowflake first is skipping AWS Lambda all together and just using Snowpipe that will listen to s3 events and then some UFD functions or stored procedures. For the Export jobs we can use snowpipe and snowflake tasks as well maybe in some cases we would need external functions and then some kind of AWS lambda service but it's probably unlikely this will be a real use-case. The snowpipes and storage integration i would create with terraform snowflake provider.

So i would like to ask the community what approach they would choose here. In my opinion both approaches have disadvantages and advantages:

advantages aws first approach (aws lambda and python)

- most of the project is in aws so aws lambda would integrate very well (IaC) etc.

- patching of software is required anyways since our project has some other tools and apps

- we could implement some additional stuff in the lambda like idempotency

- very easy to understand if the project wants to hire low cost devs in the future for programing integrations

- data is pretty small probably even fits into a python3 dataframe so all the optimisation we would get from focusing on snowflake features probably not worth it

advantages ""snowflake first"" approach (terraform and snowpipe, tasks, ufd etc.)

- leveraging the tools of a platform we already paying for anyways

- less operational overhead, less changes needed in the future

Maybe i can get some guidance on this topic. It just feels counterintuitive to me to build so many custom lambda functions on the other hand there isn't really a good argument to do most of the workloads in snowflake. What do you think? :) Thanks!

",2024-03-18 20:39:09
1bgymae,Getting the most of out an old tool?,"About 6 months ago, my company made the decision to purchase Ab initio. Based on my research, it, along with other licenseing ETL tools, seems to be on the decline. My company has been rolling it out to various teams and mine is up next. We will be expectsd to migrate current pipelines and new ones to Ab Initio. Currently, we use python + SQL scripts ran on Snowflake to transform our raw data ( files in AWS S3) and then just load into Snowflake tables.

I'm not ready to leave this company, but I'm not so excited about developing a skill set in a dying tool. Still, is there anyway to make the most out of this situation? Is there anything to learn diving deep in ab Initio and learning all I can about the tool? Should I use the tool but focus my cv on the challenges I overcame, not the tool used? Or, should I hyper specialize in ab Initio and become an expert who drifts around to different companies to maintain their pipelines, requiring a large salary for this niche and drying skill set.

",2024-03-17 14:18:28
1bahyli,Where to find good data modeling consultants?,"I am a one man data team who is responsible for too much. In an effort to make our reporting tool easier to use the data model needs to be simplified. Our analytics system is a bloated mess in PowerBI and I am planning on rebuilding it, and planning to hire a consultant to help me build a best practice retail data model for reporting using DBT core + metric flow (open source semantic layer). The source data is already being served in BigQuery with an active DBT project so it’s purely a modeling design exercise.

For serious data professionals where might be a good place to search for consultants in this domain?",2024-03-09 13:39:50
1b9rwqe,How to implement Apache iceberg on my data lake?,The docs don’t explain how to actually use Apache iceberg just how to connect to existing icebergs tables. How do you actually implement the iceberg format of your data lake?,2024-03-08 16:15:02
1b6rkhb,Quick and easy anomaly detection in SQL?,"I'd like to run some basic checks on our data in a fairly generic way. Basically, passing a table (or view) and a list of fields for dims and list of fields for measures

And have it slice by each dimension for each measure to see if there is anything that is greater than X number of standard deviations compared to same day last week or compared to rolling week average, for example.

We randomly encounter data quality issues for metrics within a certain dimension which isn't caught by more high level checks.

Example, for our Transactions fact table, we stopped received transactions for a *specific* product type. At the high level, it was almost impossible to see any deviation when charted. but when split by different dimensions, the anomaly immediately pops up.

example trendline for transaction count for Product XYZ

3/1  : 200

3/2 : 250

3/3: 190

3/4: 0   -- or tiny number like 7

We're on SQL Server/AWS Redshift. I might do something in dynamic sql to handle this but is there a \*free\* solution out there? Don't actually need anything really sophisticated, we're a small company.

We're not on dbt, but can dbt generic tests help with this?

There's a few python related solutions that I've looked at, as well but would prefer to do the analysis on the DB engine for larger datasets.

* [https://github.com/ydataai/ydata-profiling](https://github.com/ydataai/ydata-profiling)",2024-03-05 00:44:08
1b5664g,How to handle data where first and last names are the only “keys”,"We receive a large amount of ad-hoc data from customers that needs to be matched up to authoritative data. Oftentimes, the ad-hoc data doesn’t contain any IDs, and we end up matching via first and last name. I cringe just thinking about it, but we can afford an incorrect match if two people have the same name or to throw out some unmatched data because it’s all aggregated and used for reporting general trends. But I still hate that we do this.

How have you guys dealt with data where the only keys are people’s names? We can’t fix the source data and populate IDs, as it’s not our data, it’s exported and sent to us. And we process so many files, it’s not feasible to manually handle mismatched names or name conflicts.",2024-03-03 02:37:26
1bslzft,Does OOP have place in data engineering?,Are ETLs meant to be only created with functional programming or is OOP also applicable? When do you crate a class over a function?,2024-03-31 21:50:13
1bqiruk,What is the importance of A/B testing in data engineer jobs?,"In my understanding, A/B testing is useful when we make some data anlytics to observe a single feature, or when we build ML models for data scientist.

Today while talking to a recruiter for a DE position openings, I've been asked about the experience of A/B testing in the past, and I'm wondering when would a DE need to employ A/B test in their work.

While builing a data pipeline, creating 2 pipelines to test whether a feature is available or not seems not an intelligent way, may took too much time to verify.

&#x200B;

Does any one as DE have experience using A/B test in their work?

Thank you for sharing",2024-03-29 06:53:44
1blyr09,Learning Databricks ,"Any resources for learning of Databricks besides their online materials? Need to get up to speed on it quickly for a new initiative my company is looking to invest in.  I want to understand the common use cases, particularly in healthcare and life sciences, and how companies typically architect/use Databricks.  If you’re in this space already, I appreciate any recommendations or pro-tips. Thanks!",2024-03-23 18:12:38
1bhv9to,Suggestion on etl,"agenda is to extract the data from Redshift and need to load into SQL server


What are someone of the best approachs to do this , fast and cost efficient.",2024-03-18 16:41:27
1bfaon5,Are data lakes mostly used for sturctured and semi-structured data or unstructured data?,"So, data lakes unlikes data warehouses let you store unstructured data (text, binary) - does it mean that they are mostly used for this type of data? Or are data lakes mostly used for structured and semi-structured data? But then why choose data lake if we can use data warehouse. What is your experence?",2024-03-15 10:22:12
1bf57d2,I manually enter invoices in access,"Hi, I have a job where all I do is enter the $ amount, vendor name, and invoice # from a pdf invoice that is received in an outlook inbox into a Access database, There has to be a way to make it less manual.

For context I work for a government organization that receives 100s of invoices from many hundreds of different vendors every day.

Where should I start?",2024-03-15 04:07:16
1bdp3jf,"The future of Data Engineering: Beyond traditional ETL ""Connectors""","Hello folks, dlt co-founder here!

We're on a mission to shake things up in data engineering by moving away from the repetitive and into the innovative, especially with source generation. Wondering about the future of connectors and traditional ETL tools? We see pipeline generation not just as a possibility but as our current reality, supported by the case study in the article.

The real question now isn't if we're generating sources, but how extensively we're doing it. Building a single data pipeline involves numerous decisions, and we're harnessing automation, LLMs, and the OpenAPI spec to streamline this process. While relying solely on automation tends to lead to compounded errors, a collaborative approach with human oversight is proving effective.

What's the future of ETL libraries in this evolving landscape? Our experience and user feedback suggest a significant shift. With 500 private sources created last month alone by the dlt community without any code generation, we hope that empowering the community with better tooling and growing the community will 100x this process within the year.

Want to dive deeper? Check out our [latest blog post](https://dlthub.com/docs/blog/code-vs-buy) and the related case study. 

You can try the code generator we created last year (in the post), but you should probably hold off for our update towards the end of the month.  


&#x200B;",2024-03-13 11:25:28
1bb3alv,What are the tools you use to integrate AI in your dags?,"How are you orchestrating AI calls in your pipelines. I often come across the case that I have to run classifications on subsets of data, which is then processed further based on these classifications.   


The classification might run on raw documents (e.g. decide whether it is an invoice, an email, or something else) or on text. Mostly the data is either reinserted into the database or another stage of the lakehouse, but sometimes it is also passed to the next processing step.   


I would love to know what tools you are using and how you are orchestrating these AI calls. I personally often have to rely on serverless AI deployments, which I autoscale, so I try wherever possible trigger it with a simple Lambda, which calls the different AI endpoints and alters the data and then reinserts it wherever necessary. I would love to know whether you deem that a bad approach.  


If you know or have seen any other interesting libraries or tools, drop them below.  ",2024-03-10 06:11:00
1bazt75,"Roast my data project & video editing skills. I made a python script that acts like a Google Tasks plugin on Obsidian, the note-taking app. Works with the watchdog library to watch for file changes in your Obsidian vault's daily note. Code in comments!",N/A,2024-03-10 02:55:16
1b8uryg,need recommendation,"hi guys, I am in a bit of a pickle at work. I will tell you my story and I would appreciate  if you could help me with identifying, if their reasons are plausible.

So I am a data analyst in an retail company (lets call it company J) with low data maturity. 

The company has server on-premise which is handled by our IT, there is also a ""processing"" server which is handled by ERP wannabe, but actually accounting software company (lets call it company A).

The issue is, before the company hired me, they depended on the company A to build reports in their software, but it takes them forever and/or don't do it because the load will be too big on the servers.

Now, I have to export excel files from company A's software and I am very limited in the amount of data. The software allows me to download sales of 3 months at a time. 

So I talked to company J's head of IT, he said - hey ask company A to give you access to their processing RDB server, let them give you ""read-only"" permission, should be all good.

I called company A they tell me - we will give you the access, but we won't be responsible for ANYTHING that happens to the servers. Even if we give you read-only, there is still a threat of SQL injections (they said something can be done with sql that still could be a threat). Another option is you set up a server for analytics and we will transfer the data you require.

I mentioned this to head of IT at company J, he was pissed, he said - they are full of sh#$t. If we add extra VM to our cluster, the load will be too big and will crash possibly. There are no such issues that company A said in modern day...

&#x200B;

Is the SQL injection threat really a bs? Is the advice of me getting a read-only permission to a processing server a bad advice?",2024-03-07 13:34:23
1b8it55,Which courses compete with or are alternatives to Data Talk Zoomcamp?,"I'm not here to downplay Zoomcamp, just to explore new options and make a more informed decision.

The community strongly recommends Data Talk Zoomcamp for DE, and taking it is probably the next step I should take in my studies.

Are there any other alternatives that the community recommends at a similar level of:

- content quality
- preparation for real-world Data Engineering problems / jobs
- financial accessibility

Thanks",2024-03-07 02:25:32
1b7meq0,"Let’s have a talk about connascence, shall we?","I’m just shocked… flabbergasted, actually, that my entire degree had no mention of this word.

Connascence is a concept similar to dependencies and coupling. If a change in one part of something requires changes in other parts of something, that’s connascence. Apparently there’s a decent amount of academic work done on this subject, fresh for the study. 

My first impressions is that you can probably explain very well why Docker is so successful, with only a solid understanding of connascence and how servers / applications work. It minimizes connascence between the machine and the app.

There are categories: static connascence and dynamic connascence. Both have subcategories, such as value connascence, identity connascence, name connascence, …

I’m finding this neat because it makes talking about an abstract and difficult topic, easier. I’m suddenly making realizations about many of my systems and their connascence, how I could change their connascence measures…

This is cool stuff, guys.",2024-03-06 01:05:42
1b79p9w,Help me understand Test Driven Development within Data Engineering work.,"Greetings,

I am seeking assistance to comprehend how TDD (Test Driven Development), a concept from software engineering, is adapted for data engineering. As a data scientist, I want to stay informed about trends in the data domain, though my expertise is not primarily in data engineering.

I grasp the core principles of TDD in software development, but its application in data engineering eludes me, despite reading numerous articles and examining some training examples that appeared impractical to me.

My experience mainly involves external data acquisition, especially in the finance sector. In data engineering, if one constructs a pipeline to ingest data from an external source for ML models, setting up this pipeline constitutes the crux of the work, besides establishing the necessary infrastructure, correct? With TDD, the focus would be on the methodology for coding transformations along the pipeline. However, how can one formulate sample, edge, and corner cases for transformation unit tests without first experimenting with data from the source? Incoming datasets and sources seem far more stochastic, dynamic, and complex than what a typical software module faces, making it challenging to establish test cases and expected results without preliminary data experimentation. And it seems to me, one needs to experiment with the transformation steps to reach to meaningful test cases. But then the whole idea behind TDD is lost if you have the code before the test case, is this not true?

For example, I have seen some training example with Python-based pytests for simple Spark row-wise transformations on streaming CSV data, like breaking down a date column into its components. This are just atomic transformations, based on built in functions which should be rather tested by the developer building that specific module (e.g. pyspark), am I not correct?

In my experience, data pipeline failures often result from changes in incoming raw data rather than code errors in basic transformations. These anomalies in data, such as definition changes, are hard to anticipate with testable code. Wouldn't it be more practical to focus on detecting quality issues at certain points and testing the data itself?

Furthermore, most problems I've encountered were not something to capture by row-level windows. Many transformations involve complex aggregations and windows, where checking longitudinal distributions and detecting drifting is critical. A basic example is a time series panel, which should contain all business days within a range, and you need to make sure if there are no gaps in the data. However, preparing for these issues seems more aligned with data contracts and data testing (like dbt data tests), rather than TDD. While I recognize dbt's new unit testing capabilities and understand their [example](https://docs.getdbt.com/docs/build/unit-tests) of unit testing an embedded complex function in a SQL query, such scenarios seem unrealistic in my previous work environment. Failures typically rather stemmed from incoming data changes and definition shifts, making tests around these aspects more critical.

I would appreciate any insights on applying TDD to data engineering work, especially in scenarios involving the ingestion of dynamically changing, low-quality external data from various vendors.",2024-03-05 16:37:29
1b5nkfu,Data Science vs Data Engineering,"Would like insights from other data professionals about being a data scientist vs data engineer. 

I have worked in data for a few years now, currently employed as a Senior Data Analyst. Among many different roles in my career, I’ve learned a lot about gathering and cleaning different data sets to prepare for analysis. In my current role, I’m constantly partnering with other departments to gather and align data requirements for analysis. I also build and manage dashboards in PowerBI, with daily business facing analysis and socialization of trends and KPIs.

Work life balance is my top priority. I work remote and emphasize working efficiently, not long hours. I do not want to be in meetings all day.  ",2024-03-03 18:07:39
1bsqx9p,Advice needed. What do you do to understand business and data quickly?,"I have been put into a data engineering project at work and I know very little about the project. All I know is that it is a dashboard and reporting tool my company bought from a vendor, and for some reason, leadership wants to replace that tool with something I build in-house. I will be working on it alone and leading everything (requirements gathering, architecture, development, testing, deployment, maintenance). 

I am happy about the project as it will give me good experience, but **I am a little worried because I struggle at understanding the business and data quickly**. I don't know why that is. I think I am just slow. The information during meetings becomes too much too quickly, and I just get lost eventually. I don't want to look dumb in front of lots of people, so I say I understand when I really have not. Then, it becomes a struggle to eventually understand the business and data.

What do you do to understand business and data quickly? Do you have a template of questions or patterns of questions you ask? How do you know what to build quickly and correctly?",2024-04-01 01:22:18
1bqyj44,"UPDATE: Avro slower than Parquet for frequent writes, but why?","[\[previous post\]](https://www.reddit.com/r/dataengineering/comments/1bq0g7c/avro_write_time_is_worse_than_parquet_but_avro_is/) attempted to benchamark Parquet vs Avro, but you pointed out at flaws of my experiment

One comment suggested the following:

Avro should do better than Parquet in wirte-heavy workload where we do a lot of small writes to a file. Avro is mutable so we don't have to load the whole file to write to it. While Parquet is immutable and we have to load a whole file to memory, so Parquet should do worse.

My experiments shows, that Parquet is still better in this scenario:

PARQUET (1000): Execution time: 4.640063285827637 seconds

AVRO (1000): Execution time: 7.076133728027344 seconds

What am I missing?

Code:

    import pandas as pd
    import pandas as pd
    import time
    import fastavro
    import pyarrow.parquet as pq
    
    iter_num = 1000
    
    # PARQUET ###############################################################################################################
    # Create parquet with 1 record
    data = {'id': [1, 2, 3], 'name': ['John', 'Jane', 'Alice'], 'value': [10, 20, 30]}
    df = pd.DataFrame(data)
    df.to_parquet('scores.parquet')
    
    start_time = time.time()
    
    for i in range(iter_num):
        # Load parquet with n records
        df_parquet = pd.read_parquet('scores.parquet')
    
        # Append 1 record to this file
        new_row = {'id': 4, 'name': 'Bob', 'value': 40}
        df_new_record = pd.DataFrame([new_row])
        #df_parquet = df_parquet.append(new_row, ignore_index=True)
        df_updated = pd.concat([df_parquet, df_new_record], ignore_index=True)
    
        # Save it
        df_parquet.to_parquet('scores.parquet')
    
    end_time = time.time()
    execution_time = end_time - start_time
    print(f""PARQUET ({iter_num}): Execution time: {execution_time} seconds"")
    
    # AVRO ##################################################################################################################
    # Create avro with 1 record
    data = {'id': [1, 2, 3], 'name': ['John', 'Jane', 'Alice'], 'value': [10, 20, 30]}
    df_avro = pd.DataFrame(data)
    avro_schema = {
        ""type"": ""record"",
        ""name"": ""Person"",
        ""fields"": [
            {""name"": ""id"", ""type"": ""int""},
            {""name"": ""name"", ""type"": ""string""},
            {""name"": ""value"", ""type"": ""int""}
        ]
    }
    with open('scores.avro', 'wb') as avro_file:
        fastavro.writer(avro_file, avro_schema, df_avro.to_dict(orient='records'))
    
    start_time = time.time()
    
    for i in range(iter_num):
    
        # Load parquet with n records  
        with open('scores.avro', 'rb') as avro_file:
            avro_reader = fastavro.reader(avro_file)
            records = [record for record in avro_reader]
            df_avro = pd.DataFrame(records)
    
        # Append 1 record to this file
        new_row = {'id': 4, 'name': 'Bob', 'value': 40}
        df_new_record = pd.DataFrame([new_row])
        df_updated = pd.concat([df_avro, df_new_record], ignore_index=True)
    
        # Save it
        with open('scores.avro', 'wb') as avro_file:
            fastavro.writer(avro_file, avro_schema, df_updated.to_dict(orient='records'))
    
    end_time = time.time()
    execution_time = end_time - start_time
    print(f""AVRO ({iter_num}): Execution time: {execution_time} seconds"")





",2024-03-29 20:21:05
1bpwtm5,Views vs materialized views,"Sometimes you can curate a dataset virtually sometimes you may need to materialize into a physical subset.

What are your rules for when one or the other is appropriate. What are signs of leaning too much on one side or the other.",2024-03-28 14:22:48
1bonpws,Does anyone do anything with NFC and/or RFID?,"Do any of you guys work in a niche that lets you play with programming / reading from RFIDs or using the NFC protocol in order to collect, transmit, or store data? How about physical sensors?

I’m curious what flavor of data engineering you typically enjoy on a dataly basis. I have been reading about these little things and they’re pretty cool. I just bought a BuildYourOwn light sensor kit online, gonna play around with it here soon.

What do you use them for, personally and professionally (if you’re allowed to share)?",2024-03-27 00:08:05
1bo1qwr,Career advice regarding new offer,"Hi, I wanted to get some career advice. I've been working as a Data Engineer for almost 5 years, but a huge part of this job was just Python development (APIs, libs, microservices...) and another part was building pipelines, using spark, Kafka, SQL, NoSQL, data bricks,  Airflow and others. Currently, I am working on another project in Hadoop on-prem where I'm building spark streaming apps - but I don't like Hadoop and the on-prem ecosystem, and I think also that it's legacy stack.   
Because of these above, I have very poor, almost 0 experience and knowledge in Cloud.

That's why I wanted to ask you for advice. I got an offer for a Data Engineer on AWS - the stack there is very simple like s3, kinesis, glue, Athena, Ecs, Rds, and Redshift - and the team has only one Data Engineer who basically is also on his road with AWS, but already passed some certificates.  
In this role there is a very small part of coding - like 10-15% of the job. But they are okay, that I don't know AWS and they will give me time and space to learn it and to pass certificates.  
And I wonder if I should take it, on one side I will learn AWS, probably a couple of DE-related services, and I will pass the exam. On the other side, I won't code too much, and probably everything will be pretty low quality, as there is no collaboration with DevOps, SWEs, and other DEs into good swe practices (testing, high quality of code, design etc).   


I'm having a bit of a headache with this. Do you think this is a good idea and a step forward in your career - I think it's a position for at least a year. But will it be a step backward - especially since I have been working more in programming like data engineering (writing code 80% of the time) for the last 5 years?  


  


  


&#x200B;",2024-03-26 07:17:16
1bm7g5n,"""Queueing"" table inserts and deletes to avoid deadlocking?","EDIT: I'm working on MySQL 5.7


I'm new to this community so apologies in advance if this isn't allowed and I will remove. I'm kind of on my own at work so I thought I'd see what you all think because I've been noodling on this for a few days.

One of my recent projects ended with me writing a number of SQL queries that are used in tasks. They add or delete rows from a production table meant to show near-real-time analytics to customers. Depending on the trigger my queries do some transformation and add or remove rows from the table, which determines what is visible to the customer. 

If the task that is removing some rows runs at the same time as a task adding rows, the table obviously deadlocks. 

I think the best thing to do would be for the tasks to queue up and only run when the previous task is finished, but that's not my jurisdiction. The stopgap my colleagues put in is a fallback that basically reruns the tasks if there's a failure. 

For me to be able to fix it I'd have to do it within SQL, stored procedures, or adjusting the data model. I've been trying to think of a way to maybe queue up all the adds and deletes in one place and apply them every few minutes but that doesn't seem scalable. If anyone has any input I'd love to hear it!",2024-03-24 00:22:05
1bm3wsz,CI/CD and Code Versioning on AWS," Hello fellow data engineers! 

I recently switched companies and I'm diving into cloud services more extensively than ever before. Previously, I've worked with AWS but approach was waaay different, also I worked in a company that used Snowflake and BigQuery + GCS at another. This new role introduces me to a range of AWS services like Lambda, EC2, Kinesis Data Stream, Kinesis Firehose, Glue, Redshift, DMS, EMR, and more.

In my previous experiences, we always had code versioning and CI/CD processes using tools like Jenkins or GitLab. Usually, I would create a feature branch from the development branch, commit changes, and push them. After a review, the CI/CD system would handle the deployment to the development environment, and later to production. Production was managed solely through CI/CD pipelines.

However, in my current role, the approach is different. Instead of uusing CI/CD for deployments, my team directly writes and tests code on AWS, starting with development tables (code testing), then moving to a staging tables (data validation I guess?!) before deploying to production. This methodology seems to bypass the traditional CI/CD pipeline approach (hands OFF the PROD).

I'm grappling with the concept of having only one AWS environment (production) and testing everything there directly. It raises questions about the necessity of CI/CD. If the Lambda function works in the development environment, does that mean it will work in production without any additional checks or safeguards?

In my previous experience with Airflow, we maintained separate development and production environments. Changes were tested in the development environment, and upon approval, they were merged into the production branch triggering builds, tests, and deployments automatically and DAGs would be present on Prod without me ever laying a hand on it.

I'm curious to hear about your experiences with implementing code versioning and CI/CD on AWS using GitLab or GitHub. How does your company handle these processes? Thank you for sharing your insights!",2024-03-23 21:47:09
1bl3z33,How would you generally debug a heavy skew in a spark job?,"Recently I found myself having to try find the causes of slowness in an AWS Glue Spark job. Eventually I landed on a stage that had 99% skew (see screenshot).

https://preview.redd.it/pb4bgm4zywpc1.png?width=1884&format=png&auto=webp&s=8aa8e7c3c8da88bcf751f3278b6e9b73432c852c

It's my first time having to dig this deep into Spark so I was a bit out of my depth. The job itself also didn't help, because it's basically a single step like this:

    spark.sql(a_700_lines_sql_query)

The query isn't even that esoteric, but it joins many wide (and relatively long) tables, all at once at the end of it.

By looking at the DAG, I found out that this specific stage (which is also the longest one), happens in correspondence of one of those joins. Knowing that skews happen (also?) because of uneven distribution of partition keys, I've checked the join keys of the 2 involved tables, but found nothing dodgy.

I'm a bit at a loss here so my question to you is: in general, how do you address situations of this kind?

Assume that I can't touch the SQL query itself because it was written by another team we have no control over.",2024-03-22 16:51:42
1bjprcv,I’m tired of the same kind of projects,"Hi, I’ve been looking for passion as 8 years ago when I started to program in Java for a bank company, during this time I’ve realized I always had been involved in projects of the same kind, you know a monolithic application with lasaña pattern to fetch data or insert data or update in a multiple tables on the database. Since 4 years micro services appeared as the revolutionary vision to build business applications and well we now have to be proficient troubleshooting issues on kubernetes. But in the core is the same, a vast set of microservices with its databases and patterns just doing a CRUD operations.

I’m a software engineer working with Java, and I’ve been working with Java version 8 for the last 5 years. Yes I am here to capitulate that too less companies are migrating to Java 17 for example.

Nowadays I started to be interested in C# and Dotnet, when I tasted some C# elegant code I felt excited again, because the new features of the language and how it is curated the ecosystem around dotnet.

When I was a teenager I was a Linux fan boy terrorizing hard disks with Windows and formatting them with a Linux distro, the perfect anti-Microsoft guy. But when you start to work in the real world and you’re enough mature, you start to see things in a pragmatic way. 

My questions here is:

1.- how do you keep interested despite mostly of the projects that you’re involved are not necessarily exciting things that implies you have to learn new cool stuff? Such as cloud services, new tools or new versions of the framework and programming language!

2.- I’m thinking in doing a horizontal movement and try to get a position as dotnet developer or maybe data engineer with Python to challenge myself or c++ templates or Scala to be a functional ninja, do you believe am I being biased with my own opinions?
",2024-03-20 21:59:56
1bernd3,Var vs varchar for uuids,"I've recently joined a company that stores all Primary Keys for all tables as UUIDs instead of integers.

And for some reason all these columns' dtypes are `VARCHAR(64)`. I know that storing them as `CHAR(36)` would be better.

I've tried to run some benchmarking to backup my decision, but found almost no storage benefit, and minimal query performance improvement (for simple selects, and also for joins).

Would it really make a difference or should I disregard this initiative?

PS: we're using mariadb, and I've benchmarked by creating three identical 1M rows table with varchar64, varchar36, and char36.",2024-03-14 18:05:44
1beo2yq,What architecture would you suggest for a Real Time Streaming project?,"Hi all,

As a DE, I am being part of the developing process of an architecture for our next company project. Given the significant challenges presented by the project, I am seeking some advice. Here's the concept:

We need to fetch data from tables belonging to different databases to feed a Digital Twin, which must mimic the behavior of its physical counterpart. The tables from these databases can be of two types:

* Static (meaning they update or add their rows every *n* minutes or hours).
* Temporally ordered (i.e., Time Series).

It is highly likely that the observations, which are to be fetched every *n* seconds, may require processing. Once processed, this data should be pushed to a database for historical purposes AND feed the Digital Twin in real time (where, by 'Real Time', I mean that it gets updated every *n* seconds).

Also, note that the user might also want to roll back in time to view a historical window of past events on the Digital Twin itself.

I am primarily looking for tools/frameworks that you would recommend for managing such a project. The project will be handled entirely locally.

Thank you in advance",2024-03-14 15:37:46
1bd25o2,Dagster's Open AI integration,"[https://dagster.io/blog/dagster-openai](https://dagster.io/blog/dagster-openai)

The new dagster-openai integration lets you tap into the power of LLMs in a cost-efficient way.

https://preview.redd.it/ovkzr6rjoxnc1.jpg?width=1200&format=pjpg&auto=webp&s=a7ecab5ae764f9a3ce77612374c19fa2442aa8b0",2024-03-12 16:54:32
1bc9ree,On Apache Iceberg and snapshots expiration,"I'm currently experimenting with Iceberg, and there are some aspects I don't think I fully understand yet.

The scenario I'm testing involves a dataset containing a customer's information about purchases and other preferences. I need to remove (masquerade) some of this data to comply with GDPR or similar regulations. Could Iceberg be the right tool for this?

What I've done so far is load a set of pre-existing Parquet files into an Iceberg table. This generates an initial snapshot that cannot be expired. Am I mistaken?

In the following days or months, the dataset grows until I receive requests to remove a customer's information from my database. At that point, I proceed to delete or anonymize these records. Then I manually expire older snapshots. However, if this customer's information was included in the first import, it will always be present in my dataset, correct?

Regarding the statement in the Iceberg docs: 

    Data files are not deleted until they are no longer referenced by a snapshot that may be used for time travel or rollback. 
    Regularly expiring snapshots deletes unused data files. 

How can I predict if a Parquet file is referenced or not? Even if I expire a snapshot and the data is no longer queryable with time travel, there is always the possibility that it is present in some Parquet file because it contains other data that has not yet expired.

What am I missing here?",2024-03-11 18:05:55
1bapiy7,Has anyone used Scantron sheets yet?,"I'm building systems that business users will eventually rely upon to do their job. Data will be collected via Webapp, asynchronously from hundreds of remote super intendants at any time of day. This will get cleaned and funneled into an analytical database so that project managers can review and make changes over time. In a way, it transitions the state of their project from implicit to explicit, as the most current state of any project is now tracked and represented via the database. Eventually, project managers should rely completely on my system for the service it provides.

This means that if super intendants can't access my web app for ANY reason, they can't do their job. That's unacceptable to me.

As a failsafe, I'm thinking of creating a physical version of the data collection form via Scantron. I've never done this though ..

Would anyone have some insight into the kinds of problems I'm likely to face and what sort of tradeoffs are made when choosing a solution to those problems?",2024-03-09 19:12:15
1b9vh4s,Bill of materials table - SQL help," A very common problem in querying ERP or manufacturing MRP systems is that they use “self-join” tables to store their part information. These tables store “bill of materials” or “BOM” information, where complete assemblies of items can be found.

I have this table with approximately 200k rows, that's similar to a bill of materials. For example: product A1 is made of 2 A2s, A2 is made of 2 A3s and A3 is made of 2 A4s, so on and so forth. We have thousands of products in this table.

Basically I need to transform this in a table that says A1 is made of 8 A4s, and does this for every product. We need to know which and how many items the products have in their final layer. I've tried doing a bunch of joins in SQL but the problem is a don't know how many layers each product can have.

Do any of you have an efficient way to approach this?

&#x200B;

[Simple example](https://preview.redd.it/2h3ygsyul5nc1.png?width=679&format=png&auto=webp&s=1c3bcb0b000f8ad5e05c4342b7bbec474b721e7b)",2024-03-08 18:34:34
1b74adw,Data Consulting Club – Medium,"Hey everyone, we've just created a new medium publication on content for running & scaling data consultancies. The link is this one: [Data Consulting Club](https://medium.com/data-consulting-club).

&#x200B;

The first article is on selling data services to non-data-savvy businesses; I hope you enjoy it.

&#x200B;

I know it sounds like an arcane topic, but really I looked almost everywhere, there is no single source of content out there that helps with scaling data consultancies, and yet there are thousands of them just inside the US. 

*Enjoy, feel free to comment, share ideas, questions for content!*

&#x200B;

&#x200B;",2024-03-05 12:42:26
1b69w5a,How to dockerize spark and mage.ai,"Hi everyone, I'm a software developer that want to try data engineering.  
I'm attending Data Engineering Zoomcamp by [Datatalks.club](https://Datatalks.club). I just complete the week 5 about batch data processing. I just want to elaborate my knowledge from orchestration ([mage.ai](http://mage.ai/)) and spark (or pyspark whatever).  
I want to know, is it possible to dockerize [mage.ai](http://mage.ai/) and spark? because in the material spark only running on local machine.",2024-03-04 12:37:05
1b5i9t0,"Is there a paid service that can ingest csv files, then they get uploaded to my database?","I want a service which provides an sdk i can through my node js app and it ingests the data in to postgresql. The files will be csv, my current implementations crush the node.js server because they can't fit into memory",2024-03-03 14:20:45
1b5bclz,Career decision,"Hey folks currently I am working as a data migration engineer for the past 1.8 years.This is my first job too.My main job is to analyse different database technologies and maintain SQL script codebase according to business logic.The rest is automated and is just a button click.The pay is decent and I have a great WLB.But the thing is I am not learning anything rather than some business logic which ain't any use to me personally.I am getting guilt that I am wasting my initial years of career cuz that's where all the learning happens.So I have been thinking of job switch,but the problem is I haven't learned anything properly except for a little bit of SQL and c# (noob level mainly chatgpt)for automation tool development.Either way I need to grind to switch, so my question is whether I need to grind on data engineering and switch to data field, but I have been hearing the field is routine mundane and it gets boring after a time and the payment won't build up after certain years.(Is it true?)The switch to data field would be easier path for me as I need to put more effort in learning coding.So experienced people should I switch to data field or transition to a developer role(longer and harder route).What do you all suggest",2024-03-03 07:26:50
1b4r198,Is it worth Getting going for a Masters in Data Science?,"Context: I have some work experience and have freelanced in this field - Built data pipelines(data engineeiring) and BI dashboards. Mostly inclined towards Azure. I have some other clients as well that are related to that of a software architect. I am considering my options to see if a masters can give me a significant jump. 

Does it necessarily give me an edge? Alternatively, what's the next best thing to do if not a Masters? Would it be working on projects and building my portfolio?",2024-03-02 15:38:26
1b4pxjb,One data warehouse or two for different divisions of the same company ,"My company has two separate main business functions. One involving distribution of food products, and the other being the selling of custom merchandise and apparel. There’s been some discussion on if in the future it would be beneficial to have one single data warehouse that is shared by both companies or having two separate systems for both. 

The companies currently operate separately now, as one company was acquired and never entirely brought under the same umbrella as the parent from a technology standpoint. Both currently have separate systems and databases, but will be upgraded over the course of the next few years (one before the other).

Does it make sense to design a warehouse that can accommodate both under one roof? Or would it be wiser to keep these two divisions separate from a data warehouse perspective?",2024-03-02 14:48:23
1brff27,Enabling better pipeline versioning with only S3 parquet datasets,"Currently all of our data lives in parquet datasets that are stored in S3, written from python/pandas/awswrangler with no other fancy tools. While the dataset itself isn't very large (<100 million entries, 7 columns, growing slowly) the way this data is used means that some of it may be overwritten in the pipeline (because they get aggregated from other values if certain columns are empty etc..)  
A -> B -> C  
All of the dataset gets used in A, some if it gets modified in B and C but if I was ever to run this again and B and C haven't changed, I could theoretically use everything that was produced from C. The same if C has changed, I could just use everything from B onwards. I think that I could handle this complexity by using the versions of each step and storing full datasets per version. My problem with this is it multiplies the data enormously and I would need better tooling to look up what I need. I could also only store a portion of the data that has been impacted by that step, so my query would become a combination of the raw dataset and the delta of what has changed. But I would need have a much better way of querying. Currently i've explored using spark, athena, glue, different partitioning schemes (hive), but to be honest i'm kind of lost between them and i'm failing to simplify the problem. Eventually this data needs to be displayed 'live' and it is possible that the pipeline will need to be rerun in real-time, so I have every interest in improving how data is versioned along the pipeline  


Has anyone had a similar use case, or is using similar types of data that they can offer their opinion on the best stack/tools to be using?",2024-03-30 11:14:40
1bquxtu,What big data file types do you use for unstructured data?,"I mean .jpg, .mp4, .txt when we store CSV in data lakes we convert them to parquet files, and what do we do with unstructured data like pictures, video and text?",2024-03-29 17:27:59
1bqt78u,Date control logic in a PySpark architecture: better alternatives?,"I currently work with the Spark Hadoop architecture, running Pyspark for most of the pipelines. There's also autosys and shell scripts to manage job scheduling. 

I'm trying to abstract some of the logic away from shell scripts because I absolutely hate working with them, and with some of the larger files it gets really difficult to maintain these scripts without something breaking. A key operation in the shell scripts is date control. Currently, we use a file to save the date of the last successful run in a location on the edge node, which gets read every time the process is triggered.

This seems a little hackey to be honest. Are there any better alternatives?",2024-03-29 16:16:05
1boyiuc,Database - where to start?,"
Hi, r/dataengineering,

I recently started as a data analyst in a company that doesn’t have any data / IT department, so im pretty much on my own.

We get data from different sources through API’s. I have mainly used R to pull down the data with these.

My boss wants to have a database at some point, which stores all the data from our different sources, so we can extract the data from there.

I have been given time to look into possible solutions, as they know I dont have experience with this. What would be the best solution in this case? Cloud based? External? Is it possible to build databases with data through API’s?

A bit of info:
- Four different data sources at the moment (all with paid API’s).
- New data daily (some thousand observations every day).
- The company wont hire more people to the data department in the near future.
- Only a few people will extract data from the database
- Use case is to connect the database to visualization tools to automate

Where to start?",2024-03-27 10:25:32
1bogg92,🚀 Launching Mycelial: A New Way to Handle ETL Without the Cloud - Seeking Your Feedback!,"Hey Reddit,

I'm part of a team that's been grappling with a challenge I think many of you can relate to: the inefficiencies and security concerns of traditional ETL processes that rely heavily on cloud transit. It led us to ask, ""What if there's a better way?""

So, we built Mycelial - an open-source platform designed from the ground up to enable direct, point-to-point ETL. This means your data moves from A to B without being forced to detour through a central cloud, eliminating unnecessary latency, reducing costs, and enhancing data security.

**How Mycelial Works:**

Crafted with love in 100% Rust, Mycelial leverages local daemons to execute data workflows, guaranteeing fast and secure data movement. It features a universal interface between data sources and destinations, meaning once you're connected to the Mycelial network, the possibilities for your data are endless.

**Why we think it's a game changer:**

* **Security**: With data breaches on the rise, keeping your data off public cloud networks reduces exposure.
* **Efficiency**: Direct transfers mean quicker ETL processes, getting your data where it needs to be faster.
* **Cost-effective**: Less reliance on cloud infrastructure translates to lower operational costs.
* **Control**: You maintain complete control over your data's journey, end-to-end.

We're thrilled (and a tad nervous) to introduce Mycelial to the Reddit community. Your feedback, whether it's praise, criticism, or anything in between, is invaluable to us. We're committed to transparency and community-driven development, so we're all ears for your thoughts, concerns, and questions.

**Be Part of the Beta Test:**

Mycelial is in beta, and we're eager for testers to push our platform to its limits. Your insights will directly influence the evolution of Mycelial, helping us refine and perfect the platform. Visit [www.mycelial.com](http://www.mycelial.com/) to sign up and start exploring the possibilities.

Feel free to AMA about the technology, our development journey, or any burning questions you might have. Your input is crucial in our mission to make Mycelial the best it can be for the community.",2024-03-26 19:21:11
1bnkubv,snowflake+dbt," 

Hello everyone,

I have just completed learning Snowflake and dbt. Is building a project the only way to become proficient?",2024-03-25 18:24:00
1bmpt0o,☘️Green Data Engineer ✳️,"
Hi everyone,
Data Engineer here (10 years strong!) who recently shifted to a leadership role. While I love the new challenges, the environment is my true passion, and I'm itching to use my skills for good.

Calling all green data engineers!  
Are you working on any cool sustainability projects?
Are the salaries in this space are close to other DE  opportunities?
Sharing your experiences (project types, how you found them) would be a huge help!

Looking to leverage data expertise to make a positive impact on the environment.

Thanks in advance!
",2024-03-24 17:11:20
1bl73gj,Cannot realtime streaming with Spark Structured Streaming!!,"Hello All,

I have been struggling for hours to get this simple real-time streaming feature to work and desperately need some help.  


I am trying to achieve a real-time streaming application with Spark Structured Streaming to calculate the average value of power aggregated over the latest minute. So say if the time now is 18:15:00, I only want the application to write the average value calculated between 18:14:00 - 18:15:00 to the console. The frequency to write to console is set to 10 seconds and it should only output the MOST UPDATED averaged value.  


This is the simplified code: 

    aggregated_df = input_df \
        .groupBy(
            F.window(""engine_time"", ""60 seconds"", ""10 seconds""),
        ).agg(
            F.format_number(F.avg(""power""), 2)
        )
    
    # Some more transformation
    
    output = res_df.writeStream \
        .outputMode(""update"") \
        .format(""console"") \
        .option(""truncate"", False) \
        .trigger(processingTime='10 seconds') \
        .start()

The problem with this is that the averaging window depends on the event time (i.e. engine\_time) instead of the actual time now. The engine\_time can sometimes be delayed. If it's 15 seconds delayed, the whole averaging window will be shifted by the same amount.  


If the outputMode is set to ""complete"", all past rows are shown which is not helpful.

If the outputMode is set to ""update"", it will show the 10-second-window rows that are updated which is not helpful either.

If the outputMode is set to ""append"", it will show 1 row at a time which is good but there is a 10 seconds delay.  


HOW DO I GET IT TO FREAKING WORK!!?!?

Any help much appreciated!

&#x200B;

NOTE: Late data can be completely ignored which is why I am not using watermark.",2024-03-22 19:00:17
1bi1et9,When does query optimization in DBMS happen?,"What is optimized by DBMS: logical or physical plans? or both? If only logical plans, how DBMS can calculate costs without knowing of exact physical operations behind nodes of query tree? I also doubt that only physical plans are optimized.  
I have this picture (CMU DB group lectures) where given logical plan is optimized but I dunno how this may happen if logical plan is to my knowledge high-level DAG of operators without any physical specifics.

To me it seems that optimizer should optimize both logical plans (iterate through different query plan trees and choose best plan using *heuristics*) and physical plans (iterate through different options of executing the same query plan tree (different joins, access methods, etc.) and choose best plan using *costs-based approach*) but I'm definitely not sure about that.

UPD. Found docs of specific DB [https://docs.pingcap.com/tidb/stable/sql-optimization-concepts](https://docs.pingcap.com/tidb/stable/sql-optimization-concepts) . In this docs there is point about doing some ""logically equivalent changes to the query"" and they name it ""[Logical Optimization](https://docs.pingcap.com/tidb/stable/sql-logical-optimization)"". Also there is about ""obtaining a final execution plan based on the data distribution and the specific execution cost of an operator"" and they name it ""[Physical Optimization](https://docs.pingcap.com/tidb/stable/sql-physical-optimization)"".   
So to me it seems that in modern DB systems there is actually similar process to one I described earlier.

https://preview.redd.it/3p9o5illm5pc1.png?width=537&format=png&auto=webp&s=683f91e65fed6d4cc615379eb13cb35997f2578a",2024-03-18 20:45:28
1bgb5ki,"How do you prioritize performance, price and ease of use?",When building a data platform how do you prioritize these three things?,2024-03-16 17:21:26
1bg4g62,Real Problems worth solving with Data Engineering skills,"Do you know of any interesting problems in software that could be tackled using data engineering skills?  I've seen many people doing projects like having lot of AWS services involved and shifting data from once place to another, but that doesn't solve any real use case (they are kind of showing one's skills). Cases like this [https://www.investopedia.com/simulator/](https://www.investopedia.com/simulator/) where data is constantly generated and given out in the world to learn trading skills (even though they are fake) but still...Something like this which will help people to at least think in the direction of building something which can be put in the world for good.

EDIT: One more example of real life use case of DE skills: [pricetracker.app](http://pricetracker.app) which tracks the price of products on amazon, flipkart etc.",2024-03-16 11:59:27
1bdzyr0,On premise stack,"I've been data engineer (mostly working on ETLs in cloud environments) for 5 years. I started a new position yesterday in an industry that requires to work on premises. Nothing exists yet, the project starts with me, which is very enthusiastic. 

However I am a bit lost after 2 days struggling on the stack. Right now I am trying to identify which tools I should use. The load of data isn't that big yet but it's required to make something a bit scalable; that could welcome new sources of data (structured and unstructured)

My idea is the following : 

Airflow as an orchestrator (already have an experience on it), HDFS for a datalake, PySpark scripts for ETL (comfortable on it) , Hive or Presto for datawarehousing, Metabase for viz 

Therefore right now I have created docker containers running Spark, HDFS and Airflow. However the more I go further, the more I am doubting. There are so many tools existing for everything and I feel like this stack is a bit oldish ... I've also checked open source versions of Dremio, Cloudera, Iceberg/minIO but I am not sure how suitable it is for a production environment where all the data weighs probably less than 100GB. Also, I'll stay at my position for approx 6 months, I need to make something that would be easily maintained by people who don't work in data engineering field, I feel like Iceberg would be overkill. I sometimes also think about doing easier stuff with pandas/postgres but this is limiting for unstructured data. 

I would be glad to receive your opinions about it !

&#x200B;

&#x200B;",2024-03-13 19:12:47
1bc5i2z,Processing Kafka streams in Python,N/A,2024-03-11 15:13:01
1b8c1dk,Moving away from google sheets,"I recently moved to a new company where we are trying to create a pipeline for a downstream team that are going to use it for analytics. The upstream is the team directly involved with ""creating"" the dataset and their whole department works with sharepoint and google sheets. They create and maintain everything in excel manually because that's what they have been doing. So, for our pipeline we have to manually get the excel file from them  through slack or through email before we start the pipeline. I have tried having discussion with them where the file they create should be pushed to lake periodically but they don't want to move forward with it. We volunteered to build it from their side the integration to your lake but they don't want it. Now, my manager doesn't want me talking to them and do it as-is. Do you guys have experience how to deal with this situation and what can i do from side that can make this process a bit more automated? Any advice is appreciated. ",2024-03-06 21:37:36
1b7nrzd,What should I prioritize learning,"I'm a data analytics professional of 6 years who started becoming interested in DE after finding software engineering more interesting than my statistics/data analysis work. I first started out by taking python OOP, computer science fundamental and full stack courses. I enjoyed that stuff and had some full stack knowledge beforehand, but the issue was from a career perspective that I had so much experience in data that full stack dev seemed like a disconnect from my past work and I started to look at something that would blend my data skills with software engineering interests. 

I started looking into DE, but I didn't quite understand where to even start. I have automated data workflows in the past, but I was using SQL, R and Cron-literally leveraging whatever tools I had because the work previous analysts were doing was very manual and tedious. I don't have formal experience with cloud technologies, airflow, kafka, databricks, etc. I used Docker and AWS on personal projects and kind of understand it, but I'm not an expert by any means. 

I just got my first DE job after hundreds of applications for data engineer, software engineer and data science roles. I'm excited but I also don't know what I should prioritize in my learning journey. I bought the joe reis data engineering book months ago and am doing the data engineering zoomcamp, but am so behind on that due to other life things. The DE zoomcamp seems like a great resource so far and it's amazing it's free-but I'm sort of feeling like I'm going through the motions without really understanding what exactly I'm doing. I also saw that AWS has certifications and the job I'm going to be doing uses Redshift so I thought I'd look into that. 

Any suggestions on what to start with? ",2024-03-06 02:08:53
1bsh4ro,Who do data engineers report in the org?,"Hey, question : is data engineering always always in the CTO organization? If yes, how familiar was the CTO with the technical details of data engineering?",2024-03-31 18:25:47
1brrjbw,The Existence of Jr DEs,"Do junior Data Engineers exist where you work? I'm talking about 0-2 years work experience.

On the one hand it's a consensus that 'DE is not an entry level position' and many if not most DEs transition sideways into the role. Also there are very few entry level DE job postings, at least in the USA. Even before the tech recession.

On the other hand, entry level DE job postings do exist. Furthermore I have seen many resumes from India where a senior DE started their career as a DE straight out of university, perhaps initially as a DE consultant. No sideways transition. So I'm wondering if entry level DE is less of a rarity there compared to USA. ",2024-03-30 20:31:21
1brkra5,First DE architeture on AWS,"Hi everyone, I am going to build my first data warehousing  (no bigdata at the moment). I have data time series (data look like some costumer data to store per each day) . I want to build a database to query and do some ETL-ELT using python and AWS.

I just wanted to hear from people what they would do and curious to see what tools and ideas you have. 
",2024-03-30 15:39:51
1br02y0,Seeking advice for my position ,"Hello everyone,

I apologize for turning this platform into a space for my personal blog, but I feel compelled to write and seek genuine feedback on my current work situation. From what I've gathered, you all possess a deeper understanding of how things should operate in this industry.

I serve as the BI manager for a dine-in restaurant chain, a position I've held since November 2023. My journey began two years prior, when I was hired as a Power BI analyst under the CFO. I developed reports that are now utilized daily by over 300 individuals, ranging from restaurant staff to headquarters. Essentially, I've become the de facto data governance, as none formally exists within our organization.

During my tenure, the data engineering team was part of IT, consisting of four engineers, a project manager, and a BI manager. The BI manager lacked competence, displaying neither knowledge nor decisiveness. The project manager role saw frequent changes, with four different individuals in two years. Among the engineers, two were competent consultants who, paradoxically, made questionable decisions, presumably to justify their roles. The remaining two, both juniors, are still with us. Our interactions were minimal, often limited to basic data requests for the Data Warehouse.

In October 2023, both the BI manager and the project manager resigned. I proposed to the CEO that I assume their responsibilities, in addition to my own, with appropriate compensation. The board agreed, placing me under the dual oversight of the CFO and CTO.

Subsequently, the consultant engineers departed, leaving the junior engineers who can maintain our current systems but are ill-equipped for significant modifications. Technologies such as Terraform and CI/CD pipelines remain beyond our collective expertise.

Our technology stack consists of ADF -> Synapse dedicated pool -> AAS -> PowerBI, costing approximately 190k€ annually. Through improved practices, including minimizing Synapse's power and operational hours, I reduced our expenses to 90k€/year. 
I repeat myself but I have very minimal knowledge of anything outside PowerBI. I think I'm way above average in dax/m and general modeling of dataset tho.

Despite these cost savings, the board is against hiring a third data engineer to address our skill gaps, believing it unnecessary since our systems are operational.

I'm also constrained from exploring cost-reducing migrations to platforms like Snowflake or Databricks due to budgetary limitations and a lack of clarity on which would best suit our needs, despite sales pressure from both.

Despite the satisfaction expressed by both the board and the data engineering team with my performance, I often feel overwhelmed and occasionally like an imposter.

I welcome your insights or advice on navigating this situation, including any critical perspectives.

Thank you for your time.",2024-03-29 21:35:08
1bo3ryk,Hosting superset for redshift viz,"Apparently, the decision to go for superset is final.

the company is around 200.

data size around 10s of millions.

So far i've noted the below -

**Managed**

1. [Preset.io](http://preset.io/) -

* from founders of superset
* 20$ per User (14D free trial)
* additional benefits: RBAC, Slack reporting

1. AWS based [alternative](https://aws.amazon.com/marketplace/pp/prodview-c3evrh2ho3fn4#pdp-pricing)

**Self**

1. AWS fargate based solution - 📷[Apache Superset on AWS—Partner Solution](https://aws.amazon.com/solutions/implementations/apache-superset/)

* scalable
* cost friendliness TBD - [https://github.com/aws-ia/cfn-ps-apache-superset/issues/14](https://github.com/aws-ia/cfn-ps-apache-superset/issues/14)

1. Self-managed deployment on EC2 instances

Realistically the cost efficiency is what would be latched on to even if the dashboards aren't instantly loaded.

Would the deployment on EC2 ""win"" then?

The fargate based solution appears to cost 1k$ pm [just for setup](https://github.com/aws-ia/cfn-ps-apache-superset/issues/14) so i'm skeptical on how it can explode when viz work starts.

Are there any other solutions missed? Did anyone deploy via ec2? any helpful guides/tips?

TIA.

Regards",2024-03-26 09:38:49
1bnnfq8,Seeking Guidance on Advancing Skills," I'm strong in SQL, but my Pyspark, AWS, and Kafka skills are still basic. I've been working hard to learn these, but I'm stuck on what to focus on next to become a top-notch data engineer. And my second question is: Freelancing as a data engineer sounds exciting, but I haven't seen many jobs on Upwork. Is full-time freelancing even a possibility in this field? ",2024-03-25 20:04:47
1bn74g3,Data freshness and completeness,"
Hello everyone , we have different source systems sitting in Amazon rds , Mongo db instances and so on. We are migrating all the data to redshift for single source of truth. For rds instances, we are using AWS dms to transfer the data. For mongo we have hourly scripts to transfer the data. Dms is not suitable for mongo in our usecase because of nature of the data we have . 

Now the problem is sometimes the data is not complete like missing data, sometimes it is not fresh due to various reasons in the dms, sometimes we are getting duplicate rows.

Now we have to convey the SLA's to our downstream systems about the freshness like how much time this table or database will take to get th latest incremental data from source . And also we have to be confident enough to say like our data is complete , we are not missing anything. 


I have brainstormed several approaches but didn't get the concrete solution yet . One approach we decided was to have the list of important tables . Query the source and target every 15 mins to check the latest record in both the systems and the no of rows. This approach looks promising to me.  But the problem is our source db's are somewhat fragile and it requires a lot of approvals from the stake holders . If we fire count(*) query with our time range , to fetch the total no of records, it will take 10 mins in the worst case . 


Now how to tackle this problem and convey the freshness and SLA's to downstream systems.

Any suggestions or external tools will be helpful. 

Thanks in advance ",2024-03-25 06:42:06
1bmz1rx,Learning resources and courses - data leader,"Hi all - I’m looking for a little bit of guidance regarding learning. I searched the subreddit and wasn’t sure which resources to focus on.

For context - I’m the head of data at a midsized company. My background is mostly in the analytics/data science space: building ML models, visuals, insights etc to drive business strategy. I am keenly aware that I have a pretty big gap around data engineering. I have picked a few things up here and there working with data engineers, but I really want to learn more to make me more effective. The way I’m currently thinking about this (and let me know if this seems off base):

1. Getting a deeper understanding of the problems that data engineering is solving for (technology agnostic) - basically the “what” and the “why”.

2. Current industry best practices in terms of technologies, frameworks, approaches, etc to solve those problems - what are the major methods currently used to solve #1 above?

3. Some level of hands on practice - I’m someone who learns best doing it by hand in a relatively structured environment. Now, since my day to day isn’t going to be doing data engineering work day in day out (have actual technical people to do it and run the teams), I don’t (think..) I need to have advanced hands on practice, just enough to understand the high level approach and things to watch out for so I can advocate for my teams. 

Ideal scenario for me is to have a course to cover most of these to build a base, and then augment from there with reading/other items as necessary. Any suggestions? Does something like this exist? ",2024-03-24 23:34:46
1blzcx2,Looking for advice on how to efficiently create a pipeline for images,"&#x200B;

[Data flow](https://preview.redd.it/85qi28f2n4qc1.png?width=759&format=png&auto=webp&s=678aed4c017509487bfe89da3c2ed92d56aed5dc)

Hi,

&#x200B;

I am trying to setup a pipeline for processing images which will be running OnPrem. I have added a data flow diagram above. There will be 4 docker containers for individual processes. The first container (preprocessing) will fetch images from external API every 5 minutes, process them and then send them to S3 storage. I am thinking of using Airflow here. The problems that I am struggling with are:

1) how do I notify the next container(s) that the images have been processed and ready for transfer? 

2) how do I transfer the images for further processing between the containers? 

I am open to any and all suggestions but the tools/tech need to be free to use.",2024-03-23 18:38:05
1blz3t4,I built a tool to run classification directly on a database,N/A,2024-03-23 18:27:33
1bltvss,How far do I have to go to become a Data Engineer?,"Was hoping someone could give me a realistic perspective of how far I need to go to get my foot in the door as a Jr. Data Engineer. I am hoping to make a career change out of consulting - I am currently a Senior Product Analyst. Prior to this I was a mechanical engineer for five years. I've found I still don't want anything to do with the political world of business execs, I want to get back in the weeds where I belong and build glorious elegant solutions to weird problems. Efficiency makes me happy.

Where I am currently:

* I have built a few full-stack applications and am very familiar with React and Postgres.
* At work I use Python (often starting with a first pass generated by ChatGPT to save time, then refining from there -- everything has to happen fast, and everyone thinks we should just use Excel for everything) to automate a lot of my tasks and try to build leave-behind tools to help others process data. 
   * While I may skimp on the implementation step since I am cheating with AI, I do have to understand:
      * What is possible
      * What's the specific set of steps I want to take to achieve this
      * What data structures will be useful in getting me there
      * What edge cases and other issues do I need to account for to make this generally applicable
   * These have included Powerpoint slide generators that take data from an Excel file and populate pre-defined templates to save time in re-spins, an automatic Pareto analyzer for spend data, a variant tree generator for product portfolios, and a bill of materials re-formatter to transform various formats to a consistent hierarchical pattern, enabling further downstream analysis
* I also use Alteryx a lot at work for a wide variety of tasks, usually for quickly parsing and getting insights from large datasets, and setting up repeatable transformations. 

&#x200B;

I understand most of this is Analysis and not Engineering, but I'm wondering if I may be close enough that I could get into a job and learn on the fly after spending a few months doing some self-learning and practice with the basic concepts of data engineering. 

**Would appreciate any and all feedback -- don't hold back!**

&#x200B;",2024-03-23 14:45:56
1blt82x,Databricks Dimensional Modeling,"Hey everyone I joined a mid sized company working on a Machine Learning team. Our team is only 2 years old so we have relatively immature data infrastructure. We recently released a new ML product. Data is sent via a request to our API we return predictions and have another endpoint to collect feedback. Our ML service is hosted on k8s with triton inference server and MongoDB to store user feedback for improving predictions. 

Our next initiative is to create a data platform - we chose to use Databricks Lakehouse as the underlying technology. The API is writing the mongodb documents as JSON files to blob storage. These documents contain a lot of fact and dimensional data and we are using autoloader to ingest into databricks. My question is should I be building/updating these dimension tables from the json files? Or should the json artifacts only have fact data and we build the dimension model using cdc from our main application SqlServer db (which I don’t even have access to because it’s maintained by a different team and has an existing sub-par snowflake model).

Please let me know if you need any clarification and I really appreciate the support. At my previous role, I always had access to the main application db where the dimensional model was already built so I just had to maintain it. Never designed one from scratch.  Thank you!",2024-03-23 14:16:55
1bkeepi,How can I learn databricks without admin rights / or at a low cost?,"I work for a company that's heavily relying on databricks for ETL and modeling. I am expected to become an expert using databricks, but I do not have permissions to create new clusters, workspaces, etc. And, when I attempt to create a ""training profile"", the system will still only log me in to these limited profiles.  
  
Does anyone know how to access/create a student-tier workspace that capable of handling the demands of Databrick's own learning modules? Does Databricks provide access to something like this for people training to use their systems?  
  
Any help would be appreciated.  
I did check online and everything points toward having admin access, which leads me to believe I will need to set up a personal azure account and pay for databricks itself. I'm trying to avoid that.",2024-03-21 19:01:03
1bimv59,Why do people say xcoms are no longer the default in airflow,"Learning Airflow for the first time. Have been using Prefect.

Learned about xcoms today. The syntax for passing things seems very verbose and somewhat cumbersome, but I get it. However, the 48kb limit means xcoms have some limited application.

In researching posts, I saw on multiple occasions people stating (paraphrasing here) ""*back when Airflow was still using xcoms*"".

What does this mean?

* In airflow, can a task be a function that calls other functions that are not tasks themselves?
* Could your task push data to something like S3 where the next task can pick it up?
* Are xcoms still an integral part of Airflow?",2024-03-19 15:24:34
1bigwrv,Event ingestion on GCP terraform template + blog (18x cost saving over Segment),"Hey folks, dlt (the data ingestion library) cofounder here,   


I want to showcase our event ingestion setup. We put this behind cloudflare, to lower latency in different geographies.

Many of our users use dlt for event ingestion. We were using Segment ourselves as we had free credits, but on credit expiration the bill is not pretty. So we moved to dlt on serverless gcp cloud functions with pub sub.

We like Segment, but we like 18x cost saving more :)

Here's our setup  
[https://dlthub.com/docs/blog/dlt-segment-migration](https://dlthub.com/docs/blog/dlt-segment-migration)

More streaming setups done by our users here: [https://dlthub.com/docs/blog/tags/streaming](https://dlthub.com/docs/blog/tags/streaming)  


&#x200B;",2024-03-19 10:20:34
1bhhxs0,"Supercharge your compute strategy with Apache Iceberg, Snowflake, Apache Spark, AWS Glue & Project Nessie","For the past few months I have been learning about data strategies that employ Apache Iceberg with Snowflake DB and Apache Spark & I have compiled my learnings into a short article.

[https://medium.com/@pbd\_94/skiing-with-snowflake-b196e8f7e2e6](https://medium.com/@pbd_94/skiing-with-snowflake-b196e8f7e2e6)

Fire away.",2024-03-18 04:09:29
1bezk3y,Integrating Apache AGE for Enhanced Data Engineering Workflows,"Hello r/dataengineering,

I've been deeply involved with Apache AGE, a graph database extension that's proving to be a game-changer for data engineering. As a core contributor, I've seen its potential to revolutionize the way we approach data relationships and analysis, especially in environments that traditionally rely on SQL databases.

Apache AGE enables the integration of graph database functionalities directly into PostgreSQL, allowing for complex data relationship queries without the need for a separate graph database system. This opens up new avenues for data modeling, querying, and analysis within the familiar PostgreSQL ecosystem.

What's particularly exciting for data engineers is AGE's ability to handle complex, interconnected data scenarios more naturally and efficiently than traditional relational databases. This capability is invaluable for applications in social networking, fraud detection, recommendation systems, and more, where understanding the relationships between data points is key.

I'm curious to hear from the community:

* Have you integrated graph database features into your data engineering projects?
* What challenges and opportunities do you see in adopting Apache AGE for your data workflows?

Let's explore how Apache AGE can fit into our data engineering toolkit, share experiences, and discuss best practices for leveraging graph database capabilities in our projects.

For a deep dive into the technical workings, documentation, and to join our growing community, visit our [Apache AGE GitHub](https://github.com/apache/age) and [official website](https://age.apache.org/).",2024-03-14 23:33:35
1bd1ldx,Why we rebuilt our streaming SQL engine on Arrow and DataFusion,N/A,2024-03-12 16:32:31
1badyyn,SQL Database using LSM Trees,"Hi everyone,

I read that lsm trees is used by dynamo db, cassandra and scylla db. But nowhere saw link between relational db(sql db) and lsm trees. Is lsm trees only used in nosql databases if that is the case why can't we used them in sql ?  
",2024-03-09 09:33:46
1b9b9oa,Databricks value ,"Hello everyone, I just recently started working as a consultant, I'm being offered to work in a project with databricks. My main question is how valuable can be the experience I could get from that project in the look for developing my career? I've seen there's a a lot of mix reviews on databricks, so I would like to hear your opinions and experiences.
Thanks in advance :)",2024-03-08 01:26:45
1b72js6,"New to the world of Data, does this count as a Star Schema","Hi Guys,

I'm currently building a data model for a reporting system. While looking for references or best practices, I stumbled upon the Star Schema. So I'm wondering, does my picture, which is a small cutout of our model, still count as one since it has a dimension in the middle?

Thanks for all of the answers.   
Upon further consideration, I believe this project doesn't necessarily require a Star Schema just because I learned about it in university. There's no need to force it. A classic relational database is more than enough and better than dealing with tons of Excel files.

[https://imgur.com/a/e4CVooX](https://imgur.com/a/e4CVooX)",2024-03-05 10:58:18
1b62ekj,Advice - Getting back into data engineering after a slight career pivot,"Hey everyone, looking for some advice on my current situation.

I'm a software engineer with 5+ years of experience. I actually started my career in a non-software role but luckily got into data engineering after a year (I was in a rotational program and had self-taught programming so expressed my interest and fortunately got placed in a data engineering role). I worked in that role for roughly 1.5 years, briefly moved to a startup as a data engineer, then left to try starting my own software development agency. I was naive at the time and simply had this notion that I wanted to work for myself without any clear vision.

Well after 2 years now of having started and closed my agency, working freelance gigs, joining a pre-seed startup as the only software developer, and subsequently being laid off after a failed raise, I've decided I want to refocus my career on pursuing data engineering for the long haul and have started applying to strictly data engineering roles. During the last 2 years I've worked in a full-stack capacity with a slight backend bias, developing JavaScript framework-based web apps, backend tooling in Python and Rust, APIs for mobile apps, cloud services implementation, and even dabbled in blockchain smart contract development. Recently I started working on personal projects that leverage data engineering skillsets.

How should I go about speaking to the last couple of years of software engineering experience and how do I leverage it in the context of wanting to get back into the data engineering field? Are the last couple of years even relevant at all or am I essentially looking to start over in data engineering? If it's worth anything, I'm a couple of classes away from finishing the Online Master of Science in Analytics degree from Georgia Tech (I started the degree as my former employer sponsored it but took a break to focus on my company).

Any advice is much appreciated as I'm a bit lost on what my next steps are. Thanks!",2024-03-04 04:56:13
1bsf902,"How can I use Apache Airflow, Kafka and AWS Services in Databricks Community Version?","I'm planning to become a Data Engineer and currently studying Apache Spark (PySpark). I can't seem to make PySpark work on my computer and that's when I found out about Databricks Community Version. Now, my dilemma is, will I be able to make Airflow, Kafka, and AWS Services on Databricks Community Version? Or maybe there are better ways of doing this? These are the techs that I want to add to my stack. My question might sound dumb or vague please bear with me. I'm kinda new with Data Engineering and I plan to make a career on this path. Thank you in advance.

&#x200B;

My current tech stack: Python, SQL, MySQL, PostgreSQL and with a few ETL Projects done locally on my computer.",2024-03-31 17:02:48
1bs3fe8,Getting entry level DE opportunity ,"I’m currently working as an entry level DE (even though my role is closer to a BIE), I have 1 year of experience now since I’m a new grad. What tech should I learn that would make me a marketable candidate to standard entry lvl DE roles? I’m trying to leave since my current team is pretty non technical (even though they should), so I’m working with other teams to learn more. I have a CS degree and I have learnt stuff like dbt, Airflow, Terraform, AWS, Snowflake, Docker, Tableau on this job",2024-03-31 06:07:39
1brupmp,Data orchestration tool,"My company is considering to move out from Talend to a more modern data management and orchestration platform. Despite talend fulfills the company's needs it is costly to maintain. Licence costs have been a pain. 

Azure Data Factory seems to be a good alternative however research suggests that this data platform does not scale well with complex data pipelines. Compute resources tend to be expensive on Azure. 

Which python based data platform does your company use? ",2024-03-30 22:46:51
1br47wf,Opportunity for a free voucher on data certifications,"Guys, the Microsoft Learn AI Skills Challenge is still open. For those who are unfamiliar, Microsoft periodically offers an immersive and free challenge in the realm of Data and Artificial Intelligence, with the promise of a certification voucher upon completion. The challenge is straightforward: simply enroll in one of the four available tracks and complete the learning modules.

&#x200B;

[Azure Machine Learning](https://learn.microsoft.com/training/challenges?id=764de7ca-8b6d-4b3a-a491-1942af389d8c&WT.mc_id=cloudskillschallenge_764de7ca-8b6d-4b3a-a491-1942af389d8c&wt.mc_id=studentamb_360500)

[Azure OpenAI](https://learn.microsoft.com/training/challenges?id=da09d3ca-a2bb-47dc-ba42-bea77b386a3d&WT.mc_id=cloudskillschallenge_da09d3ca-a2bb-47dc-ba42-bea77b386a3d&wt.mc_id=studentamb_360500)

[Azure AI Fundamentals](https://learn.microsoft.com/training/challenges?id=3ef5d197-cdef-49bc-a8bc-954bcd9e88cc&WT.mc_id=cloudskillschallenge_3ef5d197-cdef-49bc-a8bc-954bcd9e88cc&wt.mc_id=studentamb_360500)

[Microsoft Fabric](https://learn.microsoft.com/training/challenges?id=b696c18d-7201-4aff-9c7d-d33014d93b25&WT.mc_id=cloudskillschallenge_b696c18d-7201-4aff-9c7d-d33014d93b25&wt.mc_id=studentamb_360500)

&#x200B;

You have until April 19th to complete one of these challenges and secure a certification voucher for a Microsoft exam.",2024-03-30 00:26:25
1bqmmfk,Dbt and data quality,"Dbt newby here, I’m trying to study it and got in touch with tests for data quality and I got a question. Since dbt is meant for the transformation phase of ELT (in my case I’m using it to provide data to the business layer), how do I approach data quality before in the loading phase? For example suppose I load daily orders data in my datawarehouse, of course orders with a NULL order_date in my datalake are not ok to be loaded, why should I load and then exclude in the transformation phase?
Is it better to organize data quality in two steps, one for loading and one for transformation?",2024-03-29 11:10:33
1bq4ze3,Anyone here use TimeXtender?,"Hi, newbie data engineer here, just transitioned from Data Analyst. My company has been using TimeXtender on-prem for about 2 years to build our DWH for PBI reports.

The reason for my question is, our contract with their on-prem version runs out in 2025 and we would have to switch to their cloud model. Our DWH needs a cleanup as well, and with Fabric coming along, we are reevaluating our technology stack. It's just SQL Server, AAS, some ADF, and the PBI stack.

Does anyone have feedback on TimeXtender or how it compares to other products? I've been going through their training pages and they've turned me into a fanboy, but I don't have any experiences with other DE technologies to really weigh the options.",2024-03-28 19:56:52
1bptmlf,How to build an e-commerce data pipeline,"Learn how to build an efficient data pipeline for e-commerce analytics using Airbyte, dbt, dagster, and BigQuery. Enjoy! 

[https://www.youtube.com/watch?v=PMYxeWeNoX8&t=1s](https://www.youtube.com/watch?v=PMYxeWeNoX8&t=1s)",2024-03-28 11:49:18
1bp7bdr,Examples of documentation you like working with?,"Hey all! I want to improve our company's docs, and I'd like to find examples of data tool documentation that DEs really like.

A couple I've seen mentioned on this sub:

\- [GitLab Data Wiki](https://handbook.gitlab.com/handbook/business-technology/data-team/platform/)  
\- [Snowflake docs](https://docs.snowflake.com/en/)

To be clear, I'm am **NOT** asking how you build good documentation. I've found several threads on this sub that answer that question.  


I **am** asking what documentation you like working with and what you like about it.  


Cheers!",2024-03-27 17:16:46
1bnbg99,Does Palantir Foundry Count as Data Engineering?,"Hi All,

I currently work as an aerospace engineer (nominally), although a lot of my actual work has been to do with data and software. This is definitely what interests me, and I want to move into this sort of role full-time.

However, my experience has been a bit of a mess across several different disciplines, so I'm really not sure what I'm qualified to go for or what's expected. I have some software development experience (creating custom analysis software in Python), some data analysis / data science experience, and then also some data engineering experience.

However, a lot of the stuff that I have done has been using Palantir Foundry, while all of the jobs I see seem to want things like Azure, Databricks, AWS etc.

Is Palantir Foundry an ""easier"" version of these tools, and I will struggle to move to something like Azure? Or is it broadly similar, and I should I look to do some self-learning on Azure, for example, and then I will be job ready for a data engineering role?

The work I have done in Palantir Foundry definitely seems quite data engineering related - I use Python / PySpark SQL and have created some custom dashboards using HTML / CSS / JavaScript. However, I haven't actually been involved in the ingestion of any raw data, I have usually taken existing data sets that were in a poor state and cleaned, transformed, joined multiple datasets to get the result I wanted.",2024-03-25 11:36:14
1bmqeoe,Any laptop recommendations for Data Engineering?,"I'm about to start my Data Engineering journey. Its time to put down my Macbook Pro from ............. 2012.

I know I know.... 12 years strong but it's giving me so many issues and I can't even reformat it anymore because my keys don't work when it reboots so I can't even... Command + R after reboot so .... yeah it's done for me at this point. 

I'll be honest I have to be as frugal as humanly possible but this laptop is strictly for usage on my couch and on the go for studying and programming. I will also need to dual boot Linux/Windows because I'm going to follow the Coursera IBM Data Engineering Certification. 

Is there anything you recommend? ",2024-03-24 17:36:34
1bmajc0,Netflix L4.,"Hey everyone, I'm interested in pursuing a career at Netflix, Can anyone shed some light on what level of experience they usually expect for L4 roles? I know for L5, they typically seek at least 5 years of experience, but I'm curious about what's expected for L4. Thanks in advance for any insights! #Netflix",2024-03-24 02:54:51
1bm27px,When should I prefer DuckDB over DataFusion?,"I am checking out DuckDB but it seems like DataFusion can do everything DuckDB can do except some built in extensions provided by DuckDB like postgres.

What are some cases for preferring DuckDB over DataFusion? Any experiences to share?",2024-03-23 20:35:35
1bjjdqc,Lightweight alternative to Spark/Flink/Apache Beam,"My company has a mix of GCP/on-prem architecture : 

\- for all our streaming jobs on GCP we use Apache Beam (Java/Python) + Dataflow

\- for on-prem streaming jobs we implemented custom python consumers

As part of the revamp of these consumer jobs, we want to harmonize the technologies in the data and software engineering teams.  


We are a bit frustrated with Beam, as a lot of features are only available in Java, and it is sometimes overkill for some use cases (just tasks doing simple transformations). Also, running Beam on-premise is not the simplest task (we need Apache Flink for that).  


We tried looking at alternatives, and we wondered if there were some limitations using python consumers instead of using a tool like Spark/Beam/Flink ?  


[https://www.benthos.dev/](https://www.benthos.dev/) is really interesting to us, are there similar interesting alternatives out there ? Is anyone running this in production ?",2024-03-20 17:39:46
1bibu4z,Any messy datasets for Pyspark practice?,"As the title says I'm looking for any messy datasets that would require some significant transformation for a personal project I'm doing. I have already set the architecture which involves Kafka, RDS Postgres, Docker, Debezium and Pyspark. I specify some tables from which any new-entries are captured and published to Kafka topic(s) through a debezium-postgres-connector. The next step is to ingest these real-time entries in Pyspark, perform some transformation, and publish it to another topic for subsequent processing (I haven't decided what I'm going to do yet — maybe some visualization/analytics?)

&#x200B;

Everything is working fine and I'm getting the new entries in real time. But the data that is currently present in the database is overall clean and doesn't require any major transformation. Maybe dropping a column or two, or filtering for rows that don't meet a specific criteria but that's it. Neither is the data really huge.

&#x200B;

So I'm looking for any datasets that can span multiple tables so I can do some meaningful transformation on them. I could upload the datasets to the db and proceed from there. 

&#x200B;

I'll also take any suggestions regarding the project itself. Maybe there's a flaw in the architecture I'm missing. Who knows?",2024-03-19 04:29:38
1bhzg06,Humble Tech Book Bundle: Pipelines and NoSQL by O'Reilly,"Get the knowledge required to excel in the realms of data engineering, data science, and a host of related in-demand fields with this collection of books from O’Reilly! Deciphering Data Architectures provides a guided tour of today’s most common architectures—from data lakehouses to data meshes—to help you understand the pros and cons of each. Data Science: The Hard Parts is a handy guidebook of techniques and best practices that are generally overlooked when teaching this wide-ranging discipline.


",2024-03-18 19:27:43
1bhpxx2,Analytics Events Documentation & Monitoring,"Hi All,

To be very up front: I run a small saas focused on real-time metrics. I recently announced a new feature/product I'd love some feedback on (and potentially beta testers).  [Blog Post](https://aggregations.io/blog/autodocs-coming-soon).

The idea is simple: you forward your event stream and you get a searchable schema of your events, & their properties along with statistics/distributions of the field values. 

The other element comes in the form of a per-version changelog, with alerting for things like type changes, cardinality fluctuations, etc. 

I won't go too far into the technical details, unless people are interested -- but building this is obviously has been a very intricate and complex project, I'm pretty happy with the result so far :)

&#x200B;

I've built a system like this multiple times in the past at larger companies, so I know the value it can provide -- I'm just not sure (1) how to express it well and (2) what other scenarios/ features might be useful.

For example, post launch I know I want to add in more collaboration/annotation features (to make it more of a ""documentation hub"" for analysts, data producers, etc) -- but other things I've built before, may not be super applicable. 

In the past, I enabled ""generate SQL to fetch this property in different languages"" because JSON functions can be tricky and some payloads are gnarly. I don't know if that is widely applicable? 

&#x200B;

Is this a thing that would help you? What struggles do you have with documenting your events, etc?

Any thoughts or feedback would be greatly appreciated! ",2024-03-18 12:50:34
1bgvc2w,Batching Ingestion (PostgreSQL): Sequential Transactions vs Temporary Tables,"I have an API wrapper, which accepts a batch of data, which has to be ingested into multiple tables. First the main facts are inserted and the ids are used to fill some additional metadata tables and junction tables for the dimensional tables.   


How would you efficiently handle the ingestion, when a batch of data arrives?   


I think the following options are viable, with respective tradeoffs:

1. Sequential inserts wrapper in transactions. Inserting each fact into the content table and then inserting the accompanying data into the surrounding tables. Do it in a loop for all data. Inefficient, but should give the best guarantees. 
2. Bulk insert with temporary tables. Insert the data into temporary tables (staging). Insert into fact table and get content\_ids. Fill other temporary tables and insert into dimension tables and metadata tables. 

I was looking into COPY but it doesn't seem suitable for multiple tables. Am I missing an option? What setup would you go with? ",2024-03-17 11:28:38
1bfe06k,Is SQL trigger functions or DBT execution more preferred for data transformation in PostgreSQL?,"I've a pipeline run like this:

extract data from data source -> store into raw table in PostgreSQL -> run another SQL transformation script to load into staging table.

&#x200B;

Now for the third step, I would like to create an automation method to execute the SQL transformation script instead running manually. I expect it to be executed whenever an insertion occurs in the raw table.

I know SQL trigger functions can react to insertion, and I'm not sure if DBT can react to the insertion yet.

My expectation is: can evoke to run transformation SQL script whenever insertion happens in raw table, and I'm able to record the transformation log to trace the action.

&#x200B;

Any suggestion is appreciated! THanks",2024-03-15 13:30:11
1bf2jbk,Different methods for real-time aggregation?,"I'm primarily a frontend engineer. When I need to work on backend/infra, I often run into the issue of not knowing how to aggregate data in a scaleable way. E.g. pre-computing Reddit's upvote counts so you don't have to aggregate at read time. Aggregating for the top posts feed is easy because it's ok if it's stale by several minutes. However, if you upvote a post and refresh a second later, ideally the post count should include your upvote.

Is there a standard way to do this? I don't know why it's so hard to find discussions around this problem. Afaik some ways of doing this are:

1) When inserting the ""upvote"" record, also increment an ""upvoteCounts"" record

If you do this in a transaction, it would make the insertions slower. If you don't use a transaction, the upvote counts would go out of sync. I think some companies don't use transactions, but have a job to continuously recompute the counts. Another problem is if you need to mutate multiple aggregations per insertion, it'll be confusing to maintain.

2) Use a trigger

It's basically the same as 1) with transactions, but handled by the DB instead of the application.

3) Aggregate the event stream

Stream changes, then use something like Flink/ksqlDB to aggregate the stream, then stream the aggregation to somewhere for the application to read from. This has much higher latency than 1) with transactions. The benefit is that your aggregations are declarative, so they're easier to reason about.

4) Have a batch job for old aggregations, then handle real-time aggregations in memory

Assuming batch aggregation will take a long time, you can store real-time aggregations in memory. E.g. when someone upvotes, increment a count in memory. When fetching the count, sum the count from memory with the count from the batch job. However, it seems tricky to not double count or miss upvotes.

5) Handling the aggregation at read-time with aggressive caching

This works when fetching the upvote count for a small set of posts. However, you wouldn't be able to sort posts by upvote count. Also, the max staleness is the cache timeout, which could be a long time.

6) Real-time or incremental materialized views

Ideally, we'd have materialized views that could quickly automatically respond to changes in the source tables. However, afaik the existing real-time materialized view systems are either in memory (e.g. Materialize) or places severe limitations on the source table (e.g. AnalyticDB). Materialize uses too much memory to be scaleable and I've never tried AnalyticDB, but it seems like it's too limiting on the source tables.

Are there any resources to learn more about which approaches large companies use? Have you tried some of these and know first-hand about the pros and cons?",2024-03-15 01:50:50
1bf13x5,ETL tool recommendations?,"We have been using talend open studio for our ETL needs but with that going out of support we are looking for a new tool. 

We have about 8 ETLs that extract data from various sources: powershell scripts, python scripts, MySQL database, sql server database and do some pretty basic transformations into an on prem oracle database. After that we build dashboards in power bi. 

I think cloud tooling could be good as there is less maintenance (server patching, app upgrades) but it must be able to write to on prem data warehouse. I’m not against on prem tool if it makes more sense cost wise. 

I think cloud tools are usually priced based on computation. 

One big benefit for us would be amount of resources on the tool online. If you run into issues are there lots of forums that have similar issues with resolutions? 

Any good paid tools or open source tools you recommend or don’t recommend? ",2024-03-15 00:43:27
1bdv1xv,Updated: Just launched my first data engineering project!,"[Link](https://www.reddit.com/r/dataengineering/comments/1b9hnn0/just_launched_my_first_data_engineering_project/) to the initial post. Posting this again after debugging. Since this is my first project, I appreciate your feedback on anything; be it Github readme, dashboard, etc.

&#x200B;

Leveraging Schipol Dev API, I've built an interactive dashboard for flight data, while also fetching datasets from various sources stored in GCS Bucket. Using Google Cloud, Big Query, and MageAI for orchestration, the pipeline runs via Docker containers on a VM, scheduled as a cron job for market hours automation. Check out the dashboard [here](https://aeroatlas.streamlit.app/). I'd love your feedback, suggestions, and opinions to enhance this data-driven journey! Also find the Github repo [here](https://github.com/suleman1412/schipol_flights_pipeline).

&#x200B;",2024-03-13 16:00:09
1bdovx9,Running Doom in Snowflake Container Services (again),"I tested the code that Daniel Palma created for Running Doom inside Snowflake Container Services. In this video I'll show case the same in video format and explore a little bit what Container Services are and what are their use cases.

[https://www.youtube.com/watch?v=Zd81nydtgX4](https://www.youtube.com/watch?v=Zd81nydtgX4)",2024-03-13 11:12:58
1bd1bsf,How to learn data engineering in a non-professional context?,"Hello,

I've got a master's degree in data and done several internships in data engineering. I'd like to start my career in the same field but I haven't had the opportunity to learn much about certain technologies. Are there any good exercises or projects on Kaggle to learn how to use certain combined technologies like Python, SQL,  ELK, Kafka, Airflow, Spark, terraform, with docker, Bash for example. If there aren't any Kaggle projects or that sort of thing, would there be appropriate datasets for the data sources? Maybe projects using GNS3 for the configuration part or some kind of github repository as a guided project? I don't know. Above all, I'm looking for a project that will enable me to learn or discover all these technologies. I know that a lot of these technologies can work together in a company, so I suppose it's not impossible to get a lot of them involved in this project.

As I'm considered to have little experience, I'd like to work on all these skills in my own time to see how everything fits together and to know what I'm talking about. I'm prepared to devote a lot of time to it so that I can be deployed more quickly in companies. If you can help me or even give me advice on the technology, for example, that would be really nice. Any feedback is also very welcome.",2024-03-12 16:22:08
1b8z73y,Data engineering in a biopharma/biotech company,"Hello everyone,

I just recently joined a big pharma company to do some data engineering tasks (I'm basically more on the data science side, but have some experiences in building data pipelines).

In my previous company (a typical tech startup), building a data pipeline is relatively straightforward, most data sources either have data connectors or APIs so you can easily access the data and ingest them into the data lake/data warehouse, and you can use a lot of open-source tools for data transformation.

However, this time, I work with a lot of data generated from lab instruments, which typically need to be processed with specific software (typically the software made by the instruments' vendors). These software are either desktop app, and/or do not have any programmatic way (API or CLI) to access them. The raw data format is also often non standard text format (i.e. propriatary binary data format and there were no data parser available for most of them). Moreover, data processing is also not straightforward as often times it requires specific expertise in interpreting the meaningful information out of raw data, for example manual labeling and data selection from lab scientists.

Has anyone had any experience in building a data pipeline in this kinda settings? I wonder if I can pick up some successful strategies you guys have implemented previously.

Thank you.",2024-03-07 16:45:33
1b85yc5,"Are there any use cases for Scala over Python, using Spark with the DataFrame API?","Most of the work at my job is focused around a web scrapping | ETL | ML pipeline, where almost everything is implemented in PySpark, using DataFrames. I've recently started learning Scala for my masters. My feeling is that I should be able to significantly improve the performance of some kind of processes by migrating to Scala, but I don't know what to look for. 

My guess is that any code that makes a significant use of udfs could use a migration. Does anyone have any specific benchmark numbers on this? Do you know of any other use cases, apart from this?",2024-03-06 17:41:52
1b53dkg,Iceberg Upsert Streaming Pipelines,"Hi, 
Does anyone has Experience which one of those options for a upsert (based on id) streaming pipeline from kafka to iceberg (which also has to do schema evolution, e.g. automatically adding new cols if some appear) has the best performance:

- Spark Structured Streaming 
- Flink Streaming 
- Tabulars Sink Connector

Which do you prefer? 

I am currently building one pretty flexible pipeline with spark structured streaming, multi-table support (based on column value in data) and upsert per default, running locally on my Mac M1pro Ram limited to 8gb. Current Throughput at around 7k msg/seconds. Was wondering if flink or kafka-connect might be faster and worth a try",2024-03-03 00:23:38
1brs9oj,What are some great blog posts or mini-books regarding some important data engineering topics?,"I just got done reading this blog post, [Machine Learning For Everyone](https://vas3k.com/blog/machine_learning/index.html) and I was really blown away at how much I learned in just 50 or so pages. It's given me such a better overview understanding of the whole machine learning landscape. I'm looking for content similar to this, maybe more related to programming or data engineering directly. Does anyone have anything that comes to mind?",2024-03-30 21:02:31
1bpqthk,Bigquery with AWS,"Hey there,

We are currently using AWS as our cloud provider for a marketplace type platform, and want to implement a Data Warehouse. Since we are still a start-up, we are looking to have as low a cost as possible. We want to be feeding in event data such as Google Analytics as well as SQL data from our own database, with the possibility to extend this to other data sources in the future. Researching the possible options, it seems like Bigquery is a preferred option for many. However, we are using Amazon as our cloud provider, and I am not sure if Bigquery would work well with AWS?  


So, I guess my questions are:

Would it make sense to migrate from AWS to GCP to use Bigquery efficiently, or is it not a problem to use AWS with Bigquery?

Should we use another DW solution than Bigquery such as Snowflake?",2024-03-28 08:53:40
1bp6rbk,Enterprise vs startup internship which one carry more weight?,"I'm graduating this year with a computer science degree, and our internship program is starting soon. As an aspiring full-stack developer, I have experience in both frontend and backend development, along with a background in mathematics. Building full-stack web applications is my passion.

I've fortunately received three internship offers so far:

* Data Engineering role at an enterprise company (tech stack: Python, SQL, AWS)
* Web development position at two startups (JavaScript, TypeScript, Python, Node.js, AWS)

Which one is the best one to pursue to have the best career? should I pursue an enterprise that is offering me a data engineering role, or a web development that I love from a startup? Will it affect my web dev career in the future if I chose data engineering? I am overthinking a lot as I am thinking that this could mess up my career in the future. Will data engineering internship help my career in my full stack career in anyway? ",2024-03-27 16:54:16
1bol3zg,What is minimum file size to justfy storying it as Parquet?,"Storying Parquet files comes with extra metdata and also we incur compute costs for conversion. So, at what point do we convert csv to Parquet? Do you have any specific threshold like 100 MB/1GB etc?

Or do you store all files as Parquet in your data lake (say Silver layer) regardless how big, so that file formats are unified?",2024-03-26 22:24:25
1bnuiuv,Different partitions of same table,"I have an etl pipeline that ingests IOT data hourly. The storage account, bronze, and silver tables are hive partitioned by the year month day hour of when my system received the data. This works well for some use cases.

 I have another use case where having the same data but partitioned by observation date would be way more efficient. Observation date is when my device made the reading, and the partition is based on when my system received the data. Sometimes these are different depending on late data. 

Currently I insert all the data into both tables, one partitioned by y/m/d/h and the other partitioned by observation date. Is there  more efficient way to accomplish this? I can only think of switching to structured streaming with CDC all the way through my pipeline instead of some spots batch based on y/m/d/h, and some spots already structured streaming. Then I could scrap the y/m/d/h table and only rely on date partitioned everywhere. ",2024-03-26 00:47:48
1bnstzw,Experience with Github Copilot for SQL?,"I've had pretty good success with Copilot with things like Python (Airflow), Bash and powershell but not much with SQL.

It hasn't done a good job learning how the different tables join or what I'm most likely to select or fields I'm returning. Has anyone used this with Visual Studio Code or Azure Data Studio?

  
I guess i don't understand why the AI model can't see all my existing code and procedures and database metadata to make intelligent suggestions",2024-03-25 23:35:39
1bne5wy,Trigger airflow dag based on real-time changes to MongoDB database,"Hey guys, I've researched the topic a good amount and still haven't been able to verify whether my idea is possible and is the right approach or not.

**The problem:** at my start-up, we have a mongodb database used for recording different events. We have a postgresql database used for analytics. The company is very immature, and also has cost and regional restrictions making it impossible to use something like aws or gcp for pub/sub.

One (of many) issues we had before I joined was lack of synchronization between one dashboard (in which calculations are in realtime and based on mongodb db), and another dashboard (which uses postgres db which is updated every night via apache airflow). So whatever happens during the day will not necessarily be captured by the postgres-based dashboard.

What I'd really like to do is implement a more CDC-based ETL pipeline. I don't have much experience with this but essentially my idea is to use MongoDB change streams to trigger certain Airflow DAGs (used for transforming data and writing into postgres db), as opposed to triggering such dags daily.

This would only be used on certain tables, all of which don't have millions of daily changes.

I've found lots of change stream examples but none using Airflow. Is this even a good approach? If so, can someone direct me towards how to go about writing the relevant DAGs? I'm confused as to how to use a change stream-based sensor operator?",2024-03-25 13:50:57
1bmqnku,Freelance Data engineer!,"Hi I'm a last year undergraduate student. I have been working on my DE skills. I know for a fact that I won't be able to get a good (remote) job in DE as a fresher. I'm thinking to do DE freelance type work for now. 

I would like to know if anyone has done it before, I would be happy to know your experience, what went wrong, what went great,etc. 

I checked the survey I don't see much of DE freelancers in this sub, but pls let me know if someone has done it before as well.

Thank you ",2024-03-24 17:47:00
1bmompk,Future of OLAP,Been trying to understand the future for on prem OLAP engines in the world of cloud computing. I understand some of these engines are favored by “power users” who value configurability and speed. IBM/Cognos has a tool called TM1.  Seems to make sense to do some analysis on premise if possible (save cost). Interested the community’s views. ,2024-03-24 16:21:10
1bmgi4u,ERD for AWS Athena?,"Hello, 

Im looking to create an ERD for around 700 Athena tables. Is there an easy way to generate one?  Any help here would be great. Thanks! 









",2024-03-24 09:09:49
1blglio,Where can I find some good resources based on Apache Airflow? ,"As a professional in automation testing, I've encountered a new requirement in my current project that necessitates learning Apache Airflow. I'm seeking high-quality references or courses that could guide me through this learning process. If you're aware of any resources, I would greatly appreciate your recommendations.",2024-03-23 01:50:25
1blccwy,Apache beam (batch based processing) vs. docker + kubernetes cluster,"Hi all,

I am having a little trouble understanding the tradeoffs between two dataprocessing methodologies.  I am not a data engineer by trade, and I am trying to understand the cases where different ETL approaches are warranted.

Suppose I have a whole bunch of files that I want to process and then dump the output into a typical SQL db.  I want to understand the differences between deploying a cluster of pods and using ETL tools to process this data.

&#x200B;

1) I can dump my processing code into a docker container and deploy the container in kubernetes to process the files in parallel.  Each pod would process a single file, upload the result to the table, and exit.

2) I can use something like apache beam to define a pipeline with a single batch corresponding to a single input file (Is this true?).  Is there any advantage to going this route?  Will dataflow/Glue handle resource scaling for me, or is it pretty much the same as provisioning a kubernetes cluster?

&#x200B;

I guess the final question is if I changed the input data - think one giant parquet file or an SQL database with all the data from the fragmented pieces in the first scenario as the input - is this where the batch processing approach with beam would really shine, or would I be better off in the original scenario with smaller pieces that are already broken up?",2024-03-22 22:38:57
1bka37v,Advice on restructuring of data infrastructure,"Was recently hired as a data analyst in a small company and I want to help out in restructuring our data systems (we don't have data engineers or the like). We're pulling CSV's regularly (about 4-5 times a day) and we use it to update our BigQuery tables. These CSV's however, are highly overlapping and duplicates across multiple CSV's are common. Our approach right now is just outright deleting a portion of our main table corresponding to the recent previous pulls and then repopulating them with the latest one. This main table is also what we use for regular reporting and visualizations. There are a lot of joins across other tables as well but all tables don't have keys and indices. The joins are also usually on derived columns.

Just started learning about data warehousing and dimensional modelling and I wonder if this is applicable in our case. Should I: create a more efficient fully-functioning database (not necessarily conforming to dimensional modeling frameworks), or design a data warehouse pulling the CSV's that conforms to dimensional modeling, or both, ie. having a database and a separate data warehouse pulling from that database?

Our team is mostly using Google Cloud and I was wondering if BigQuery or Cloud SQL or maybe a combination of both would be the ideal tool to use. Thanks!",2024-03-21 16:04:53
1bk10ch,Need help on a data problem,"Hi, I currently new in this field and want to ask for some advice on this problem.

  
Given N items (N \~ 10\^8), each item has a list of unique items that is ""related"" to it. The average size of the ""related"" list of an item is about 10^3. The problem is, each time, a list of items is given with size \~ 10\^3 items, we have to return the number of unique items in the concatenated list of all the ""related"" items of at least 1 item in the given list.

* Input: Each line is the item id and its ""related"" items. So the input matrix is around 10\^8 \* 10\^3. 
* Output:
   * When given a list of X (X \~ 10\^3) items, we have to concatenate the lists of ""related"" items of X items, and return the number of unique items.
   * For each query, the inference time is <= 1s.

Example:

Input: 

1 2 3 4

2 1 3 5

3 1 2

4 2 5

5 1 4

So the item 1 is related to 2, 3, 4. item 2 is related to 1, 3, 5. item 3 is related to 1, 2 and so on.  
If the query is (1, 4), then the answer is 4. (the list is (2, 3, 4, 5) = (2, 3, 4) + (2, 5)).

&#x200B;

Requirements:

* Exact solution with inference time <= 1s
* Cannot use cloud computing (must run with my own hardwares)

Priority (top to bottom is most prioritized to least)

* Inference time
* Use the least memory
* Simplicity
* Scalability...

What might be the most probable solutions for this? Thanks in advance.  
",2024-03-21 07:37:26
1bjxqc2,How do you manage/notify usage in the cluster?,"I'm in an org which has a 80 node cluster and run lot of data pipelines (pyspark jobs) at different times. 

Recently we have been having lot of issues effectively managing the jobs. There are lot of adhoc analysis which goes on and because of this resource usage reaches more than 80-90%, this results in everything running slow. 

I'm trying to come up with a mechanism to notify high resource users.

My current solution is to run periodic script which check the YRAN resource manager and picks jobs which cross certain threshold, Ex: Cluster usage > 30%, elapsed time > 5hrs etc. 

https://hadoop.apache.org/docs/stable/hadoop-yarn/hadoop-yarn-site/YARN.html

Is there any better way to do it?",2024-03-21 04:04:57
1bjocci,"The ""pip install data stack""",N/A,2024-03-20 21:03:23
1bisp92,How to Load Test Environment Data?,"My company currently has separate dev/test/prod environments for our data warehouse. Using one data source as an example:

We have scripts on a test VM that extract data from the data source's test server and loads the data to our test environment. We have the same scripts on a prod VM that extracts data from the data source's prod server and loads to our prod environment.

Problem: 

1. The test server on the data source isn't routinely updated, and is thus out of sync with prod and makes a 1 to 1 validation of data for new dev work difficult when users look at the prod data on the data source application. 
2. It seems redundant to have scripts extract data from both test and prod servers, when we could alternatively extract just from prod data source server into our prod data warehouse and then push straight from our prod environment to our test environment so that minimal resources are used for ETL and test data aligns perfectly with prod data.

Question:

How do you guys typically manage ETL for your test environment? Am I right that it would be better to only load data into the prod data warehouse and push from there into the test data warehouse?",2024-03-19 19:21:09
1biqsae,Open Source Kafka connector to send kafka topics data to 200+ destinations,N/A,2024-03-19 18:03:46
1big8k5,Turning sql pipeline into flowchart,I'll be working on a migration project and I have a ton of etl pipelines with 2k+ lines of sql and I was wondering if anyone knows a tool that can gate the sql script and turn it into a sort of flow diagram to understand all the tables that are being created and their relation ,2024-03-19 09:33:46
1bhfmka,How to move from SAP BW to Data Engineering?,"Wondering if anyone here moved from a SAP BW background into core Data Engineering field? Is this even possible? Looks like SAP experience is not strongly considered as a transferable skill to other tools. I have been learning data structures, streaming tools, databases etc. because its a very interesting space but unsure if I could ever switch to a DE job complementing with my SAP BW exp. ",2024-03-18 02:09:49
1bgy3ct,Looking for technical blogs sharing solutions,"Hello,

I was browsing internet and found some blogs describing a problem, and giving sample propositions on how to solve it. Do you know of any bloggers or technical content creator who does the same thing with Data engineering problems, modeling and/or Architecting solutions ?

Thanks for sharing",2024-03-17 13:54:54
1bfxq8p,Is a Master's in Data Engineering Worth It for International Students in the US?,"Hey everyone! So, I'm currently working as an entry-level Data Engineer with 3 years of experience under my belt, and I'm really eager to take my career to the next level. On top of that, I'm about to become an international student in the US this fall.

Now, here's the thing: I've been researching Master's in Data Engineering programs, and honestly, there aren't too many options out there. The ones that caught my eye are UNT and UW Madison. I've applied to Madison for the MS DE, along with a bunch of other colleges for MSCS, but I've got some burning questions.

What's the scoop on the reputation of a Master's in Data Engineering in the US job market? It seems like a bit of a niche compared to CS. Will having this degree put me at a disadvantage when competing with CS grads for Data Engineering positions? Is it even worth it to pursue a Master's in Data Engineering?

I'm feeling pretty good about my chances at Madison, but I want to make sure I understand how the market views this degree. Any insights or experiences you can share would be super helpful!",2024-03-16 04:20:59
1bfsb8o,Fundamentals of Data Engineering vs Designing Data Intensive Applications,"I'm a beginner data engineer and I wanna read Fundamentals of Data Engineering but I'm not sure if it's right for me at the moment? From my research the biggest criticism of the book is that it's bloated with technical details while not being a textbook and doesn't have enough useful use-cases.

I'm reading DDIA at the moment and I'm really enjoying the pace and find it more or less easy to digest. Is FoDE more difficult than DDIA? Would you recommend it to a beginner who's read DDIA?",2024-03-15 23:52:21
1bfpn8q,Data science to data engineering,"Can you please suggest what pathways one could pursue after doing a masters in data science do data engineering?

Thanks for all your suggestions.

&#x200B;",2024-03-15 21:55:15
1bfdjvj,Preventing pipelines from breaking because of upstream changes ,"Hi

I recently hit this scenario -

Pipelines expect a certain schema to exist, this schema changes because life (out of DEs control (prod db?)) => pipeline breaks => downtime 

Does this happen to you?

What can I do about it?",2024-03-15 13:07:44
1beza3c,Comprehensive Guide to Optimize Spark Data Workloads | Databricks,N/A,2024-03-14 23:21:20
1bedblt,DBT source for production and testing,"If you guys are using DBT, I would like to know if my approach is good or not.

I am using dbt with bigquery. In same gcp project, I have dataset\_staging and dataset\_production. In production target, I use dataset\_production as source. And for local testing or staging, im using dataset\_staging. The tables in dataset\_staging is just the copy of tables from dataset\_production with limited data from [table sampling](https://cloud.google.com/bigquery/docs/table-sampling).

To use them as source for a dbt model, this is what I am doing in my source yml:

    sources: 
      - name: some_dataset
        schema:  ""{{ 'dataset_production' if target.name == ""production"" else 'dataset_staging' }}""
        database: gcp_project_id
        tables:
         - name: table1
         - name: table2

I am not sure if this is the standard way. Or am I supposed to use dataset\_production as source even for local testing and staging purpose? The actual goal is not to scan whole table partition during testing, as each partition is more than 5TB.

&#x200B;

&#x200B;

&#x200B;",2024-03-14 05:00:40
1be7oi7,RDS to S3 migration,"What services do you recommend to make this migration as fast and smooth as possible, just once? First though is glue (used before but different purpose), or DMS service (have not never used it).ed daily, storing only 90 days. This is one of the problems of the migration, that querying from it is impossible my first approach was with a simple lambda, MySQL connector, and python script and chunk it, but would take me about 2 days if I do that. Also, the idea is to have this data somewhere else before thinking of a Lakehouse solution. My questions are:  


* What ETL do you propose to make this process daily (1.5M records) comes to glue again if i am succesfull with the first bullet pointe but for different purposes), or DMS service (have not never used it).
* What ETL do you propose to make this process daily (1.5M records) come to glue again if I am succesfull with the first bullet point
* Lastly, this data is desired to be used for analytics, initially will be in S3 to make queries using Athena while the team gains an idea about the KPIs they want to track, in the future the idea is to have it somewhere else (I though of a lakehouse) that makes it fast to query and build sql models with it. The whole company env. is in AWS so my first thought is RedShift but I like the efficiency and how GBQ handles this amount of data

Thank you so much for reading!",2024-03-14 00:24:36
1be2xai,Virtual Workshop: Real time data streaming + Analytics + Visualization,"Hello,

I'm organizing a workshop next Tuesday, March 19, 2024, at 4 PM Pacific Time. During this workshop, instructors will build a real-time dashboard for analyzing pizza shop orders using OSS products: Bytewax, Pinot, and Streamlit.
For more details and to RSVP (attendance is free), please visit:
https://bytewax.io/events/real-time-pizza-analytics

I believe this workshop is especially noteworthy for those interested, as I've personally enjoyed migrating the previous solution from Kafka Streams to Bytewax. The solution is now more Python-friendly and the complexity of the codebase is significantly reduced. For me, it felt truly refreshing to rewrite it 😄 I believe many data engineers would prefer Python solutions over JVM based.

I am still in the process of updating the repository and will share it later, but here is the previous version:
https://dev.startree.ai/docs/pinot/demo-apps/pizza-shop",2024-03-13 21:10:29
1bdvhll,"Apache Doris 2.1.0 is released, with doubled out-of-the-box performance","Hi data fellas, with version 2.1.0 available, we take this opportunity to re-introduce Apache Doris as an open-source data warehouse that provides:

* **Higher out-of-the-box query performance**: 100% faster speed proven by TPC-DS 1TB benchmark tests.
* **Improved data lake analytics capabilities**: 4\~6 times faster than Trino and Spark, compatibility with various SQL dialects for smooth migration, read/write interface based on Arrow Flight for 100 times faster data transfer.
* **Solid support for semi-structured data analysis**: a newly-added Variant data type, support for more IP types, and a more comprehensive suite of analytic functions.
* **Materialized view with multiple tables**: a new feature to accelerate multi-table joins, allowing transparent rewriting, auto refresh, materialized views of external tables, and direct query.
* **Enhanced real-time writing efficiency**: faster data writing at scale powered by AUTO\_INCREMENT column, AUTO PARTITION, forward placement of MemTable, and Group Commit.
* **Better workload management**: optimizations of the Workload Group mechanism for higher performance stability and the display of SQL resource consumption in the runtime.

This very informative release note provides details of the performance tests and the implementation of the new features and mechanisms that make the difference. Take a look and tell us what you think!  
[https://doris.apache.org/blog/release-note-2.1.0](https://doris.apache.org/blog/release-note-2.1.0)",2024-03-13 16:17:36
1bdtjwb,Keeping org tools in sync,"Long story short, we are trying to keep company tools in sync. today it is a mess that comes to bite on the data end as it creates low trust in data quality. 

Now some tools have native integrations, which makes things easy, but for some reason, the toolset that our company settled on does not have this (outside of a few mainstream apps)

The way I see it we have three options 

1. Build our integrations between tools and/or integrations between our warehouse and the tools (reverse ETL), seems like lots of work to build and then maintain 
2. Use an IPaaS solution, here I would have to have someone learn the tool as each one of these tools usually uses some proprietary way of doing things. 
3. Reverse ETL tool, this is new to me, haven't really worked with one in the past but I had a call [Hightouch](https://hightouch.com)Reverse ETL tool this is new to me, I haven't really worked with one in the past, but I had a call with Hightouch, and it seems promising. It seems to fit the data team a bit better than the other 2 options as most of the work will be done within warehouse dbt transformations, which anyone in the team can do   


Any recommendations on how to tackle this? Have you guys worked with a Reverse ETL tool? If yes which one and what's your feedback   ",2024-03-13 14:58:53
1bdcdex,Has anyone deployed Streamlit for some intense workload?,"Hello everyone,

Please feel free to stop me in my tracks if I’m going down a bad path. I want to develop my data input forms as webapps with Streamlit. They won’t be dashboards as is typical, but data collection tools. I love the minimal look of Streamlit and it’s quite easy to put out a working product quickly with it. However, that ease makes me concerned a bit- is it safe for deployment?

Some background:

I’ll probably want to deploy several replicas of the application and distribute load among them for high-availability. Perhaps only 2-3 instances is fine though, as this isn’t for high load.

Users will access the app from remote locations, potentially up to 10-15 at a time. Their internet connection may be poor at best, but if their internet goes out it’s not technically my problem anymore. I’ll just want to make sure (for now) that I can perform well under medium latency.

I’ll launch it was k3s to automate a lot of the infrastructure management. FastAPI will sit between Streamlit and the database.

I guess if nobody knows, I’ll give it a shot and let you guys know how it goes. Can anyone offer some insight ahead of time though?

Thanks!",2024-03-12 23:38:20
1bcihcy,Data Warehouse to Lakehouse Evolution,N/A,2024-03-11 23:48:40
1bbwnbq,GCP dataproc cluster related question,"1- In production, what's the best approach ? To create and delete cluster with each job or use the existing one ! 

2- if we go for using existing cuslter and restart and stop approch then if 3 teams accessing same cluster then how to stop cluster only after completion  of 3 different jobs ?

What happens in real world production pipelines ?


",2024-03-11 06:26:46
1banv68,Data engineering student in need of help,"I'm currently working on a patent analysis project and I'm seeking insights on how to efficiently scrape data from the United States Patent and Trademark Office (USPTO) website. We aim to stream this data into a data lake for further analysis. I'm particularly interested in methods or tools that can help streamline the process and ensure accuracy in collecting patent information, while also facilitating the integration of the scraped data into our data lake infrastructure. Any advice or recommendations on best practices, relevant libraries, or specific techniques would be greatly appreciated. Thank you in advance for your insights!",2024-03-09 18:02:37
1badj0t,Question on Vaccum data to save storage,"Currently we don’t have vaccum of data happening on regular basis and so size of object storage is growing in huge scale. We need actual data’s retention to be 1 year at-least -  in that case do I need to vaccum delta table (object storage bucket) - by setting  vaccum retention period as 1 year? This is so when in-case 1 year old data is needed we can time travel using timestamp.

deltaTable.vacuum(8766) 

# 8766 is 1 year in hours

As it is delta table though it’s object storage bucket - rather than life cycle policy, vaccum is what should be done? Any thoughts or suggestions? ",2024-03-09 09:02:57
1b9jo2d,Productizing data services: Removing fears with the LEAP framework,N/A,2024-03-08 08:55:25
1b9alt0,Need advice on ELT: MS SQL SERVER to PostgreSQL,"I won't give much context here as I work with classified data, I'm a software engineer(not a data engineer).

What are some viable options for me to move big data from MS SQL SERVER to PostgreSQL for a daily batch process. The data is large enough for LINKED SERVER to be bottleneck while transferring to PostgreSQL.

I thought of Apache SPARK, if so can you give me the drawbacks.  


Thank You!",2024-03-08 00:57:09
1b8ytdj,"Best practices for Terraform, AWS Glue, and CI/CD in data engineering","Hello everyone,

I'm working on streamlining our data engineering processes using AWS Glue, and I'd love to learn from others' experiences regarding:

* **Terraform for Glue:** Do you use Terraform to manage and provision your AWS Glue jobs and related infrastructure? If so, would you be willing to share some insights into your setup and any helpful tips?
* **CI/CD for Glue scripts:** How do you integrate CI/CD (e.g., GitLab CI/CD) to synchronize Python scripts with S3 buckets? Do you employ a monorepo strategy, or do you have a preferred alternative? What triggers your CI/CD pipelines (file changes, scheduled updates, etc.)?

I'd greatly appreciate any details on your best practices and any challenges you've overcome.

Thank you in advance for sharing your expertise!",2024-03-07 16:25:24
1b8xm1f,Data Transformation Comparison,"Tools like dbt are becoming a popular alternative to the legacy and platform tools like IBM DataStage or Azure Data Factory. Here is a quick comparison of five leading contenders to own the transformation stage in your data platform. (I leave off the dbt clones as largely derivative)  


||[Coalesce](https://coalesce.io)|[Coginiti](https://www.coginiti.co)|dbt Cloud|[Prophecy](https://www.prophecy.io)|[SQLMesh](https://sqlmesh.com)|
|:-|:-|:-|:-|:-|:-|
|Transform|LowCode|AaC|AaC|LowCode|AaC|
|Versioning|✅|✅|✅|✅|✅|
|Scheduling|✅|✅|✅|✅|✅|
|SQL|✅|✅|✅|✅|✅|
|Python|❌|❌|✅|❌|✅|
|DQ Tests|✅|✅|✅|❌|✅|
|Obj Store|❌|✅|❌|❌|❌|
|AI Assistant|❌|✅|❌|✅|❌|
|Orchestration API|✅|✅|✅|✅|❌|
|Data API|❌|✅|❌|❌|❌|
|Lineage|✅|✅|✅|✅|✅|
|Ad-Hoc Query|❌|✅|❌|❌|❌|
|Supported Platforms|Snowflake|Netezza, DB2 Warehouse, Databricks, Redshift, Athena, BigQuery, Yellowbrick, Starburst/Trino, Synapse, SQL Server, Postgres, Snowflake, Spark, Hive, Greenplum|Redshift, BigQuery, AlloyDB, Databricks, MSFT Fabric, Starburst/Trino, Postgres, Snowflake, Spark|Databricks|BigQuery, Databricks, DuckDB, MySQL, PostgreSQL, Redshift, Snowflake, Spark|

&#x200B;",2024-03-07 15:35:36
1b8sa5g,skyffel - prototype for generating Airbyte connectors,N/A,2024-03-07 11:22:07
1b8hrsh,"Iceberg + Dbt + Trino + Hive : modern, open-source data stack"," To provide a deeper understanding of how the modern, open-source data stack consisting of Iceberg, dbt, Trino, and Hive operates within a music streaming platform, let’s delve into the detailed workflow and benefits of each component. 

[https://medium.com/@stefentaime\_10958/iceberg-dbt-trino-hive-modern-open-source-data-stack-3567568d6597](https://medium.com/@stefentaime_10958/iceberg-dbt-trino-hive-modern-open-source-data-stack-3567568d6597)",2024-03-07 01:37:40
1b8duhr,How to make this system design,"I am presently using [draw.io](https://draw.io), and I have been unable to locate icons resembling those in the provided image. I would greatly appreciate your assistance. Thank you.

https://preview.redd.it/qo3c75lflsmc1.png?width=1018&format=png&auto=webp&s=131a2a872005cd7f3543497c0d7c8e21b80f9505",2024-03-06 22:49:17
1b6rxl1,Is there a guide for how to choose tools based on DB size?,"I need to build a DB that will house years worth of 15 or 30 minute data for a couple hundred facilities. So about \~15M rows/year with about a dozen columns and some dimension tables. End goal is using the data for visualization and reporting

Rather than just ask what tool I should use for this specific project I wanted to know if there are general rules for what tools to use based on expected table size. I feel like for smaller data like my setup something like SQL Server Express would be fine but I figured I'd ask. I also want to use tools that will help me self market for my next role.",2024-03-05 01:00:10
1b6k06l,Mage AI experiences?,"Hi,

Trying to give guidance to a company in mid market 600 FTE. They have heard about Mage AI and are interested in it. Any experiences with it? Does it solve certain problems that Airflow/Prefect/Astronomer doesn’t? ",2024-03-04 19:38:33
1b67asb,Can data fetched on a per day basis be treated like timeseries data and stored in Cassandra/HBase?,"Hi, I have data coming from an API for the buses running daily within the country. The data for a particular bus looks like this:

    [{'busNumber': 1,
    'departureDate': '2024-01-01',
    'operatorCode': 1,
    'busType': 'ev',
    'busCategory': 'Long-distance',
    'timeTableRows': [{'stationCode': 'B01',
                       'type': 'DEPARTURE',     
                       'busStopping': True,     
                       'confirmedStop': True,   
                       'scheduledTime': '2024-01-01T12:24:00.000Z',     
                       'actualTime': '2024-01-01T12:24:58.000Z'},
                      {'stationCode': 'B02',     
                       'type': 'ARRIVAL',     
                       'busStopping': True,     
                       'confirmedStop': True,  
                       'scheduledTime': '2024-01-01T12:29:00.000Z',     
                       'actualTime': '2024-01-01T12:30:53.000Z'}
                     ]

I understand that because of the already present structure, relational db like postgres, mySQL would be the best databases. But, can this data be considered like a timeseries and be stored in NoSQL databases like Cassandra/HBase?",2024-03-04 10:01:27
1b5k1kf,Experience with Sling for data ingestion?,"I came across Sling and was wondering if anyone has experience with it. Any pros/cons?

https://slingdata.io/",2024-03-03 15:39:46
1b533il,How common is it for low-code drag & drop tools to provide the option to work with code?,"A bit of clarity. In my company we use the Infosphere suite for all the major ETL pipelines. I hate it with a burning passion, all the advantages it provides are mostly circumstantial, and I especially hate dragging icons on a screen when the same task could be done ten times faster by using code. Luckily, it gives you the option to export and import jobs via xml, so I built myself a script that makes what I need by taking in input an excel spreadsheet with some parameters.

Do most tools give you the option to import/export stuff in a human-readable format? I was thinking about improving the script so that it does the same work for other tools. Also, and this is a long shot, is there some open-source software that does the same thing, so that I can study it?",2024-03-03 00:11:06
1b4p1t1,Big Data Computing in Data Engineering,"Hi, I wanted to ask how important it would be to learn (theoretically / practically) the following topics with in mind a data engineering future career:

Distributed framework Apache Spark.

Clustering for data analysis and summarization.

Analysis of data streams.

Similarity search.

Association Analysis.


Thanks!!",2024-03-02 14:06:10
1bryn1j,Tools for azure dw?,"My current company is asking me to propose improvements to our current stack,

We currently use azure synapse with DBT . Using mostly adf to load from blob storage to synapse then DBT to bronze , silver , gold.

I'm asking on suggestion of what could be proposed? 
So far I'm thinking:
Dagster for DBT we don't have an orchestration tool so this will work

Great expectations , our data needs to be validated step by step and I believe we need something like this to valdiateit and document it.

We don't have anything for migrations but might be an overkill. However I will probably propose Flyway

What other tools or plugins do you recommend as a must have for a new data lake/ data warehouse?",2024-03-31 01:45:27
1bqpk6a,What’s your experience with Policy based access management? Any tips,"We have used RBAC and ABAC, starting to discuss PBAC - any thing you can share if you have implemented PBAC?",2024-03-29 13:40:43
1bqiwgl,How to turn into Data Engineer from Backend Developer,"I want to change my path from .Net Backend Developer to Data Engineer.   
I have 8 YOE in .Net Framework (API,SQL , Azure) with decent pay package.  
I want to turn to Data Engineering so what path do I need to follow and  
 should I even switch considering my experience as if I land a job in DE , I would get paid as close to a fresher?  ",2024-03-29 07:02:07
1bppi9f,Data governance engineer role,"Hey guys, I recently got an offer for the role of data governance engineer (junior) at a company which is involved in digitalisation of supply chain and manufacturing. They have a nice cloud team and the topics are interesting. Since I’m also quite new to this area of data governance and inexperienced. 

I’d like some advice on this. What’s the future of this role? What career opportunities will I have in the future in this field? What can I also jump into later? And how relevant is this role in today’s market? 

Thanks in advance! ",2024-03-28 07:19:33
1bpp4ia,"What is better for your career, learning some DS techniques or going all in on DE (specifically a question for DEs in azure shops: DP100 or DP600)?","This month you can earn a free exam voucher for both the certifications from microsoft. DP100 is the data scientist associate, while DP600 is for a fabrics analytics engineer. 

Would it be better long term for your career to have a cert showing that you know at least the basics of DS or is it better to have another cert that shows that you have more depth in DE (especially if you have the DE cert already dp201). I am thinking about opportunities with startups/small shops that maybe need more of a DE/A/S (aka a data god) 1 or 2 person doing everything data team, or with bigger firms needing some DEs to link with their DS departments.",2024-03-28 06:53:44
1bpgecu,"Starting my career as fullstack(java/react) or Data integration(Boomi, Talend ...) for a Data Engineer career","
Hello guys,

I hope that senior people or people who were in same position can help me to take the decision.

I am a Master Software engineer graduate and I did internships in Data Engineering ( Airflow, Python, SQL...). After I graduated I didn't find a proper data engineer role so I have two offers now.
Starting my career as a full stack engineer (Java/React) or Data Integration role( Boomi, Talend ...) .

I know that using ETL GUI tools are not good in general and I don't like web developement a lot but which role is more relevant to me as I want to be a data engineer in the future and immigrate to Europe because data engineers roles are rare in my country.

Thank you in advance.
",2024-03-27 23:23:25
1bp36tt,"First DE project, help deciding on infrastructure (Prefect/Dagster/Others?)","Hello, I'm setting up my first DE project in my homelab and trying to figure out what software to use for my infrastructure. I'm the only one working on this project part time, so I want something powerful enough to make me efficient building this out, but not so overly complex that it takes a lifetime for me to setup. My project at this point is pretty simple, mainly collecting data from APIs, doing some cleaning and calculations on it, and storing it in Postgres database. I'll also have some Anvil dashboards to view the data.

I'm running this from a single linux server at home and would prefer keeping most of this self-hosted with docker containers. I've set up and started using Prefect for orchestration and like it so far but it seems that it doesn't play nice with classes and OOP. That's not a deal breaker for me, other than I've been trying to push myself to write more OOP since I tend to think of things in more of a procedural way (and maybe procedural is best here?). I do like how Prefect pulls from my Github account so updating my scripts is very easy.

I'm also looking at Dagster, as it looks pretty nice but I've seen the learning curve is steep. Not a problem if that is going to pay dividends in time savings later, but I don't want to introduce unnecessary complexity.

In my software stack, is there anything else I should be adding? I've seen DBT quite a bit, but not sure if it that will help me or just be even more complexity I don't need. Thanks for the tips!",2024-03-27 14:26:36
1boia8q,By how much is storage in data lakes cheaper compared to data warehouses?,"In AWS Redshift data warehouse it costs 0.0256 $ per gb to store data and in S3 (data lake) it costs 0.0018$. So, 0.0018/0.0256 = 18 times cheaper? Does this estimate allign with your experiences?

  
 ",2024-03-26 20:33:49
1boc91z,What to use for an open source ETL/ELT stack?,"My company is in cost-cutting mode, but we have some little-used servers on-prem. I'm hoping to create a more modern ELT stack than what we have, which is basically separate extract scripts run through a custom scheduler into a relational database. Don't get me started.

I'm currently thinking something like the below, but would be very happy for some advice. Nobody on our team has any experience with any of them, so we're (a) open to new, but (b) wary of steep learning curves:

\[Sources\] (many, sql/nosql/flat) -> \[Flink\] -> \[doris\] -> \[dbt\] -> \[doris\]

Currently approx 5TB of data, will probably double this year as more is added.",2024-03-26 16:33:47
1bnt8o9,Analytics to Big Data Engineer Tips,"Hi all, moving from basically a completely SQL based analytics engineer type role to a technical big data engineer role utilising Scala and Spark.

There’s a lot more experienced engineers here than me so from your own personal experiences or that of your colleagues:

- What will be the biggest shock/change?
- How long did it take to feel comfortable in the new role?
- What did/could you have done that made the transition smoother? 

",2024-03-25 23:52:54
1bnm4w0,Ground Truth vs Gold Standard vs Baseline vs ???,"How do you call the curated dataset that you use to validate or unit test your transformation?

Example, say you have a manual process that you are automating. The users have their way of extracting the raw data, and doing their thing in Excel with procV's galore, or something like that and you are automating all the business rules and making it run daily. Now, you're going to validate if your pipeline recreates exactly what the users used to do in Excel.

How do you call the dataset that you have to mirror? baseline, ground truth, gold standard, template?",2024-03-25 19:14:52
1bnis0n,Potential Issues with Data Virtualization,"I run a data engineering consultancy and we do straightforward work of creating data architecture, data pipelines, data warehouses and data lakes. 

One of the potential customers we are talking to require Data Virtualization rather than traditional approach. Has anyone worked in this field? I read that the biggest nuances of data virtualization is performance at scale because most of the data are loaded to some sort of virtual instance at runtime for consumption. 

The prospect told me they already have Data Lakes, Warehouses and multiple other data sources with gazallion data and don't want to move anything to a new storage. So I'm a little worried if we are biting more than we can chew, but I don't want to lose a business that is already at the doorsteps. 

What are some implementation techniques you guys used during virtualization to address speed and performance issues?",2024-03-25 17:02:14
1bnbtne,Spark Architecture,"Hi All,

As many of you reached out to me and said to continue the blogs I have decided to get back at it. However, 1 glaring issue was that all my blogs would be lost with time on reddit hence I have decided to post them on YouTube going forward. This will ensure all of you and the future folks can access it whenever you want as well as I can teach with more creative freedom than being restricted to writing.

I have posted my first video today which is about Spark Architecture: [https://youtu.be/8JPu1CECtPE](https://youtu.be/8JPu1CECtPE)

Note: As this is my first video there are some problems which I am aware of like maybe sound, getting stuck for a short time while explaining at times, flow may seem a bit off, rest assured I will keep improving, for now goal is to get these videos out to world and improve myself in the process rather than waiting for perfection and never getting started :) ",2024-03-25 11:57:35
1bl0kxw,BigQuery IAM dilemma,"In my company, we restrict certain tables to certain people in the company, depending on their clearance to see the tables.  


We have implemented IAM permissions at the table level. Say data engineer X should not see finance records, so he is not on the list of people who can view the tables.

The dilemma comes in when using service accounts. Airflow needs access to the tables for extraction and DBT for modelling. What's stopping engineer X from using such to gain access to the private tables?
",2024-03-22 14:28:23
1bkq5yk,Asking for help with side Project,"Hi everyone, 

I am currently a Junior in college and am doing an end-to-end analytics project that requires data extraction (web scraping), data cleaning, EDA, etc... Right now I was wondering if there's any way to schedule the [extraction.py](https://extraction.py) file to run every 2 weeks, then trigger the data\_cleaning.py file to run after the [extraction.py](https://extraction.py) file. Also, I am open to any feedback regarding my project. Since I am an MIS major instead of CS, my code might not be as clean as it is supposed to be, but I am trying my best to work on it daily. Truly appreciate the feedback and the help.

[Project Link](https://github.com/MarkPhamm/British-Airway)",2024-03-22 03:47:13
1bjjte9,Small Data Architecture,"In my current position (and for personal projects) I find myself doing a lot of bespoke problem solving as team of 1 or 2. Examples:

\- Scrape and munge some amount (<10GB) of historical data from this website. Have it in a format where we can calculate \[potential metrics\] if we want to write about it in the future.

\- The metrics on this government website don't give us what we need -- monitor the site for updates and trigger a pipeline that downloads the data, calculates our metrics, creates charts, and emails a summary.

There's no online application, no warehouse, not much technical expertise on the team. The objective is to get the task done in a reliable, maintainable manner while minimizing cost.

I have found myself often favoring a data lake (S3 + medallion architecture) + serverless functions (AWS Lambda). Email alerts when things break, Cloudwatch for monitoring. Feels so ethereal not worrying about servers and databases while still delivering valuable data.

Who else does ""small data engineer"" work and what architecture(s) do you typically use to get the job done?",2024-03-20 17:57:37
1bjd57h,Peer Driven Projects for Experience and Learning Purpose,"I'm looking out for any website/platform which allows users to interact and users will be able to work together on the project. If it's not present, I'm proposing following:

A platform or website which have functionality to support following workflow: 

  
**1**. **Ideation Pitch In** \- Anyone can come and add their project/task ideas in the posts. For ex. Need to build a dashboard from this [dataset](https://voaratinglists.blob.core.windows.net/html/rlidata.htm). 

**2**. **Peer Gathering:** People gonna see the post and gonna add the enhancement that can be done on this. ex. in dashboard, can we add this feature?

**3**. **Task Division:** OP can divide the tasks initially once the project is finalized (ofc they can add enhacements in between).

**4**. **Task Ownership:** Peers who wanna pitch in can take these tasks and own them (if that owner drops off, someone else from community can pick that up)

*Advantage*: They can show up these projects in their portfolio as they are openly available in public Github, Also if some ideas are really useful to everyone, they can easily showcase them. 

Let me know what you guys think of it. Any feedback is appreciated.  


&#x200B;",2024-03-20 13:14:19
1bj6pp2,"Leetcode, programming capability and approaching technical problems","I have been working as a Analytics Engineer/Data Engineer for a year and been solving Leetcode problems lately (to prepare for new opportunities). The following questions just came up in my mind regarding Leetcode and how to approach solving technical problems in general.

**1. About intellectual capability to program:** I was able to crack some Easy level LC problems, but this one (https://leetcode.com/problems/subsets/) made me doubt my ability. I came up with a general solution (pseudo-code) and a runnable program (using recursion) for that problem within an afternoon, but only until the next day did I find a way to write it without using recursion. I do not have a CS background, but I know that interviews are very time-intensive and my colleagues with CS background can solve this kind of problem in minutes. With my age approaching 30, am I screwed with regards to my ability to program well and effectively? I'm really afraid that I'm not intellectually capable of programming *effectively* (i.e., coming up with efficient and smart solutions to problems).

**2. About approaching technical problems:** For me, it takes quite some time to solve LC problems and lots of tweaking/debugging to pass all test cases. I feel like if I come back to those problems, I won't produce the exact answers. How would you organize the ideas that you learned from solving technical problems like LC? Do you recommend remembering all the details/edge cases of the code or just remember the general approach to each problem or the kind of the problem? Should I read algorithm textbooks as well to invigorate the ideas learnt?

Thank you for reading.",2024-03-20 06:10:52
1biw65l,Property listings data modeling,"Hi folks, i´m trying to model my data from a scraper that i made. I´m able to retrieve around 26 columns from an api and put togheter in my dwh in bigquery, obviusly the first table that i have is one OBT table but i realize that along the time the prices maybe change and if the owner make renovations some characteristics around the property also can change, the status if it´s sell can change.

https://preview.redd.it/2rvqo23o1dpc1.jpg?width=701&format=pjpg&auto=webp&s=16d470f03ebffc901b4beeeeefe526318242598c

In the obt is hard to mantain these logic and i want to use scd type 2 to track this changes in the dimensions. So the first thing that i need to do is model my data as you can see in the image.

But i´m a few undecided if my schema it´s OK or something can change.

* In the location table, i see that if is a condo maybe have the same adress and geographical data, but i´m not sure if i can put the adress and geo data into the fact table
* for the attributes dimension also i only think that is 1 to 1 relation because there is no chance that 2 property listings have the same property\_id and url. So i don´t know how to aproach this dimension
* the other dimensions i think are ok

What do you think about my schema and if i need to do some improvements?

thanks for your support folks.",2024-03-19 21:40:01
1bige78,dbt vs databricks dot,"Hello Engineers, I am kinda starting to learn and use dbt and have used delta live tables from databricks(dlt) in the past. I am trying to understand the point of using dbt when I am already in databricks environment.

Could someone who has the experience point out some of the scenarios where you found dbt more useful than using dlt. I assume dlt will be bit costlier but would love to hear your thoughts.",2024-03-19 09:45:41
1bi83x0,Understanding Dimension and Fact Table References: Seeking Practical Insights,"Starting a new project, I'm grappling with some debugging challenges:

1. **Dim tables referencing fact tables:** Is this standard practice or a potential pattern to reconsider?
2. **Multiple fact tables referencing other fact tables:** How do we effectively manage this complexity?

Seeking practical advice: Are these situations acceptable, or should we prioritize a data model refactor within our team? Your insights would be invaluable!",2024-03-19 01:23:15
1bi4oww,Practical Data Engineering: A Hands-On Real-Estate Project Guide,N/A,2024-03-18 22:55:48
1bghnuc,Optimizing Multi-Tenant B2B Integration with Apache NiFi: Seeking Feedback and Insights,"Greetings,

We're currently in the process of developing a multi-tenant B2B integration platform, and I'm exploring architectural options, particularly regarding the use of Apache NiFi. I'm considering whether to implement separate NiFi clusters for each tenant or to leverage NiFi's capabilities for multi-tenancy within a single cluster.

Our envisioned data flow involves routing data from various sources such as Amazon Marketplace, Shopify Marketplace, ERP systems, and SAP orders to our Order Management System (OMS) via NiFi. We anticipate a data movement pattern from API to staging and then to another API, with NiFi serving as the translator between these APIs.

For instance, orders from different marketplaces or systems may need to be directed to different OMS instances, and our integration platform would facilitate this process seamlessly.

I welcome any insights or suggestions on optimizing this approach.

Looking forward to your input.",2024-03-16 22:16:06
1bg6jfl,A Taxonomy Of Data Change Events,N/A,2024-03-16 13:51:02
1bftu0k,Building Serverless ELT (that can also run locally),N/A,2024-03-16 01:02:13
1bew1z0,We used Temporal IO to build data pipelines & it just works ;),"At [Multiwoven](https://github.com/Multiwoven/multiwoven), When building our [open-source reverse ETL platform](https://github.com/Multiwoven/multiwoven) on Ruby on Rails & Postgres, as part of the ETL process running SQL queries in background and extracting data from data warehouses was critical and we needed a way to handle long running workloads.

We wanted something that was durable & that could handle retries, our initial thoughts were to use Rails Sidekiq, but the lack of durability and the need to handle retries made us look for other options, also we didn't want to create a dependency on Redis.

When we first did a POC with [Temporal IO](https://github.com/temporalio/temporal), we were amazed by the performance of running long tasks using Postgres as data store, we were able to benchmark by running a workload that tool **3-4 minutes** to complete and processed **100K records** from a data warehouse. Of course, we had to make some changes to our code to make it work with Temporal IO, but it was worth it.it also provided a nice looking UI to monitor the Syncs and the ability to retry failed tasks, which was a big plus for us.

https://preview.redd.it/1qemhu4h7doc1.png?width=3824&format=png&auto=webp&s=6deb4d423b3f8b6a7761d6d6e98570f968fafa33

**If anyone looking to build a long running workloads, I would highly recommend Temporal IO, it just works ;)**",2024-03-14 21:06:21
1bes500,Can you help me understand checkpoints in structured streaming ,I have pyspark structured streaming code which writes to hive using in micro batch mode. I am using foreachbatch. I want to know if that particular batch failed then check point is commited or not. When exactly checkpoint is commited? Is it when function passed to foreachbatch runs sucessfully or just after passing df to that function .,2024-03-14 18:26:04
1be395d,"Redesign ETL process, DBT table materialization","Hi,

Please give me any suggestions what will be the best way to redesign this data pipeline, to ingest data from the BI tool back to the source table in Snowflake.  
Current setup:

https://preview.redd.it/94pdir9f16oc1.png?width=1096&format=png&auto=webp&s=b0f3c45506cf2a3c288d780fead9f33d979e3267

 I have a Google spreadsheet that is transformed daily with DBT and stored in Snowflake DB as a mart-ready table for analysis. This table undergoes a full refresh daily. Later, this table is connected to the BI tool where Data Analysts analyze the data. However, they now want to manually input data in Power BI so that this data will be saved in the initial source table in Snowflake.

How should I approach this, considering that the initial table is refreshed daily? I'm thinking of changing DBT materialization to incremental or creating a new extra table in Snowflake that will be a combination of raw source data and input from BI. Maybe there is a better way yo do it?  
",2024-03-13 21:23:02
1be2kwd,How to integrate Great Expecation Data Quality tests in Airflow?," 

&#x200B;

# How to integrate Great Expecation Data Quality tests in Airflow?

📷[**Blog**](https://www.reddit.com/r/dataengineering/search?q=flair_name%3A%22Blog%22&restrict_sr=1) Orchestrate Modern Data Stack

Vlog on how to how to integrate Great Expectation Data Quality tests in the Apache Airflow.  We use the Great Expectation (GE) provider for Airlow and run the Great Expectations suite. The target data asset is a PostgreSQL table.  We use Airflow as the orchestrator for the ETL (ELT) Data piepline and GE as the testing framework

[https://www.youtube.com/watch?v=WAgbFrHUk50](https://www.youtube.com/watch?v=WAgbFrHUk50)

Topics covered:

* Data Orchestartion
* Airflow & DAG
* Airflow GE Provider
* Run Modern Data Stack with Airflow

Tech Stack: **Airflow, Great Expecations, Data Quality, Python**",2024-03-13 20:57:04
1bd7bfo,How to stream from SNS to Iceberg table on AWS?,"I would like to stream data from SNS into an iceberg table on S3. However I am a beginner and I feel quite overwhelmed with the amount of possible ways of how to do that.

SNS has around 100 thousand messaged per day (ca. 1/s) which I would like to stream to an Iceberg table. I was thinking Data Firehose however, I think it can not push directly to an iceberg table on S3.   
Does it make sense to first put data onto a landingzone in s3 and then run a streaming glue job in spark to stream into the table?   
Furthermore the problem is that the table should be somewhat up to date (lets say 1 to 5 minutes delay) because customers use it for querys in Athena. However, as the streaming throughput is that small and the table is partitioned for every customer, it leaves me with the small file problem in the table. (the messages about every second can be from different customers)   
I think running a compaction job every hour could be overkill if you have to do that for hundreds of customers with always just some little files.   


I'm thankful for every pointer in the right direction and I can answer more details if I forgot anything as I am still a beginner. ",2024-03-12 20:15:58
1bcvbdr,Apache Atlas Review ,"I have data assets on multiple clouds (sql ds in aws, databricks and adls gen2 in azure), and I want to build data lineages on top of them, is Apache Atlas good tool of choice. 
If yes please share some resources, that might be helpful. Thanks in advance🙌

Ps. I don’t want to use ms purview ",2024-03-12 11:48:37
1bcch7b,Expedock replicates data from Postgres to Snowflake with <1 min latency and 5x cost savings with PeerDB,N/A,2024-03-11 19:52:56
1bbzmpj,Have anyone taken dbt analytics engineering certification?,"How is the exam? I am considering to take it. But there are less resources available online. 

Have anyone take it? Mind sharing some inputs?",2024-03-11 10:01:25
1bbxmhe,Should I Separate API Endpoints from Databricks?,"Hi there, over the past couple months I've been delving into creating some ELT workflows using Databricks and dbt, but I've recently ran into a use case which is abit less standard, and I'm pretty conflicted around the best way to handle this (even after researching this extensively).

This sub has been really helpful in my learning so far, so I thought it might be worthwhile seeing if anyone might have any guidance/suggestions on what I should do here.

&#x200B;

Basically, I have an existing frontend app which allows users to insert/update/delete data for a few different Postgres tables (using backend API endpoints). Now that we've setup Databricks, we're required to include these tables alongside our other Databricks data (so it can be joined together).

&#x200B;

Additionally, some of these tables contain data that comes from other Databricks tables (where these other tables are part of ELT workflows from various sources). For these tables, any changes that come through need to be displayed in this same frontend app.

&#x200B;

From my initial understanding, these are my options to handle this:

1. Directly query and modify the Databricks tables from our API endpoints (although I've read this isn't a great idea, and instead I should separate API endpoints from directly interacting with the data warehouse?).
2. Continue using postgres to store this data, and then setup a job to sync the data between postgres and databricks:
   1. This would involve syncing all postgres tables into Databricks (which I'm assuming I'd just do using regular ELT loading patterns?)
   2. And then when one of these Databricks tables is updated, any changes would need to be synced back from Databricks into postgres (where I'm assuming I'd just use a standard SQL database connector?)

&#x200B;

Note latency isn't really an issue here (it's fine if the data shown in the frontend is only synced/refreshed daily), and these tables are quite small in size (biggest one has \~20,000 rows).

&#x200B;

I've also read briefly that cache layer or CDC might be relevant, but my understand is too limited to know whether this is appropriate for my use case.

&#x200B;

Any help at all would be greatly appreciated, and sorry in advance if this is difficult to understand (am happy to provide more context if helpful).",2024-03-11 07:36:42
1ba7vl3,Azure Synapse Analytics - Editing notebooks with VSCode?,"Hi,

Is there a way to edit Azure Synapse Analytics notebooks in VsCode? I can clone the repository, but the code is stored as JSON, eg as follows:



	{
		""cell_type"": ""code"",
		""source"": [
			""\r\n"",
			""\r\n"",
			""\r\n"",
			""import http\r\n"",
			""import logging\r\n"",
			""import azure.functions as func\r\n"",
			""import requests\r\n"",
			""from azure.storage.blob import ContainerClient\r\n"",
			""import sys\r\n"",
			""import pandas as pd\r\n"",
			""import json\r\n"",
			""from datetime import datetime\r\n"",
			""from tqdm import tqdm\r\n"",
			""from notebookutils import mssparkutils\r\n"",
			""\r\n"",
			""\r\n"",
			""\r\n"",
			""\r\n"",
			""# Grab API key\r\n"",
			""# (ETC - All code is foramtted like this as a list of strings)\r\n"",

		],
		""execution_count"": 23
	}


So, none of my code formatting / linting tools will work out of the box.

I thought about importing a .py file, but this is difficult as the nodebook code is run on an Apache spark cluster.

Any ideas on this would be appreciated.

PS: Azure Synapse Analytics is a trash product.",2024-03-09 03:27:52
1b9rmyv,Pipeline using .bak file and move data to an Azure SQL DB,"Firstly I am new to the Data Engineering role, but have been working with SQL and data for many years. My knowledge on all the tools out there is limited.

The company I work for are regularly supplied with a .bak file of a third party system the company uses and I have been tasked with extracting data from it and loading it to our Azure SQL DB. I have a local docker container with an instance of SQL which I use to restore the .bak file to. I have tried to export a bacpac file from the DB but there are many errors due to unresolved references to objects.

I can extract the data and load to Azure SQL manually using the restored DB in docker, but want to avoid this and automate the process. Even if the bacpac file worked I would like to automate the process rather than relying on a manual process.

Has anyone done something similar to this, or have any ideas on how I could go about accomplishing this? ",2024-03-08 16:04:37
1b9pcs2,Constellation schema problem : One or two Time Dimensions ?,"Hello, I'm a business Intelligence student and Data warehousing isn't my strong suit. I'm working on a project in which I need to evaluate the performance of members in a test. I created a constellation star schema where I have fact tables that mesure the overall performance of that type of test, with mesures such as number of participants, number succeded, number in progresss , etc ...

I also have one fact table that mesures the individual performance of a participant rather than the entire test, with mesures such as score, startdate, finishdate , durationForCompletion, etc ...

Each attempt made by a participant has a timestamp. I want to mesure the performance of the test by using the test fact table, and the time axis of analysis would analyse by quarter, year, month, etc ...

I also want to mesure advancement by individual so the time axis would analyse progress by hour, day , month , etc ...

Due to the granularity being different between both fact tables, do I create two time dimensions or is there a better approach ?",2024-03-08 14:32:40
1b9niyy,Data ingestion tools to keep databases in sync,"Hello,  
(Im personally quite new to data engineering)  
one of our customers has hundreds of distributed databases, each with one important table of sensor records. We have an existing data pipeline to process this kind of data (with Dagster), but never ingested data at this scale.  
Normally we do ingestion with a simple Python script, but at this scale, we would need to build a set of features that any database-to-database ingestion framework would need (keep a list of databases, connect, check table exists, check what's new, fetch it, allow the user to add a new database table to be synced).

I know about Airbyte and Meltano, both are quite powerful and heavy frameworks with many connectors. I was hoping for something simpler. I just need keep the local table in sync with the remote one via SQL at regular intervals (not realtime). Im tempted to build it myself, please stop me.

Would be happy if you would share your ideas.",2024-03-08 13:08:38
1b9ic2j,Multi-Armed Bandit Simulator [https://github.com/FlynnOwen/multi-armed-bandits/tree/main],"&#x200B;

https://preview.redd.it/b4zv93asb2nc1.png?width=1610&format=png&auto=webp&s=20b4e39ec6d2c9fa3ac4d43b64660ef6fc443d79

I created a multi-armed bandit simulator as a personal project: https://github.com/FlynnOwen/multi-armed-bandits/tree/main  
I work as a data engineer/scientist but don't often get to play around with new software, and sometimes work on projects outside of work hours to stay fresh and learn more about the space. I thought members of this sub may appreciate this piece of software I worked on.  
Stay cool data devs 8)",2024-03-08 07:28:55
1b9bnak,Cassandra or Praquet for structured data,"Hi team,

I have a basic question today, so please bear with me. I've been working with Azure Databricks for just a few months.

My question is about storage options. Initially, I was under the impression that using ADLS and storing files extracted from SAP S/4HANA in the raw layer as Parquet was the optimal approach. I believed that Parquet was the most suitable format for this purpose. However, I've recently learned that aside from ADLS, it's also possible to store data in Cassandra. This discovery has led me to wonder: if Cassandra is capable of storing highly structured data from SAP, might it be a better choice?

Am I oversimplifying the issue by not fully considering factors like the cost differences between saving data in Parquet versus storing it in Cassandra, or potential use cases like analytic reporting versus write preference?

I would greatly appreciate your perspective on how to make this decision.

Thanks.",2024-03-08 01:44:11
1b98jy5,Databricks certifications advice,"I've just got the Databricks Data Engineering Associate certification and I'm thinking about keep studying for the Databricks Data Engineering Professional certification.

I've been using Databricks at work and I think it's a powerful tool. What's your advice? 

To go for the professional certification or to learn another stack like snowflake, DBT or another one?",2024-03-07 23:15:04
1b90nap,How would you ETL JSONL data with different schemas in the same file?,"I have a dataset that consists of thousands of JSONL files (one JSON object per line). About 800GB in total, and 288 files of 15-50MB are added each day. Each file contains JSON objects with different schemas. Example:

&#x200B;

[Each 'LogAnalyticsCategory' has a different schema \(not visible in the picture, but you get the point\)](https://preview.redd.it/t6hmtz0i6ymc1.png?width=1332&format=png&auto=webp&s=84105e3453bfaed8019ec8f9dadebbeccc8d8d5c)

Eventually each category needs to go into a different destination table (sink = Postgres), but I'm unsure how to do this efficiently. I can do it easily with BigQuery, but given the volume of the data that will become too expensive, especially as the data will be queried for analytical purposes.  

My toolkit for now is basically limited to Python on a single machine. My source files are stored on GCS. Looping over the rows to figure out the schema of each line will probably be horribly slow. How do I best approach this (in the most simple way)?",2024-03-07 17:42:19
1b86dyj,Parallelize Tasks execution in Airflow," Hi everyone! I hope you're all doing well.

I'm working on a pipeline to scrape data from SofaScore. On average, I'll need to scrape 36 matches daily, requiring 72 total requests (two per match – one for statistics and one for highlights). To optimize this process, I'd like to parallelize the DAG execution.

Could you advise on the best practices for this? Should I define a task for each match, or create two separate DAGs (one for statistics and one for highlights) and trigger them both for each match?",2024-03-06 17:58:46
1b81nx5,Expose data from S3,"Hello everyone,

we have data stored in an S3 bucket in the form of raw CSV files. Adobe is currently accessing this data directly from the S3 bucket. However, these files contain sensitive information. I’m exploring options to ensure that we maintain ownership of the data while allowing Adobe to access it securely.

Any suggestions or use cases or right way to get this done propely . We are having mulitple third parties using the data not only adobe. Just asking what are my options here please help.",2024-03-06 14:54:23
1b7k1yc,Survey questionnaire metadata industry format?,"I'm working for a small market research company and I'm trying to do some machine learning on survey data. Most of the data we get from clients is in bad shape and I need to be able to format it into a particular shape for this. I'm looking for a general metadata, machine readable format for survey questionnaires. There doesn't seem to be an industrial standard or even any attempts at one?

The closest thing I've found is what [google forms exports](https://github.com/stevenschmatz/export-google-form), but this is very Google. I thought there would be an RDF schema but there is none (that I have found). Can't see anything from the W3C?

Surely this is a ""good idea"" as it will allow the same survey to be taken on different platforms or reproduced at different times/places/ languages or even act as a description of the resulting data?

Hoping that there is something I've missed...

Update: Given the lackluster response I take it there is no such standard. Sooooo, do you want to join me in making one?",2024-03-05 23:22:31
1b78fwh,Data Engineering Education Justification,"I’m from the Mechatronics and Indistrial Technology (Supply Chain and Data and Analytics) education background with a couple years of Data Engineering Experience. I’m currently applying for new opportunities and getting rejected, i believe because i’m not from the standard CSE/Statistics/Mathematics background.

I’m looking to take the CS50 course from Harvard, will that help me justify my education for employers to consider me?.

Or are there any other suggestions?",2024-03-05 15:49:06
1b6woqe,Cannot drop table in Trino using Hive connector for no good reason,"I have configured a local environment that consists of the following containers:

* Trino
* Hive
* Minio

I have successfully set up these three containers to create and query external tables using Trino, which are stored in the Hive Metastore and on Minio as Parquet.   This all works great for me.

However, when I try to drop the table in Trino that I created, I get this error:

>Access Denied: Cannot drop table schema.tablename io.trino.spi.security.AccessDeniedException: Access Denied: Cannot drop table schema.tablename

This doesn't make any sense because there is no user setup in Hive Metastore, nor security setup.  If I connect to the Hive metastore using Beeline, I can drop the table with no problem.

Do you have any clue why Trino is unable to drop the table?",2024-03-05 04:44:02
1b6kgk6,Data Engineering for Computer Vision - Advice,"Hi all,

&#x200B;

I'm looking for some advice and I'm hoping this is the right spot. 

&#x200B;

I've recently started a new role as a Machine Learning Engineer for a R&D team focusing on computer vision applications. It has become clear that the biggest weakness in our operation stems from our approach to data. We have \~terabytes of proprietary image and video data in various formats and no system to efficiently process and label it.  I have been assigned the task of developing a data pipeline to curate custom datasets for our CV models but I don't have any real expertise in data engineering other than some experience with SQL/NoSQL databases. 

&#x200B;

I'm aware that a team of dedicated Data Engineers is really what's required for such a task but unfortunately I am the only one available to work on this problem. With that in mind, I'd very much appreciate any guidance on the following: 

1. What resources are available for learning Data Engineering for CV? I have spent a lot of time researching this area now but most resources that I've come across really only focus on structured tabular data. I haven't come across any resource that discusses best practices when dealing with video or image data. 
2. How do I decide on what tools to use? I am overwhelmed by the choice of tools relating to the 'modern data stack'. I have come across Activeloop's DeepLake which seems promising but then Delta Lake seems to be more popular. Do I even need a Lakehouse architecture? 
3. I have set up CVAT for image annotation with semi-automatic labelling to improve efficiency. Are there better tools out there for labelling? I have seen LabelBox mentioned as a viable alternative. 

&#x200B;

Of course I have many other questions but I'll keep this short as the main thing I'm looking for is advice on how to improve and learn. Any information would be much appreciated! ",2024-03-04 19:57:18
1b6fxq9,Seeking Advice for a Sr DE's Learning Journey Post-FAANG,"Hey everyone,

Up until recently, I was a Sr DE at FAANG (7YOE). However, I was part of the recent layoffs and found myself contemplating my next steps sooner than expected. To add a twist to the plot, I just tore my achilles tendon playing basketball, which means I'll be spending quite a bit of time on my butt at home recovering. Not exactly the kind of break I had in mind, but here we are.

Post layoff, I was already planning to take a significant break to reset and figure out what I want to do next. The injury just extended this timeline a bit. I'm pretty comfortable with the Leetcode grind, system design, and all that jazz. But for this break, I want to focus on learning and exploring - diving into new tech, maybe picking up some skills that are just for fun, and generally getting reinvigorated about the field.

I've spent a good part of my career leading projects, primarily working with AWS, Python, SQL, mainly with ETL platforms. While I'm pondering whether to return to FAANG or venture into something smaller where I can have a bigger impact (perhaps lead a larger team), I'm not quite sure where to direct my learning energy.

So, I'm reaching out for suggestions on how to make the most of this time. Are there any books, courses, or personal projects you would recommend? Any emerging tech or tools I should get my hands on? Or perhaps some advice on making a transition that could allow me to be more impactful and fulfill my desire to work closely with a team?

I'm all ears and really appreciate any thoughts or guidance you can share. Thank you!",2024-03-04 16:58:26
1b69inb,"What's ""Modern"" in the Modern Data Stack",N/A,2024-03-04 12:16:45
1b5tuv6,Team design?,"Hello fellow kids, ::insert Steve buscemi meme:: I have a management oriented question about data eng if you'll indulge me. Do you have any ideas about the optimal team design? I'm thinking mesh, team topologies, or some of the ideas Ben Rogojan has talked about. Any frameworks or example of good team/org design for a data eng practice? I'm sure I could look at Meta/FB for an example but what else would you suggest?

In full disclosure, I'm researching this idea because I'm trying to write a newsletter and eventually book on the subject. But honestly I would like some ideas about how to help make my own small team at work more data engineering oriented. Would love your advice. Thanks.
",2024-03-03 22:18:30
1b5a9fr,Has anyone used that “Lowcoder” platform,"This platform here:

https://docs.lowcoder.cloud/lowcoder-documentation/

I constantly find that I’m needing to build some kind of frontend for various purposes. Usually because I need data that can only be collected from a human. Like a date value, an integer, a whole file, a CSV with validation, …

I build the database, the backend logic, provision the servers, set up the network, deploy the code, back it up to code storage, … i just don’t understand web development. Not sure if I should or if it’s out of scope for my career… but this comes up often.

I kind of just want an easy tool that can build quick front ends and I can move on with my day. In a perfect world it would be version controlled but that one requirement seems to make a world of difference by complexity. 

However, lowcode too quickly becomes unmaintainable monstrosities of which I also want no part in… Ive tried, they are notoriously bad. Too rigid and not at all professional, resulting in strange tools.

This platform says it’s different and more modern. I think I want to trust it because it’s new and open source. However, I also think I know better…

Has anyone tried this specific platform? I’d love to hear about your experience.",2024-03-03 06:19:19
1b4ti4e,How do you solve these adf scenarios ?,"So i have being giving interviews for azure data engg. Most of them ask me scenario based questions. The problem of scenario based questions is there may be multiple ways of achieving the solution. 

Below are some scenarios. Please do share your insiggts if you might have came across in your work

#1. You have multiple csv files in adls. You want to apply transformation on them and generate a single file. How would you acheive it.

Sol 1 - using adf mapping dataflows can take multiple source, do  the transformation and generate one file with one partition using sink transformation

Sol2 - using metadata activity to get the list of files and then using copy activity to get all the files from source and sink as one file by setting copy behaviour to merge



#2. You have to store data on adls. How do you store the data in optimised way. Like partitioning indexing etc
(Dont know about this. Please provide your input here)


#3. How will you incrementally load data from file in adls.
Sol - probably using a metadata activity by getting all the last modified date 


#4. How will you copy data from onprem sql server to adls
Sol-  setup a Self hosted IR and then using a copy activity. But what if i dont want to copy all the data and just a section of data how do i do that ??


#5. How do you Transform data without using a dataflow. 
Sol - probably copy activity. But as far as i know it can only map source and dest cols and cant do any complex transformations


#6- how do you connect to a on pem SQL server from a synaspse notebook without using adf pipeline
-i dont have any idea abt this


Would be great if experts of ADF share their insights",2024-03-02 17:24:51
1bssp27,Self-taught transition for Analytics to Data Engineering,"For background, I have 3 years of experience as a Data Analyst with bachelors in CS. I am fairly proficient in all things analytics (Python, SQL, Power BI, SAS) with some exposure to Azure Databricks and PySpark from my current employer's tech-stack. I have done some machine learning, but soon realized it's not something I want to do long term. And so here I am, working my way into Data Engineering.   
I have made myself a study path after spending days on the web going through all the guides/resources (I was completely taken aback by the sheer amount of content on this domain). As a complete newbie to DE, I wasn't sure if this is the right way to go. Please give your opinions or suggestions on how I can improve my plan, add something to it or completely change the path. I am completely open to new ideas and perspectives.   
My Study Plan:  
Step 1: Start with Martin Kleppmann's -Designing Data-Intensive Applications to build up a solid foundation.   
Step 2: While working my through the above book, get hands-on training in Azure Data Engineering. The resources I have decided for this are [**John Savill's**](https://www.youtube.com/@NTFAQGuy) Youtube series and Microsoft Learn.   
Step 3: Build projects in Azure to get my hands dirty. (Not sure how to go about this. In data analytics, I was able to work on self-projects all by myself after getting familiar with the tech. I am not sure how this work for Data Engineering or Azure)  
Step 4: Formally learn Spark framework with a focus on PySpark (Haven't decided on resources for this yet, please suggest some)

&#x200B;",2024-04-01 02:49:18
1bsgshv,How do you start as a Contactor in DE?,"Coming from an IC role, what made you switch to Contractor? How hard was that? And do you recommend?",2024-03-31 18:10:57
1bs8qrc,Help needed for cracking MAANG data Engineering positions.,"Hello everyone,

I'm reaching out for help in finding a good resource to practice SQL for a Product-based Company. I have two coding rounds coming up with one of the top product-based companies, and I am eager to crack them.

I am looking for someone who has curated a list of the most frequently asked Python and SQL questions among FAANG companies for data engineering interviews. I understand that compiling such a list requires a significant time investment, and I am willing to compensate for it if needed.

Since time is my main bottleneck, any help would be greatly appreciated. Feel free to DM me if you can assist. Thank you.

&#x200B;",2024-03-31 11:59:02
1brpoht,Switching from .Net development to data engineering after 7 years?,I want to switch to data engineering beacuse I think there is demand for it. My only motivation is good salary. I think .Net is not rewarding and I cannot go to europe with .Net skills where as getting a job in data engineering in Europe will be easy. What do you guys think. Is it a good career move? I have the option in my current company to switch to data engineering.,2024-03-30 19:11:30
1brp03z,What are your thoughts on roles focused on Data Security/Governance?,"I see more job descriptions that are all about security, governance, compliance. Anyone here in a role like this? How do you like it? I have the experience for it but not sure about going down that road.

I guess the pros are it's probably not too fast-paced, and maybe opens doors to other security-related roles.

Cons would be you might get less experience building stuff, and audit requirements can be mind-numbingly boring. Any thoughts?",2024-03-30 18:42:14
1brhys3,What are current (best in class) solutions for very big data companies (where data is proprietary asset) to store and utilize data? ,"Please explain like I am 5, with business degree and am chief data officer 

",2024-03-30 13:31:56
1brbjkc,"What AWS/Azure services would you use to list files by their folder path in S3, retrieve each file, and upload each file with durability and scalability to an external API of another service provider?","What AWS/Azure services would you use to list \~3,000 files by their folder path in S3, retrieve each file, and upload each file with durability and scalability to an external API of another service provider?

In the past, I would use AWS Lambda / Azure Cloud Functions. Are there new services that can do this with no-code today?",2024-03-30 06:52:53
1br0onj,What would you call the collections of data loaded between 'refreshes'?,"Hello Data Engineering Reddit!

I'm looking for the right term a specific activity.

Say you are loading data from a source that doesn't share deletes (like Github's API) - the deleted content just disappears  - no deleted\_at column or CDC ""deleted"" events come your way.  You are doing the right thing and loading data incrementally:

Sync 1 (Monday):

* Issue A (first time seen)
* Issue B (first time seen)

(Time marches on: Issue A is deleted, Issue B is updated, Issue C is created)

Sync 2 (Tuesday):

* Issue B (updated)
* Issue C (first time seen)

At this point in your warehouse you have 3 records after de-duplication:

|Issue ID|Synced At||
|:-|:-|:-|
|A|Monday||
|B|Tuesday||
|C|Tuesday||

Now, you *know* that Github's API has doesn't give you information about deletes, so you periodically rewind your cursors/high-water-marks to see if stuff was deleted.  The data above is in ""bucket 1"", and the data you are about to sync is ""bucket 2"".

Sync 2 (Wednesday):

* Rewind Cursor
* Issue B (first time seen this ""bucket"")
* Issue C (first time seen this ""bucket"")

|Issue ID|Synced At|""Bucket""|
|:-|:-|:-|
|A|Monday|1|
|B|Tuesday|1|
|C|Tuesday|1|
|B|Wednesday|2|
|C|Wednesday|2|

We don't want to deduplicate across buckets because there is meaningful information here - if Issue A didn't appear in the second bucket, we can infer that it has been deleted in the source, maybe adding a \`is\_source\_deleted=true\` column

|Issue ID|Synced At|""Bucket""|is\_source\_deleted|
|:-|:-|:-|:-|
|A|Monday|1|true|
|B|Tuesday|1|false|
|C|Tuesday|1|false|
|B|Wednesday|2|false|
|C|Wednesday|2|false|

So the question: What is the right term for these ""buckets""? We are currently thinking that each bucket here is called a ""generation"". Is this a good term?

|Issue ID|Synced At|Generation|is\_source\_deleted|
|:-|:-|:-|:-|
|A|Monday|1|true|
|B|Tuesday|1|false|
|C|Tuesday|1|false|
|B|Wednesday|2|false|
|C|Wednesday|2|false|

&#x200B;",2024-03-29 21:59:14
1bqyquj,"What is the difference between the data engineer role and ML, devops, MLops, infra engineer roles?","It's not a lazy question but after a lot of researshing it seems that it's expected from data engineers to be able to do some of the work of those roles. Honstly, should data engineer have a decent background and knowledge of those areas?",2024-03-29 20:33:36
1bqx848,How to integrate Databricks Unity Catalog with AWS Glue Catalog?,"I'm just starting on a new project at work where two larger IT lines of business have created two separate data platforms. The first is a custom platform built around AWS Glue Catalog and the second is Databricks using Unity Catalog. I've been tasked with integrating the two so that any single dataset in either platform is governed at a central place.

For example, if I create a data product in Databricks and specify specific set of users and groups have access to this Dataset from Databricks, this same set of users and groups would have the same access from the custom platform. And vice-versa.

At a high-level, my first thought here is to just use the AWS Glue Catalog as an external metastore as documented here (https://docs.databricks.com/en/archive/external-metastores/aws-glue-metastore.html), but as this is listed as legacy I'm not sure how safe I feel building architecture around something that could be deprecated...

Curious is anyone has seen a similar use case and how you tackled this challenge?",2024-03-29 19:10:23
1bquu9l,Confused on Data Pipeline Orchestration,"Hi everyone,

I've been getting familiar with Azure Data Factory *(just orchestration in general)* for data pipeline orchestration, and I've noticed something that's been confusing me. Most of the examples and tutorials out there focus on end-to-end orchestration, taking you from data extraction all the way to updating a data model, all within a single pipeline/DAG. They typically run these pipelines daily, pulling new data, running it through notebooks/procedures/functions for cleaning and transformation, and ending up updating the final data model for a specific use-case.

Here's where I'm confused: these entire pipelines are really tied to just one use-case, very tightly coupled. It doesn't seem efficient to build a whole new pipeline, starting from extraction, for every single business use-case, especially when that means extracting the same data over and over.

For example, what if I want to keep my data lake fresh by extracting and cleaning data every 15 minutes from a particular source, but I only need to update a data model once a day? In the tightly coupled pipelines that most examples show, you'd have the data transformation happening right after cleaning, every single time, which isn't what you'd always want. Or what if I have another use-case requiring the same data as the first use-case, but needs to update every hour? 

I'm thinking separating out the stages could be a lot better. This would allow you to extract and clean your data as frequently as needed without disrupting the downstream processes that rely on it. However, this approach can become complex, particularly when most use-cases and data models necessitate data from various sources. For instance, if SAP data is extracted every 15 minutes and Salesforce data every hour, synchronizing these systems for a downstream use-case could be challenging due to them being out-of-sync.

I don't see a solution on extracting and cleaning data independently of the eventual downstream use-cases while also being able to keep data from multiple sources in-sync so that the downstream use-cases can reliably combine data, especially when there could be dozens of independent source systems.

So, I've got a couple questions:

1. Do you usually keep the extraction/cleaning of data separate from how it's used downstream?
2. Are there any strategies or best practices you've found helpful for setting up your systems like this?

I'd really appreciate hearing about your experiences or any tips you might have.",2024-03-29 17:23:56
1bqsyyc,Amazon AppFloww,"Has anyone here used Aws AppFlow, and if so was it worth it compared to manually coding and deploying integrations? I'm thinking about using it for data ingestion from Salesforce.
",2024-03-29 16:06:13
1bqrukz,Seeking advice on warehouse-native marketing tools,"We are a b2c business with with 3.4 million monthly active users. We employ RudderStack for collecting all our first-party customer data, which we then sync to Snowflake. For our marketing automation needs, we then use Reverse ETL to transfer the customer data to Iterable.

However, as our user base grows, we're encountering challenges in efficiently moving all customer data to Iterable, and the costs are becoming prohibitively expensive with the increasing volume of data. One of our colleagues in the data team suggested to try out a warehouse-native customer engagement solution like Castled([https://castled.io](https://castled.io/)) or MessageGears([https://messagegears.com](https://messagegears.com/)), which will directly consume the data from our Snowflake to run marketing campaigns. 

Both look decent for our usecase from the website - Have any of you tried out these platforms before? How would you describe your experience? Any feedback or advice would be greatly appreciated as we consider our options.",2024-03-29 15:19:49
1bqq986,Alternative tools like Talend and Data warehouse tools,"hey guys I am new to data engineering Are there any other alternative tools like talent(it was free till jan31 2024) 

if there were any free tools or trial tools for data warehousing and for etl that would be great",2024-03-29 14:11:34
1bp60ft,Is the availability of too many tools an issue??,"I recently got to know from few DE regarding the issue with the availability of too many tools out there. Often times when working on a project, in the initial phase most of the time goes into choosing which tools to use. There are so many good tools out there and eventually becomes hard to choose. 

And I got to know about this from few really smart/experienced DE. I was wondering if you all have faced this issue? And if yes how are you all dealing with it!",2024-03-27 16:24:05
1bp1nzs,Separate Kafka producers and topics for getting data for analytics?,"Hello everyone! 

We are tasked with getting data for the purpose of analytics and reporting from an application created in-house. The application uses Kafka for messaging between microservices. 

We are thinking of creating a separate set of producers (triggered by user or business actions that are likely different from operational ones) and topics for the purpose of getting data. 

Is this a good idea? Is there a better way to do this? The application does not have a strong data backend, so Kafka is being considered as a potentially easier way to get data accessibility.

Thanks in advance!",2024-03-27 13:18:29
1bp1bf9,Globally available database ,"Hi, I’m trying to find a solution to make a database available globally. As if now we are using mongo, there is the option of replicating and sharding mongo but that would require both time and money. 

Are there other easier options available for having document based data, I have been reading about scyllaDb, do you guys think it would be a good option? 

The use case is mainly for games with over 5 million users. ",2024-03-27 13:02:00
1bnunsa,Geographic info is stored in FACT or DIM table?,"Hi folks, so i´m facing some troubleshooting when i make a dimensional model. The data that i collect have some geographic fields lat-lon that is the exact point of a listing property, that makes that if i convert to a dim table with exact adress have similar size of my fact table eg my fact table has 172k rows and the dim table for locations have 150k rows (because one apartment have the same location as others)

It is OK if i treat as a degenerated dimension and put into my fact table that is a factless table?

If i do that my dim table has only the city and  neighborhood granularity and have around 10k rows

&#x200B;

Thanks for your time",2024-03-26 00:54:03
1bm2fm9,Maintaining separation of data for different clients,"Hello,

We are going to start on a new project where we will be provided data for 9 different clients in csv formats. Data will then need to go through a fairly normal dataflow

1. Data passed to us via the clients' ""drop boxes""
2. Data validation and ETL stuff
3. Stored in Azure SQL
4. A fairly complex calculation will be applied to the data
5. Results of calculation stored back in to Azure SQL
6. Various reporting etc
7. Process repeated periodically (say twice a month)

Other than being different datasets, the logical steps will be the same for each client. It is likely that throughout the life of the project there will need to be changes to the methodology in step 2 and step 4, when that happens any changes will apply to all clients simultaneously.

Lastly, it is critical that there is never a mix up between the 9 clients' data.

The most robust would seem to be to have completely separate instances of everything, but conversely having 9 copies of python / Spark ETL and calculation code doesn't seem like a good way to do things when it gets updated.

Appreciate any thoughts or any reading I could do.",2024-03-23 20:44:57
1bl90gl,Best Courses for Data Engineering System Design?,"I am looking to move up in the ranks at my current organization to the next level. Therefore, I need to further my knowledge in the data engineering topic; in regards to system design. I am looking for good courses that teach system design for data engineering related systems/applications. Can anyone recommend me a good course to pursue on sites like Udemy, Coursera, Pluralsight, etc? I have a background in DevOps, so I am experienced with some engineering (automation, CI/CD, scripting, etc.) Thank you in advance!",2024-03-22 20:18:46
1bl39nc,Data mesh/ virtualization using tools like Starburst and Dremio for data virtualization when the data sources are on prem or cloud relational databases? ,What are the pros and cons of using tools like Starburst and Dremio for data virtualization when the data sources are on prem or cloud relational databases? Can MPP help with such sources in any way or will it just act as performance bottleneck? Anyone has any experience with such setup?,2024-03-22 16:22:29
1bkuszf,Data Generation and Validation Tool,"Hi everyone! My name is Peter and I've been working on a data generation and validation tool called [Data Caterer](https://data.catering/). The latest version comes with a UI and you can now run it on Windows, Mac or Linux (also via Docker). I hope this tool makes generating and validating data across any data source simple and easy.

Key features:
- Batch and event data generation
- Maintain relationships across any dataset
- Create custom data generation scenarios
- Clean up generated data
- Advanced validation options
- Suggest data validations

[Quick start to run it yourself.](https://data.catering/get-started/docker)

[Demo of the UI.](https://data.catering/sample/ui/index.html)

[Github repo.](https://github.com/data-catering/data-caterer)

Happy to receive any feedback.",2024-03-22 09:02:49
1bku4jp,CDC logs - __$command_id,"Hi all, 

I have a quick question about command id. I need to pull cdc logs to Storage account from on prem sql server and table is badly designed so I do not have any watermark column I can grab onto. Is command id good choice for know ing what I ingested with copy activity (ADF) since I do not have any other options. Also would date without timestamp be somewhat acceptable choice? 

Thanks! ",2024-03-22 08:11:43
1bkt04n,Apache Spark Distributed File System and DB queries ,"Hello,

I have the following doubts related to Spark.

1) Is it really needed to install a distribute file system (HDFS) on top of the regular file system the nodes are using?

I have only seen examples where HDFS is included as part of the solution and if this is true, wouldn't the file be present in the local file system and then distributed across the nodes taking more space? 

2) Is it true that spark does the processing distributing part of that load across the nodes and persist in memory? How does the distributed file system and this memory processing relate to each other?

3) Is it true that the main difference between haddop and spark is that the first one does not rad and write to disk for the dataset processing? 

4) When we say that spark can distribute sql queries, does that literally mean that you make a query to an external database and the nodes do the calculation? Wouldn't you have to bring the entire sql database locally first? I'm little lost there and an example would be great to illustrate better. 

I appreciate in advance your help as I am really new to all of this. 
",2024-03-22 06:48:51
1bkrjby,Database solution?,"So I work in Data and mostly have employees working on google sheets where data is processing step by step by diff people before its been locked and then we move to next sheet for another day. 
We are looking for a database which automatically stores this data from time to time and where I can connect this database to a visualization tool say PpwerBi. This way if I need to look at 2023 data I can code and extract all data from this database. 

Sorry for lame tech lang but Im not a tech person and work for a disorganized and small healthcsre company. 

Question is what show we really get? We only have sheeets and powerbi. No place where we store our data. If we need to do analayses on yearly basis, need to copy paste values from several sheets into excel and import to powerBI. Ofcourse can connect bi to sheets but thats going to be 500+ connections on yearly basis which powerbi cannot handle
Thanks",2024-03-22 05:07:52
1bkpfx3,What is your Postgres setup for data warehouse?,"Hi, I am developing a medium data warehouse with size about 100 GB, using Postgres as database. At the moment I  running a generic postgresql server for dev environment, but I wonder if that version will hold up on production.

So, what is your setup? Please share!",2024-03-22 03:08:03
1bkiai1,Extracting from API strategy - what to do?,"Hey folks, we're recently undergoing a bit of a project to extract data from a SAAS platform. Just a few API endpoints. It's pretty niche, so no managed EL services (Fivetran, Airbyte), so we gotta do it in-house. Fine.

Because the dataset is relatively small, for 9/10 endpoints, we can just do a full extract (~300 pages) every 6 hours or so using a Google Cloud Function and overwrite our BQ tables. 

For one of the endpoints, however, we're talking 3000+ API calls for a full data dump. Not great. Cloud function execution time is 10 minutes so it times out. We could move the script to Cloud Run, but I feel we're going to come up against the same issues.

Now, there is a /changes endpoint, which only returns records that have changed since the provided date, but in terms of EL strategy, we still need to somehow consolidate the data into a source of truth that mirrors the source.

I'm for a bit of a sense-check. I propose that 

* Every 6 hours (or any arbitrary cycle time), we ask for any data that's changed at source for the last four* cycle periods.
* We take those changed records and dump them into Google Cloud Storage in JSON/Avro, whatever.
* A BQ data transfer job appends all records into a table
* We create a Dataform/DBT SQL pipeline that picks out all of the most recent records for every record ID into a staging table.
* Downstream BI can use that staging table as a 'mirrored' source of truth from the SAAS platform.

Why four*? Contingency. If an EL fails, it can fail for 4 cycles before we need manual intervention.

Am I overcomplicating this?",2024-03-21 21:39:33
1bkeojr,Difference between DA and BI,"Hi! I was wondering if yall can explain the difference between data analyst and business intelligence analyst. From the job descriptions, they seem really similar. ",2024-03-21 19:11:52
1bkd6t0,What to read to get up to speed with AI? ,AI is almost like a buzz word these days and I understand hopelessly little. What should I read to get up to speed? Ideally related to data engineering. ,2024-03-21 18:11:05
1bkaqt4,What is the best way to run airflow tests on local windows machine,"So, I recently started playing with airflow since the project I work on is migrating to GCP where we will be using Composer.

Till now I was playing on my personal device with Pop OS. Recently I tried to create some dags from my organisation's windows machine and found out airflow tests won't run on Windows, it just supports POSIX systems.

I saw people running airflow on windows with Docker or WSL, but since I am using a company provided laptop, due to restrictions I cannot enable WSL, and not sure how tedious it will be to setup docker here.

Just being able to run unit tests on DAGs and tasks should do the job for me. Anyone have faced similar issue ? Any suggestions here are welcome. 

If nothing, I will have to maybe setup some scripts that will sync my code with a compute engine instance which will run those tests for me. Not sure how feasible this is though ..",2024-03-21 16:31:59
1bk8637,Performing delta/incremental loads with header/detail relationships,"Note: This isn't about Extract/Load data in delta mode but rather the Transform part.

Let's say I'm loading orders and I'm joining the header and detail together.

I'm trying to figure out how to handle scenarios where either table gets a change.

Let's say the Order header changes, I'd want to reload the entire Order by joining the header/detail but, only the order currently has delta data.

There's also the scenario where a new order line appears and I need to reload the entire order because, let's say I'm performing window functions or any similar workload.

&#x200B;

I also have scenarios where more than 2 tables get joined together and all of them are deltas.

&#x200B;

I'm currently using SSIS with most of the workload being pure SQL or Stored Procedures.

I'm aware the tool is old and we are looking into new tools but I still need to develop new stuff right now, plus I assume the issue would remain in other tools.

&#x200B;

Thanks!

&#x200B;",2024-03-21 14:43:34
1bk45ct,Organizing scripts in Apache NiFi,"Hello,

I am the only data engineer at a small company with not too much experience, and I have to figure out a system to supply the data for our BI platform and other reports, from various sources (APIs, csv files, SQL databases, etc.).

I looked around and I found Apache NiFi (which has already been recommended by other people on this sub). I've already played around with it a little bit and it seems to be what I'm looking for. It was also important that I can run python scripts with any dependencies, and it seems to be allowing it through the ExecuteStreamCommand processor, but what I don't know is how I should organize these scripts and where to store them.

We will run NiFi in Kubernetes, and according to our DevOps guy, it would be very bad practice if we just kept adding these python scripts / virtual environments (in case we really need that complexity) to the container of NiFi. 

What would be the best practice here? We want to keep the python scripts version controlled, so it would be pushed to our own gitlab. Of course we don't want to deploy each python script individually and create a bunch of pods either, but we're struggling to find our way.

Thank you in advance!",2024-03-21 11:17:40
1bk1j5e,Do you guys know any online certification for junior engineers?,"Hi guys,

I’m a Software Engineer and in this last month in my company I’ve been working as a Data Engineer manipulating data and setting pipelines with Postgres, Airflow, ELK and Kafka.
The thing is I need learn more and get a certification to improve my early expertise.

Do you guys know any cloud or platform certification the could be worth it for freshmen in the data engineering field?

Thanks in advance",2024-03-21 08:16:29
1bjl4fq,"AWS Glue + Athena Data Catalog, int column all nulls if one row in source data was null but float is fine","So I receive a CSV in s3, glue ETL grabs it and transforms the schema and to parquet and drops it back into another bucket and registers the data in the catalog (Athena). Fine.

But I noticed columns marked for transformation into int data type will get written entirely as nulls if just one row has a null value in that column. If it transforms to float, this is not a problem. Everything is being converted from string, but the csv will have just a blank cell not “null” or whatever.

Am I missing something here? Is this expected behavior?",2024-03-20 18:51:52
1bjbsqu,Divesting from Informatica & Hashing,"Howdy folks. The team I’m currently on has a plethora of Informatica processes built up that are being considered for divestment out of Informatica. Currently those processes use a hashing function in Informatica that creates a hash off predefined attributes, checks that hash against the current table, and if there isn’t a match: load the record.

The problem: the hash function is a black box. Attempting to recreate the hash, even by the namesake of the hashing algorithm (think MD5 and other hashing algos), the same result is not reproduced. There seems to be some baked in logic to the Informatica logic that modifies the algo one way or another.

So with that in mind, divesting from Informatica will cause every row to be new. The compute and storage of an entirely new dataset across all of the processes is out of scope.

If anyone’s been down a similar road, curious on the solution you used. One thought is SCDM2 and indicate the columns being used as the hash input today as the changing dimensions for SCDM2. In theory, simply scanning those columns and timestamping changes ought to provide a solution that mirrors the legacy dataset and not cause an entirely new dataset. ",2024-03-20 12:03:51
1bjbmk7,ETL setup for the backend for the data products?,"What are some commonly used tools for integrating and transforming data with different GTM SaaS?   
For instance, I built a SaaS product that helps to construct PnL for B2B SaaS. I want to extract data from Stripe and Hubspot, perform some metric calculations across two systems, and then send it back to the original systems; what tools should I use? I considered using Airbyte for ETL, DBT for transformation, and some API wrapped to communicate the results back to the systems. 

Does this approach sound reasonable or I'm looking in the wrong direction? ",2024-03-20 11:54:27
1bj9ua8,File storage format for tensors,"This is more of a Data Science problem, but I found surprisingly little information on it and I thought it would be a good idea to ask the epxerts.

I have a large dataset of strings and corresponding embeddings (fixed-size torch tensors). I would like to save them in some kind of tabular format, to be re-used when experimenting with hyperparameters, model architectures, etc. If possible I would like to avoid signing up for cloud vector DBs or setting up DB servers (like Postgres).

The solutions I see now:

- store as `.npy`, `.pt` or `.hd5`: straightforward & efficient. Cons: not so straightforward retrieval without link to the corresponding string, no built-in appending / batching solutions.
- PyArrow has a `FixedShapeTensorArray` extension format which you can save into Parquet. Con: it does not seem to be interoperable with tools I would use to retrieve embeddings for a particular set of strings, like Polars or DuckDB.
- DuckDB & Parquet have a 1D fixed-length array type. Con: would require reshaping before saving and after retrieval.
- Finally, Parquet could also store a serialized bytes column. Con: again, requires (de)serialization on read and write.

I would appreciate any insights.",2024-03-20 10:01:44
1biukdk,DDL for COPY and UNLOAD commands in DBT?,"I’ve been asked to make a plan/road map to migrate my job’s pipelines off airflow + Spark + Redshift to airflow + dbt core + Redshift

Currently our keep raw data  in S3 to use DBT I would need to load that raw data into using the COPY command in Redshift.

So I’ve been tinkering with DBT but it doesn’t let me run CREATE, COPY, and UNLOAD anywhere except the pre-hook and post-hook of models, it expects everything to be a SELECT statement.

This seems like a terrible practice because the SQL “hooks” in these needs to be a quoted string meaning you get no sql linting,no syntax highlighting or checking etc.

Is there any way to schedule arbitrary SQL that isn’t models with dbt?

 I can roll my own but it just seems like a tool like this that’s literally for running SQL should have something more than these pre and post hooks to run DDL SQL commands.

**tl,dr:** We have over 200+ raw data tables in S3. Making empty models with only pre and post hook to use the CREAT, COPY, and UNLOAD commands to load the data into Redshift seems like a bad idea. Is there a way to run DDL from DBT that’s not pre and post hooks.
",2024-03-19 20:36:15
1biua2g,Problem to design a fact table,"Hello Guys! I need help

In my business, we typically don't rely on concrete facts, but rather on daily snapshots. We operate within the educational system, focusing on aspects like ""Students enrolled in courses on date X"". These student records may remain constant (I lack a specific date column for updates, only a cancellation date). Therefore, every day we capture a snapshot at 11:59 PM, which results in a significant amount of duplicate data, given the minimal changes from day to day. Perhaps one or two cancellations and two or three new enrollments might occur.

Its something like 50K rows per day, it is not so much but we are on prem 

How can i handle this type of fact?  Usually directors request to me like ""How many enrolled students we have on day X""",2024-03-19 20:24:43
1biu10m,Seeking Advice on Open Source Ingestion Tool Compatible with Delta Lake and Spark,"Hi everyone,

I'm currently tasked with setting up an open-source ingestion tool for two main use cases within my organization:
1. Self-service capabilities: for our non-technical users to easily ingest data.
2. ELT data pipeline: construction to streamline our data processing workflows.

We were initially considering **Airbyte** as the primary tool due to its versatility and broad connector support. However, we've encountered a significant compatibility issue with our current architecture, which heavily relies on **Delta Lake tables** and **Spark** for data processing. Specifically, we're facing challenges with Airbyte's integration with this setup, as discussed in this GitHub issue: https://github.com/airbytehq/airbyte/issues/16322

Given this context, I'm reaching out to the community for advice:
- Has anyone successfully found a workaround for integrating Airbyte with Delta Lake and Spark, as per the mentioned issue?
- Alternatively, are there any other open-source tools you would recommend that could meet our needs and seamlessly fit into our Delta Lake and Spark-centric architecture?

Any insights, experiences, or suggestions you could share would be immensely appreciated. Our goal is to find a reliable, open-source solution that can accommodate our specific requirements without compromising on functionality or ease of use.

Thank you in advance for your help and looking forward to your recommendations!
",2024-03-19 20:14:13
1bistcf,Legacy Oracle Data Warehouse to Azure Cloud Migration Strategies,"Hello data engineers,

I'm seeking advice on migrating our on-premises Oracle data warehouse to the azure cloud. Our on-premises oracle db is 80tb with thousands of etl information jobs . 

 * What was your approach (business-driven vs. replicating on-premises processes)?

 * Any recommended migration strategies or tools?
Our current approach feels scattered. 

Any tips or lessons learned would be greatly appreciated!
",2024-03-19 19:25:41
1bhwr9o,How are people building a impression and conversion tracking system?,"Hey!

Not sure if this is the best place for this question, but I thought it would be interesting to hear how people are building models with require joining across large impression and conversion tables to calculate stats.

The main issue I am trying to figure out best practice for right now is how to handle the fact that conversions can happen up 30 days days in the future past impressions. This means each time I run my jobs, I need to select a whole 30 days of data from the tables. This seems extremely inefficient, especially when running this hourly.

Would love to hear how people are tackling this!

I am using DBT + Bigquery.",2024-03-18 17:41:10
1bhgung,Cataloging vector databases?,"Hi DE folks,

Created this account to gather feedback and insights from y'all. I'm fairly new to data engineering/governance. My company has been using Informatica Cloud for quite some time now.

&#x200B;

My organization wants to dive into the whole Generative AI segment. This is at a very nascent stage where nothing is really decided or planned, beyond the database. And the DB of choice for this initiative is Pinecone. Other than this, nothing else has been decided yet, what the architecture would be like, how and where we are planning to use Pinecone. I'm aware vector DBs are great for extending the capabilities of LLMs especially since they're infrequently trained, but again.. I have no idea if that is the use case here, or something else entirely since the company has not revealed anything around that.

&#x200B;

That being said, I am trying to figure out what would be the best way to catalog the metadata from Pinecone. Like I said, we're on Informatica and are using the Cloud Data Governance and Catalog tool for our operations - however this tool does not have any vector DB cataloging capabilities, nor does Informatica have any Pinecone connectors planned for the near future.

&#x200B;

Can you share some insights, maybe articles or just your thoughts on how I should be approaching this problem given the context around it? Apologies for such a broad question.",2024-03-18 03:12:10
1bhdssw,having trouble moving to MDS,"Hi Team,

like all data engineers, we want to have the modern data stack for a data transformation initiative. My context is that I'm having trouble getting management approval on this (mostly due to costs) and I think it is mostly due to data literacy gaps. We're doing a lot of that (teaching them) but it seems it would take more than just informing them to get them to invest. It's a large enterprise that doesn't have a lot of consumer data, but lots of other data opportunities to explore for analytics use cases in the form of supplier, vendor, partner, location data and varying industries. They currently don't have an enterprise wide implementation of anything. ALSO, they don't trust the cloud and currently is a heavy MS user.

so my question to the group is, given this context, what would you advise me to do as next step? Here are my thoughts but I'm open to ideas and stories. I would recommend to at least start with building a business intelligence platform with datawarehouse first. I'm open to using ms products if it will help speed things up a long e.g. power bi and ms sql as the datawarehouse. Then I'll probably just look for an open source tool (maybe - i'm not sure what ms has in terms of on-prem integration tool) to use for ingestion then a separate server for machine learning deployments (we can use local laptops for development). my only problem as well is the encryption/ hashing tools for PII, i think. use this for a few years to prove the use cases, then hopefully gain enough approval to invest in an on-prem mds.   
  
I was thinking the tech debt would be worth it just to get things started and prove that having these things in place works to get more buy in later on. would really appreciate everyone's thoughts thank you!",2024-03-18 00:42:48
1bgptkk,Choosing between Data Engineering and Strategy Consulting,"Dear sub,

I have a quite heterogeneous CV, but was always drawn towards coding, in particular data science & data engineering. Following my interest, after my studies, I have started as a Data Engineer at a boutique consulting firm. For the past six months, this is where I program ETL pipelines for varying clients and industries.

Now, as I mentioned, my CV is quite diverse, and just recently I have received an offer from a leading (Tier 2) strategy consulting firm. My job there would be on a much higher abstraction level, talking to clients about digital transformation and AI strategies.

I really enjoy the work I do right now as a Data Engineer, but the consulting firm is offering me twice the pay. On the other hand, right now I am aquiring a lot of ""hard skills"", therefore I am confident in what I talk about and work is fun.

If I go into consulting, the path would be quite clear: Work there for 2 years, do a (paid) PhD, work for 1 more year (or if it is fun, even longer) and then hopefully exit into a management role at a tech company or a corporate tech division.

As I do not know much about possible career paths in data engineering, I am interested in the later career options and how you would choose.",2024-03-17 05:10:43
1bgf8zo,Oracle WMS and Oracle SaaS - one depends on the other | seeking data sync advice,"Hi All,

I noticed this subreddit has Oracle WMS related posts so wanted to inquire on that same topic.

My Organization has implemented Oracle SaaS ERP and Oracle WMS solutions. Both are interconnected to each other.

Every quarter, Oracle deploys SaaS patches on Fridays. Similarly, WMS also quarterly deploys their patches, but on Saturday.

 From what we are being told, WMS and SaaS has reliance on each other. With one of the two being offline on both Friday and Saturday, how does one go about ensuring if anyhing not able to being passed to the other system, how to you sync that up? 

Is there a provision in SaaS that allows to queue up data intended for WMS, and vice versa.

thanks",2024-03-16 20:25:37
1bg7ell,Question on learning Snowflake,"I am currently learning Snowflake from their course site by creating a trial account.   
I already work with Databricks as part of my job. 

My question is will Snowflake be helpful in the future as a tool added to my knowledge?",2024-03-16 14:32:42
1bg44l6,Is the salesforce pub/sub api in replacement of Kafka or just the Kafka connector? ,Working on a pub sub api poc and im the end goal is the get salesforce change events into s3. I have the api setup and listening to events but do I then just write the event to s3 or do I put it in a Kafka topic and gain some added benefit to Kafka? ,2024-03-16 11:39:09
1bg221u,Is it required to create ADF CI in Dev Env?,"Hi, new Data Engineer here!

I'm tasked to create CICD for ADF and Databricks.

I understand the importance of pipelines from Dev to UAT and then to Prod. But, I'm a bit unsure about whether I need to create CI for ADF and Databricks in the Dev environment since all the pipelines are already there.

Thanks in advance!",2024-03-16 09:15:42
1besj42,Ingesting data from GA4 with a few Python statements,"In [this demo](https://github.com/airbytehq/quickstarts/blob/main/pyairbyte_notebooks/PyAirbyte_GA4_Demo.ipynb), we use the Airbyte’s Python library to ingest data from Google Analytics 4, followed by a series of transformations and analysis using pandas.

This demonstrates how you can do fast data ingestion prototyping of a complex data source, without having to start from scratch.

https://preview.redd.it/z9kyphw6hcoc1.png?width=1052&format=png&auto=webp&s=8756a27a3ca8e790281300ea550e28a04406558e",2024-03-14 18:41:35
1berurq,Looking for open-source libraries for database schema migration and suggestions on implementation,"I'm working on a project that involves database schema migration,  and I'm looking for some guidance and suggestions from the community.  
The  goal of the project is to create a tool that can migrate the schema  from one database to another. Initially, we're targeting support for  MySQL, PostgreSQL, and MongoDB. The tool should be able to retrieve the  schema information from the source database, generate the necessary SQL  statements or equivalent commands to recreate the schema in the target  database, execute the schema migration, and perform validation to ensure  the migration was successful.  
I've started implementing a package  structure for the schema migration process, which includes a retriever  package for retrieving schema information from different databases, a  generator package for generating the migration statements, an executor  package for executing the migration, and a validator package for  validating the migrated schema.  
Here are my questions:  
Are there  any existing open-source libraries in Go that already provide schema  migration functionality similar to what I'm trying to achieve? If so,  could you recommend some libraries worth checking out?     


If there aren't any suitable libraries available, I would appreciate  any suggestions or ideas on how to approach the implementation of the  schema migration tool. Are there any best practices, common pitfalls, or  important considerations I should keep in mind?     


For those who have worked on similar projects or have experience  with database schema migration, could you share any insights or lessons  learned that could help guide my implementation?  
I'm open to any feedback, suggestions, or recommendations that could help me move forward with this project.   

Thanks   ",2024-03-14 18:14:23
1beqtw9,Partitioning in Snowflake vs Regular Data Warehouse,"I am reading about Snowflake and understand that it does Micro partition which is different from how partitioning is done in regular data warehouses.

Let's say in a regular data warehouse I do

    CREATE TABLE table_name 
    .... 
    ... 
    PARTITIONED BY datestr

Is the equivalent of this in Snowflake

    CREATE TABLE table_name 
    ....
    ... 
    CLUSTER BY datestr


&#x200B;

As Snowflake does micro partitioning by itself?",2024-03-14 17:32:00
1bem5cq,data architecture for home project - crime preduction in streamlit,"Hello community. I like my job so much that I would like to use some new tools to build complete architecture for my project. input: world crime data > output: streamlit app on internet. So far I'm using complete MS environment (SQL, ADF, PBI) or for simpler projects (KNIME, PBI). Have ""weak NUC"" as server like 8GB ram 4 threads and so on. My picture is something like this, only open source: Local PC (64GB RAM, RX 6900XT): extract data via API with dagster and load to DuckDB, do modeling using polars and then prediction with pytorch. Pipeline between local PC DuckDB and server streamlit app maybe via jekins? I woul like to use most modern tools even if they are overkill, but willing to learn. Do you have any recommendations or comments? I am not 100% sure for usage of Jenkins as CI/CD between duckdb and streamlit. Thank you for every comment and recommendation gyus!",2024-03-14 14:13:40
1belgv1,Query + Script to identify unused columns in Snowflake and other data warehouses,"[https://blog.twingdata.com/p/identify-unused-columns-in-snowflake](https://blog.twingdata.com/p/identify-unused-columns-in-snowflake)

  
TLDR: If you're on enterprise Snowflake you can do it via a query. Otherwise there's a simple Python script that uses the sqlglot library to extract the physical columns and compares them against the info schema.  
",2024-03-14 13:42:48
1beg9e1,Informatica : career help,"Hi guys, I am working as an Informatica developer with a  total of 7 yr of experience but now I think my career is not progressing as much. Please guide me what should i do next...TIA. ",2024-03-14 08:21:01
1be9w0z,When do you feel to move next?,"Like title says.

When do you feel that it is a time to move next company? Here is a list that I can think of

- want to play with different type of data
- hard to get promotion
- not learning much
- your function is less appreciated in the org
- stayed one place too long
- just want to try something new
- want to relax and not working crazy


I have been with one startup about 5 years. Working with decent size of data with Python, airflow, snowflake, spark, aws, k8s, docker, etc.

I have been putting a lot of effort in this company although I'm at lower level. Several rounds of layoffs happened over the last 2 years and my team lost more than half.

Also a few of good team members left while company's revenue is still growing decently. Salary is lower side of median band.

I still don't know which sector I want to try, but want to hear from other DE's when they decide to move. Do you have your own indicator? Or mostly goes with your feelings?",2024-03-14 02:06:27
1bdbeqc,ERP System to Data Visualization Tool ,"Hi everyone,

The company I work at is currently in a system transition. We are transferring to a low code solution (CMiC) that seemingly offers little to no API support.

After transitioning systems, the company plans on integrating data visualization software to help supplement the short-comings of the system selected. CMiC offers some data visualization capabilities, but it does not come close to full fledged data viz softwares available. It does have the capability to output tabular data, so I’m thinking it will be a limited ETL process if I design the standard reports appropriately. 

Company would include 20+ business users that would need access to these reports. 

Number of rows can be expected to be 10,000+. 

My question is: Would Power BI or Tableau be a better solution for this situation? With the limited capabilities to export data, I’m thinking it would be a better solution to use Power BI since it seems like we’ll have to manually export the reports anyway. 

Please let me know if this is the wrong sub and I’ll happily report to another. Thanks!",2024-03-12 22:58:40
1bdah3x,Iceberg vs Delta table?,"Which one is better? 

Consider below points:
-more read operation vs write operation 
-due to late arriving data, we may have to update some of the transactions or table partitions. 
-schema evolution; e.g. int to bigint, new columns being added in the table. 
-rollback to previous states when something goes wrong during data processing 
-storage optimisation 

I would love to know how you guys are using it in production? Highlight some major disadvantages in both of cases. ",2024-03-12 22:20:12
1bd5pwp,Fivetran and GA4 Data... useless?,"We've recently purchased Mozart, which includes Fivetran for data connectors, then brings everything into a Snowflake DB. From there, we connect Tableau to Snowflake for dashboards. 

By no means am I a data engineering expert, so there's likely a miss on my side (and hopefully an easy solution), but I'm currently unable to find a single way to bring in GA4 data that contains dimensions for date, source, medium, campaign and page title, then the metric of sessions. Ideally, we'd have a bit more than that (events, conversions, views, engaged sessions, etc.), but I'd be happy with that as a starting point. 

From what I've seen, there's no page title available at all within any of the tables from Fivetran's connector, and I can't get any of the data to align with what we see in GA4. 

&#x200B;

Is this a known issue? Are there areas to explore that I may be overlooking?",2024-03-12 19:12:22
1bctnhy,Best data modeling tool,"Currently, I am writing a report comparing the best data modeling tools to propose for the entire company's use. My company has deployed several projects to build Data Lakes and Data Warehouses for large enterprises. 

For previous projects, my data modeling tools were not consistently used. Yesterday, my boss proposed 2 tools he has used: IDERA's E/RStudio and Visual Paradigm. My boss wants me to research and provide a comparison of the pros and cons of these 2 tools, then propose to everyone in the company to agree on one tool to use for upcoming projects. 

I would like to ask everyone which tool would be more suitable for which user groups based on your experiences, or where I could research this information further. 

Additionally, I would want  you to suggest me a tool that you frequently use and feel is the best for your own usage needs for me to consider further.

Thank you very much!",2024-03-12 10:07:12
1bcnb0v,API get data which one is faster,"We have a project wherein we need to fetch data from ALDS Gen 2 to Excel via API.

My question, is it faster to read 5 out of 5 columns from the source compared to reading 5 columns from a total of 100 columns? 

I'm thinking wether to create a separate data model for the application or the silver layer (which contains more columns) would suffice.

Thank you in advance.",2024-03-12 03:29:00
1bclc8c,SOP for enabling end users to enrich data?,"We have a client who currently categorizes data in a spreadsheet. When new data appears, they need to assign a category.

As we move them to a warehouse, I'd love to give them a way to notice data that needs to be assigned a category + an easy way to assign. 

They are in the Microsoft ecosystem.

Seems like this should be a common need, what the standard operating procedure here?",2024-03-12 01:55:01
1bcijs0,Azure Data bricks  live project ,Need help. I have been working on Abinito . Recently an opportunity came for me to work on azure data bricks. we will be moving some of the on prem jobs . For this purpose i have done certification in Azure and databricks as well.  Have no experience on Python. Does anyone know of any online course where i can get chance to create 1 or 2 projects where data ingestion using ADF and transformations in Databricks using scala or pyspark. ,2024-03-11 23:51:25
1bc3u16,Integration Platform,"Do you treat data integrations separate of analytics/data warehousing?  Meaning do you have a IPaaS and team managing integrations (i.e. application to application), and another team managing ETL/ELT and Data Warehousing?  Obviously there’s a lot of overlap in functionality, not necessarily in tooling or processes.  I understand the nuances and differences, but often leadership doesn’t.  If you have successfully merged those disciplines, can you talk about how you did that and the tools you used?",2024-03-11 13:58:54
1bbhvxs,[Help] Market order book storage for further analysis,"I'm looking for advice on efficiently storing historical market order book data for analysis purposes. Eventually, I'll have to deal with around 400GB of data (10 pairs, 1 year worth of data, with an entry per second, and each entry being roughly 1 kilobyte). I need to collect this data manually since the API I'm using doesn't provide historical data.

At the moment, I'm storing the data locally in a Parquet file containing a Pandas DataFrame. However, this file is getting too large to fit into memory. I'd like to be able to scale to the cloud in the future.

I've thought about two approaches:

1.Recording into a traditional database and then converting it into a columnar format.
2.Recording directly into a columnar format.

I'm unsure about which tools to learn for this task and don't want to reinvent the wheel. 

Can anyone suggest tools or approaches that would be suitable for this?",2024-03-10 18:55:59
1ba8g6b,Opinion on Data Engineering Bootcamp- WeCloudData,"I'm looking to enroll in the DE bootcamp by WeCloudData- the self-paced one and wanted to know if anyone has gone through their bootcamp. Would like to know your experience- how was there course material, TA support, and career support? I'm currently working as SWE and looking to pivot into a de role. I know there are other ways of developing those skills either by watching YouTube videos or through Udemy but I like to have a structured course.

Please mention If you tried other boot camps and found it helpful.",2024-03-09 03:58:09
1b9tv07,Share Your Big Data and ML Adventures!," Hi team,

I'm relatively new to the Azure data engineering world and have taken a leap of faith to step out of my comfort zone, which was primarily in SAP data warehousing. I am thankful for how this role has supported my family and me, but I am eager to learn about what everyone else is up to in the field. For instance, I'd love to hear about the exciting projects you are working on, like using big data to tackle poverty or advanced machine learning projects in search of extraterrestrial life.

I'm curious about the challenges you face daily and the successes you quietly celebrate when things go smoothly. With the rapid introduction of new tools, how do you cope with the uncertainty of the future while maintaining a balance? It must be quite a juggling act.

Let me share my experience to start the conversation. Currently, I am working in the agriculture sector, extracting SAP data into ADLS and processing it with Databricks. It's not overly complicated or fancy – the output is a straightforward PowerBI dashboard. The most engaging aspect so far has been writing simple PySpark scripts according to specifications. The main challenge lies in coordinating among multiple teams, ensuring that SAP views are available for data orchestration. As for the future, considering my background in SAP BW, I aim to delve into the open-source realm. It's a new frontier for me, and I remind myself to stay focused and aim for my Databricks certification before venturing further.

While I'm not yet tackling global issues like world hunger, I'm really interested to know about the complex problems you all are solving. Thanks for sharing your insights!",2024-03-08 17:31:52
1b9qfw8,Master's dissertation research project ideas,"Sup lads, 
For context: I am currently in my second semester doing a Master's in Big Data and data science technologies in London. 
I m new to the data science field and whilst i v done my undergraduate in computer science and have decent madtery of certain base skills, m still realtively new to the data science field. I am very interested in data engineering in contrats to typical data analysis and vizualisation. 
For my Master's final dissertation, I noticed that most students just pick a research topic in healthcare for instance, look for a dataset, do analysis, show findings and thats it. 
To me I find this aspect of data science uninteresting and trivial and i have no idea which research area to aim for anyways. 
Since i still lack experience I would very much like for my research project to be slightly more technical and can sort of nudge me to the DE field. 
If anyone has any ideas or past experiences i would very much like to hear anything, this choosing topic phase is my least favourite and I need some sort of brainstorming to get me started so I can snowball!",2024-03-08 15:16:59
1b9p7eb,Turnkey Data Analysis Using DuckDB and Metabase for PostgreSQL & CloudWatch,https://medium.com/gitconnected/turnkey-data-analysis-using-duckdb-and-metabase-for-postgresql-cloudwatch-1255b925b7b4,2024-03-08 14:26:26
1b9b188,Dealing with Management's Lack of Awareness (and Potential Underappreciation of Technical Contributions),"TLDR:

Feels left out of the handover process after team lead resigned. Concerned about career growth & potential reassignment of tasks to less technically skilled coworker. Time to look for a new role or tough it out while trying to get buy-in from management?    
.  
.  
.  

---
Hi all, relatively pretty green & junior here (esp. with regards to navigating the human side of things in a corporate setting). Looking for advice/perspective from the more experienced DE folks here. 

My team lead has decided to resign recently & we're about to start the handover process. I've been fortunate to be working with him as he's been a great manager who can advocate for me to management & thus far I'd like to say that I've sufficiently contributed to the data team. AFAIK I've been doing just fine since nobody has raised any issues regarding my performance.

But I just caught wind that I was not meant to be involved in the handover meeting. My team lead, bless his heart, does want to involve me so he will invite me in but it's strange to originally leave me out as the rest of data team are required to be in attendance. 

Some of the newer, currently in development reporting/dashboard stack that's definitely right up my alley is even to be assigned to a coworker who's not as technical as me. I don't mean that as a demeaning remark as said coworker does an amazing data admin-related job (which is his main role in the data team), but in a more DA/DE capacity, has to frequently ask me for ad-hoc, quite simple queries. Just doesn't make sense to me.

It seems like for now there's a still a chance to convince management that I have something to offer from a more technical standpoint to the data team through the handover meeting but the whole thing just leaves a sour note on my end. Is it time to start looking for a new role just in case management remains unconvinced?",2024-03-08 01:16:01
1b8vzri,How can one practice (alone and for free) with DE tools or frameworks?,"I am referring to all those that involve any kind of subscription or pay-per-use formula. 
Any suggestions?
",2024-03-07 14:28:31
1b8qxva,Dagster Cloud vs Dagster on GKE/GCP,"Coming from an airflow background. Just started using Dagster. What are the benefits to Dagster Cloud? Is one going to be significantly cheaper than the other? I know off the bat, that self hosting the webserver and all that will take a little more Ops work and config but I am up for it.

I'm reading that it enables ""Embedded ELT"", which is pretty much just dealing with the ingestion step. Currently I am using external queries in BigQuery to get the data in from Postgres (on Cloud SQL), which some might call a little hacky.",2024-03-07 09:59:16
1b8m5e7,What are some things you enjoy about being a data engineer?,"What the title says. Currently a DA, looking to move into DE and looking for some perspectives. While DA is interesting, I think I will enjoy the Increased technical aspects of DE. What are some things that appeal to you?",2024-03-07 05:07:07
1b8hpn3,Replicate Remote Desktop MySQL instances to snowflake,"Working with a company who has several locations, each with a separate Remote Desktop, using the same software to manage their business. I’d like to replicate that data from the MySQL databases to a unified location in Snowflake. Seems like options include fivetran / air byte etc. are there any more custom solutions that are easy-ish, reliable and cost effective?",2024-03-07 01:34:57
1b8fibk,European research groups,"Hi everyone, I’m a student that will graduate in a few months and I’m attracted by the research field. I was wonder if any of you know about research groups (universities/colleges or also professors) in EU or UK that are focused on the data engineering/management field. Topics that I’m passionate about are parallel computation, like anything about spark/dask/ray/etc., and something like the work of the DataLearning group (the only group that I know of).
Thank you all in advance ",2024-03-06 23:56:23
1b8e24b,Scraping over 1200+ links.,"Hi everyone, I'm struggling with scraping over 1200 graphs from a server. For context, I'm using Playwright in Python, and I used Chromium to scrap data of a single graph (by the way, is a server in which I have to login with my username and password).  Scaling this for a 1200+ graphs is driving me crazy, I do not even know if this is even possible (for every graph I have to scrap, I launch either a new Chromium windows or tab).  Obviously the best way to do it is asking for the data itself to the webmaster, but this is off the table at the moment.

Any idea how can I do this? 


[Edit: I made it guys! :D]",2024-03-06 22:57:47
1b8bscy,Spark Connect + EMR (Remote connectivity to spark clusters),"Background: With Spark 3.4, Spark Connect was released allowing individuals to remotely connect to remote spark clusters using the DataFrame API. They claim that it can be embedded in modern data apps, and even be able to use IDEs like VS Code. I think it's cool.  


Link: [https://spark.apache.org/docs/latest/spark-connect-overview.html](https://spark.apache.org/docs/latest/spark-connect-overview.html)

Question: I want to be able to do this with AWS EMR. Did anyone try this and was successful or mind sharing pointers on how to set it up? ",2024-03-06 21:27:52
1b8ajea,Test coverage in dbt,"In dbt, I am trying to make sure that all our fact tables have relationship tests on their dimension fields.

Is there a tool which would do this? So far I've only seen packages which check the coverage for all fields, but I do not want to enforce that, only for the dim\_\*\_key patterned fields.

Do you have any similar solutions?",2024-03-06 20:38:28
1b88yj3,Data Warehouse - Fabric vs Opensource vs ?,"We are moving from a data lake of sorts, or better put, a bowl of spaghetti, to creating a new data warehouse. Our base system is  Dynamics 365 Business central. Open source or Fabric? Or is there a better alternative?",2024-03-06 19:37:03
1b885o0,How to read 10k small CSV files or 1 massive Parquet file efficiently (blogpost),"Hey r/dataengineering!

The Daft team published a blogpost about how Daft is able to handle ""adversarial"" file reading cases -- from 10k small CSVs to 1 giant Parquet file :)

See blogpost: [https://blog.getdaft.io/p/adversarial-file-reading-from-10000](https://blog.getdaft.io/p/adversarial-file-reading-from-10000)

I think it's pretty interesting and have come across this problem many times in my personal data engineering career so far. Some situations I've seen this happen:

1. 100k JSON/CSV files in AWS S3, dumped from an ingestion service every minute
2. 1 giant 1TB Parquet file dumped from an export process from a database

Historically, reading this type of data from the cloud has been really painful, and so the Daft team built in functionality to help you do this really efficiently. Happy to answer any questions about our approach!",2024-03-06 19:06:09
1b8657k,Concerns About the Future of My Position,"Over the past year or two I've set up a postgres product database for a small-medium sized business that I work for from scratch (and with no previous experience). On top of this, I've streamlined several very manual and time consuming jobs, originally in VBA but now more in Python. As I've progressed I've become increasingly anxious about the fact that I'm the only one who knows how our systems work, and given that I plan to leave the company within the next 5 years, that I'm carving out job requirements that will render my replacement too expensive for the company. How common are roles within the industry that require skills like SQL and Python (plus a bit of PL/pgSQL and VBA) that pay something like £30000-40000 a year? And what are some best practices or considerations that I can put in place to future-proof my job role for after I've left?

To provide a bit more detail for my responsibilities, I have set up and I maintain a database full of our product data. This database is mostly queried for different formats of product data and pricing to share with customers. I'm also currently working on a Python project to maintain product-file associations and aid in managing product files (keeping directories clean, making sure naming conventions are followed, etc.). I would like to implement some further systems to maintain data quality across different platforms (eCommerce, finance systems, stock control systems, etc.). I would also like to make editing our product data easy for non-technical staff, as currently all updates are done through me using pgadmin. Lastly I would like to document how all of our systems interact as a whole, as currently everything is divided between our IT supplier, an in-house IT guy, external contractors who code systems for us, and me. Everything is currently glued together in a very ad-hoc fashion.

I've taught myself all the skills I've used above as I've gone, but I'm still very inexperienced and so would really appreciate any guidance as to what kinds of technologies, practices and skills are appropriate for small-medium sized businesses, as well as if there is any standard procedure I should follow to identify business needs and communicate these to future employees, as I really don't want to leave an expensive mess behind that only I know how to operate.",2024-03-06 17:49:11
1b7vmpd,Need suggestions on using Athena ,"Hi,
I have parquet files arriving at S3 regularly. So I created folder structure as root/yyyy/mm/dd/ and in the day I have multiple small parquet files.
The parquet file contain time series data.
I want to generate a histogram in grafana for one of the columns.  How can efficiently achieve this. I cannot use grafana histogram plot because it requires all the rows. 
For couple of days it's fine. But when I want to calculate over multiple months I get timeout error. And even in Athena it takes more than a minute to calculate the buckets and counts. What I was thinking is to use ctas command to create a table with  statistics for each column and insert or update the fields every day with stats from new data. Is this the correct approach?",2024-03-06 09:31:40
1b7juwd,Data format translation tools,"I’m curious what people use for translating various data formats. My team has been using gdal (python bindings) converting spatial (shapefile, geodabase) and non-spatial data (csv, json, excel). we are currently exploring other tools that could perform a similar function and that:

1) has more readable code
2) work with wide variety of data formats
3) performant 

",2024-03-05 23:14:33
1b7i2u5,Question on architecting data solutions for newbie,"I am a new BA (have always been more IT-adjacent and hobbyist programmer) in a company with a very immature IT department and no data warehousing. 

What are some ways in which I can build applications for the team/department level that can utilize the stability of databases and move us away from running operations via Excel. Are there best practices or standard toolkits that I should look towards when building solutions?

From a cursory search I saw that Docker + Python + Postgres may be an approach to consider, but as this is my first foray I wasn't sure if that is overengineering or even a standard approach, and didn't know whether there is a simpler tech stack that might be more business friendly / stable / easy to maintain.

&#x200B;

&#x200B;",2024-03-05 22:04:23
1b7d4qw,How to Grab Keys of a Nested Dictionary in a Pyspark Column? Put Them as Values in New Column?,"I have a pyspark dataframe that has a column with values in this format (read.json on json files):

{50:{""A"":3, ""B"":2}, 60:{""A"":6, ""B"":5}} (This field is a StructField with StructTypes)

I have been trying to figure out how to get the data into this format:

Columns: |value|A|B|

|\[50,60\]|\[3,2\]|\[2,5\]|

This is my immediate issue, but to those who are interested in even more of a challenge I actually have two columns with nested dictionaries:

column1| column2

{50: {""A"":3, ""B"":2}, 60:{""A"":6, ""B"":5}} | {""value"": 16:{certain\_info1: 16}, ""value"": 60 : {certain\_info1: 42}}

my ultimate goal is to have the data in this format

Columns: |value|A|B|certain\_info1|

|60|6|5|42|

To be clear, the ""value"" info is not in the same order in the two columns, and the ""value"" info is not a key but the value TO a key in the second column.

I have been banging my head on this all day. Would love some advice or help. Thanks!",2024-03-05 18:50:56
1b71mkb,Why Your Clients Are Resistant To Data Literacy,N/A,2024-03-05 09:56:42
1b719yv,Ask the advice for (EL) process in ELT,"Hi guys
I'm just learning data engineering by joining DE Zoomcamp. I want to create a end-to-end project from processing files until creating a dashboard. I will be using mage.ai as workflow orchestrator and DBT as data modeling tool.

Here is my dataset from kaggle. https://www.kaggle.com/datasets/antonukolga/cyclistic-bike-share-data-12-months
In my mind, I need to download all files then upload to google cloud storage after that to bigQuery. Then trigger DBT to build it.

I'm confused what should I do in the process of extracting and loading? Just as simple as download and upload?",2024-03-05 09:31:44
1b6i9st,Free RAW Json to View tool,"Hi all!

We made a tool for BigQuery users. You can generate a view from a JSON object if you are storing your data as us in a JSON field, it's very useful!

Any feedback is welcome! :)

The tool is here: [https://vg.persio.io](https://vg.persio.io)",2024-03-04 18:29:10
1b6h1cm,"Comparing Postgres Managed Services: AWS, Azure, GCP and Supabase","At [PeerDB](https://peerdb.io/), we commonly get asked from customers on the preferred Postgres managed service. In that spirit, we are releasing our first part of Comparing Postgres Managed Services.

The blog includes 4 popular options  - AWS RDS Postgres, Azure Flexible Server Postgres, GCP CloudSQL Postgres and Supabase and compares them against 3 main dimensions of Performance, Costs and Features. [https://blog.peerdb.io/comparing-postgres-managed-services-aws-azure-gcp-and-supabase](https://blog.peerdb.io/comparing-postgres-managed-services-aws-azure-gcp-and-supabase)

A couple of disclaimers:  
The blog serves as an initial checklist for developers considering managed services. It's an overview, not an exhaustive analysis.  
The blog doesn't cover other managed services like Tembo, Crunchy, Neon, TimescaleDB, Aiven, etc. We'll include them in future posts.",2024-03-04 17:40:31
1b633he,Any good/bad recent experiences using Matillion elt for Snowflake?,"Hi all,

We have been looking at Matillion for ELT solutions (writing to Snowflake).  We looked at 2-3 years ago then our projects got delayed due to other priorities so we haven't looked at it.  Wonder if anyone has recent experience they could share on how well it works for you (or doesn't). 

&#x200B;

Thanks",2024-03-04 05:33:32
1b5z1l1,LSM Trees vs B indexes indexing,"I was reading Design data intensive applications and in 3rd chapter they mentioned 3 types of indexes in databases.

1. Hash based.
2. B Trees.
3. LSM Trees.

In lsm trees first we check in memtable if data is not present there then we check sparse index to check in which sstable segment will be present. And then we can apply binary search on that segment  for our key.  
My question is, Binary search we apply based on address arithmetic right because if we load complete segment in memory and then do bs then it will be a costly operation ?

Edit 1 - Sorry I messed up title, now i can't edit :).",2024-03-04 02:06:15
1b5tc2q,Advice For A Current Student,"Hi everyone! I'm looking for some advice and/or some direction about what to do.

Here is where I'm at:

I'm currently getting my degree in Information Systems with an emphasis in data engineering. I'm going to be graduating in December of this year (woohoo!). I've taken intro and advanced classes in SQL and Python. I'm currently taking a Data Warehousing course where we are using Snowflake (gotta love student licenses), airbyte, dbt, and a few other things. I'm also taking a class where we are leveraging AI to build ""full stack"" web apps using React and JS. I'll also be taking a data pipelines class that I believe covers CI/CD pipelines and will cover extensively things like Pandas or Numpy.

As far as projects go, I've created a stock trading bot that would run trading strategies on different stocks I picked and submit orders to a broker. Another project was retrieving COVID data and performing analysis. I also created a super simple website that was connected to a MySQL database, and all of it was run through AWS.

I'm currently working part-time as a web developer/mobile app developer. I use Kotlin for Android and Swift for iOS, and I've also recently been working on a website project using HTML/CSS/Javascript. I've worked on a wide variety of projects, most of which have been front-end work.

I’m trying to land a more backed/data-focused internship for this summer, but if I can’t, I’m pretty certain my current position would take me on full-time for the summer.

My question is, what would you do in my position? Would you get better projects, if so, what projects? Would you consider going for a master's degree? What jobs would you be looking for? It seems that true entry-level positions for Data Engineers are limited. 

Thanks in advance!",2024-03-03 21:58:04
1b538qj,Retaining Delta Table (like) Versions Forever,"Hi,

I'm reading that in Delta Lake the deltas are periodically cleaned up. One of the reasons we're looking at Delta Lake or Iceberg as a technology is so we can retain all versions of our data forever. Is there a way to do this in Delta Lake, or any disadvantage performance wise? I see Iceberg has a tagging feature we could run on every change. Since Delta Lake doesn't seemingly offer a way to turn this feature off there's a tonne of concerns I need to address:

* How does running \`VACUUM\` affect the ability to time travel back to versions older than the retention period? It deletes them.
* Does Delta Lake trigger the data retention process automatically, and if so, under what circumstances? It claims not to run VACUUM, but it does look like it will clean up the log automatically, which will prevent time travel.
* What are the performance implications of not running \`VACUUM\`, and should these be considered in schema design decisions?
* How can the risk of table vacuuming for older versions of data be mitigated, such as through backing up the version history prior to compaction?
* How does the retention period setting affect versioning strategies, once we VACUUM do we reset the version back to 1 or does it keep going?
* Once set, how robust is a retention period, and what impact does it have on data management?
* Is there a concept of version pinning we can use, e.g. a version 'tag' that gets kept forever.

Thanks in advance.",2024-03-03 00:17:41
1b50byv,Resources for Best Practices in Software Architecture,"Any suggestions for online resources and/or books explaining ""all-start"" software architecture design? I can use Lucid to make clean flowcharts, but I'm interested in learning key attributes and industry standards",2024-03-02 22:13:06
1b4pj29,Reverse Engineering a multi-database model,"I've inherited a large data warehouse that has many different SQL Server Databases.

I'm trying to understand the structure and relationships of the objects, but there isn't a data model I can refer to.  The system has grown over many years and has inconsistent names etc. which makes it more difficult to follow.

The largest issue, is that there are an abundance of nested views which, in turn, reference objects in other databases.  When you dig down through the views, you may even end up back in your original database.

I'm thinking that I need to reverse engineer it in to a data model so I can see the dependencies and work out what is/isnt required when implementing its replacement.

Has anyone worked with data modelling tools that handle multi-database dependencies?  I briefly looked at ER/Studio, but it seems to only handle dependencies within the same database when reverse engineering objects.  Even when adding the objects manually, I couldn't see how the database could be specified for an object, as it didn't seem to go above schema level.

Anyone worked with a modelling tool that can reverse engineer dependencies like this?

&#x200B;",2024-03-02 14:29:11
1b4n50k,"Mistral AI, Klarna AI customer support agent, extract and load still unsolved",N/A,2024-03-02 12:26:20
1bspum8,Managing data linkage across relational databases,"I’m working with a project where we ingest a bunch of data into one db, then normalize it and store the results in a second db, and finally make the normalized data searchable and editable in a third db. Internally, each of the databases has uniqueness constraints and a bunch of structure to make sure that duplicates are kept in check, etc. 

But, there’s nothing to maintain linkage of data records across the three databases. Of course, the data pipeline that is ingesting records into the second and third databases includes includes primary key values for the related remote database records. But, the three databases don’t know about one another.

I would love to have something like a foreign key constraint across two relational databases (in our case, mysql instances). Does anything like that exist?

And, more generally, what are some best practices for maintaining data linkage in  this type of setup with multiple databases storing related (and often identical) data?",2024-04-01 00:32:48
1bskle9,Where do you get to deploy your dbt-duckdb project?,"
I've playing with a project that pulls data from an API and store it as raw jsons on s3, then I transform these jsons to parquet files (store it in s3 as well) and from there I consider the parquet files as my raw sources and use dbt-duckdb to transform and get my final models in parquet files then move them to postgres and expose a read only API for a nextjs project.

Most of my pipeline is automated running in railway. The only piece I don't know where to deploy is the dbt-duckdb. I tried to do it in railway but it consume 'a lot' and I want this to be a cheap personal project. At the moment all dbt-duckdb stuff I run it manually and locally whenever I want to refresh my nextjs dashboards.

Thanks

EDIT: I'm very ignorant on infra related stuff. I guess I'm asking where would be a good idea to execute dbt-duckdb a python runtime serverless provider? Am I wrong? ",2024-03-31 20:52:11
1brp9rz,CDC: Aurora MySQL -> Athena,"Disclaimer: this is a batch job, analytics won’t die if they don’t have live data, 1 day updates is fine.

I know… I know… CDC question are one of the most common on the subreddit, I have gone through many option you guys have mentioned here like debezium,AWS DMS and delta lake. But I feel my solution is just simpler. Thinking of:

1. Read binlogs lambda (can be airflow if long time run or just step function)
2. Create updates for each table 
3. Load changes into memory pandas per table, if comes to be too large ( just one table 2gb daily data) it will read chunks from the raw files created using SELECT INTO S3 
4. Write .parquet files using aws wrangler which loads data into Athena table at the same time 
5. Query Athena (Temporary solution to allow analytics teams to access this data without waiting 30min for a query from prodDB)

This batch job have some flaws like there are some users tables that changes not only to insert but to delete, and these tables are important!! how have you approached this in the past when working on Athena? This thing only reads what is in s3 already and who the heck will know in which UUID file of the ones that awswrangler create in s3 that row is…(?) Is there an easy workaround for this? What would you propose differently? Should I change the solution and go a Data Warehouse? If so I would push my company to go with google big query, since this charges by query as Athena does and integrates very nice with data studio. Looking up to your suggestions :) :) :)",2024-03-30 18:53:48
1brndx4,Delayed Data Validation Best Practice Question,"Hi Reddit,

&#x200B;

I'm a Data Scientist at a company who has been tasked to do close to real time analytics/projects.

Except the data pipelines they have are basically non-existent, so I've had to detour into the wonderful world of data engineering first.

&#x200B;

Summary of problem:

In order to validate any given data point, I need the N previous readings, and next M readings.

N and M are different depending on sensor type, and data validation I'm trying to accomplish.

How would you go about solving something like this in Databricks?

&#x200B;

Longer Version:

We have these giant silos full of material, ranging from 0-40,000 pounds.

Every 15 minutes we get a reading telling us how much is left in each silo.

&#x200B;

The silos are periodically refilled so you can jump from 10k -> 40k.

0 -> 20k, any assortment of big deliveries.

&#x200B;

Here's the tricky part. Some of the sensors that tell you how much is in the silo will occasionally send a bad reading.

You could be somewhere around the 3k mark, and the sensor will read 27k but only for one or two readings.

&#x200B;

Current architecture:

IoT Hub writes to a Raw Telemetry table.

Enriched telemetry (multiple different sensor types) reads from Raw Telemetry and attachs master data about the sensor (location, timezone, etc.)

&#x200B;

I would then love to have some sort of ""silver\_silo"" table where every reading from each silo is accompanied by a number of 0/1 flags depending on the number of tests needed to run on it. 

Because from here, I can than have any number of projects branching off from auditing reports, data quality, projections, etc.

&#x200B;

Within Databricks, I have tried doing a forEachBatch, and depending on the streaming data we've

received query the enriched telemetry for the last N hours. However this feels clunky and expensive.

&#x200B;

I've also tried doing a wild Groupby(Timestamp, deviceId) collect\_list() to then pass everything into a UDF, but grouping

incorrectly leads to the reading and timestamp not being in the correct order. The only way to make this one work is

to then start making the data structure more complicated.

&#x200B;

Any advice or even keywords to google would be incredibly helpful!

My biggest issue is that I can't validate the most recent data point in whatever time window.

&#x200B;

Best!",2024-03-30 17:33:31
1brmutc,can we deploy web apps on databricks clusters?,if yes/no - how does one go about it?,2024-03-30 17:10:50
1br3cm4,Accumulating Fact Table,"How do I merge join If I have more than 2 sorted dataset ? I was trying to merge join 3 sorted dataset but I do get an error. 

https://preview.redd.it/m19agdxb1drc1.png?width=1126&format=png&auto=webp&s=28b979255888853ceb03f3b16ed8481e56845412",2024-03-29 23:48:28
1bqxv9a,Data Reconciliation Stack,"A company I’m working with is building a data solution that will process upwards of two million financial transactions per day.
Solution will contain business logic that will match (reconcile) 96% of transactions leaving the remaining 4% for manual reconciliation (matching). 
This manual matching will be done via 3rd party vendor app.
Primary source of data are .csv files, destination will be Snowflake and .csv files.

If you were to build, scale and maintain such a solution what tooling would you use?

(solution and tooling needs to be self hosted, meaning no PaaS or SaaS offerings can be used)",2024-03-29 19:46:54
1bqu6y3,"L-L-M-M-L-O-P-S, what are y'all doing?","Curious to hear about design patterns emerging here, or what you guys are converging to architecture pattern and framework/tool wise",2024-03-29 16:57:00
1bqu4iw,On request ML endpoint?,"
Hey, I have successfully implemented a ML endpoint with SageMaker and connected it to Lambda and API Gateway on AWS. It worked perfectly - I was able to send request and receive classification back. However, the next day I found out that this approach is extremely pricey for my needs due to the fact that the ML endpoint needs to run 24/7 in order to provide this kind of service.

So my question is - is there an alternative where I pay on-request basis instead of for the time the endpoint is running? 

Let’s ignore any business needs such as scalability and whatnot. I am only playing around with it for now so I am the only person sending requests. Therefore, I’d prefer on-request pricing. But I haven’t been able to find any suitable solutions.

As for the description of my project. Image multi-label classification case, I am sending binary image data and requesting classification back.

Sorry if I was not technical enough or if my question is too dumb. I am fairly new to this.

Thanks!",2024-03-29 16:54:10
1bqslqn,How do a data scientist should expand into MlOps / Data Engineering? ,"I have been working in data science in the retail industry for almost 3 years, the first 1.25 years as a data science intern & later 1.25 years as a data scientist. Till now, I have mostly worked on projects from POC to market test / backtest. I have not had a chance to push the model into production. But with the advent of AI automation, I do not wish to just stick to being a notebook data scientist & expand my expertise to ML engineering / MlOps. I am confused about how & from where should I start since it is such a vast ocean. I would be grateful if you guys could provide me with some starting points. Thanks !! ",2024-03-29 15:51:01
1bqs661,What are your teams using for Data Security?,"What are the best ways to secure your data? Are there any tools/processes your teams use? Also how did you secure PII? 

I was at a small company (<40 people) and we had a small team. We mostly restricted access to data and had roles people would use to access data. We kept PII restricted as best as we could but we could have been better. 

&#x200B;",2024-03-29 15:32:58
1bqpk6g,Strategy for batching Redshift to Postgres,"We have a query that pulls \~10M records from Redshift.  
The process was written as an MWAA DAG. It queries Redshift, processes the data with pandas, and pushes it to Postgres.   
We are concerned that the airflow server will get crushed by the amount of data.

If there is no obvious candidate field for batching (such as date or an evenly distributed dimension). What would be a strategy to get around this problem?

If we had a decent batch field candidate such as date, I would suggest pulling a date range of manageable size, processing as written, and pushing that to S3. Once all the date ranges are processed, have another task pull each of those files and push them to Postgres. But I haven't worked with some of these technologies so I'm looking for recommendations on best practices.",2024-03-29 13:40:44
1bqoram,Career Question. Very Unsure and Confused on Route. Please Help! :),"Hello. I am 23(M) working as a Business Intelligence Analyst at my parents house in a LCOL area. I have the opportunity to leave to work as a Platform Engineer in a HCOL area for not that much of a pay upgrade. I currently use a little bit of Python and a lot of SQL at my current job, and this new position is supposedly a bit of Python and Kafka. 

If I want to eventually be a Data Engineer / Platform Engineer in a LCOL area to buy a house, what are your opinions on the decision I should make? Not sure why but something is telling me to stay at my current job and keep searching..",2024-03-29 13:03:01
1bqmmj6,Metadata columns you've found useful in raw/landing tables,"For example:

Source file name, source file checksum, source file timestamp, ingest time are obvious, and some platforms have built-in ways to capture them.

Edit:  I am thinking specifically about landing data into columnar cloud storage, and more specifically into Snowflake tables \[and more specifically yet into table stages but that last detail is an unrelated set of tradeoffs\]

What about columns about anomalous things like ""why was this record loaded""?  The objection I see to columns like below is that if they aren't used consistently they might have zero/negative value.  

I'm thinking it might be valuable to a column like ""Discarded"" (bool) or  ""Discarded Reason"" (nullable) or ""Discarded time""

Is\_Reload ?  (bool)

Notes? (list/json)

Issue/Defect # (null)

Edit: I am envisioning adding a certain set of columns to all staging tables whether those columns are potentially useful for that data source or not (e.g. I could imagine soem sources couldn't possibly ever be discarded, or have a source file name).   ",2024-03-29 11:10:43
1bqjxth,Trying to create a local development environment using Docker,"I’m trying to set-up an environment using Docker to have a way to create test ELT pipelines for learning purposes. 

Currently I have:
- Postgres as a source database with data from pagila
- MinIO as an object storage (to use as a Data Lake)
- Airflow for scheduling pipelines
- Jupyter Labs for faster development and idea testing

I would like to add a few other tools, but I’m not sure what I could use.
- A way to use SQL on the data in MinIO buckets for data exploration.
- A Data Warehouse, where data would go to from MinIO after I do transformations. 

Do you have any suggestions?

Also if you have any suggestions for alternatives to tools I currently use I would gladly hear them.
",2024-03-29 08:15:34
1bqi1dm,Databricks from Dev to UAT Env,"Hi All, 

New data engineer here! 

 I'm setting up a CI/CD pipeline for Databricks from development to UAT using Azure DevOps (ADO). I'm wondering if there's a feature in Databricks similar to ARM templates in ADF, allowing me to seamlessly convert all dev-specific parameters within my notebook to UAT file paths.

 I'm specifically referring to modifying the code inside my notebook, as I've already configured the necessary UAT resources in ADO using variables.

Thanks in advance!",2024-03-29 06:04:46
1bqgoas,Senior members here - > Please guide !,"I am having 13 year exp now. Based out in India

I mostly work on big data tech.

Currently working on below tech stack in Data platform -

AWS , spark , flink, k8s, python, snowflake etc.

Mostly I am involved in hands on dev work + designing the solutions , creating HLD, LLD for junior dev

Currently seeking the architect role. However in current org , due to restructuring chances of promotion are low and I have about 6 months before next layoff cycle. I never practiced Leetcode , Do I need to solve these questions for arch positions ?

I am currently here seeking the guidance , What I need to focus on for architect positions ? Now, I am not officially architect but staff Engineer.

I am mostly focusing for data related tech stack as I have background in it. I am open to all suggestions from senior devs here.

If detailed path for next 6 months is provided , it will be very greatful.",2024-03-29 04:41:25
1bqda0i,Academic Research: ChatGPT data analysis and insight generation,"Hi r/dataengineering, I’m a graduate student who is conducting academic research on the use of ChatGPT for data analysis and insight generation. The goal of this study is to understand how professionals and enthusiasts leverage ChatGPT in comparison to traditional data analysis methods. I am looking to have a 20-30 minute Zoom chat with individuals who have experience using each. If you are interested in participating, have any questions, or just want to discuss the topic, please feel free to reply or DM. Thanks!

P.s.: Mods, please let me know if I should edit this post",2024-03-29 01:50:52
1bpxyfk,Help in Creating Synthetic Dataset,"I am to create a dataset that has all of world's dishes (By dishes I mean things like Chicken tikka, Thin Crust Margherita Pizza, Classic Margherita Pizza. It should include the dish along with all its variances).

Now I am to build a model that can tell the categorical hierarchy of the dish when I enter its name.
For example if I enter Chicken Tikka it should tell me:
Indian -> North Indian -> Mughlai -> Chicken tikka

I was given a hint that researching on Knowledge Graphs might help in building the model.

Can anyone give me more insights on how to go about making it?",2024-03-28 15:11:29
1bp604p,DE Hybrid Role Projects,"I want to build some cool stuff, just for the fun of it. DE is interesting and pays the bills but there's a different type of satisfaction building something brick by brick, or getting some cool answers out your data.

&#x200B;

Looking for DE projects ideas that also some other specialities to do something with the data. Maybe some visualisation using analytical tools, some spatial data, displaying the data in an interactive front end. I've actually got a fair bit of expereince in Front End as I originally took this path, built a few basic sites/apps in HTML CSS JS and React before deciding to go for DE professionally.  


Any one got any suggestions? Looking to find the joy in programming cause I'm currently a bit of an SQL monkey trying make sense of huge totally undocumented data sets every day in my role currently.",2024-03-27 16:23:43
1bp5bev,"Edginary - Your Data, Globally.","Hey Engineers,

We're thrilled to introduce you to [Edginary](https://www.edginary.io/), a new groundbreaking approach set to change how developers access and scale their data globally. We've all felt the pain points of dealing with centralized data sources: performance bottlenecks, reliability issues, and the dreaded revenue loss due to downtime. Edginary is here to tackle these issues head-on.

[Edginary](https://www.edginary.io/) is a serverless platform that moves your data close to your users in real-time so they can access it in ultra low-latency. By reducing the load on your servers and databases, we aim to not only improve performance and reliability but also save you money.

**How it works:**

Edginary pulls data from various sources like databases, data lakes, and data warehouses using Change Data Capture (CDC) or periodic polling mechanisms. This ensures up-to-date data ingestion in real-time or near-real-time.

Once the data is captured, Edginary enables users to combine, transform and aggregate it using its own internal real-time transformation engine. It supports Streaming SQL, with future plans to incorporate WebAssembly and TypeScript.

After processing, the processed data is replicated to hundreds of nodes around the world, stored and indexed in a low-latency datastore, queryable using REST API with millisecond response time globally.

**Join Our Beta:**

We're opening up our closed [Beta](https://www.edginary.io/beta)! It's a fantastic opportunity for you to get early access and see how Edginary can revolutionize your work with data. Visit our [web](https://www.edginary.io/) and be part of this exciting journey with us.",2024-03-27 15:55:56
1bp1g5r,"Variant VS JSON: a new data type 8 times faster, good fit for semi-structured data analysis",N/A,2024-03-27 13:08:08
1bowx4x,Naming conventions in BigQuery ,"I've had the opportunity to rebuild our entire data warehouse on BigQuery. My predecessor did some work in BigQuery and while everything functioned he did it in such a way that wasn't really scalable (no dbt/dataform, everything with just schedueled queries). While I think I did a pretty good job in general, I'm not to sure if my naming scheme for the datasets I've created makes a lot of sense. Tables and views are whatever, since their naming is primarily based on the dataset they belong to.

So what I have done is the following:
staging_source_name is where I have incremental tables that store historical data.

ids_ source_name is where I do some cleaning and SCD2.

dm_ is where I have domain specific tables that form the basis for most queries that are used for reporting.

reporting_ for tables/views that are used 1:1 in reports and dashboards

I think the general idea is fine, but my naming scheme might need an update so that if I leave in a couple of years my successor doesn't get confused. Especially the ids naming doesn't seem to be common, but this is what it looked like at a previous employer and I just went with it. In more modern terms it would be the silver layer I think?

How do you guys name your datasets and keep it scalable?
",2024-03-27 08:30:45
1boty8g,AWS data engineering or...? ,"I'm just starting out trying to learn data engineering but confused on the route to go. 
Should i just go through the AWS route (that is taking the SAA & then Data Engineering certification) or learning these individual tools (as described on awesomedataengineering.com)?",2024-03-27 05:09:20
1bojruu,Manage ELT pipelines with code using Airbyte’s Terraform provider,N/A,2024-03-26 21:31:39
1bogde0,How do you handle replication of data into development environments? Do you replicate everything or do you implement some sort or reduction strategy?,"Something I have been wondering for a while. As long as I have been at my current job we replicate all the data in every table for a set of schemas from our production cluster to a development cluster to facilitate development work. This seems unnecessary to me though. 

I feel like we could implement a tiered structure where we keep all data from the current year, half the data from last year, a quarter of the data from the year before, etc. in order to shorten our dev runs while retaining enough information to ensure that we are consistent with historical data as much as current data. 

Curious if anyone has implemented something similar.",2024-03-26 19:18:14
1bo056j,Urgent deadline : resources requested-Using Airflow (local or AWS connect Talend & Snowflake to build ELT pipeline,"Hi there, I’m doing a project for work and needs to be resolved asap. 

Using talent 7.1 open studio for reference 

Steps done so far: 

*Extraction*

1. Python script to get some data from API into a .json file 

2. Using talend: TSystem ->tFileInputDelimited. I created a schema and mapped out the fields accordingly and it generated a csv file 


* load*

3 . Connected tFileInputDelimited->tDBInput_1 (snowflake) . Mapped out data base and connected the schema

1st QUESTION: how do I do these steps using airflow? 1stly having an issue installing airflow in my local system. So should I use EC2 instance or lambda? If not please help provide some resources. IF not, what orchestration  tool is recommended to capture metrics?!? 



2nd question ( dependent on 1st): if you’re able to solve the 1st, then I want to do some transformations in Snowflake but right code for that so I can capture metrics.
",2024-03-26 05:30:54
1bnskfq,Career advice,"Hi everyone. So a quick back story- I am a self taught data engineer currently working at a small company as a junior data engineer. I’ve been here nearly a year now and I don’t think I’ve developed as quickly as I’d hoped. I like where I work and they’ve been very patient with me but I don’t think it’s the right place for me to progress. I am proficient in SQL, Excel, Power Bi and a little bit of Python (still learning). 

Just wanted to see how you guys have navigated your career journey from where you started and the skills you had back then to where you are and the skills you have now.

What do I need to do to become a senior or better my self? Skills ? Experience ? 

Thanks in advance.",2024-03-25 23:25:03
1bnpwgz,Continuation to structured blogs.,"Hey All,

I have launched my own channel where I'll be talking about DE related stuff, if you guys found the previous blogs to be useful please do consider visiting and subbing to my channel. 

This is the link to it: [https://www.youtube.com/channel/UCKqjpKQg7Um60HD8zqUDRGQ](https://www.youtube.com/channel/UCKqjpKQg7Um60HD8zqUDRGQ)",2024-03-25 21:39:34
1bnlwjj,"Parquet for reads, avro for writes","Parquet is better for read operations, while avro is more efficient for write operations. Do you ever  decide to save some data as avro because you think it will be written to more often in your data lake or do you prefer to have all files in one file format like parquet? What are your experiences here?",2024-03-25 19:05:42
1bnljan,"what is the difference between exadata database vs exadata warehouse? Also, can we migrate both of them to Databricks?",Too specific of a question but would like to know the nuances,2024-03-25 18:51:18
1bnl9gw,what is the prime use case behind migrating oltp/ods on prem systems to databricks?,title,2024-03-25 18:40:36
1bnf67w,"A Python package to help Databricks Unity Catalog users read and query Delta Lake tables with Polars, DuckDb, or PyArrow.",N/A,2024-03-25 14:33:32
1bnbtkc,dbt basics - final model,"Not having used DBT for anything other than demos, this is likely a 'basic mindset' question.

Is it normal to have a destination final target in your data warehouse that is defined outside of dbt?  

In other words: would it be the ""dbt way"" to for an organization to say: ""the schema of ACCOUNTS\_DB.HVAC.PERRY is maintained by some team other than DEs, and DEs use dbt to update that table, but dbt user does not have sufficient grants to modify the table's metadata""

&#x200B;",2024-03-25 11:57:27
1bnb4p5,Tools,"I have recently started working on a project where the source data is delivered in CSV or fixed-length file format, with individual files approximately 1 GB in size. The total dataset size could be large, and the data quality and transformation on this dataset are minimal. However, some of these source columns need to be mapped to another data model in PostgreSQL. Currently, there are COPY scripts to import the data into the database and Python/Java scripts to map it to the new data model with minimal transformations.

So, my question is, what's the ideal data pipeline for this scenario? Should I continue using the same scripting methods or are there any efficient ways to do this? This is a small team in a large organization, so computational resources are fine, but we have to rely on open-source data tools. Additionally, a small POC revealed that we don't necessarily have to rely on a relational database; data stored in Parquet format is equally efficient. Any advise is appreciated.",2024-03-25 11:17:53
1bn9edz,Fivetran RBAC - Terraform or REST API based implementation?,"I am curious to poll if anyone has implemented RBAC on Fivetran itself, via the usage of groups, teams, roles, etc.  


Have you found it necessary for your team, and if so, how does the general RBAC design look like from your side?

&#x200B;

Thanks.",2024-03-25 09:27:23
1bn0gvk,Any tools to generate lineage for Oracle PL/SQL Stored Procedures and Packages?,"Dealing with a large unwieldy code base that has no documentation. Looking for tools that can generate lineage and help with data governance/cataloging. 
",2024-03-25 00:37:45
1bmfuqh,Mock NAV data generator,"Hi guys

I am currently working on a demo database for NAV 18 (Cronus international)

Unfourtunately there is some issues in the dataset

General ledger entries from one year and sales invoices from another (so I cannot draw a line between the two)

Not enough data (only 30 invoices etc.)

Does anyone know of an expanded demo dataset for BC/NAV that can be downloaded or better yet a data generator that will allow for some streaming showcase

My business purpose is analytics and data modeling demos for new customers to my consultance business.",2024-03-24 08:24:34
1bly2lq,Is there good scope in data engineering in a non profit healthcare company?,"Most of the work is done on Epic systems like Cogito, Caboodle and Clarity using SQL to source data.
Teams are transitioning to Snowflake and dbt but also mainly focused on Epic suite.",2024-03-23 17:44:02
1blsz6g,Will learning kotlin be beneficial in my data engineering career,"So at work I was handed a spring boot application to do some minor changes to inject a service to manipulate and load  some kstream data into a topic and load some data to postgres.
I only use python and HiveQL otherwise for the past 5 years.
Will learning kotlin be of any use or should I just not bother too much with it.",2024-03-23 14:06:03
1blhvrx,How would you allocate an (positively) ambiguously capped professional development budget?,"If you were to have an agreement with your manager that the organization would support you in your own professional development, and the context bespoken included *courses, seminars, workcenters, associations, conferences, certifications, …* and the like, without any particular budget, where would you start looking?

I’m curious what sort of data engineering, devops, software architecture, events or programs are highly regarded for its value and currency with modern practices. Are there any renowned speakers who you’d look out for at events, or specific events in general you’d want to plan for? How about books, classes, boot camps, etc? 

I have such an agreement with my employer. Before asking I had some books and a course in mind, but my bosses response made me feel quite hopeful that they’d support me in much larger endeavors. I’m not sure exactly where to start looking given the broader scope.

For context on my aptitude for this stuff; I have never managed a data team, though that’s a possibility for the future. My only management experience is years ago in the military. I have always worked in smaller businesses where I wear many hats. My duties include analysis and automations, but I also manage the infrastructure and networking to make that happen. I feel comfortable with things like terraform, docker, Python, sql, vanilla JavaScript, Linux, some pretty advanced webscraping, marketing concepts like customer acquisition and implementing the client side and server side tracking for it, …

More recently, I’ve grown a liking toward the concepts of engineering distributed systems, devops platforms, data meshes, Kubernetes, react (yes, the JS framework), and streaming. These are all areas that I would like grow in, though I’m not suggesting I need to start there.


What do you think? Thanks for any input. ",2024-03-23 02:54:10
1blbtxo,buckets of blobs and cloudy functions,"I'm inheriting a system that is all buckets and functions with an SQL flavor for running queries. There are a few VMs for some persistent jobs. Are there any tools that would help this system with observability, maintenance, extension, and reliability? It's more data heavy then it is compute heavy.  
Thanks for your thoughts.",2024-03-22 22:16:41
1blao2n,Multi-tenant dynamic DAG options for a series of requirements?,"Have a unique case where I cannot find much information on how to architect or provide a good enough solution (which may be due to **how** I am looking for a solution). Most if not all of my background comes from the standard BI/analytics/internal use case DE, and I'm presently working in an environment where DE supports a product itself.

Essentially, we have the following requirements:

1.) Dynamic DAG generation, taking the data and either aggregating it further into a very denormalized structure and then also into OLTP to be consumed by an RDS instance. By dynamic, I mean that the tables created in the database follow a DAG generated by variables passed plus a configuration template. So if customer XYZ signed up, we would create table ""agg\_XYZ\_stuff"".

2.) Multi-tenant (customer) DAG execution, where the DAG executions need to be separated by the specific customer, So from the first requirement, customer XYZ's DAG would execute on an interval set by their timezone (likely daily batch, but again, by their timezone).

3.) As little human involvement beyond the config files or templates. Trying to stay away from manually having to create customer specific files/tables/models in order for ""agg\_XYZ\_stuff"" to exist. 

Most of my recent experience is using dbt-core, Meltano+Dagster, GBQ/Snowflake, but this is a whole new beast to begin with. I know that dbt can support pre-post hooks, but I don't know if it could handle the complexity of generating new models at runtime without invoking a PR to manually create the models. It seems to me that this kind of work falls under ""software engineer, data"" type work.

Any advice is very much appreciated.  ",2024-03-22 21:28:08
1bk78d2,Help deploying prefect to EC2,"    FROM python:3.10.6-slim-buster
    
    WORKDIR /orchestration
    
    COPY . /orchestration
    
    RUN apt-get update \
        && apt-get -y install libpq-dev gcc \
        && pip install psycopg2 \
        && pip install --no-cache-dir -r requirements.txt
    
    EXPOSE 4200
    
    # Run the Python application
    ENTRYPOINT [""python"", ""data_orchestration/staging_workloads/main.py""]
    RUN prefect server start

I can't build the docker image because it gets stuck in the ""prefect server start"" endlessly. I've seen other repos and I think there is nothing wrong with my code.

&#x200B;

Can you please help me?",2024-03-21 14:01:26
1bk57gv,Paralelize requests to insert,"Hey guys, maybe you have faced this. I have to send some data to an endpoint where the batch size limit is 10k, we were doing ok using python on airflow (single core) but now the data size increased to 10M so it’s taking too long because of the 1000s requests

We could use spark but I would like to avoid it (as 10M fits in one worker perfectly) but not sure how, not really used to python multi threading or how to implement on airflow ",2024-03-21 12:19:35
1bk50f2,Apache Doris,"I am looking for information, documentation plus videos preferable in English Apache Doris. This is an upcoming field I am venturing to and would like to have as much info. as possible on the subject matter. ",2024-03-21 12:08:32
1bk36hb,Is this coursera specialization still relevant for DP-203?,"I have been preparing for Microsoft's DP-203 certification exam with the modules provided in Microsoft Learn. I came across a specialization (by Microsoft) in coursera, here's the link for it: [https://www.coursera.org/programs/learning-program-for-family-iwira/professional-certificates/microsoft-azure-dp-203-data-engineering](https://www.coursera.org/programs/learning-program-for-family-iwira/professional-certificates/microsoft-azure-dp-203-data-engineering) . 

But the specialization's videos seem a little old. I'm just wondering, is this specialization still relevant for the exam? Has anyone finished it? ",2024-03-21 10:15:27
1bk11x5,Spark docker-compose questions,"Hi!  
I am currently trying to run Spark through Docker as part of my learning journey. It's giving me a bit of a headache though. Therefore, here I am, asking for some knowledge I know I'm missing.

The goal: Get Spark UI to work in order to analyze how tasks are handled  
Purpose: Get Spark to work as part of a larger Apache Airflow setup.

Problems:

* Not sure I understand the difference between a Spark Master and a standalone Spark Cluster that has access to Spark UI. Is the Spark Master UI a 1:1 functionality match for Spark UI? Is it something different altogether?
* Not sure if using the bitnami version of the docker image is the best way forward. How do you set it up usually (or do you go for another version)?
* What is not right in the way I set up my spark?
* Is there a need for a Spark UI dedicated container?

Bonus round:

* While exploring worker tasks I get hit with web addresses I can't access (either localhost/172.x.x.x type of networks or docker internal links). How can I work my way around that?  
Kubernetes seems to be a suggested solution but I am still unsure if I should go ahead and sink into that.

If anyone sees this and answers, it would make my day. Thank you very much! :)

For reference, this is a snippet under services of how I set it up:

 

`spark-ui:`  
 `image: bitnami/spark:latest`  
 `environment:`  
 `# needs to be updated whenever I relocate the raspberry pi`  
 `SPARK_DRIVER_HOST: ""0.0.0.0""`  
 `SPARK_DRIVER_BINDADDRESS: ""0.0.0.0""`  
 `SPARK_MASTER: spark://spark-master:7077`  
 `MAIN_CLASS: Main`  
 `ports:`  
`- ""4040:4040""`  
`- ""4041:8080""`  
 `networks:`  
`- spark-network`  
   
 `spark-master:`  
 `image: bitnami/spark:latest`  
 `ports:`  
`- ""9092:8080""`  
`- ""7077:7077""`  
`- ""4043:4040""`  
`- ""8998:8998""`  
`- ""8887:8888""`  
 `networks:`  
`- spark-network`  
 `spark-worker-1:`  
 `image: bitnami/spark:latest`  
 `depends_on:`  
`- spark-master`  
 `environment:`  
 `SPARK_MODE: worker`  
 `SPARK_WORKER_CORES: 1`  
 `SPARK_WORKER_MEMORY: 4g`  
 `SPARK_MASTER_URL: spark://spark-master:7077`  
 `ports:`  
`- ""8081:8081""`  
`- ""4042:4040""`  
 `networks:`  
`- spark-network`  
 `# # spark-worker-2:`  
 `# #     image: apache/spark-py:latest`  
 `# #     depends_on:`  
 `# #       - spark-master`  
 `# #     environment:`  
 `# #       SPARK_MODE: worker`  
 `# #       SPARK_WORKER_CORES: 1`  
 `# #       SPARK_WORKER_MEMORY: 4g`  
 `# #       SPARK_MASTER_URL: spark://spark-master:7077`  
 `# #     networks:`  
 `# #       - spark-network`  
 `# #     ports:`  
 `# #       - ""8082:8081""`  
 `# #       - ""4043:4040""`  
`volumes:`  
 `spark-data:`  
 `name: spark-data`  
`networks:`  
 `spark-network:`  
 `name: spark-network`",2024-03-21 07:40:45
1bjvp4z,⚙️ Automating Database Schema Change workflow Using GitHub Actions 🐙,N/A,2024-03-21 02:21:43
1bjq03j,Dynamically Copy Data from SQL Server to Azure SQL Database,"Organizations worldwide are beginning to adopt cloud services. As a result, the painstaking task of moving data from on-premises to the cloud has been on the rise for most data professionals. Using the best platform and technique for moving data is more crucial than ever.",2024-03-20 22:09:36
1bjmmox,Native Debezium alternative for replication from Postgres to Azure Event Hubs,[https://blog.peerdb.io/enterprise-grade-replication-from-postgres-to-azure-event-hubs](https://blog.peerdb.io/enterprise-grade-replication-from-postgres-to-azure-event-hubs),2024-03-20 19:53:42
1bisyjw,How to use learning budget?,"Hi everyone,

I have $500 to use as part of L&D from my company.

Just wondering what are the best resources to invest in.

I read most of my books on Kindle and my computer, so not really a big of physical books.

Thinking of educative.io membership?

Any suggestions?

Thanks!",2024-03-19 19:31:24
1bi1qnn,Athena Partitions and .parquet schema,"Hey everyone, I have athena partitions for date like:

year=2012/month=01/day=01 and inside each folder around 30 parquet files

The schema of the parquet files does not have the columns year, month, and day, it has a single column called 'date' which is a timestamp, when I created the folders just did it by filtering on the spark df and then writing on the S3 folder. Does this affect performance in how Athena queries this data? Should I have within the .parquet files a schema with the year, month, and day? should I change my partitions to be 2012/01/01 2012/01/02 and create a column within the .parquet files called just ""actual\_date"" and keep ""date"" timestamp? any thoughts? :) TIA!",2024-03-18 20:58:07
1bhsa88,Is it possible to use DataHub as UI for OpenLineage backend?,"I'm trying to integrate OpenLineage to my airflow server, but I wanna know if I can use other metadata visualization tools instead of marquez",2024-03-18 14:37:39
1bhj9jk,Starrocks + glue + iceberg performance vs. starrocks native storage,"We are looking into starrocks as the Open Lakehouse platform as query engine over glue(catalog) + iceberg open file format. My question is can anyone comment on the performance perpspective of this setup vs. the Starrocks with its own native storage? Will it still be levrage its outstanding Multi table Join performance as well SIMD based vectorized query engine architecture?  With its distributed MPP in memory query engine, is there any hard memory size constraints when dealing with Extremely large table tables (say billions of rows) when using the LakeHouse Iceberg table as the storage?  Thanks,  ",2024-03-18 05:28:27
1bhexl3,How should Redshift columns of type super be queried?,"I'm currently PoCing the new AWS Zero ETL DynamoDB/Redshift integration, and infrastructure-wise, it seems very straightforward and simple. However, the table created by the integration holds all columns of the DynamoDB source table as a single column as a large [super type](https://docs.aws.amazon.com/redshift/latest/dg/r_SUPER_type.html) named `value`, which I have zero experience with!

My first thought here is to create a view where I extract elements of `value` into distinct, normalized columns:

For example, suppose the value of one row of `value` is:

    {
      ""id"": {
        ""S"": ""RcVpEFPNIAMFytg=""
      },
      ""timestamp"": {
        ""N"": ""1705090208167""
      },
      ""files"": {
        ""L"": [
          {
            ""S"": ""s3://example/file1.raw""
          },
          {
            ""S"": ""s3://example/file2.raw""
          },
          {
            ""S"": ""s3://example/file3.raw""
          }
        ]
      }
    }

I could theoretically create a table/view to query this like:

    with s1 as
    (
        select
            json_extract_path_text(json_serialize(e.value), 'id', 'S')::varchar as id,
            nullif(json_extract_path_text(json_serialize(e.value), 'timestamp', 'N'), '')::bigint as timestamp,
            json_extract_path_text(json_serialize(e.value), 'files', 'L') as files
        from ""example"".""public"".""example"" e
    )
    select *
    from s1
    order by s1.timestamp desc;

However, I've never worked with super variables before, and I have two big questions:

1. Am I right to be extracting `super` type columns into normalized columns like this, or should I be querying them as they are?
2. This is a bit more of a detailed question, but how do I successfully extract arrays of `super` type columns? For example, in the above, the new `data` column would still be a `super` (`[{""S"":""s3://example/file1.raw""},{""S"":""s3://example/file2.raw""},{""S"":""s3://example/file3.raw""}`), and it's not clear both how this should be extracted and 3N normalized into separate columns?

Any help is appreciated, thanks!

&#x200B;

&#x200B;",2024-03-18 01:36:32
1bh5cf1,Recreating document layout in GCP,"I'm working on a pipeline in GCP that involves extracting data from a PDF, modifying it, and then reconstructing the PDF with the original layout. Process-wise, this isn't terribly complicated, but I'm struggling with a tool to preserve the layout information to help with reconstructing the PDF after all is said and done. Does anyone know any tools, GCP-native or otherwise, I could use to capture/utilize the layout?",2024-03-17 18:55:25
1bh3fsi,Docker operator alternative,"Similar to the docker operator we have in airflow do we have something similar in mage, prefect or Dagster? ",2024-03-17 17:38:55
1bgwk0l,CI/CD Databricks DBT and Github Actions,"New to DBT. 

I  want to run a CI/CD pipeline to perform DBT jobs against a test set in  Databricks. I want to do this in the most inexpensive way possible using  a very small amount of data so it can be triggered for every PR we  raise to the data models. Ideally this workflow would do DRY run tests so we don't need to launch any clustering. 

* Has anybody got any experience with this setup, can point me in the right direction, and ran into any issues? 
* Is this all possible in the  latest open source version of dbt-core?

I also want to create a metadata ingest to list files from various blob store sources e.g. Azure Gen2, into a synchronised raw table of metadata of those files (for visibility/querying/and ontology). 

* Is this something DBT can help with? Original plan was to do this with isolated Autoloader Spark jobs, not 100% ideal. 

Ideally if we can shove as much as possible into DBT to provide managed CI/CD testing and deployment, and overall a structured workflow for our data tasks, that'd be great.

Thanks for any help! ",2024-03-17 12:38:50
1bgfhg6,Guidance in leveraging cloud,"Hi all. Throwaway account because I’m paranoid.

My DE team is slowly shifting away from older msft-beholden tech, to more newer cloud based stuff (Azure). At the end of the day, a huge chunk of what my team does is file ingestion (or output) that either goes into SQL Server or Snowflake in some form or fashion. We are also starting to dabble with some APIs for some core business products that have data needs that have to go through us in some way or form. Management says we're trying to ""modernize"" our stack (for me going to C# from SSIS isn't modernization, but that's what they call it here). 

The nature of my team is B2B interactions. I guess my question is, what kind of cloud tools/offerings can I leverage in this type of environment? I want to be ahead of the curve in my team, but sometimes it feels so pointless to overengineer stuff for something that a simple process can do to upload files to a table and call it a day, or to extract data and create a file. We're not doing anything majorly cutting edge, and Snowflake is handled by a separate BI team. We currently don't do any type of streaming either. The business is currently expanding heavily, and I feel like we need to focus on scalability and producing performant solutions, but when my main experience so far at the company is just reading files and inserting and using SQL to get the data I need, I feel a bit lacking, and management isn't exactly great at providing vision and guidance.",2024-03-16 20:36:37
1bgby5z,Any data migration tips,"Pls share your experience on data migration especially structured data from single to multiple different systems. For example eCom kind of solution contains customer records and transactions. 

Thanks.",2024-03-16 17:56:39
1bflawu,Streaming + Realtime + Dashboard ,"I’m playing with streaming data from f1 games. 

Basically I get telemetry from the car via udp, and parse the bytes to float.

What solution would you use to:

1) Store this data
2) Create realtime dashboards ?
3) Create historical analysis?",2024-03-15 18:48:16
1bfkhdp,Has anyone used Salesforce Pub/Sub API to get cdc events into a kafka topic on AWS?,"I followed the quick start guide and i'm able to subscribe to objects, just waiting on an event to come through. Wondering what things I can do to get this data into a kafka topic preferable using AWS resources. ",2024-03-15 18:13:18
1bfjwdt,Favorite orchestration tool for prod?,"How do you handle orchestration in prod, and why?",2024-03-15 17:48:42
1bfiznn,"Total Novice, need advice.","I have been tasked with getting data that is regularly emailed to us in the form of an xml file into a database. I have gotten past the first step - I can get the file automatically put into a folder (OneDrive/SharePoint). But now I am on to the part of how I can get this data into either a SQL database (which we have) or something else. We are Microsoft leaning, and there are so many tools and options and ETL/ELT options that I'm not even sure what the best thing to do is. I would appreciate any advice that can be given. Much appreciation in advance. If this is the wrong sub for this, please let me know.",2024-03-15 17:10:19
1bfevv2,Snowflake FinOps Center - Control and monitor costs with this free Streamlit app,N/A,2024-03-15 14:12:33
1bf7wdx,"scalable, performant and cost optimised approach needed for data preprocessing","Hello. We have a spark cluster on emr to preprocess the data and move from staging to production layer( as part of lakehouse architecture on s3). The data in staging is on s3 partitioned by downloaded date and hour (date at which the data comes to our system) in increments every hour. We need to preprocess the data and write it to production partitioned by report date (date at which the event occurred). 

The challenge is the source system may update the previously sent data. I.e, for the event recognised by event id which happened last week and was ingested last week, may come again with updated values. We need to update it in out production layer. 

We are currently trying to do it in spark. Every hour we get the staging data (one hour incremental data), get all the data from production layer by distinct report dates and upsert in spark and re write back to prod layer

&#x200B;

Data volume.

around 100 million records per day in raw layer (10 gb parquet files)

around 30 million records per day in prod layer (2-3 gb parquet files) (after preprocessing)

Everyday we get around 60 dates older dates data (or 40 dates older dates data per day) which we have to upsert

So we have to read in spark for every hour data in staging layer (30-40 report dates data from production to upsert which amounts to 180-200GB compressed parquet files, on de-serialization will expand to 1000GB-2000GB)

&#x200B;

The spark job takes lot of time to complete (1:30 hours to 2 hours on a 7 node cluster with 32 gb machines and 8 core per node). Most of the time is on reads and writes. We have done all the possible optimisations from the spark code and config perspective

For us it's too long and costly and also not scalable

Is there a better solution for this? How does industry approach this problem? Should we do upsert over any datawarehouse instead of spark?",2024-03-15 06:57:41
1betqlh,Need help with SharePoint Online list attachment refresh issue in PowerBI Desktop!,"I've been encountering a problem lately while refreshing a PowerBI Desktop report from a SharePoint Online list. The refresh gets stuck indefinitely, even after leaving it running for an hour with a stable connection. This issue arose as my SharePoint list grew substantially (around 2k items), although the online report continued to refresh successfully through the service.

&#x200B;

After investigation, I discovered that the problem lies with attachments in some of my SharePoint list items. When expanding the AttachmentFiles column in the SP online list connector to retrieve the URL of the first attachment, the refresh becomes sluggish. Removing this column resolves the issue, but I need to access the attachment URLs for my analysis.

&#x200B;

Has anyone encountered a similar issue or have advice on how to pull attachment URLs from a SharePoint list? The conventional SP connector doesn't seem to provide a solution. Appreciate any insights or suggestions!",2024-03-14 19:30:45
1bese1h,How do you debug DBT models ?,"Hi,

We are transitioning from executing Bigquery stored procedure to DBT (or maybe Dataform).

There are many advantages but one things that bothers me is how to easily debug a model.

For example if some data are not shown properly in the end table, backtracking to find the faulty model is quite cumbersome.

Having CTEs everywhere makes it annoying because unless I manually change the CTEs to a CREATE TABLE, which is annoying, I need to wait for all the CTEs to execute and it can take some time depending on the models.

Having real tables instead of CTEs is not helping either because I still need to write the CREATE TEMP TABLE etc..

I am probably nitpicking but since it was easier before, some coworkers do not like it.

Previously I just had to copy paste the CREATE TEMP TABLE statements from the stored procedure to BigQuery and I was good to go.

Do you have any tips or tools to improve that ?

I have just found about Count which seems great but is also expensive.",2024-03-14 18:35:58
1berdf5,Data policy / ethics,"You would think this would fall under data governance (internal and external). How ever it’s really not, because obviously I wouldn’t be asking this if there was proper governance, in the ethical form not business logic. More a question for  senior members. I’ve been through this before at banks and parted ways. 
What would be the best route, if the data team is manipulating data (intentionally), albeit their objective is stockholders perception but I see this could have repercussions beyond the company.  How would u recommended absolving my involvement and not losing that contract because I don’t know how far it climbs to the e-team. Sorry but I can’t give details of the data. I was thinking just a registered letter to my lawyer (he agrees). I’m also not into this whole “ I was being told what to do” deal. The last time this happened it was pretty bad, I parted ways with the bank and their team tried to use me as a scape goat and the bank was considering pursuing charges against me (no joke) but the fact I left and gave reasons of “team dynamics” and “unethical data manipulation”, which I guess they didn’t read because it was on the second page of the voluntary contract termination I drafted up, my lawyer basically told them to take a hike but this back and forth a  few weeks costed me a few grand.
",2024-03-14 17:54:31
1beq90y,How to automate/make azure dataflows dynamic?,"Hi,

For a project I'm required to build a datamodel using only the graphical components of Azure Synapse, meaning only pipelines and dataflows.

The problem is that this datamodel will have around 100 tables and each table would require a dataflow. That's of course a lot of manual ""drag and drop"" work.

Is there any way I can lessen that burden? I was thinking of making configuration tables to dynamically fill them in but I don't think that that's possible (and a good idea).

&#x200B;

Any tips? Thanks for reading",2024-03-14 17:08:20
1bep67v,Cluster Library in databricks,"I was wondering what the benefits of this are?  For example, our current cluster has S3 could we install the library directly to databricks? ",2024-03-14 16:23:39
1begn3l,XLSX/CSV file 'dropbox' for data cleaning before being pulled into Airtable,"Hi All,

I am looking for a solution whereby XLSX/CSV files can be 'dropped' in a certain location where they will then be processed and cleaned before being fed into Airtable. The data being dropped will be in the form of tables with specific headers.

Any ideas?

Thanks!",2024-03-14 08:49:31
1be7qvk,dbt alternative using Python,"For folks out there looking to build data infrastructure using python: [https://docs.turntable.so/quickstart](https://docs.turntable.so/quickstart)  


Supports federated ETL across warehouses and DBs, data contracts, embeddable into other BI tools/API and a metrics/semantic layer  


&#x200B;",2024-03-14 00:27:36
1be7ccx,Data Engineering vs MLOps,"I'm a traditional backend engineer with lots of cloud experience and I'm interested in making a transition to the AI/ML world and I think a lot of my experience could be applicable best towards the aforementioned roles.

From my limited research so far my understanding is that data engineers don't necessarily work with ML or even data science teams but just data in general whereas MLOps/ML platform/ML infrastructure engineers require a good understanding of ML and work on the entire ML-lifecycle infrastructure which may or may not include ETL pipelines. Did I get it right? What's your perspective?",2024-03-14 00:10:00
1be75eb,How do I progress my career working in manufacturing? Look for something new or wait it out?,"I joined this company two years ago with next to no experience in data. I did some self-learning 6 months prior to joining and I already had some experience with Python from college, but that was it.

Over the first year I was essentially a data analyst with <5% data science work. I worked with controls engineers to get data off the equipment and into SQL tables. From there I built out dashboards in Power BI that showed relatively real-time metrics out on the shop floor. I then started to build out tables/queries to aggregate that data and send out daily reports. During this time I was learning a lot and honestly having a blast.

After the dashboards I did some dabbling with condition monitoring sensors to try to drive towards predictive maintenance solutions. This totally flopped, we just didn't know enough about predictive maintenance and the sensors/math required to make a viable production ready solution.

The next year, I had the opportunity to dabble in Azure to better understand how we could bring some of this production/machine data into the cloud. This would help the business bring all our data sources together in the future. I dabbled in ADF, Azure Stream Analytics, and a few other Azure tools.  But my knowledge felt half-baked and disconnected. 

Since then I've basically hit a brick wall. Systems were changing, equipment was moving, and I was still a sole employee working alone in manufacturing. We began switching our ERP to Oracle so ERP data has essentially been off limits for me while IT works. It was bad data to begin with. then we wanted to start using a historian for manufacturing data so I again had to wait for IT to make implementations. Now, I'm at a point where I feel trapped. The historian is cool but ultimately does not allow me to learn traditional data engineering techniques/tools, the PLC programmers have limited time so I can't get new data from them, and extracting data from other data sources is something that IT likes to keep for themselves since I fall under the ""operations"" bucket. 

I'm alone, feel like I have very little mentorship in this space, and see roadblocks in nearly every direction. However, I can tell my director has a significant amount of trust in me. She always asks for my input to provide feedback to a ""global I4.0 committee"" that is forming. Additionally, she seems open to me getting professional training but data engineering in manufacturing feels super niche. It feels like normal courses don't apply. Networking, the tools used, etc are all different and create a unique set of problems.

How do I continue to develop? I like DE and may even see myself as a good software engineer in the future (not sure how common that switch happens), but as of right now I'm learning a totally different skillset and I'm unsure how many years it will be before a true ""data team"" exists that I could learn from in the manufacturing space.",2024-03-14 00:01:49
1be1kpo,Data in Life Sciences: Are We on the Same Page?,"Hey everyone,

I've been thinking a lot about how we, as data professionals, are tackling data adoption hurdles, especially in areas like drug development and personalized medicine (Life Sciences); so I wrote this article. 

[https://www.datacoves.com/post/benefits-of-digital-transformation-in-healthcare](https://www.datacoves.com/post/benefits-of-digital-transformation-in-healthcare)

Are you facing similar challenges? How are you managing them, and what strategies or tools have made a difference in your work?

Really curious to hear if your experiences align with mine and how you're navigating this complex landscape.",2024-03-13 20:16:50
1bdyu6v,Flink&Spark - which OTF most commonly used with each?,"Are folk seeing a general trend in the particular pairings of open-source compute platforms and open table formats (OTF)? 

Specifically: 

1. My _assumption_ is that #ApacheSpark [Databricks] is most often used with #DeltaLake [Databricks], and less so than with Hudi and Iceberg. Is that true?

2. Does #ApacheFlink get used with #ApacheIceberg / #ApacheHudi / #DeltaLake equally when an OTF is involved, or weighted towards one or the other?

I know that folk at the various respective vendors in this space are going to have a bunch of data points to share illustrating the pairings in permutations of favourable light ;) But I would love to hear from folk in the field too who have anecdotal evidence to share too :D

TIA.",2024-03-13 18:29:18
1bdy4lv,Data consistency checking,"Hi everyone, I have question like that How do all you guys check if your data consistent. Let me be more detailed. Think about that you have two tables one in A server another in B and based on A table, B table is being updated. I want to check if all the values in B table matches values of A table. Is there best practice to check it ? Currently I have developed my logic, checking both tables with python code. But wonder what if tomorrow the size of tables growths to 2 billion rows. I would like to get your recommendations",2024-03-13 18:01:56
1bdxy8e,PySpark Structured Streaming,"Hey Guys,

Need some help on writestream with foreachbatch, so my code runs perfectly fine on a single user cluster but when i test it out on a shared multinode cluster it break with an error

&#x200B;

`def process_batch(df, batch_id):`  
`AT_original.alias(""t"").merge(`  
`df_batch.alias(""s""),`  
 `""s.key = t.key""`  
`).whenMatchedUpdateAll().whenNotMatchedInsertAll().execute()`  
`query = df.writeStream \`  
`.format('delta') \`  
`.outputMode(""update"") \`  
`.foreachBatch(process_batch) \`  
`.option('checkpointLocation',)\`  
`.trigger(once=True) \`  
`.start()`

Cannot serialize the function \`foreachBatch\`  


what am i missing here?",2024-03-13 17:55:00
1bdvhzv,Introduction to ELT with CloudQuery — a declarative data integration framework for developers,"(I have no affiliation with the tool)

If you are tired of drag-and-drop or heavy client-server applications for data ingestion, worth checking this alternative open-source framework written in Go. Simple, fast and stateless.

[https://kestra.io/blogs/2024-03-12-introduction-to-cloudquery](https://kestra.io/blogs/2024-03-12-introduction-to-cloudquery) ",2024-03-13 16:18:05
1bdv8ix,"5 Common Myths About Massively Parallel Processing, Debunked",N/A,2024-03-13 16:07:17
1bdql0m,pyspark coverage," For those of you who use PySpark consistently, 

do you have code coverage for PySpark UDF/RDD? 

Which tools do you use?",2024-03-13 12:46:16
1bdppvx,The Power of Perseverance: Starburst’s Journey from Open Source Vision to $3.35 Billion Valuation,N/A,2024-03-13 12:00:39
1bdp93l, How to read Trino datas from PowerBi &|or SSAS,"Hello there,

I look for advice on how to connect a powerbi to a trino(oss) to query trino datasets.

(In a corporation environment)

Is there someone to share any tools, proposal, advice,or RetEX on a such subject ?

Thanks a lot by advance.",2024-03-13 11:34:14
1bdlq97,Joining two data sources without common columns,"Hey y’all! 

I’m trying to join two data sources; one for the customer subscriptions and the second is for the finance to monitor the payments and transactions with the subscription period. 

First tool have the following: 
Company name 
Contact info 
Subscription info

Second tool: 
Company name (stored in another language) 
Transactions 

The solution came to mind is to force the finance to add contact email and join if the email domain is the same and gave it a surrogate key. 
However, not all clients are using their work email, so a lot of them are using their gmail account. 

I’m not finding any common data/columns to join on… any ideas that might help? 

",2024-03-13 07:42:22
1bdcmb1,Data cleanup for USA addresses,Anyone know of a service that cleans up addresses not only of USA but also other countries. ,2024-03-12 23:48:54
1bdaodb,Recommendations for Multidirectional Real-Time Data Pipeline,"I'm working on building a stitched-together ERP system, where I want actions taken and data created/received in one piece of software be reflected in the other ones. (Ex: make a purchase order in one piece of software => can see same purchase order in another). Most data pipeline and integration services I've seen have syncs on a daily basis or some other time-frame, they aren't event driven. Anyone have recommendations for something like this?",2024-03-12 22:28:19
1bd8k71,Need help mapping freshness as a metric for upstream data sources,"We run different batch jobs via Qubole/Airflow setup which run at different cadences for different sources. We also have a datadog integration available for observability.   
For eg, there will be a pipeline which is running every 3 hours and then there will be one which is running once every 24 hours.   


I need to map the data delay that the upstream is having. For eg, if I am expecting the upstream to update their data by 11am PST, but the data is delayed and arrives at 11:30am PST. I will run my ETL/batch process after 11:30am. Need to track this 30 minute delay on datadog.   


What is the best way of doing it?",2024-03-12 21:05:18
1bd6tif,Help need to with passing a unique key in a workflow,"Hello Folks,

I currently need a solution for a particular scenario. I Have three jobs to run the pipeline(assume they are set up using Data bricks workflow). Now I want the first job (1) to produce a unique key and pass that one to the next job(2) which will pass to the next job(3). When this workflow runs again the key generated should be different compared to the previous one.  


This is for databricks

Any Ideas!!

Thanks.",2024-03-12 19:56:27
1bd2389,Any resources for monitoring and alerting? What tools do you use? ,"Just want to learn more about how to monitor pipelines in a more efficient way and what tools I can use to set alerts.

",2024-03-12 16:51:58
1bcy5ou,Education funding requires I ask questions to professional DE [Urgent],"I'm currently applying for government funding for a Software Engineering education. The application requires that I find people in the occupation I'm pursuing (Data Engineering) and ask them some questions about their experience.

My program starts in a couple weeks, and I just found out that approval has to come through before the program starts! 🙃 

If anyone who works as a DE could shoot me a DM so I could ask a few short questions, it would mean everything! 

Also, I need to find 2 employers who have DE working within their company to do the same with. If that is also you, or someone could point me in that direction, that would also help immensely!

I feel awful for the huge ask, but this community is the best place to find active data engineers.

Thank you!!",2024-03-12 14:10:02
1bcy126,Iterating terabyte-sized ClickHouse tables in production,"Interesting take on doing schema migrations on very large production OLAP tables with streaming ingestion and lots of concurrent reads.

[https://www.tinybird.co/blog-posts/clickhouse-schema-migration-while-streaming](https://www.tinybird.co/blog-posts/clickhouse-schema-migration-while-streaming)

Disclaimer: I work for Tinybird, but did not write the post.",2024-03-12 14:04:20
1bce00m,Validating data types before merging a dbt model,"Hi, could the community kindly advise on how to address data type inconsistencies after merging a dbt model? Here's a little background:  following the creation of a staging model that explicitly casts each field, I ensure that ""dbt build"" completes successfully and subsequently execute ""dbt sql"" to validate the records. The dbt extension in VSCode defaults to 500 records for this process. If this subset of data looks accurate, a pull request (PR) is created. However, we are currently facing an issue with records that violate the defined data types, aside from the 500 records displayed. This becomes apparent only after executing a ""select \* from"" query on the table. This approach seems a bit heavy-handed, and I'm confident that there is a more programmatic way to handle this. ",2024-03-11 20:51:33
1bcbj0k,"Job situation in QC, Canada","What’s the DE situation in Montreal at the moment, or sentiment for future? I know they’re pretty strict on what an engineer means. How do DE positions advertise to circumvent this? Any experience would be appreciated ",2024-03-11 19:16:08
1bc78mk,Opinions about this course ,"What do you guys think about this course?
I've been doing it, but I would like to know what's your opinion? Have you heard about it?

",2024-03-11 16:25:20
1bbiqwk,Extract CV infos automatically with NLP,"Hi everyone,

I want to exctract infos from CV such as first name, second name, gender, ... and fill them into an excel table automatically, the scenario is having many CVs inside a folder, and then have an excel table where each row represent a candidate data. the CVs can be either in English or French, pdf or .docx formats

Does anyone have an idea how to achieve that ? or have previously done that before ? I saw there is an AWS service that can help ? what are your thoughts ?

Thanks",2024-03-10 19:31:41
1bax86r,Databricks Delta tables and YOLO computer vision,"Hi all,

I would really appreciate if a fellow data engineer could help me out.

I have been doing training in Databricks, as we plan to use it at work for our ML tasks.

One thing we use is YOLO for object detection. I’ve managed to run YOLO by loading data from the blob storage, but I’ve seen that the best way to do deep learning tasks in Databricks is to train your ML models on Delta Live Tables.

I currently have my training dataset as a Delta table, and I was wondering if anyone has managed to train computer vision models on Delta tables.

I’ve read the documentations and have seen repos such as petastorm that try to implement training on delta tables, but I can’t for the life of me understand how to actually run yolo this way, especially since YOLO uses yaml for config.

Thank in advance for your help! 😇",2024-03-10 00:46:35
1b9kfaw,The Hunt for the Missing Data Type,N/A,2024-03-08 09:48:26
1b91k2e,Onelake as DWH replacement ,"Hey everybody,

So I recently got more into the Microsoft stack and before that I had a ""best practice outlook"" on  setting up small/medium sized DWHs:
- onprem/cheap cloud DB
- EL: custom / Free tools like airbyte, pentaho.. 
- T: dbt

On my new job we only use azure for everything. Namely azure functions/data flows for ELT and a datalake blobstorage as a DWH.
The datalake sources powerbi, which is possible since onelake?  I am pretty new on the onelake/fabric stuff and asking myself if this is ""best practice"" for small/medium sized DWHs?

I actually like nothing about this stack but that might be due to me not knowing/understanding it's capabilities?",2024-03-07 18:17:59
1b90jmh,Databricks - Delta Live Tables Bronze to Silver,"Hey all!  


I've been around the forum for a while now but haven't ever posted anything. I've read through quite a few of the old posts on Databricks Delta Live Tables (DLT) and haven't quite gotten the answers that I'm after.   


Context: Our team is working on standing up a data lakehouse to better support analytics. We've made the decision to go with DLT to make use of the documentation it provides as well as it making SCD relatively simple. We are not able to use Unity Catalog but we're moving forward with DLT pointed at the hive metastore. I've written an initial pipeline that generates six dims and four fact tables. All of these tables are completely refreshed daily right now. I had done this to get something stood up in the short term while I continued learning DLT and to support some short-term data science efforts on our team.  


I now need to loop back and adjust my process to use SCD and ensure that I don't lose data when the source starts losing data (some of the data will disappear from my source later this year).  


In light of this I have a few questions I was really hoping the community may be able to help me with.  


1. My bronze tables at the moment are simple overwrites. We are considering making these append-only tables to support incremental and full loads as well as being able to track what data we actually processed. What do folks typically do here? Files are loaded from ADF and then processed into Bronze using Databricks.
2. I made an initial attempt at adding SCD-2 support to one of my dims in silver. It worked on the first run but the run on the following day errored out indicating that it detected an update or delete to one or more rows in the source table. My source table is a full overwrite and per examples I had seen online I was reading it with readStream. I switched it to a simple read and got an error stating that the view was not a streaming view. Does DLT apply\_changes require a streaming table for it's source? The documentation doesn't ever really indicate one way or the other but does talk about needing CDC data.
3. The final thing I haven't been able to fully wrap my head around was how fact tables are intended to be handled in DLT. I assume all dims should be SCD 1 or 2 but with regards to the fact table, am I intended to write a merge statement to continuously upsert records into the fact table (this way when my source stops sending me some data, I don't remove it with a full reload)?

Any help is greatly appreciated on this! Happy to clarify anything if necessary!",2024-03-07 17:38:23
1b8vlma,Google Cloud Storage Subscription,"Came across this service last week and thought what a great way to no longer having to setup an extra job to ingest messages from PubSub to GCS. It is supposed to support batching of messages (e.g. time window or file size). Anyone been able to get this to work as it's writing every single message to a file for me no matter what I configure it with?
",2024-03-07 14:11:13
1b8uth2,"Snowplow community/Cloud - events forwarding, alternatives?","Hey all, does anyone have experience with the snowplow CDP ([https://snowplow.io](https://snowplow.io)), either community or cloud edition? My biggest question is how hard is event forwarding to e.g. Braze, Facebook Pixel, etc., when hosting the community version on own infrastructure (azure)?  


Any advice/learnings, or alternative solutions would be much appreciated! Thanks!",2024-03-07 13:36:20
1b8spuz,New Kid on the block?," 

Hey What’s up Reddit - this is a product launch announcement from me (Hugo) for Orchestra. Been a fan a while of the Reddit community and r/dataengineering even as a lurker so I’ll keep it brief

**What problem are we solving?**

Data Teams experience huge pains trying to create a single place to monitor the value of their data initiatives. As a result, we struggle to see what’s going on, and to be seen : communicating the value to business stakeholders is a daily struggle

**What technical challenges are there?**

We’re spending too much time building pipelines and infrastructure. For example, managing Kubernetes infrastructure for orchestration tools is overkill, data pipeline metadata is everywhere and can’t be aggregated, there is no nice UI for aggregating this metadata at a data product level. The list goes on..

**How do we do it?**

* Version-controlled, GUI-driven Data Pipeline Builder. Expand existing workflow orchestration tool specs to be conscious of data assets, data quality testing, and collect metadata, plus:
* Awesome UI for visualising all of this, plus:
* Custom alerting so you can stop using datadog / the thing you hacked together
* How does this compare to existing things?

The existing way to do this would probably be: Platform engineer + Workflow orchestration tool + Observability tool + custom alerting (and you still can’t really see how any metadata aggregated by data product). Not an easy undertaking for any team of any size

**The best bit**

We’re releasing a Free Tier so you can get all this for free as long as you don’t try to run 1 million pipelines and bankrupt us. Check it out here [https://app.getorchestra.io](https://app.getorchestra.io/) and on producthunt [https://www.producthunt.com/posts/orchestra-data-platform](https://www.producthunt.com/posts/orchestra-data-platform)

Questions concerns? Let me know in the comments!",2024-03-07 11:48:16
1b8py81,Webinar: 5 Styles of Modern Data Integration,"I thought I would share this webinar. Data Virtuality is hosting a webinar on April 16 with The Eckerson Group.

Some of the key topics that will be covered include:

* The top five integration styles: data virtualization, ETL, CDC, ELT, and streaming
* Understand the pros and cons of each style and their ideal use cases
* Strategies to bridge the gap between business demands and IT realities
* How these integration styles enable architectural approaches like the data mesh, fabric, and lakehouse

Reserve your spot here - [https://go.datavirtuality.com/five-styles-of-modern-data-integration](https://go.datavirtuality.com/five-styles-of-modern-data-integration)",2024-03-07 08:51:34
1b8nf7g,Native Unit Testing with dbt,N/A,2024-03-07 06:15:59
1b8l2z1,dbt - Generating models from a DW,"Does anyone know if there is a way to clone your current DW into dbt models? 
I have years of regular SQL saved that I want to convert to DBT models for the docs.
This is my biggest barrier to adopting dbt so I'm sure there must be a package or Python script out there but I haven't found a way yet.
Needs to be either straight from the database or from saved SQL in git.
Would be cool if it grabs tableau dashboard sources from a server and saves them as exposures too.",2024-03-07 04:12:01
1b88no5,Help to find the best way,"I'm trying to find a solution to determine the best way to deliver data to my Curated/Data Warehouse layer. Context:

* There are thousands of pages(html) saved as text in the raw layer.
* The daily ingestion isn't significant, around 10-15 files per day.
* Each txt file is a page 

I have conducted some studies, and the information I need to extract from these HTML files generate a nested json with 4 levels.  
Ship(only 1) > Owners(many) > Containers(many) > orders(many)

My question is... How do you guys think would be the best approach to deliver this information at the end of the pipeline?

raw(html) > refined(parquet) > stage table > facts (ship, containers, orders) > analysis OR raw(html) > refined (parquet - ship, containers, orders) > facts (ship, containers, orders) > analysis OR raw(html) > refined (parquet) > curated(OBT) > analysis",2024-03-06 19:25:25
1b839io,Parsing dbt's manifest.json for DWH Logging?,"hi r/dataengineering

I'm looking for advice on parsing dbt's manifest.json and logging the extracted metadata into our data warehouse for monitoring purposes. The use case is that we'd build some kind power BI report to visualize all our models and tests. we've already done that with run\_results.json which we parse via  
an run-on-end hook with macros. We need manifest.json because it contains tags which we'd like to vusalize in our monitoring dashboard. We attempted to use the same logic with manifest (use hooks to call macros), but from my understanding, you cannot directly parse the manifest within 'on-run-end' hooks. I know that some of the information i'm looking for will already be visualized within the dbt docs, but not under the format we would like it to be. 

Has anyone done something similar with parsing the manifest? How did you approach it? I'm thinking of some kind of Python script to just parse the json into a lakehouse/warehouse and then join it with the run\_results via the invocation ID + result\_id? Using python scripts does feel anti dbt tough..

Context: we're calling dbt via bash commands in fabric notebooks.

",2024-03-06 15:59:20
1b81qqc,Best practice how to create and maintain custom roles in Azure?,"Hi y'all,  


Is there a best practice how to create custom roles in azure? Or better how to choose from thousands of grants and making sure the custom role contains exactly what it needs - not to much and not to less?  
Or is everyone just reusing/combining built-in roles?

Details and background:

We, a smol team of data engineers, are doing our first steps in setting up our azure platform, currently.  
We created a pipeline with Azure DevOps and can create resources via Terraform.  


So far so good, but now we realized we want to create multiple resource groups in the future. And setting up devops service principle on a higher level -> on the management group level is not working because Terraform has n open enhancement that stops us, see here:   
[https://github.com/microsoft/azure-pipelines-terraform/issues/81](https://github.com/microsoft/azure-pipelines-terraform/issues/81)

Now we are thinking of implementing the mentioned workaround in Git Hub, for which we need to create a custom role. And that leads me back to my opening question.  


Furthermore, we are concerned that creating these custom roles might cause n endless debugging loop in the future as MS loves to change and rename things, or add new stuff. And from one day to another your application/pipeline is offline and you start researching which role needs adaption or addition.",2024-03-06 14:57:40
1b7ze88,What would make your life easier?,"Testing around some ideas for a project, happy to get your feedback

[View Poll](https://www.reddit.com/poll/1b7ze88)",2024-03-06 13:14:27
1b7x543,Data Engineering Solution,"So, I'm working on my thesis, and the problem is integrating diverse Big data (mainly JSON) from multiple providers effectively. My goal is to develop and process a versatile ETL Framework that integrates and handles diverse data formats. I have a Postgres database with the ideal output format that the input file has to be transformed in. I have a table with global rules, mainly regex expressions, to apply to every file. I have a providers table with the names of the providers that I receive, and the last table is the providers\_rules that have a provider\_id associated, the id of the attribute of the input file, and the id of the attribute of the output that corresponds to a match.   
Example: I want to tell my framework that the [`event.4314985.id`](https://event.4314985.id) from input\_file corresponds to [`data.id`](https://data.id) in the output file.   
The problem is, when the attributes aren't in the same depth, how can I do this matching?   
PS: I'm using Python and Streamlit (for now)",2024-03-06 11:11:27
1b7oap5,Team meeting demo topics,"On your data team, if you were to implement a weekly meeting to demo new tools or techniques, what are some of the things you would want to see?",2024-03-06 02:32:58
1b7l586,Help me transform my company data stack,"I've worked at a small startup for a couple of years as a data analyst, though I have a background in data science and software engineering. I want to completely transform the data stack at my company as the current system is haphazard and causes a lot of overhead and operational issues. However, I often feel overwhelmed and don't feel like I have enough knowledge or experience to go about this properly (I am the only data analyst, and have no similar experience in such a digital transformation). 

Let me break it down into a few details.

My company is an urban research consultancy. They collect data in the form of surveys which contain respondent demographic information, and scores given by respondents to aspects of a neighbourhood. When data collection for a project is complete, a CSV file is extracted from the survey platform, the data is cleaned (more on that later), and some R scripts are run on the clean data to produce a JSON file with all the insights needed to make a report. 

For some of these JSON files (i.e for type 1 reports):

This JSON file is then paired with an existing InDesign template, and some JavaScript (Adobe ExtendScript) runs to produce a report with the data integrated into it. 

For some other of these JSON files (i.e for type 2 reports):

The JSON file is loaded into a web platform built using VueJS. This then creates an online report that is interactive, supports drilling down by several dimensions and looks quite spiffy if i might say so. The catch? All values are precalculated into the JSON - the web based report is completely static and could run offline. No calls to any API are made in the report. This results in minified JSON files being upto 21Mb's each, not to mention the whole thing being rather inflexible and tightly coupled. 

At the end of each month, we take data collected during the month and append it to an Excel file which contains all our data (Let's call that AllData). Sometimes we want to run bespoke reports on data from AllData on some XYZ dimensions with ABC filters. I then have to manually go, snip out that data from the Excel file and paste it as a CSV before writing my R code on it. I know there has to be a better way.

I promised more on the data cleaning. It consists of some or all of the following:

1. Removing and/or renaming columns until the schema matches that of the AllData - this is because our scripts recognise the AllData schema, and because the ultimate destination of this data will be the AllData file. This part is easy.
2. Flagging and/or removing dirty records. This part is somewhat tricky. We eyeball responses that we feel might not be honest or well thought out. These could be where all/most scores are similar, some columns contradict each other etc. 
3. Flagging and/or removing spam. This is the trickiest. Spam records may look like legit responses but often we recognise them as coming in batches, so I can recognise and eliminate a batch of spam records with some shared characteristic, rather than 1 spam record by itself, if that makes sense. Sometimes individual spam records do give themselves away with characteristics like email address (optional) and name (also optional) being completely and laughably different, or open ended answers (also optional) being extremely vague and unrelated to the question.

What I want to ultimately pull off is some kind of RDBMS in to which our responses are collected. I also want to build an ETL/ELT process that runs monthly - or on trigger, performs all data cleaning and builds some kind of a data warehouse from which all our reports can be based - whether from InDesign, or from our SaaS - which would be fetching data in real time via a RESTful interface.

I think i've just blurbed out a whole bunch here - will add more details later. Does anyone have any thoughts that come to mind?",2024-03-06 00:09:26
1b7k784,Anyone transitioned from Project Management to DE successfully?,"Hi folks,

My current position is project coordinator, which mainly focuses on administrative tasks, such as reporting, budgets, forecasts etc.

How smooth would it be, if I make a transition into DE, within the same organization? Do these two have some overlaps? Any advice will be appreciated. Thanks in advance.",2024-03-05 23:28:40
1b7exqa,Running serverless ML with Knative And Ceph,N/A,2024-03-05 20:01:35
1b7ew2v,Databricks Data Engineer Associate,Curious how difficult it is to pass this without any databricks experience at all? How many hours would be needed to put in to pass?,2024-03-05 20:00:03
1b79vy6,Primer materials for Data Governance and Observability?,"I have an upcoming job meeting (we can't say inter-view in post body?) where data governance and observability were specifically called out as topics for discussion. 

I haven't gotten much exposure to the concepts in my career so far. I was hoping this sub could point me toward articles, videos, courses, etc. that you've found helpful on the topics. Thanks!",2024-03-05 16:44:43
1b77l4q,Differences in setting Primary Key and Foreign Key constraints in Redshift using CREATE TABLE vs ALTER TABLE,"I'm interested in setting Primary Key and Foreign Key constraints in Redshift as I've read it can help the Query Optimizer create more efficient queries. 

I've noticed that when setting these constraints in the table creation they show up in the INFORMATION\_SCHEMA.TABLE\_CONSTRAINTS table, but when setting them with an ALTER command instead, they do not show up there. However, they both show up in PG\_CONSTRAINT table. 

My intention is to eventually set these in DBT, which also only shows up in the PG\_CONSTRAINT table.

Being somewhat new to optimizing queries on Redshift, I'm curious if anyone knows:  
1. Does this difference matter to the query optimizer (ie. can it only optimize queries if it has the constraint info in the INFORMATION\_SCHEMA.TABLE\_CONSTRAINTS table)  
2. Has anyone seen noticeable performance gains using these constraints vs just focusing on DIST and SORT keys?  
3. Do you have any other general advise on optimizing Redshift Queries with DBT in mind? ",2024-03-05 15:14:43
1b6smez,Needing advice for replication software,"The business I work for needs to replicate a database server (MySQL) (in our AWS infrastructure) to another DB server (MySQL) in a customer's AWS infrastructure over the internet. Originally they wanted log shipping however that's not feasible given that the machines are not locally connected.

&#x200B;

The business has decided to look into purchasing a solution as Microsoft makes this extremely difficult given the gap between infrastructures. 

&#x200B;

Was looking into fivetran/airbyte however I am looking for other solutions/recommendations as I am new to this myself.",2024-03-05 01:31:19
1b6niyw,Complex CDC ETL from RDS MySQL to Aurora Postgres," Greetings all. Experienced data engineer, but AWS Novice and looking for best practices (if any)

I have a MySQL RDS db, which I want to create a real- or near realtime CDC into a new Aurora PostgreSQL db. This is a database replatform, and both databases will be in use for some time. (The old MySQL db will still be having data written to it, but over time, we'll be migrating code over to the Postgres db.) In addition, the Postgres db is not even close to the same schema as the MySQL db, and significant ETL will be needed.

It doesn't look like anything in AWS really accomplishes both a realtime data migration *and* complex ETL. What are tried and tested third-party tools that can accomplish this?

Many thanks in advance.",2024-03-04 22:00:19
1b6gq4k,Setup CDC For changing source replication slot,"Hello Guys, I am trying to setup cdc for our warehouse.  
I have set it up on dev but the challenge on stage is that the stage db i.e source gets refreshed bi monthly that means the db is restored from prod anonimysed db snapshot.  


The challenge that i see is, every time that refresh will happen we will loose those publication and replication slot. We can create both the things at time of restore but the LSN would be different and might cause some problems.   


The subscriber at the other side is a debezium connector on top of kafka.

Please don't suggest any architecture changes. Can't implement those.",2024-03-04 17:28:41
1b6cazx,"What do we mean by ""framework"" in the context of Hadoop being a big data open source framework?","The term ""framework"" is thrown around a lot, and I would like to understand, what do we mean by that.",2024-03-04 14:32:33
1b6929o,PostgreSQL: Protect tables against accidental deletion,"🔥 New Article @ Tela Network

PostgreSQL: Protect tables against accidental deletion

[https://telablog.com/postgresql-protect-tables-against-accidental-deletion](https://telablog.com/postgresql-protect-tables-against-accidental-deletion)

👉 There is a risk of accidentally deleting an important table whenever we interact with a PostgreSQL server.

👉 We want to add a protective guardrail that prevents accidental deletion.

👉 We create an event trigger that fires when the DROP TABLE command is entered.",2024-03-04 11:51:40
1b67aqj,Syncing One database to another using python script ,"Hi Guys,

I’m currently trying to copy the table data from one database to another.

I have successfully done that, where I’m getting problem is that when I’m executing the script again if any values change in the initial database it should reflect in the new database.

Please help with the updated code.

Current code:

def insert_data_into_test_table(client, rows):
    try:
        for row in rows:
            row_values = [str(value).replace(""'"", ""''"") for value in row]
            name = row_values[0]
            insert_query = f""INSERT INTO Test1 SELECT '{row_values[0]}', '{row_values[1]}', '{row_values[2]}' WHERE NOT EXISTS (SELECT 1 FROM Test1 WHERE Name = '{name}') LIMIT 1""
            client.command(insert_query)
 
        print(""Data inserted successfully."")
    except Exception as e:
        print(f""Error inserting data: {e}"")",2024-03-04 10:01:23
1b61s6v,AWS Glue: Column-wise Append Parquet Files?,"I'm using Glue and running lots of pandas operations on a dataframe with a few million rows and around 100 columns. When I do this, I get the error:

    AWS Glue: Command failed with exit code 10

This happens because my dataframe is too big. If I drop a lot of the columns, and only perform the transformations on the necessary columns, the error goes away.

The issue is that if I want to join the other columns back in at the very last step, the pandas merge operation takes so much memory that I run into the same error again.

The final output file is a parquet that lands in S3. I'm wondering if it's possible to do some type of column-wise appending to the parquet file so I don't have to do such a memory-intensive merge in pandas which causes the glue job to fail. I'm also open to any other possible solution. Any ideas on what I can do?

I should mention that I can solve the issue by increasing the worker type from G.1X to G.2X, but this is not ideal from a cost perspective.",2024-03-04 04:23:03
1b5ee2p,May I ask for suggestions for the data pipeline?,"I'm working on a project for stock analysis, attempt to get 10 years of data from stock historical data source to store in data warehouse, then update the latest daily stock price after system go-live.

I'm using Postgres as database which I'm familiar with, the overall data ingestion pipeline looks like this:  
`data source -> raw_table (Postgres)-> staging table (Postgres)`  


The significance of raw table is that when I gather data from stock data source, it may get duplicate data that have already captured before. 

I plan to gather data to a raw\_table, then insert into staging table which removed all duplicates

&#x200B;

The problem here is that there are too many rows in raw\_table and whenever I want to do the insertion to staging table, I run this:

    insert into stagin_Table
    select * from stock_raw_table sr
    where sr.stock_code not in (select stock_code from stock_raw_table);

where stock\_code is the unique key for single daily stock price.

The above insertion cost too much time, and I'm wondering if my pipeline is inefficient.

Any advice for this scenario?

Thanks for any advice!",2024-03-03 10:46:12
1b5bfh9,Combine Spark's output files?,"Hi team,

I want to let my Spark partitions write to CSV or JSON as usual. As we know, this means that each partition writes its own file.

I've got the challenge of writing a statement for each customer. That means, 1 customer gets 1 file with their info in it.

I want to avoid bringing all the data to the driver node and getting python to write the file, but I also have to manage these multiple partition files. Even if I \`coalesce\` the name is not correct. For this case, let's pretend the data is so large that coalescing would be a bad idea anyway.

Question is: what is the best practice for combining + renaming these partition files?

We have airflow in the works, so perhaps a step in a DAG? Or maybe a CRON to catch the files and do the work?

Any ideas? Please and thank you.",2024-03-03 07:31:47
1b4w7pv,Need help with Spark and Mongo,"Hi all,

I am trying to read from Mongo using spark mongo connector trying to load 10M rows

Anyone know how I can ignore rows if there is datatype mismatch using my predefined schema. There are some date field which are timestamp but someone inserted String type. There are similar issues with other field. 

I want to know how we can handle datatype mismatch, The DROPMALFORMED functionality is not available in Spark Mongo Connector.

Any advice or pointer will be helpful.",2024-03-02 19:18:29
1b4k4hr,"SAP ECC, S/4 HANA and ARIBA",Can anyone share their experience and tech stacks used dealing with these three above. Automation experience will be a plus! Really interested to know what’s the workaround ppl are using to streamline the ETL process to BI reporting / dashboards.,2024-03-02 09:11:31
1b4gdam,Recs for materials to help with cloud migration resource planning,"Does anyone have any recommendations for materials, e.g., blog/textbook, to help with resource planning and design/architecture for cloud migration?",2024-03-02 05:17:00
1b4e5fn,Do relational databases offer a way to require serial order from manually entered data?,"Better yet, what am I actually looking for here?

I’m designing a system that handles schedules in a novel way that, based on our review, isn’t offered elsewhere. Honestly the magic boils down to just a handful of presentation choices and feature integrations.

I’m using a relational database and the data, for the most part, fits the model well. One thing that stands out is tasks though… The tasks have a serial order to them meaning that one gets started only after the prior finishes.

I need to support a query that, given a task and a location, the response is the number of incomplete upstream tasks. This is done by getting all incomplete tasks with an expected start date of less than the given task, within the given location, and returning the count of records.

However, tasks are global… Every single space has the same tasks. Therefore, the serial order for tasks is also global. It’s wasted effort to slice to tasks within the given location, recalculating which are upstream every time as based on a date column. 

I could just artificially maintain a serial order column within a tasks table. Values would just count upward, and I as the user would just insert them this way. This is an implicitly maintained rule that’s critical for the system’s functionality though… I think not.

Then there are trigger functions which likely can be used very effectively. However, they carry a negative connotation due to undesired side effects. It can be difficult to reason about the synergic behavior of relational and functional capabilities at large scale, and humans bite the bullet here eventually. I’d like to avoid triggers if possible.

Are there data models that support this kind of precedence between entities more naturally? 

Are there data models that blur the line between relational and what I’m looking for, potentially solving both needs at the same time?",2024-03-02 03:19:10
1brkvfn,Requesting Feedback,"Hi All,  


A little over a year ago, I initiated a project aimed at creating a Power BI report and dashboard for our Operations (OPS) team to monitor and analyze drive-thru performance. This tool has since become a daily resource for both our operations and executive teams. As I revisit the entire project with an eye towards enhancement, I'm particularly interested in incorporating Machine Learning (ML) and Artificial Intelligence (AI) capabilities. The current system architecture involves a data pipeline within Azure Data Factory, which extracts data from various APIs in JSON format. This data is initially transformed, then stored in a SQL Database, and finally, it's pulled into Power BI. Here, Power Query facilitates further data transformation before it's visualized and analyzed in our reports and dashboards. I'm now looking for feedback that could guide the integration of ML and AI into this workflow, aiming to elevate our data analysis capabilities with forecasting etc.   
Bear in mind I'm no expert and this was my first project in this area of IT.",2024-03-30 15:44:53
1brgv7m,Do you build rags too?,"Hey folks, do any of you extract data and load it to storage or to places like vector stores or lancedb for llm use? Or any of you working for really rich companies that can afford to train llms?

I'm wondering about the state of the role (how much it goes in that direction from DE) and what kind of applications you are working on.

To contribute to the discussion myself, I was at data council and after talking to people smarter than me/top of their fields there seems to be quite some overlap, moreso than there was between ml eng and classic DE",2024-03-30 12:36:12
1brc320,Looking for early adopters for disruptive next gen streaming solution,"I’ve built custom C++ Lambda runtime for data ingestion with DuckDB and it runs efficiently with smallest arm64 Lambda. I’m very excited about the findings. Looking for early adopters and design partners for ubiquitios data streaming! 
No strings attached and all free. I’m thinking of adding Kafka support with kafkajs too, and would love any feedback and start collaborations.",2024-03-30 07:28:13
1bquanx,Looking for Advice: File Types and Sizes Specifically Working with Azure Data Factory/Data Lake Storage Gen2,"Hopefully this is allowed, I am an Application Analyst that is looking for advice in trying to develop data solutions for my company. I have a unique opportunity in that we don't have anyone else at the company that can do this, so I have taken a bunch of courses on Data Engineering hoping to leverage myself as more of a data specialist, and perhaps a data engineer (I don't want to get too ahead of myself, though.) 

For background, my company sells e-learning courses and in-person training events in the mental health industry and I was the main person to configure our Learning Management System so that we could sell these programs online. With the LMS, we now want to extract the data and store it in a database so that it can be accessed mainly for reporting, but it will also need to support our integrations with Salesforce and our accounting software in the future. 

My boss has given me the opportunity to experiment with Azure Data Factory to ingest the data from the LMS REST APIs and store it in a Data Lake Storage Gen2 account, to then be accessed for reporting. I have read that Synapse is likely needed to properly serve up the data from the gen2 storage account to something like Power BI, but I feel like I am getting a few concepts mixed up in regards to exactly how to sink and then serve up the blobs in the storage account in the most efficient way. I do have enough knowledge to build a relational database in Postgres, but it just seems like a lot more work with not much gain in developing all the tables/schemas, as we don't have complex data. Although transactions occur on the LMS, we are not trying to extract these in real time or anything like that, just get the basic info for now on what people have purchased. 

I have read a lot and experimented quite a bit with Azure Data Factory, but I feel like I just need to run this by some humans to see if I am even close to the right track. ChatGPT/Bing isn't much help either as these things seem to be changing so quickly. 

My main question is:

\- When ingesting the data from the REST API, I have to loop through pages (using Until in Data Factory) which results in multiple files. Right now, I am storing these files as individual parquet file blobs in a folder on the gen2 storage account. My first inclination is to consolidate these files into one large parquet file, however, data factory seems to want to do the opposite and is limiting my size of file to only 400 rows (17 columns.) This is where I struggle and feel like I am missing something because a lot of the learning I took says that you should have one large file, close to a GB for efficiency. Data Factory, however, is limiting my file size to KBs. Am I missing something or does the one large file really mean a folder of individual blobs? I also know that partitioning plays a factor but not clear on how. When accessing these individual blobs for reporting, are they queried as one? Conversely, is Data Factory perhaps restricting file size unintentionally through a configuration that I can change?

I am a total noob at this and I don't have a CS degree, so any help would be greatly appreciated and apologize if this is a really stupid question.

One thing to add, is that parquet files seem to be the preferred way to store blob files but any advice to the contrary would also be appreciated.",2024-03-29 17:01:12
1bqu5zm,On request ML endpoint?,"Hey, I have successfully implemented a ML endpoint with SageMaker and connected it to Lambda and API Gateway on AWS. It worked perfectly - I was able to send request and receive classification back. However, the next day I found out that this approach is extremely pricey for my needs due to the fact that the ML endpoint needs to run 24/7 in order to provide this kind of service.

So my question is - is there an alternative where I pay on-request basis instead of for the time the endpoint is running? 

Let’s ignore any business needs such as scalability and whatnot. I am only playing around with it for now so I am the only person sending requests. Therefore, I’d prefer on-request pricing. But I haven’t been able to find any suitable solutions.

As for the description of my project. Image multi-label classification case, I am sending binary image data and requesting classification back.

Sorry if I was not technical enough or if my question is too dumb. I am fairly new to this.

Thanks!",2024-03-29 16:55:53
1bqrjca,Data infrastructure for a small company (Azure?),"Hi, I’m starting a job at a small company (around 40 ppl) and I’ll have the responsibility to create and maintain a data infrastructure (for ETL purposes), find market insights, and create dashboards (with scheduled refreshes). Since I will use Power BI for reporting, I was thinking of using Azure for the infra (I’ve read Data Factory is good for E-L), but I’m pretty new to creating data infrastructures from zero, so any advise is very appreciated. 
It’s worth to mention that the company already has some processes and reports they run in Excel and they’re planning to double the quantity of employees by the end of the year so scalability is a must.

My main questions are:

- Is Azure SQL database + Azure Data Factory (just for E-L) a good approach? 
- What is recommended for the “T”?
- How much time is this infrastructure gonna take to create? What is an approximate monthly cost? (For planning purposes)
- Is this approach gonna be enough for me to develop data pipelines with schedules executions, reports in PBI with scheduled refreshes, and a platform to query the database in order to satisfy specific analysis the business may ask?
- What do I specifically need to do to create this infrastructure? Maybe there’s some video or course I can use as a reference?

Thanks in advance! 
",2024-03-29 15:06:30
1bqmrmg,Business Process Automation,"I offer automation services to companies using tools like Zapier, [Make.com](https://Make.com) (formerly Integromat) and n8n. While most people would refer to these as automation tools, I think from a DE perspective they are kind of just low code ETL tools- they move data and automate processes in various cloud platforms.

I'm looking to build more complex automations for various business processes.

Do you think gaining some core DE knowledge could be utilized for business process automation? I think it certainly will, but I just wanted to run this by people here as my DE knowledge is very limited.",2024-03-29 11:18:54
1bqlknr,Iam currently trying to setup a realtime pipeline in which i will read from kafka and write into iceberg table using spark structure streaming. Currently iam using Hudi but iam not able to see much documentation on spark structure streaming using iceberg. Anyone who is using iceberg with spark?,Anyone who can help or currently using iceberg with spark streaming i have some general config related doubt.,2024-03-29 10:05:04
1bqjb2u,Aggregating hdfs data using spark streaming ,"Hi

I have 5TB of ORC files on hdfs. I want to calculate multiple aggregations on the entire data set and then write it back to another hdfs location. 

Is it possible to do this using spark streaming i.e read hdfs data using spark streaming, aggregating it then writing it back to hdfs?",2024-03-29 07:30:35
1bq4g6z,Altair SLC Hub -NAS drives access,"Setup: 
I've Altair software installed on my server. I'm able to login to it using my domain credentials and I can access the shared NAS drives in my programs. This is working fine.  

Now I've Altair SLC Hub also installed on the same server and synced with LDAP authentication. This is for ETL pipelines. Here also I can login with domain credentials. But the same programs are unable to access NAS drives when automated through it.  

I couldn't find any Google results. This is a new tool. If anyone has ever faced such an issue with this software or a similar ETL automation tool. Kindly help.",2024-03-28 19:34:50
1bq3izh,If anyone needs this..,N/A,2024-03-28 18:56:49
1bpx9lx,TUTORIAL: From Postgres to Dashboards with Dremio and Apache Iceberg,N/A,2024-03-28 14:42:06
1bpduw3,"Index Roundup- Search at DoorDash, Small language models at Swiggy and more","Hey! We're introducing a roundup of engineering blogs, research and talks for engineers in search and AI. The first edition of the newsletter can be found here: [https://index.rockset.com/p/index-roundup](https://index.rockset.com/p/index-roundup)

We're covering the following search and AI news:  
\- DoorDash's new search engine: DoorDash's move from Elasticsearch to an in-house search engine using Apache Lucene. Reasons for the new architecture include challenges with the document-replication model and modeling complex relationships between items and stores.  
\- Using small language models to improve search relevance at Swiggy: How Swiggy adopted a two-stage fine-tuning approach to matching search terms to local dishes from restaurants in India.  
\- RAFT for adding domain-specific knowledge to language models: A new approach to adding domain-specific knowledge to language models for improved relevance.  
\- Evaluating GenAI products at LinkedIn: Real-life case studies from LinkedIn on how GenAI products are evaluating using human reviews, in-product feedback and product usage metrics.  


Why a ""new"" newsletter?  
The availability and accessibility of AI models introduces new ways and means of building search and personalization systems. The goal of the newsletter is to aggregate and explain how new technologies are being adopted, share best practices from engineers and discuss design tradeoffs. The newsletter is technology agnostic featuring open-source tools, in-house infra and serverless technologies. We want this newsletter to be rooted in the community and would welcome feedback, articles to share and best practices for any engineer jumping into the search space.",2024-03-27 21:39:36
1bpaz4b,Open Data Lakehouse Evolution: Powering the Future with YugabyteDB & Apache Hudi,[https://www.yugabyte.com/blog/apache-hudi-data-lakehouse-integration/](https://www.yugabyte.com/blog/apache-hudi-data-lakehouse-integration/),2024-03-27 19:44:16
1bp7eft,Do you have to decompress Avro before using it?,"I haven't used Avro before and considering it for something. I read that Avro does not have built-in compression like Parquet, so we have to first decompress Avro before using it. Is that right? Does it mean that Avro negatively affects performence because of cost of decompression before loading it?",2024-03-27 17:20:14
1bp4iv6,Feel Like Learning is Limited / Pigeon Holed,"I am asking others how they feel, if they experienced similar, if their first major job kinda feels like a bottleneck at times. My first job big job is a junior DE now hitting where I feel very comfortable pushing myself to and have achieved more of a mid-level dev role.

I have been excited to try and push myself, and see where there are areas that could use work (code needs rewriting, better documentation, procedure issues) and have been put on other projects. But I don't feel I am gaining any experience with certain tech I want to gain actual industry experience with (Jenkins/Airflow/Kubrenetes).

While I use them, the procedures and servers are kinda already set up and not needed to be worked on as its a large corp with established roles / code base. I don't mind the pigeon hole to an extent, but feel I can easily get cut out from some of this stuff that feels important to know beyond ""playground"" level.

What would others do or feel of this?",2024-03-27 15:22:44
1bp39mt,How does your team indicate data completeness in object storage?,"In scenarios involving data writes to object storage, without utilizing formats like Iceberg, Delta Lake, or Hudi—which offer transactional support—how does your team signal to downstream processes that the data load is complete? Do you use a commit/manifest file, or perhaps leverage streaming ingestion directly from the bucket? Mostly curious approaches the community has tried and went well or didn't about the approach.",2024-03-27 14:30:04
1box1jy,Automating the dev environment on Azure Databricks working with a dbt project,"Hey, I'm a former software engineer and fresh data engineer, and I'm kind of losing my mind when I see all the manual steps we take in DE. I knew the DE domain was less mature than SE but I didn't expect such a big fallback. My new team is pretty ""young"" technically, I'm the only technical person, so I thought it explained it all. But the more I search for answers on the Internet, the less confident I am that there are viable solutions out there.

We don't have a test environment at all, but we do manual tests using Databricks (eg checking that the number of rows doesn't explode after a change in the relationships). What really annoys me the most is having to create all our tables in the Databricks environment manually.

So basically, I would type my dbt run command, see it complains about one missing table, go create it in my notebook with a simple ""create table foo as select from source.foo"", then rinse and repeat for 15/20 times (or I'd crawl our files to find all the needed table but I find it even more annoying).

Isn't there a simpler, automated solution for this?",2024-03-27 08:39:46
1bosbx8,"Looking for software/app for data entry, organization","I am looking for software that can help me with collecting and organizing data for research. 

The data I am collecting can be grouped in 3. Patient information, implant information, follow up information. I would like all the implant and followup information linked to the patient. The issue I am having with doing this in excel, is that I have one patient, but I could have multiple implants with multiple descriptive values that I would like to sort/filter/group by as well as multiple follow up dates with their own multiple descriptive values that I would like to be able to sort/filter by. So for example, John Doe might come in for one implant. The location, implant type, date of placement, timing, augmentation of that implant would be recorded for that implant. He might come in a year later to get another implant and I would need the same information on that implant. Then the patient might come back a year later for a check up, and I would need to get information regarding that appointment like the time difference between the placement of implant 1 and implant 2, the xrays that may or may not have been taken of implant 1 or 2 and other data pertaining to those 2 implants. I would like to have the implants and the follow up visits linked to that specific patient. I would also like to be able to filter/sort by the information in each category, patient, implant or follow up, like gender or the amount of time that has elapsed since the placement of the implant. Is there a program that makes this easy or am I going to have to use excel and have 3 worksheets where I have patient ID and information in worksheet 1, multiple patient IDs to link multiple implants in worksheet 2 and multiple patient IDs  to link multiple visits?

So in a list format, it would be like this:

Patient Name > Age, Gender

Implant #4 > date of placement, surface type, etc

Implant #5 > date, surface, etc

Follow up #1 on date > Implant #4 > time elapsed since initial placement, etc, etc

\> Implant #5 > time elapsed since initial placement, etc, etc

Follow up #2 on date > Implant #4 > etc

Follow up #3 on date > Implant #5 > etc

The implant(s) would be linked to each unique patient. The follow up visit(s) linked to each unique patient. I would be able to search for ""only 28 weeks or more"" or surface A. Any suggestions on how to get this done?",2024-03-27 03:39:26
1bocu8c,SQL Server to BigQuery ,"Hey,

I’m looking for some input for an upcoming project we are about to undertake at my company. We are looking to replace our Azure SQL Server data warehouse with BigQuery. We are traditionally a Microsoft shop and much of our transactional data comes from other SQL Server databases via Azure Data Factory. We are currently looking at how to best ingest this data from SQL Server to BigQuery without breaking the bank. 

Some of our considerations are:
1. Write and deploy our own Cloud Functions which we orchestrate via Airflow. Sounds easy enough, the challenges we see with this approach is private network connectivity and backfill. 

2. Try out Datastream which is now in preview for SQL Server. The CDC approach sounds tempting, but we don’t have a clear requirement for streaming data at the moment. 

3. Go with Airbyte or Fivetran to ingest the data. If Airbyte, is self hosting a valid option?

It’s small amounts of data. Roughly 100 gb of changed data each month and 1-2 tb of historical. 

We are a small shop with me and a new junior DE. We are both quite proficient in Python. The rest of the stack is basically BigQuery, Airflow, Dataform and Power Bi. Any input would be greatly appreciated! 😊
",2024-03-26 16:58:02
1bo85gj,Databricks TPCx-BB 30TB,"Hi,

I'm trying to benchmark the TPCx-BB on 30TB scale factor. Any recommended type of nodes and amount of nodes on Google Cloud? I have no experience in this cloud provider.",2024-03-26 13:39:42
1bo5yzw,Application vs Data integration tools," Currently on the market there are integration tools with different capabilities. If we deep dive more into it is that they are tools suitable for application integration ( APIs, event-driven, real time) and data integration (batch, big amount of data). Now, for example, for application integration the suggested tools are Mulesoft or boomi, while for ETL is in general informatica or talend. 

Can someone educate me why the design architecture of tool like boomi is not suitable for ETL but informatica is? I guess there must be something, just was not able to find anything concrete.
",2024-03-26 11:52:00
1bo5a7j,Processing Duplicates with PySpark,"I'm currently working with a transactional database that authors records that change retrospectively.
EG a supplier may have an attribute that changes each day (as an example).
This means a record with the same supplier ID is changing each day, but I dont want this duplicated in my final suppliers table.
Is there any way to approach this without doing a window function and hence reading/shuffling my entire dataset each time? I'm finding compute limits on my dataset (approx 30gb) because i am deduping and then partitioning by date (resulting in a massivle shuffle) and disk spillage.",2024-03-26 11:12:50
1bntcf2,Transitioning Data Sourcing: Pros and Cons of Shifting to Production-Exclusive Approach with Service Account Integration for Transformation in dbt,"We have a bigquery datawarehouse on 2 environments dev and prod. the datawarehouse is on medallion modelization. (Bronze - Silver - Gold)   


  
What could be the drawbacks and advantages of ceasing the extraction of source data into the bronze layer across both the development (dev) and production (prod) environments, opting instead to exclusively gather data in the production environment? Additionally, how does utilizing a service account in dev to extract data for transformation in the silver and gold layers impact this process, considering the use of dbt for transformation?",2024-03-25 23:57:24
1bnpp91,Advice on the best way to store and process the large volume of data. ,"Hello guys,
I am working for a particular research lab. That uses an application to collect survey data. This survey data has multiple fields and they keep adding question even after creation.
The survey are partitioned into different group and each group has different different survey with similar fields. 

The current work flow is as follows; 
1. The data is stored in mysql database with one of the field taking majority of the data from the survey in JSON form. 
2. A script is writing to pick the data from mysql to mongoDB as documents and flat file. 
So all the fields from the the different survey are all put as a single flat file with a lot of missing values. 

3. Using Tableau to connect through mysql connector to mongoDB BI connector and pull the data to tableau as a single file. 

Problem;

1. The fields size increases with increase in survey information from the application and Tableau has a limitation of 700 columns. 
Which posed a problem for some fields missing. 

2. We try to partition the data based on the groups so we can use the Union method of blending, we encounter a problem of having two values on  a particular field for a single instance which is a problem. 

3. The modeling of the application is not in a relational approach that we can use joints. 

What is the best approach to solve this  problem? 

",2024-03-25 21:31:51
1bnldjq,Data Pipeline- Near Real Time + Agregations,"We have multiple data sources that are integrated into single Data Warehouse on Redshift. Those modeled data is then used by other teams. So far batch approach was more then enough but, new stakeholder, new requirements (5 min latency)

Now: ELT We ingest data with glue, DMS or Api Gateway depending on source to S3. Then we have Glue Crawler that creates Glue Catalog. With Redshift spectrum we create external schemas in Redshift. Whole processing is Done in Redshift/DBT. Json explodes, types assertion, Creating current state of Fact / Dimension. group by, window functions.

We incorporating lambda approach for the biggest Fact Tables but it is not fast enough for Near Real Time since procession of those in Redshift takes \~30min.

I am wondering where should I move data processing especially Aggregations and Window functions to make it as fast as possible…

I want to move heavy lifting to something suitable for Near Real Time and parallelization. But at the end whole data must be available in Redshift.

I have seen something like this:  Kinesis Data Stream + Lambda + DynomDB: [https://aws.amazon.com/blogs/database/build-a-fault-tolerant-serverless-data-aggregation-pipeline-with-exactly-once-processing/](https://aws.amazon.com/blogs/database/build-a-fault-tolerant-serverless-data-aggregation-pipeline-with-exactly-once-processing/)

  
I was also thinking If I shouldn't learn something about Spark ?",2024-03-25 18:45:08
1bngsi2,Any requirements/configs that I should give to DevOps for setting up services in a cluster?,"New job, and I'm kinda new at being in this capacity.

I'm planning on refactoring a lot of our data infrastructure, at least insofar as a few teams are concerned. I was thinking of going into this by having DevOps provision me an EC2, S3, and Redshift instance and I'd just set things up myself on an as-needed basis; i.e., start with dbt, Airflow, and the means to connect to our S3 and Redshift, then anything else I may happen to come across.

DevOps is currently trying to just give me something set up in a cluster and they're asking for what services and relevant configs/requirements I need set up. So, I guess this is a two-part question:

1. What other services and relevant configs should I consider adding to this setup?
2. Should I insist that I be given an EC2 mostly because DevOps has admitted to being understaffed and I wouldn't want to have to wait a while for additional services to be added if I come across some that would help?",2024-03-25 15:41:02
1bnf899,Downstream model testing in the dbt dev environment,"Hi Folks,  
We are considering dbt core for transformations. I have a question, When we modify a model in the development environment, is there a way to ensure that it won't cause any issues with downstream tables before we merge it into production? ",2024-03-25 14:35:55
1bmjolc,Do you actually use window functions or recursive queries in your job?,"I spent endless hours mastering windows functions for interviews and when i got a job i found out that they are not even needed. Sole exception some very fringe cases when i want to rank, but even that can be done with other means. 

 As for recursive queries...lol",2024-03-24 12:33:13
1bm2imt,Copy data into dedicated SQL pool from Databricks or create External tables in Synapse?,"Hi folks,

We are working on a project where the architecture will be like:

ADF --> Dabtabricks(ETL) --> Synapse(BI layer)

And everything is stored in ADLS gen2.

The reason we chose Synapse was because our BI team wanted something similar to SQL as they have reports datasets written in SQL and wanted a platform to access databases same way they do now.

&#x200B;

And now we have dedicated SQL pools for them and they would access data stored in silver and gold layer. 

&#x200B;

So either we copy the data every day and do duplication of data. ( more development effort to upsert the data and since we copying data, more time taken to finish the whole process.)

OR

write external tables for each table. ( less development effort but network latency and not as optimized as internal tables)

We process close to 4-5 million records in a day to give you estimate on the volume.

&#x200B;

Or if there is another solution I am overlooking?

&#x200B;

&#x200B;

&#x200B;

&#x200B;",2024-03-23 20:48:28
1bls2gy,Column level Business rule along with Lineage in Databricks ,"I have an use case where I have to create STM document(Source to Target Mapping). 

I want to get the business logic which is being applied on a column, along with the column lineage.

Using databricks api, I got all the required lineage info of each column, but I want to get the business rule as well. For example, Col A from table-A is an union of Col B and Col C of table-B, I want to document this business logic which is union. 
Can that be done in Databricks? 
I wonder Alation/Prophecy would be able to do that? Any thoughts or help on this? Thank you",2024-03-23 13:23:11
1blph0c,AI plugins for DE,"What is your take on AI plug-ins for Databases ? Do you use it regularly and has it improved productivity? 
",2024-03-23 10:55:46
1bloqle,Consuming API data into Spark,"I need to consume some data from an API. One of the endpoint will return a list of regions in the UK. The data returned from this API will likely be the same 99.999% of the time given that these won't change very often. However that being said, I need to check this daily to ensure any changes are picked up.

When consuming this data into the raw layer of the medallion architecture, what's best practice? Do I save the data every time the API is called, meaning having daily copies of the same data?",2024-03-23 10:07:21
1blfxnq,Help me ask questions about the tech stack,"I interviewed for a law firm that had consultants build a data warehouse for them. It isn't exactly clear to me what the law firm is doing and my questions in the first round didn't uncover much. I do know they are using azure synapse and power bi for visualizations. They want me to improve the efficiency of existing pipelines and bring in more information. I plan to ask how much bi work will be expected of the role. Can you help me think of other questions to figure out if this role will help me level up technically? The pay would be a significant increase per the recruiter, but I'm very wary of what I'll be doing. Thanks for the help.

for contrast in my current position i work in python, airflow, and sql in an Oracle on prem database. We are moving to Snowflake but that isn't being handled by my team. I'll have an opportunity to work in snowflake. I am looking for a position that will allow me to use a cloud platform, maybe databricks, and work within a modern data team.",2024-03-23 01:18:09
1bl2psp,PSA: Calling Apache Parquet and Apache Orc a table format is like calling an analog camera 📷 a cell phone 📱,"PSA: Calling Apache Parquet and Apache Orc a table format (like Apache Iceberg and Delta Lake) is like calling an analog camera 📷 a cell phone. 📱 I wanted to take a moment to clarify the difference between a file format and a table format so we stop comparing 🍎&🍊.

FD: I work at Tabular (vendor on Iceberg) but has little relevance to this post.It makes sense why people conflate file and table formats based on how they experience the technology. Both formats provide interfaces to read and write columnar data with a schema, and if your only experience interacting with Iceberg/Delta and Parquet/Orc is through a query engine, you are none the wiser...and hey, that's by design: [https://www.youtube.com/watch?v=\_GW3GYZK66U](https://www.youtube.com/watch?v=_GW3GYZK66U)  


That said, file formats can’t evolve schema, they don't provide transaction semantics (like you would expect over a SQL table), and one of the biggest value adds of a table format is to track multiple files and enforce the table schema as a contract to new data written to the table abstraction (which then creates a bunch more files). There's a lot more nuance to get into, but that's hopefully enough to help you remember this as table formats grow in adoption. Now you can recite this fun fact at your next company offsite!Note: Some query engines like Apache Spark would supplement some of this metadata and do a best guess to merge schemas of multiple Parquet files but no guarantees on how it would handle added or dropped columns between schemas (see image). Unless there's an accurate `created_at` column in the data itself and assuming the files weren't created by two different systems or processes. This was considered bad practice early on and Spark now disables schema merging by default: [https://spark.apache.org/docs/latest/sql-data-sources-parquet.html#schema-merging](https://spark.apache.org/docs/latest/sql-data-sources-parquet.html#schema-merging)",2024-03-22 16:00:01
1bl26ct,How are you tracking all tooling / technolgies to check out later?,"For several years now, while reading Hacker News and Xitter every day, I've been collecting lots of tools, projects and technical blog posts to ""try out later"". Most of them are never used, or stop being developed.

But quite a few end up resurfacing, or being useful for new projects I start.

**What do you use to keep track of these things you want to check out later?**

Bookmarking services is the usual answer, but I don't feel like they're good enough. I want something that can extract the value prop when you save it, and ideally auto-categorise it.

Designers have a ton of options like https://eagle.cool. Is there something similar for tech / data tools?",2024-03-22 15:36:48
1bkxj2n,The usage of data eng. in mineral processing,"Hi, im a mineral processing engineer and i want to learn where can i use data and ai engineering in my field.

İn mineral processing, we are crushing, grinding,  hydrometallurgically and pyrometallurgically enchanting the raw ore. 

We are currently using USIM PAC and minitab for predicting plant feed and concentrate.

Thank you.",2024-03-22 11:59:41
1bkvww9,Best open source tools for tagging a digital archive,"For those whose archives contain mixed data types, e.g. video, images, texts, even bookmarks, which tools would you suggest using to tag the archive?  
  
Is there much from AI that can help with the tagging these days?  
  
In a use case where you wish to self-host and keep part of your archive private at home, but share the rest to the world, ideally without sharing your home ip address, what tools are there to help you with this?

What is there to provide a landing page, somewhere to browse the categories, and highlight new additions to the archive?  
  
Here are some cool tools that might help others:  
  
Tropy:
https://tropy.org  
  
Tagspaces:
https://www.tagspaces.org/",2024-03-22 10:20:36
1bktsyz,Consolidating Data nightly?,"I’m working for a firm that has to pull in new financial records each night from a series of about three dozen SQL databases at semi-independent branches, and I’m looking for the smartest way to do this.

Currently we’re pulling data nightly from each branch’s database as linked servers into our on-prem MSSQL data warehouse, doing a little cleaning/mapping, and combining all of those branch tables into one huge reporting table. We’ve got the go-ahead to migrate to Snowflake/BigQuery/some cloud solution this year, and I’m trying to figure out what the best solution for getting data up to those cloud architectures would be.

Is it better to pull all of the branches’ data into our server and then move those staging tables up to S3 and then feed them into Snowflake, or is it more effective to just pull each branch’s data (10k~15k rows per day) into a Pandas/Polars dataframe in turn and then push that up to Snowflake? Is it possible to pull the data up to Snowflake from each branch’s database like a SELECT INTO query against a linked server, or does data need to be pushed up to Snowflake from the branch’s server side?

Honestly, I’d appreciate any help or advice you folks think would be relevant/useful.",2024-03-22 07:47:05
1bktn9d,Operational Data Store(ODS),I am working in an asset management company and we are looking at some of the data management tool. These vendors often talk about Operational Data Store and I still can't figure out what's the purpose of these system. Looking at it i just feel like it provides real-time analytics and perform some mastering on the data. I just wondering that can't these features be implements on Data Warehouse such as Snowflake?,2024-03-22 07:34:59
1bktkt6,How are you streaming data into Snowflake?,"We use Databricks and structured streaming in most of our regions for Telemetry data streaming, storing them as Parquet/Delta as needed in ADLS. The Source system uses Event Hubs. There are other Data Products where the Event Hub connection happens via ADX where near real time data is needed 

But one of our regions we are bringing in Snowflake and moving the Azure solution there. The only reliable option to get data from Event Hubs  seems to be  ADLS capture and then Snowpipe via event grid notifications. 

While this might serve our purpose (we don't really have a near real time usecase at the moment) , I am wondering why Snowflake don't seem to have an equivalent to Spark Streaming. Other options we looked at were deploying the Kafka connector in a VM and then use the Snowpipe steaming. Also the SPCS solution which sounded good but it's still in public preview

So how are the others doing this in their projects. Or is the recommendation that when have a near real time usecase , Snowflake may not be the right solution? ",2024-03-22 07:29:53
1bkqnmn,Needs some feedbacks/advice on the feasibility of building a data pipeline for my deep learning project.,"Hi guys,

So currently I'm building a deep learning model to detect & classify some brain-tumors using CT images. The dataset I got was retrieved from Kaggle, and I'm trying to replicating a research paper using transfer learning from EfficientNet. However, I've been thinking that I want to do something with ELT the data from a more reliable source and came across the NCI portal for Imaging data commons. It appears that they do provide RESTful API for mining their data (which are open-access), so I've been wanting to create a pipeline for extracting, downloading them and use the data for my model instead. I did learn a very beginning tutorial about creating a data pipeline for this, so in my head it's these steps:

1. Create a script to query and search for images using the API with the keywords I want, in this case, a specific brain-tumor types.
2. Since this dataset might be large and complex, I need some place to store them so I'm thinking about using MongoDB or BigQuery?
3. Then I'll build another script that fetches them down and run them through the model I'm building and output some prediction/classification.

I wonder if that's a good approach or not, if you have any experiences regarding those topics, I would be extremely happy to learn and know! Thank you so much!",2024-03-22 04:14:31
1bkmpk7,How to enable CDC in SQL Server to implement Debezium connector?,"I'm trying to connect my database to SQL Server with Apache Kafka, but I can't receive update messages to my Kafka topic. I would like to know if I should do anything additional when enabling CDC in my database",2024-03-22 00:51:41
1bkemcv,Extract insights from job posts,"Im trying to get insights from job posts. I scraping Upwork and want to get common pain points and repetetive tasks. I upload csv for gpt 4, i iterated a lot and the best that i can get is high level insights for example ""many struggle with data entry its a common pain point"" but of course its very geberal, I would expect of what is the use case, fliw, personas and more, for example ""many lawyers strugle with manual rename files for discovery, they posting job posts to automate it, its a reql pain point"" idealy i will want to be able to keep chat with the llm to ask more questions such as how much tge job posts offer for that, feom what countrues are they and more 
I would like to get your advise, regarding how to execute it, since the straight firward llm approach dosent work, I will be happy to get your advice of how to do it.",2024-03-21 19:09:28
1bkae55,"Hashquery, a new Python SDK for data modeling and querying","At my company we built a Python SDK in-house to serve as a replacement for data modeling DSLs. Think LookML, but you can use it anywhere you can run Python (jupyter / hex notebooks, python ETL jobs, etc). Internally we're finding it useful as a general-purpose modeling layer so we decided to open it up: [https://hashquery.dev/](https://hashquery.dev/)

Still pretty early but we have a few ideas on where to take it from here -- interested in hearing if this seems useful?",2024-03-21 16:17:23
1bk5c9b,Data Lake Questions,"Hello, we're currently in the process of pushing structured data into a Data Lake, as it currently resides on Azure SQL Server and, at present, is using around 250gb. This would be in a blob storage on Azure.  


The plan is to opt for parquet files, with the data pulling from one database and approx 15 tables. On average, each table has approx 200k rows, none of them are very wide.  


I have a couple of questions.  


A) Would I be better pulling the data every 15minutes, creating a larger quantity of parquet files, or would I be better doing 1 file for each table, at the end of the day.

B) We'd like to introduce an analytics tool to allow users to query the data, what would be the best tool to run queries directly against the blob storage (therefore avoiding hitting the current SQL instance).  


&#x200B;",2024-03-21 12:26:51
1bk4ixa,What topic would you like to see a webinar on?,"What topics are interested in hearing about in a podcast, talk, webinar, blog, etc.",2024-03-21 11:40:25
1bk3m6f,Best practices for storing additional data for timeseries,"What is general best practice to store additional data related to devices etc.

For example  
A customer has an asset (A factory building) where many temperature sensors are installed.  
Along with time series for each sensor, we need to store additional data about devices and asset,  
for example

* Device Id (generated when device was provisioned), device description, manufacturer, installation date etc
* Device location coordinates
* Factory building related meta data

Do ppl use another relational db togather with timeseries db to store this kind of data ?  
What is general best practice, how others do it ?",2024-03-21 10:44:27
1bk1fgc,Question about reading from Delta Lake,"Hi team. I'm currently assessing a potential use case for Delta Lake (OSS on-prem) and I think I've got my head around most of it, but there is one aspect I'm unsure of.

If I've got this correct, every update to a Delta Table will create a new version which is essentially a snapshot as of that update execution. I can access data from the current snapshot or a point in time from either a specific version number or timestamp, and I can partition my data along date lines to increase the performance. Sounds so simple :D

However, I can't work out how you perform do a read across multiple versions of a table e.g. how you could write a SQL or Python script to perform a time-series query to do aggregations or determine trends over say a 6-month range. It could be blindingly obvious, but Google hasn't helped.

Can anyone familiar with it please advise?",2024-03-21 08:08:30
1bk02hv,"Gravitino: Next-Gen REST Catalog for Iceberg, and Why You Need It","REST catalog for Iceberg is very useful, but it lacks a deployment service in the community. Gravitino is a good practice. Gravitino will be donated to ASF soon.

[https://datastrato.ai/blog/gravitino-iceberg-rest-catalog-service/](https://datastrato.ai/blog/gravitino-iceberg-rest-catalog-service/)

&#x200B;",2024-03-21 06:30:55
1bjzhbp,KafkaConnect and SchemaRegistry. How does it handle this case?,"Hi team,  
I am writing a very simple custom connector. To test it, I am using the Confluent Platform docker compose (which gives me all the relevant services). Great so far.

Now I am tackling schema. My intuition is to simply create a topic in advance, set its schema in the Schema registry as Avro, and then have my connector simply produce string messages to the topic. Having tested it, I don't think it works that way now.

After reading, ChatGPTing, etc, some things suggest to create the Avro record in my connector. But to me, that's counter-intuitive. Isn't that taking the ""conversion"" away from the KafkaConnect platform and jamming it in my java code? Isn't the converter specified as configuration? Moreover, what's the purpose of having a schema registry if I have to repeat the schema in my java code?

I tested this by trying to manually produce an ""invalid"" message to the topic (one that doesn't match the schema). But it was accepted!

Can someone help me understand:

1. Where should I keep the topic's schema?
2. What kind of Record should my connector be producing?  
Bonus: Please just generally explain who does conversion in the KafkaConnect setup? And who does validation?

Please and thank you.",2024-03-21 05:50:50
1bjxiay,dbt Cloud - MFA/2FA support,"Does anyone know if MFA can be setup for the Cloud IDE?  I am not meaning dbt core, but the cloud instance.  I am also not meaning mfa to source data - just the cloud env where you manage projects, connections etc",2024-03-21 03:53:28
1bjdyd2,Alternate of Redash,"Hi, I am a Data Engineer at a startup and we are using Redash as a platform to query from multiple database sources, schedule queries and link result of queries with Google sheet.
I am looking for a tool that can handle all these use cases, please suggest some alternatives of Redash. Currently I am exploring datagrip. 

",2024-03-20 13:52:11
1bj9s7i,Data Lake folders and structure,"Hi, I have little experience in designing the architecture/structure of a data lake/lakehouse. When I load a table from a source with spark, it ends up in multiple folders with multiple parquet files. Is this normal or am I doing something wrong? How is the usual folder structure for a table? Is it a folder with the table name that contains multiple other partitioned folders with files? Is it problematic to have multiple partitioned folders for the same table if I want to use the delta format? If you have any resources regarding folder structure in data lake, please link it.",2024-03-20 09:58:00
1bj9838,I wrote up a blog post on how you can use GlareDB for query federation (e.g. running SQL joins across a bunch of different data sources) and thought it might be useful for folks with company data scattered to the four corners.,N/A,2024-03-20 09:16:29
1bix6kx,AWS Lambda Runtime.,"Hello all,

I’ve been running some jobs on AWS Lambda, and more than 75% of the time, the job timesout during runtime initialization. Ive switched some of my jobs to use Batch/Fargate tasks instead but I would prefer if some jobs could have those HTTP endpoints to trigger with.

I’m using python 3.11-slim-bullseye as my base image and can figure out how to fix this issue? I’ve tried provisioned concurrency and the like.",2024-03-19 22:19:38
1birk8h,Book Recommendations on Streaming Systems,"Hello Datadogs, 

Your expert opinion would be appreciated if you could provide me with book recommendations on Streaming systems, NRT (near-time pipelines ) and related literature.

&#x200B;",2024-03-19 18:35:13
1bipbf7,Any data engineers working at a hedge fund? I got a couple job interviews coming and would like some insights.," Do you normally build APIs?

I have good gasp of reading and parsing data from APIs but I have never build any. Not sure if building APIs is common for hedge fund DEs? Thank you!

What are the common data sources where DE pull data from?  Besides files, APIs, ftps, sql servers, what else? Thank you.  I posted this yesterday but not sure why the content is no longer available. I am kind of new here. Not sure If I am breaking any rules? ",2024-03-19 17:04:05
1binect,Text to Teradata SQL with LangChain and OpenAI,N/A,2024-03-19 15:47:06
1bikw3k,Teradata to Databricks migration,"One of the clients we are assisting is moving from their 20 year old Teradata system to new Databricks system. While doing so they are changing the data models as well. We are tasked with mapping ~300 tables across 6 databases in Teradata that a particular department currently uses for their reporting. The idea of mapping is to map columns and tables from Teradata tables to the columns in new Databricks model and eventually report anything that is missing. Being on such first time migration engagement, I’m struggling to start moving forward. Any suggestions on how i can start and push forward to resolve this problem for client ? Thanks in advance ",2024-03-19 13:57:44
1bihhh1,Power Bi datasets into Palantir foundry,"Hello everyone actually I want help from you guys in one of the subject. My company now have a new requirement they want me to get the data sets that are present in power bi and put them in Palantir foundry but i am not finding anything related to this so guys do you have any idea or suggestion like how can I get the datasets from power into Palantir foundry any help would be  appreciated thanks a lot guys.

&#x200B;

In summary i want to connect powerbi into Palantir foundry and get datasets from power bi into foundry",2024-03-19 10:58:12
1bigye8,Recommendations for which Data Msc in UK? Planning to move to Germany and a qualification would help.,"I've been working as a DE for 2 years or so and would like a degree as it'll help me get a job in germany.
There doesn't tend to be a degree in data engineering itself. Only cloud computing or data science/data analytics. 
Analytics seems to be the most desirable from German companies.

I want to do an open university but unsure whether to look at analytics or data science for the degree. I do have an interest in both and machine learning is a cool concept but understand that data science/ML can be extremely complex mathematically. 

Any advice? 
",2024-03-19 10:23:30
1bidrvp,Godel Grid Connect platform,"Hi guys, founder of [Godel Grid Connect](https://www.godelgrid.com/) platform here. We run data pipelines and transfer data in user's own cloud. This is faster and more secure than traditional vendors. Currently we have integrated AWS on our platform and transfer data from postgres to s3. We're taking requests for new cloud integrations and connectors. What are your suggestions and opinions on this. What areas I can work on to stand out from similar services (AWS DMS in particular). What do think is a weak area in AWS DMS service.

I am also looking for potential clients. If you think its useful to you message me on chat.",2024-03-19 06:32:10
1bidc92,Data purging strategy,"Hello Experts,

Putting it here as its more related to data. We are using AWS cloud for multiple applications and using multiple databases like AWS aurora postgres, Mysql, redshift, Dynamo, Snowflake etc. and also in many cases data is stored S3 storage simply for long term retention and occasional querying purpose. 

We want to implement data archival and retention strategies across all the databases or data stores. Want to know, if we already have any common framework available for data archival and retention across all the databases or data stores in these AWS databases or data stores.

 Or 

Is it possible to build some common framework like having a common config table(something as below) and then invoke either DELETE command (if the table is non partitioned) or else drop partition (if table is partitioned). The lambda with the business logic can be scheduled (through aws event bridge) will connect to the database and then read the config table and do the data archival and purge as per defined setup?

  ""Table\_name""

 ""retention\_days\_number""

 ""retention\_column\_name""

 ""Partition\_type\_name""

 ""partitioned\_yes\_no""

 ""partition\_column\_name"" ",2024-03-19 06:02:35
1bi30ja,Seeking advice from my veteran brothers,"Hey everyone,

Hope you're all doing well!   I'm trying to learn how to scrape data from job sites like Glassdoor and Monster using Python. 

I tried to use BeautifulSoup and request but I'm unable to find the correct class and parse the data.

Any tips or advice on where to start? Would appreciate any help!

Thankyou",2024-03-18 21:47:39
1bi1uwp,Denormalize Tables into CSV?,"I have a client that insists they want to see normalized data in a denormalized csv. If I were to do this it would be hundreds of columns and the number of columns could change so they would need to be dynamically generated.  


I don't suppose a tool exists to do this?",2024-03-18 21:02:26
1bhxugv,Fivetran lever connector initial sync taking too long,"I am using fivetran to sync all my data from lever to snowflake. I have followed the fivetran connector guide and created an api key and provided it, however it has been 13+ hrs and still the sync is going on, I can see some of the data populated in snowflake and it is getting populated gradually as I can tell by observing the fivetran sync times and I thing the problem is with the api not allowing too much data to be extracted by the connector. I have read somewhere that if this is the case we can know by the warnings thrown by fivetran but in my case no warnings were thrown now I am just confused on what to do should I let the sync be running as this doesn’t imply any charges or should I pause it until furthyer investigation with fivetran and has anyone faced anything similar with this connector ??",2024-03-18 18:24:27
1bhvpyv,Photon alternatives? I need something open source,"[https://www.reddit.com/r/dataengineering/comments/1bf8k8q/explain\_like\_im\_5\_databricks\_photon/](https://www.reddit.com/r/dataengineering/comments/1bf8k8q/explain_like_im_5_databricks_photon/)

Hey guys I saw this post the other day and we have a similar set up. except I want something open source instead of a full fledge product like Photon. from what ive read, spark3 already converts data frames into columnar formatting in the catalytic optimizer. 

so im even wondering apart from the C++ translation if I were to use photon would there be any other benefits? 

&#x200B;

also if you know of any open source products that can be used in OP's tech stack to accelerate my project lmk please

&#x200B;",2024-03-18 16:59:41
1bhumcc,Help needed on Delta Live Table ,"We have delta live table which reads the CSV Files from ADLS and loads into bronze and silver. Your typical Data Pipeline.

It’s not very complicated pipeline. With auto loader it instantly picks the files and loads to bronze in Unity Catalog but in between I am doing two operations -

1) Usual Deduplication based on the Hash value 

2) Based on a configuration CSV file , I pick operations like convert a elements in a  column to Upper case , or trim the elements of the column or you replace all null/empty values with NA. Small operations which I don’t think will take much time for like whole 77K records I have for just prototyping 

My question is , loading the Silver table just take a lot of time , 15 minutes and that too for 77K record. 

I am very new to spark , being from your  traditional Software engineer background.
I would expect that any kind of transformation can be done parallel in workers but I see only 1 worker running in Spark UI. I see DLT uses structured streaming so does it just run jobs in serial way like one after another and then gather all data set in one data frame and return ? 

Can’t see logs in workers because of whole Unity Catalog and I don’t know how to enable those. Gave some configs but it dint work.  


If you guys have any alternative to this or pattern which is better than this , please suggest. That will be a great help for me. I don’t seem to find a lot on internet about this particular use case. 


",2024-03-18 16:15:00
1bhprz9,Seeking Advice on Automating ERP Data Transformation to Silver Layer,"Hey everyone,

I'm seeking advice on automating the transformation process for ERP data from raw (csv) to silver layer in Microsoft Fabrics. Here are some factors I'm considering for automation:

-Incremental load

-SCD2 handling

-Automatic adjustment for column changes (Insert, Delete) in the underlying data structure while still supporting SCD2

-Generating artificial surrogate keys for streamlined joins in the gold layer (any optimization suggestions, like z-order for delta tables?

-Automatic adaptation of column metadata from ERP


What are your thoughts on which of these factors can be effectively automated and which might require manual intervention?

Looking forward to your insights and recommendations. Thanks in advance!",2024-03-18 12:41:39
1bhdbdc,How to enable self-serve analytics?,"There are times when I face an unusual amount of tickets being raised for reports and dashboards. I wonder what might be a good way to give some sort of self-serve analytics.

Have you guys faced a similar bottleneck? If yes, please advise on how you navigated it.",2024-03-18 00:20:02
1bh4lau,"Separate DB vs Separate Schema for Raw, Processed & DWH?","We are using SQL Synapse Dedicated Pool which is essentially SQL DW with ELT processes

So current process look like

Load raw incremental data from files to staging tables in vendor specific schema in DW

Post that we have stored procs which load that data into main table where incremental data get appended to historic data

Final layer would be another set of stored proc which move data to dwh schema tables

So currently everything is in same DW but in different schema and dwh supports various reports (reports are import mode so each interaction does nit hit DW) and some external vendors

This system could run into issue if number of external vendors that it needs to support increases beyond certain point and if this vendors keep hitting DW with their queries throughout the day as that would give less processing power to other processed

We are thinking of either creating read only copy of just dwh schema or taking dwh schema entirely on different DW

However if we move it out entirely we will have develop equivalent of existing stored procs in either databricks or some other ETL tool and if we just copy dwh to either parquet files or different db then copy process will have to built

Ideally copy would be simple to do but daily copy might take time

What would you do in this situation?",2024-03-17 18:25:57
1bgoilu,different ways to generate a batch id for a pipeline,"My Azure Data Factory DEs were trying to get a vendor to append a GUID for a batch ID when they extract into a shared location that has multiple file drops from different systems.   

This is just some vendor widget no code so seems to me like ADF should be doing it.  Seems like a common file wrangler kind of use case.

I suggested moving the files to another folder and separating it that way.  Was told that took 'too much logic'.  What are different ways we can separate a batch of files on the ADF end?

And can ADF generate a GUID batch id and append or prefix to the filenames?",2024-03-17 03:57:45
1bg7oz0,Databricks network question,Databricks + azure newbie. I’m looking into connecting my azure databricks workspace to an on prem server. Do I need to first establish a vpn gateway to our on prem network for this to work? ,2024-03-16 14:46:36
1bfg9u8,What or how can i show or use for this Data?,"I have this type of data, but mostly it's Qualitative values and not numbers, and i need to showcase this on Plotly somehow, any approach you guys would think of ?

&#x200B;

Context on the data: Every Bank in the world has a Rating according of how well they did on the year of Rating, the Rating goes in an alphanumeric value, as you see on columns Short and Long Term Rating(F1+,F2+,BBB,A,etc)  
I've done several stuff on Dash(Plotly) but mostly my data has been with numbers where i can play with.  
 I just would like to show this data to know which Banks have a good,bad or great Rating and not to show just a table(People non related to numbers who just want to see graphics and all those stuff).  


https://preview.redd.it/utyjjat6kioc1.png?width=958&format=png&auto=webp&s=126d04b0962008c7646058ed4103e1507e7d2891",2024-03-15 15:12:55
1bfeq9l,What do you use for EMR cluster management?,"We used to use the python package mrjob but it is now dead. Before raw dawging boto3, I wanted to see if there are high level python packages or high level approaches that you guys suggest. 

&#x200B;

Thanks! ",2024-03-15 14:05:17
1bfdc90,Why Y42,"Has anyone used Y42? I've been looking into it, it seems fairly nice. It's a fairly new tool (created in the past 4 years), so I'm wondering if people have had any issues with reliability or anything like that. I'd love to hear anyone's experience with the tool.

Also, does anyone have any idea of what the pricing looks like for Enterprise? 

Furthermore, can the tool only be deployed for cloud or VPC environments? Or is it possible to locally deploy Y42?",2024-03-15 12:57:20
1bfaffo,High-Efficiency Data Migration Under Shared Storage Architecture,"Hi community, I'm working on our distributed time-series database which leverages a shared storage architecture. In its newest version, we support migrating data table partitions (Regions) from one datanode to another.

Since we utilize a shared storage architecture, where data files are stored on object storage and shared across multiple Datanodes. **Region Migration only requires the migration of a small amount of local data from the Datanode Memtable** compared to databases employing a Shared Nothing architecture, resulting in reduced overall migration durations and a more seamless load-balancing experience at the higher layers.

I share our technical specifics here in [this article](https://www.greptime.com/blogs/2024-03-15-region-migration) and welcome open discussion.",2024-03-15 10:05:09
1bf9bff,Good tool for near real-time analysis of manufacturing plant on RDS or on-premise SQL?,"I'm working in operations in a manufacturing plant. We have lots of systems which produce data:

* ABB robots
* DC tools
* Testing equipment
* Camera quality control
* ...

These all generate data, some of this is used for reporting purposes in meetings or to get a historic overview. For this Power BI is used and it's a good fit.

However we also use this data to steer production, we're not dependant on it but it improves efficiency and we spot faults earlier. However Power BI isn't a good fit for this and we currently abuse it to fit this purpose.

What tools are a good fit for frequent (5-10 minute) update intervals to track what's happening in production? Preference goes to lowest cost one, even if it's worse. Expensive things of thousands a year wouldn't get approved at the moment. Source would be RDS for this tool.",2024-03-15 08:44:03
1bf8ppo,Why should you standardise across date columns?,"Hi all,

I’m working as an entry level data engineer and the team and I currently are pumping out data products (cleaned datasets related to specific business lines, some aggregations I.e QTD, YTD, all from the same source system) on a delta lake, dbt and airflow architecture using an altered version of the  medallion architecture.

All date columns from source are in datetime format, and are also ingested and retained in datetime. However, we have created some derived columns from these source columns (I.e quarter end date, financial year end etc). These derived columns are materialised as date format ( yyyy-MM-dd). We now have date columns with two different formats - one is the source that are in datetime (yyy-MM-ddTHH:mmzzz or something). The datetime format is kind of useless I suppose as it’s always the same (12 AM).

I had a thought and shared with the team that shouldn’t we standardise these? At least keep them all datetime, or all date, that way the other systems that pull our data for analysis and reporting purposes, do not have to do any standardisation themselves, and don’t run into any annoying issues where their code doesn’t behave as expected because datetime and date might respond differently to specific transformations or logic. However, my teammate (the other members don’t really care or respond in discussions), believes there is no need to change them as they currently behave exactly the same in our system. I suppose he is also of the opinion that it’s one extra transformation step for all date columns that enter our pipeline, and it’s a waste of work. He is also a junior. 

I wanted ask you guys and girls and see what your opinions were on this topic. Maybe hear from your experience. Is there any real benefit here to forcing the format to be the same? Appreciate any advice. 
",2024-03-15 07:58:20
1bf7qc1,How to dive deep into Gitlab Metrics with SQLite and Grafana,N/A,2024-03-15 06:45:27
1bf1ncf,Simple explanation of SCD,[https://chengzhizhao.com/unlocking-the-secrets-of-slowly-changing-dimension-scd-a-comprehensive-view-of-8-types/](https://chengzhizhao.com/unlocking-the-secrets-of-slowly-changing-dimension-scd-a-comprehensive-view-of-8-types/),2024-03-15 01:08:41
1bf15ft,What is Your Favorite Data Conference?,"What is the best data conference you’ve ever participated in?

Also, check out and register for Free Virtual Subsurface Data Lakehouse Conference May 2nd/3rd at Dremio.com/subsurface.",2024-03-15 00:45:23
1beocg8,Help me design this database ,"Hello, a junior data engineer here.

I am working on a mobile application that's similar to true caller but designed for the gulf countries (this kind of apps is famous there). The app allows users to search by phone number and name (full text search).

I am using postgres as a database and I am planning to have this design :

* a table for phone numbers : the table will have phone numbers, name associated with the phone number, country code, and some meta data. The table will be partitioned by the country code.

* a table for names: the same schema of the phones table, but I am planning to add 4 more columns for the first letter of each word in the name. Like that, I can filter out some rows before applying full text search.

For example, if I have ""Ahmed El Said"" I would store the name in the name column then store ""A"", ""E"", and ""S"" in the the 4 columns (with the last one null).

For indexes, I will add an index on the phone number for the first table and another one for each column of the 4 columns in the names table + an index on the name column.

What do you think of this design? Do you think I can use one table for both searchs?

Thanks in advance for your help.",2024-03-14 15:49:23
1bdwz3h,#Altinity #Webinar: Deep Dive on #ClickHouse Sharding and Replication,N/A,2024-03-13 17:16:11
1bd2ic4,DLT CDC,"DLT CDC

Hi all,

I have been going around investigating this but so far no luck. I have to ingest a table from on prem sql server and then proceed with databricks DLT cdc.

I am trying to understand how this works. So do I extract data from CDC table using ADF to landing zone and then ingest those files with DLT or I directly connect to cdc table through notebook. 

Also, do i have to create first external table (I need this) with inital load data and then apply cdc? I know we need to create streaming table but I am not sure how inital load data fall into all of this. Also source table currently does not have primary key and it is a huge table. 

How does DLT cdc work? 

Thanks! 
",2024-03-12 17:08:08
1bd1pkb,Databricks Delta Live Tables 101,N/A,2024-03-12 16:37:01
1bbhg34,Hadoop Ecosystem,"I am in my first data analytics course, and I have been assigned to present on HADOOP. It's been a challenging task, learning something without actually putting my hands on it. After many many hours of reading and watching videos, I still can't seem to figure out some major pieces: 1) Is there a definitive list of what is considered part of the Hadoop ecosystem? and 2) if not, then what exactly would qualify a piece of software as part of the ecosystem? Just if it can integrate with Hadoop?

I feel like these should be simple questions to answer, but every time I think I find an answer, I find a different list with different software packages.",2024-03-10 18:37:51
1bbdii9,Making Businesses Realize Data potential ,"You had to convince any business they were underutilizing their data in order to keep your job 

How would you do it?
What data is probably being overlooked?
What’s your solution?
What’s the impact?

Go!

P.S a big tech company asks this questions to staff data engineer ",2024-03-10 15:52:53
1b9qsqx,Select columns in ADF’s CDC connector,"So, I have a pipeline loading data from SAP CRM via a dataflow with SAP CDC as a source. I was able to parameterise most of the details but I am struggling with selecting only a given list of columns from the source instead of the entire object. 

The setup:
In my pipeline, I have a lookup activity that returns the ODP name, context, connection, etc. as well as a list of the columns I need from that particular ODP. Then the dataflow to which I assign these parameters and which sinks the data into .parquet. All ODPs passed to the dataflow (around 500) are fully loaded on each run. 

Any insight on this would be greatly appreciated!!",2024-03-08 15:31:24
1b916uw,Open Source Data Contract CLI to generate code or enforce the contract via testing,N/A,2024-03-07 18:03:25
1b8zv2u,EL Tools for QuickBooks Enterprise,"All, 

My company currently contracts a vendor to handle the extract and load from our QuickBooks Enterprise database. We’ve searched time and again for an EL tool with QuickBooks Enterprise connectors, but have come up empty. 

I know Fivetran has a QuickBooks Online connector, but unfortunately, we’re not able to make the switch from QuickBooks Enterprise to QuickBooks Online (some functionality loss, supposedly). 

I figured before I give up the search, I’d come here and see if anyone had worked out a way to extract data from QuickBooks Enterprise? ",2024-03-07 17:11:45
1b8xyq6,Choice of Job Title,"My boss came to me and asked me what job title I wanted. Their suggestion was “Data Architect” but I’m leaning more towards “Data Engineer”.

To give background, I recently transitioned from mechanical engineering to this role within my company, where I am introducing a new PostgreSQL database and SQL Server data warehouse. I have been doing data modelling and am trying to get ER/Studio (or similar - open to suggestions) into practice at my company. I work with scientific data and am starting to write a Python script to import data from inline a software program into the database. In the near future I think I will be using tools like SSIS and Airflow.

What other job titles should I consider? Or should I just go with Data Engineer",2024-03-07 15:49:37
1b81nob,What do you think about Oxia: a new high-performant metadata store?,"**Disclaimer:** I'm NOT a StreamNative employee, but recently became an Apache Pulsar committer.

Oxia is a new metadata store and coordination system similar to Zookeeper or Etcd. But in comparison with the others, it can store 100s of GBs of data and can handle millions of reads and writes per second.

Oxia GitHub repository: [https://github.com/streamnative/oxia](https://github.com/streamnative/oxia)

It's licensed under Apache License 2.0.

The Java client has been just open-sourced. Here is a blog post: [https://streamnative.io/blog/the-oxia-java-client-library-is-now-open-source](https://streamnative.io/blog/the-oxia-java-client-library-is-now-open-source)

Oxia is already integrated with Pulsar but isn't a default option yet. Folks from StreamNative claim that they used it in the cloud for a few months without any problems.

I didn't see any independent benchmarks yet, therefore I can't validate the performance and stability claims. Probably this post may change it and attract engineers who are interested in trying Oxia for their projects and publicly share the results.

I understand that it may be not a very interesting topic for many data engineers, but it may be interesting for engineers who build distributed systems FOR data engineers.  
I would highly value hearing your thoughts on the project.",2024-03-06 14:54:07
1b6zu8j,The Hunt for the Missing Data Type,N/A,2024-03-05 07:49:58
1b6ose8,DBT Runs,"I am integrating DBT into a RDS instance, a reports need to run daily for aggregated metrics from a big table, and spit it into other tables within the DB. Which service do you recommend running these DBT scripts and how to orchestrate it at noon and update the tables without the instance. Hopefully the solution is AWS, scalable as more models are added or time of triggering. I am familiar with lambda functions and ECS but not sure how not done this before. TIA!!!",2024-03-04 22:49:51
1b6ndyb,Reverse engineering using Erwin,Erwin can reverse engineer a view but it doesn't seem to handle CASE statements and errors out. This is for Hive. Does anyone know if reverse engineering of views can handle CASE statements?,2024-03-04 21:54:41
1b6k7nw,A simple way to optimize queries?,"We've been building a new tool called Chicory to help data engineers. Right now, Chicory can speed up SparkSQL by 50% and reduce resource utilization by 25% on average. We use GPT4 under the hood to analyze, rewrite, and optimize code while checking for errors and hallucinations.  

Check us out at chicory.ai",2024-03-04 19:47:05
1b6exrn,"Seeking Guidance on Designing an Online Data Analytics Pipeline with Kafka, Apache Flink, and ClickHouse"," 

&#x200B;

Hi everyone,

I'm in the process of designing an online data analytics architecture and could use some guidance. My current setup is based on the Kappa architecture. Here's a brief overview:

* **Source:** Events are published to a Kafka topic.
* **Processing:** These events are processed using Apache Flink.
* **Storage:** Processed results are ingested into ClickHouse for analytics.

I'm aiming to create a robust and scalable solution. However, I'm facing a challenge regarding data storage. Specifically, I want to store raw data in a deep storage solution like Amazon S3 without duplicating the processing steps already handled by Apache Flink.

My question to the community is: Is there an efficient way to integrate deep storage (like S3) into this pipeline, ensuring that raw data is preserved without reprocessing it? I'm looking for strategies or best practices that might help in achieving this, ideally without complicating the pipeline or introducing significant overhead.

Any insights, experiences, or advice on tools and techniques to streamline this process would be greatly appreciated. I'm particularly interested in any configurations or integrations that have worked well for you in similar setups.

Thanks in advance for your help!",2024-03-04 16:18:35
1b6bjgz,Easy Introduction to Real-Time RAG,N/A,2024-03-04 13:59:28
1b5y4b3,What's the most challenging part of data modeling?,"Every DE I've worked with seems to struggle *so* much with data modeling, myself included, yet I find it's probably one of the most important skills a DE can master.

What makes data modeling so difficult?

I'm a SDE with 5 years experience, studied database design in uni, read up on all the typical theory (Kimball, Inmon, etc.), and I still find data modeling is the most challenging part of my job. I treat it as an iterative design and over time with experience, I got better at knowing what questions to ask the stakeholders, testing prototypes, etc. But it will still happen when the product goes to prod, there's a random edge case that's not compatible with the model.

I work in a pretty niche industry and really gained my experience in one company, so want to hear other DE's thoughts on this

[View Poll](https://www.reddit.com/poll/1b5y4b3)",2024-03-04 01:22:18
1b4fngj,Salary reports,"Can you all please suggest what are some good sources to understand salary trends for data scientist, engineering or even data analysts? - preferably with over the years data as well.

Many thanks for all your suggestions.",2024-03-02 04:38:21
1bsqvz5,What is a good way to catch errors in a google sheet which is used for downstream tasks? Can I leverage ChatGPT?,"Hi there! My company's product team uses a 'rule-based' google sheet which is then used for downstream logic. We've had cases where despite there being guidelines on how to fill this sheet, new joiners have invariably ended up overwriting formulas or missing certain steps.

I could use conditional formatting in the sheet but I was also thinking of ingesting the google sheet into a Databricks job and letting the ChatGPT API review it and give out the cells in the sheet which are flouting pre-determined rules.

Does anyone have experience handling a situation like this before? I would greatly appreciate any advice here. Thanks! ",2024-04-01 01:20:41
1brewdy,Knowledge Graph of All Dishes,"I want to create a knowledge graph of all the dishes in the world. This knowledge graph should give me information like:-

Indian dish -> North Indian dish -> Mughlai dish -> Chicken Tikka

Italian dish -> Pizza -> Thin Crusted Margherita Pizza

Any other information that this graph may also be able to give like a description for the dish and an image is also welcome.

Currently one way I am thinking of doing this is through scraping a bunch of dish-related sites and feeding all that unstructured data to Neo4j + LLMs to build the graph.

Another approach is to use some algorithm or model to make synthetic data and then further make a knowledge graph out of that.

Please guide me on how to collect the data, build the knowledge graph or tell me about any insights that you may have.",2024-03-30 10:41:26
1brbasx,EE major and CS minor will give what potential career opps? AND do employers care where you get degree?,"\*\*\*\*\* someone commented that it might be good idea to to ask data engineering sub this question instead of electrical engineering sub \*\*\*\*\*\*

I am an incoming undergrad student, I am worried that majoring in Electrical engineering and minoring in computer science will put me at a disadvantage when it comes to some careers I have in mind that are more CS heavy:

* The careers I am interested in are:  

   * computer engineering,
   * machine learning engineer
   * software architect
   * software engineering
   * electronics engineer

I haven't really had the chance to explore my fields of interest which is why I want to cast a wide net to ensure I have some flexibility when I finally declare a major/for job search.

I got into UCSC as EE major. I was wondering whether I should go through with going to CC college instead, and then transferring to a more prestigious university like UCLA's EE department -- would It be worth it in the long run, in terms of impact on potential jobs and salary? Or should I just stick with UCSC?

I've heard mixed messages on whether employers prefer experience over where you got your degree, how does this vary which a more EE focused job vs a more CS focused job?

I would appreciate any advice to any of the questions I have.

Thank you.",2024-03-30 06:37:18
1bqnpds,Sell data warehouse to small business,I am skilled in creating a data warehouse. How can I sell my abilities to help a small business setup data and analytics for their website ? ,2024-03-29 12:10:02
1bqfuk9,Understanding Google Cloud AlloyDB Pricing,N/A,2024-03-29 03:57:44
1bqc7if,Update- figuring out what tables are what in a database,Ok I finally figured it out the database I was looking at. I found a table called sql_templates which has sql queries inside it. A sql inception if you will. Looking at that query I figured out that I have to do a 5 table join to get the data that I need. I don’t think I would have figured it out without finding this query. It has a lot of inner joins and is slow and I think it keeps causing the server to close the connection so now I have to figure that out.,2024-03-29 01:00:32
1bpsmur,Tasked with designing full product data model,"I'm a PhD data engineer with 3 years industry experience (moved from chem PhD to fintech).
Hired as DE a year ago.
My experience is in ML/data wrangling/ETL pipelines.

My work has been redirected to data architecture. I have been made the person who makes decisions on the full data architecture for the entire product, which compromises 4 apps/portals.
All decisions get directed to me.

I've taken it on but I am just going by online advice etc. I'm feeling a bit like there's a whole area in my education/skills that I've missed by not having a CS degree and I'm expected to be fluent in data architecture.

I think the problem is the team are insisting the whole software architecture design should be data driven. They want to base their software architecture decisions off a data model I lay out for them.
The product isn't really data-centered product though. 

Is there anything I can do to improve my confidence here? I've already done loads of online data architecture courses now but they largely seem to focus on building around desired functionality of the product - our team want the data model before the details of the functionality so they can decide how to build the software.",2024-03-28 10:52:28
1bpnrrb,Migrate data from one API to another API,"Hello
I have to get data from one API and migrate that data to another API. There will be many transformation involved between this process.
I have to build a custom connecter for one application to migrate data from another application

Need help in deciding tech stack. Need to use open source.
it would be helpful if you guys can advise some best practices to follow or how should I approach this work.

I have 2 yoe and I have worked with API before.

Edit:- tech stack will be Aws/azure

",2024-03-28 05:24:46
1bonmf4,How can I reorganize my football data for Social Network Analysis? ,"Hello, I'm doing a project on football data to detect the most related players in the match based on the passes played. And how those players contribute to the formation.

The data I have include the playername, x and y coordinates of the pass, end x and end y coordinates of the pass, receiver, I also have data based on the angle of the pass, chipped or not, and I can calculate the Euclidean distance of the pass bases on its coordinates. 

My professor told me to do this SNA making the players as nodes and the passes as edges, he also insisted on the passes to be weighted

This is my first time doing a SNA, my problem is mainly how can I conduct the dataframe.
Should the columns be exactly the data I stated above?

I was considering also using multiple passes from multiple players in each row, for example player A passed the ball to B which again passed to A which finally passed to C where the ball was intercepted.

But I have no idea how can I organize this in my dataframe, what should the shape of my data be? 

Can anybody provide some tips, I would be really thankful 🙏
",2024-03-27 00:04:06
1bok0e9,Ingestion of Excel files,"Hey everyone! I am in the middle of ingesting some excel files and i need an opinion on the architecture.

So i have the excel files in one drive. My ideia would be to either ingest the files pass them to a database, then from that use airbyte to retrieve the data put it into snowflake using dbt, and make some dimensional modelling and then visualize.

&#x200B;

Now this seems kinda stupid to have a database in the middle right, as well as having the files only in one drive?

&#x200B;

But i dont see the use of making the transition from one drive to azure storage for example.

&#x200B;

I need help xd",2024-03-26 21:41:10
1boayjv,French Contract Data Engineering daily rates,"Hiya !   
I am a contract Data Engineer in the UK and know very well the UK market as I have been contracting more than 6 7 years in the field . However I keep receiving more and more requests on the other side of the sea , mainly France,  Netherland and Belgium.   
I would ask what is typical senior data engineer contract day rates for someone working for Sodexo , Airbus etc ?",2024-03-26 15:40:39
1bo9ghz,Could you recommend learning materials about Data Cleaning/Cleansing best practices?,"I am working with snowflake, dbt, dagster stack. Didn't find such a tag in the data engineering wiki. Could you suggest something?",2024-03-26 14:36:33
1bnoiq5,"Databricks, SnowFlake, others, are they competitors or complement each other.","I went in with trying to compare Databricks and SnowFlake thinking they were more or less competitors to each other. However, it seems from some comments I've read on other posts, am I correct to assume they complement each other?

&#x200B;

I'm not familiar with either product so an ELI5 to these would be appreciated. What are the use cases for going for one vs the other? What are the use cases to implement both? Are there other services worth looking into?",2024-03-25 20:46:40
1blps4n,Thoughts on WashU St. Louis DE bootcamp (shown in Springboard)?,"Talking about [this one](https://careerbootcamps.tlcenter.wustl.edu/programs/data-engineering/), also found in [Springboard](https://www.springboard.com/courses/data-engineering-career-track/). They seem to provide a lot more career coaching/mentoring which I think I need, let you fast-track to complete in less time, and make you do portfolio projects. Their mantra is that portfolio projects (involving AWS, Azure, Hadoop, Spark, etc) will set me apart.

[dataengineercamp.com](https://dataengineercamp.com/) seems like a similar one but I haven't talked to them yet.

About me: I'm a May 2024 grad experienced in Data Science, ML, Python, SQL, etc, but not much else. I did have 1 DE internship involving Kafka/Confluent though. Didn't have success in the market even after several interviews, so I'm looking into DE bootcamps + certifications to expand my skillsets. I have the funds and I want coaching, so I'm just trying to choose the right program.

&#x200B;

&#x200B;",2024-03-23 11:14:53
1bl203n,Strategies for Identifying and Migrating Databases to a Data Engineering Structure.,"Hi everyone! How's it going?  
This project involves integrating the company's business areas with Data Engineering, where we'll map out opportunities for digitizing and automating databases. This ensures that all company sectors maintain data quality, security, and operational efficiency.  
The company utilizes various data sources; some departments use Excel spreadsheets, others rely on SharePoint, and some utilize our database system.

To clarify our infrastructure:

* DataBricks:  Database construction
* Data Lake:  Storage for completed databases
* Data Factory:  Automation of our code

I've mapped some things that i found useful like:

1st Phase: Define how many areas we have, which one will be chosen and who we will talk to

  
2nd Phase: Define standard processes to be used in all areas

* Interviews with key employees for development.
* Documentation

3rd Phase: Opportunity mapping

* Mapped opportunities
* Future opportunities
* Prioritized opportunities 

4th Phase: Development

* Develop
* Validate
* Quality Assurance
* Implement
* Clone
* Document 

5th Phase: Final evaluation and improvements for the next ones

* Stakeholder evaluation and feedback
* Observations (How to improve?, Errors encountered, Continue what worked well)

Considering this, I'd like to draw from your experience: What actions can we take to ensure success in this project? We're grappling with how to initiate, execute, and conclude it.

I welcome any comments or insights. Thanks for your time and assistance!",2024-03-22 15:29:39
1bku2m7,How can I get the context information like 'logical_date' within task?,"I have an airflow DAG running with BashOperators:

    default_args = {
        'owner': 'jack',
        'start_date': datetime(2024, 3, 1, 0, 0, 0, 0)
    }
    
    with DAG(
        'testing_dag_1209', # Name of DAG
        description = 'An example of DAG_1209', # Description shown in UI
        schedule_interval='0 * * * *',  # Job would start at start_date + schedule_interval
        default_args=default_args,
        tags = [""Testing_dag_1209""]
    ) as dag:
        start = EmptyOperator(task_id = 'start')
        task_11 = BashOperator(task_id = 'task_11', bash_command = '{} {} --logical_date {{ params.logical_date }}'.format(stock_env_python_dir, task),
                               params = {'logical_date':'{{ds}}'})
        end = EmptyOperator(task_id = 'end')
        
        start >> task_1 >> task_2 >> end 

I run a simple python script test\_task1.py which has content:

    from airflow.decorators import dag, task
    from airflow.operators.python import get_current_context
    
    
    
    @task
    def get_date(**kwargs):
      print(""Show current keyword args: "")
      print('Logical date: ', kwargs)
      return kwargs
    
    
    
    print(""Start test_task1.py..."")
    
    date = get_date()
    print(date)



The python script may execute every hour from 2024/3/1 until today.

What I want to do is to print the 'logical\_date' which is the scheduled date for execution from 2024/3/1 until today in the log.



I'm checked the airflow document about the context access, however it doesn't work as expected, the output log message didn't show anything in the Console print in test\_task1.py.



1. Can anyone give me some advice what is the simplest way to print airflow context within the task?
2. Although I have another question, is it possible to print airflow context outside of tasks? Like printing the context as 

&#8203;

    with DAG( 'testing_dag_1209', # Name of DAG description = 'An example of DAG_1209', # Description shown in UI schedule_interval='0 * * * *',  # Job would start at start_date + schedule_interval default_args=default_args, tags = [""Testing_dag_1209""] ) 
    as dag: 
    
      print(""Current logical date is: {{ ds }}"") 
      start = EmptyOperator(task_id = 'start')  
      task_11 = BashOperator(task_id = 'task_11', bash_command = '{} {} --logical_date {{ params.logical_date }}'.format(stock_env_python_dir, task),                         params = {'logical_date':'{{ds}}'})  
      end = EmptyOperator(task_id = 'end')    
      start >> task_1 >> task_2 >> end 



Thank you for all the advice!

",2024-03-22 08:07:38
1bkd2i1,Guidance for which certs/tech to begin with. ,"I’ve worked on oracle databases in the early 2010s as a hands on solution engineer writing backend code, integrations and etc. Evetuwlly I made a move into program management and it’s not as interesting or fulfilling as I thought it would be. 
In the past I’ve passed AWS solution architect certs but they are expired at the moment. I want to get back into hands on solution architecture around data engineering. Would anyone be kind enough to give me pointers to certs , techs to begin and learn? 

Since I want to find a job in this field, I would love to get going. I was thinking of starting with aws data engineering cert followed by something in azure. 
",2024-03-21 18:06:13
1bkaj0b,NEED FEEDBACK: Data Analyst to Data Engineer (With an Arts Degree),"Hi everyone! Just a quick question and needing some feedback on my current situation in life lol.

I have an Arts Degree, and for the past 3 years I've been working as a Data Analyst, mainly using PowerBI but not that great at SQL or even Python. But I am familiar with my way around the languages. I usually find myself just searching up solutions online. But have been doing a really good job since.

I want to make the transition as a Data Engineer, so I decided to enroll in  [IBM Data Engineering Professional Certificate](https://www.coursera.org/professional-certificates/ibm-data-engineer) using Coursera. I'm really liking the course so far. But I was thinking after this certificate - doing these certificates:

1. AWS Data Engineer Associate Certificate
2. AWS Certified Machine Learning - Specialty
3. AWS Cloud Solutions Architect Professional Certificate
4. Python Certificate

I'm only 24 years old and I have a lot of time to put my heart and soul into learning everything into becoming a data engineer. I'm also planning on to display all the side projects and case studies I'm interested in as a Data Engineer on my LinkedIn.

I would really like some feedback on this is a good plan to follow, or what I should do differently?

TLDR; Will these 4 certificates and showing my projects help me in my career as a Data Engineer?",2024-03-21 16:23:00
1bk2e5u,Auto labellisation for AI / Right choice of database/datalake,"Hello everyone,

I'm a junior data engineer and my start-up company is scrapping a large amount of imagew to train some IAs on it. the problem come to put the right label on it. A label is like a ""tag"" you put next to the image so the algorithm can classify the image he is looking at basically. Our data is VERY specific so it is really hard at the moment to use a generic model to make an Image2Text helping us classifying our data due to its high specification. Some of our images are actually labelised but most of them aren't.

I have then 2 majors questions:

&#x200B;

**How can I auto labellised ?**

As you can think auto labellisation is where we want to tend and I'm seeing some potential solutions:

\- Use a vector database and tag the non labelised image with the labelised image it is the nearest (with some limit of course)

\- Train an Image2Text and label the non labellised image (depending of % of probability)

\- something else ?

&#x200B;

**Which database or/and datalake to choose ?**

As I say we scrap a lot of image and store them at the moment in a MongoDB but we are at almost 30 Terabytes of images and are thinking of moving into a more efficient way of storage. Here some of my ideas:

&#x200B;

\- Dump the raw images into a snowflake, clean the data and put it in a MongoDB 

\- Use of a vector database like Milvus ?

\_ something else ?

&#x200B;

Thanks for reading. Sorry for my broken english and if all my explanations seem a bit clunky. I'm just a passionate junior trying to figure out !",2024-03-21 09:19:42
1biwk9j,Data Warehouses vs Data Lakes,N/A,2024-03-19 21:55:20
1big613,Enhancing Data Operations: Streamlining Processes and Implementing Innovations," As the sole data analyst within the finance team, I rely on Alteryx for conducting ETL operations, culminating in the loading of data into MS-SQL and the creation of CSV files as output. Across our operations, we manage 33 distinct ETL workflows, each meticulously tailored to accommodate various vendor datasets, each with its own set of independent tables. Despite this diversity, all data eventually coalesces into a standardized master table.

Our infrastructure is strictly on-premise, devoid of any cloud integration, and notably lacks a Data Warehouse (DWH), relying solely on standalone tables.

The data flow follows this pattern:

* txt files --> Alteryx --> Source tables (33)
* Alteryx --> Masterdata table (1)
* Alteryx --> Output tables (24) --> CSVs

While our current process meets requirements adequately, the burgeoning volume of data is escalating, although it hasn't yet reached the realm of big data.

I am keen to implement mechanisms for data validation notifications, alerts, and sharing succinct product summaries with the team. Furthermore, I am eager to optimize and fortify our existing data pipeline. Any insights or recommendations for enhancements would be greatly valued. I'm also open to acquiring proficiency in new technologies if they are deemed beneficial for our operations.",2024-03-19 09:28:38
1bi8aot,HS Code Classification Help,"We're currently knee-deep in a massive data processing project and could really use some expert advice. Our team is tasked with classifying millions of product descriptions into HS codes, and let's just say, it's proving to be quite the challenge.

We've already looked into Transiteo, Zonos, and Quickcode but their pricing is out of our budget and they offer a lot of features we don't really need. We're on the hunt for some classification-focused solutions.

Anyone who has leads or has worked on similar projects previously? Would love to hear from you, cheers!",2024-03-19 01:31:56
1bi1mny,"What is the most effective way to create, implement, validate, and document logging specifications?","**TL;DR**
What processes or tools do you use to create logging specs, get the specs implemented, validate their implementation, and then maintain documentation for that logging?

**Background**

As a Data Engineer, I spend a lot of time with my software engineers asking them to implement new or modify existing logging. In order to do so, I provide a logging spec to them with instructions on what I'm looking for.  

**My Process**

My company currently uses unstructured google docs to collaborate on the spec, but then there's no way to validate the implementation and google docs are easily lost over time and documentation for these events become non-existent.",2024-03-18 20:53:58
1bhwf27,About Meltano,"Hi,  
I’m starting implementing a data engineering project for my company. I considered Meltano for my extract/load pipelines.
I started with a very easy case, extract data from MySQL and load it into Postgres. 
For it, I am using tap-mysql and target-postgres. However the job seems (very) long as it takes more than 6 minutes for 100MB (500k rows, 40 columns or something like that). The query takes 0.004 seconds to be executed and 3 secs to be fetched on MySQL Workbench. I also tried to load it into a JSON only (target-jsonl), it takes more than 3 minutes. 

Moreover, when I am launching the job, the application that the database MySQL is based on crashes and stays down for the duration of the job.
Is there anything I am missing ? Like parameters for batches or anything that could explain this behaviour? ",2024-03-18 17:27:28
1beluak,No More Surprises in Snowflake with Resource Monitors,N/A,2024-03-14 14:00:12
1bdu22d,Need help with SharePoint Online list attachment refresh issue in PowerBI Desktop,"I've been encountering a problem lately while refreshing a PowerBI Desktop report from a SharePoint Online list. The refresh gets stuck indefinitely, even after leaving it running for an hour with a stable connection. This issue arose as my SharePoint list grew substantially (around 2k items), although the online report continued to refresh successfully through the service.

After investigation, I discovered that the problem lies with attachments in some of my SharePoint list items. When expanding the AttachmentFiles column in the SP online list connector to retrieve the URL of the first attachment, the refresh becomes sluggish. Removing this column resolves the issue, but I need to access the attachment URLs for my analysis.

Has anyone encountered a similar issue or have advice on how to pull attachment URLs from a SharePoint list? The conventional SP connector doesn't seem to provide a solution. Appreciate any insights or suggestions!",2024-03-13 15:19:11
1bdodd2,Stop Hiring Senior Data Consultants: Leverage Expertise Instead,N/A,2024-03-13 10:41:44
1bbcqvo,Any data engineers in Canada working in the US?,Curious if anyone is in that situation? Or do you have to move to the US? ,2024-03-10 15:19:48
1b85aca,Manage ELT pipelines with code using Terraform’s Airbyte provider,N/A,2024-03-06 17:16:51
1b6wpnk,GreenGauge Analytics: Competitive Edge for Eco-Friendly D2C E-Commerce Businesses," Hi everyone,

I’ve been working on something that’s really important to me, and I think it could be valuable to you too. It’s called GreenGauge Analytics. The idea came from my own struggles and desires to make my e-commerce operations more sustainable without just shooting in the dark. I wanted to make decisions based on what consumers really care about when it comes to sustainability, not just what we think they do.

So, I started building a platform to provide actionable insights into consumer sentiment on sustainability and the latest trends that are shaping the e-commerce landscape. But here’s the thing - it’s not just about what I believe is needed. This is about creating something that truly serves our community of eco-conscious brands and consumers.

I’m reaching out because I need your help. Before we go full steam ahead, I want to make sure we’re on the right track. If you have a moment, I’d really appreciate it if you could sign up to stay in the loop and maybe share your thoughts through a short survey. Your feedback would mean the world to me and really help shape this into something that can genuinely support businesses like yours in making a positive impact.

You can sign up [here](https://123carmartin321.wixsite.com/greengaugeanalytics). Thank you so much for your time and for considering being a part of this journey. Let’s make sustainability at the core of e-commerce together.

Looking forward to hearing from you,

GreenGauge Analytics",2024-03-05 04:45:20
1brb769,Short intro to Python,"Hey folks,

I plan on delivering lectures for Python. I know there are already A LOT of python videos out there so with my channel I intend to keep it short, concise and quick to the point so no time is wasted and you can hit the ground running without feeling overwhelmed. 

Here is the first video: [https://youtu.be/rDBllIp1zmM](https://youtu.be/rDBllIp1zmM)

In this 11 minutes video I've covered  
1. Variables  
2. Data Types  
3. Print statements  
4. Casting  
5. Commenting  
6. Input  
7. Comparison operators  
8. If condition  
9. For Loop  
10. While Loop",2024-03-30 06:31:01
1bonkwg,Is it worth pursuing DE as a new grad?,"Current junior in college interested in DE. I’ve done some basic ML work in personal projects and really enjoy it (plus I love Python and SQL).  Furthermore, my internship this summer involves an ML + data pipeline project. I was thinking of trying to leverage this internship + my projects + enjoyment of DE tasks/tools into a new grad DE job but the more and more I look around this sub the more it seems like a new grad job isn’t very likely? I was thinking about really cranking out some data projects this summer for this new grad job but I’m not sure if I’d be better off just pushing out traditional SWE projects if a DE job is really this hard to come by?",2024-03-27 00:02:30
1bobzpk,How do I implement (ABAC) based data governance using DATAHUB and RANGER,"I want to control which users can acces which columns, example, price should only be visible to billing team and address should only be visible to delivery team.
This is just a super simplified scenario, I could have tables with 100s of columns, and each is accessible by different group of people (except primary key)
We use trino, hdfs, ranger to enforce RBAC. 
Ranger has native abac integration with apache atlas, but as we're heavily uaing datahub, im looking to implement using that. ",2024-03-26 16:23:18
1bo9na7,How do you leverage LLM in your Self-service BI stack?,"Hi folks,

As the hype around AI and LLM continues to grow, I'm genuinely curious about how you are currently utilizing LLM and whether it truly helps in enabling self-service BI.

I want to hear from you about your experience / opinion with LLM. Are you still planning to implement it or have you already implemented it? Does it meet your expectations or have you encountered any challenges?

Thank you.

[View Poll](https://www.reddit.com/poll/1bo9na7)",2024-03-26 14:44:46
1bo628t,"44 Best Resources to learn Data Engineering (YouTube, Courses)",N/A,2024-03-26 11:56:55
1bkkm3f,Vinyl - the fastest way to build AI data products,"Hi Reddit!

Vinyl is a next generation platform for building AI-powered data products. We’ve been using Vinyl internally for a few upcoming products and it has changed the game for us.Here are a few reasons why Vinyl is special:

&#x200B;

🐍 Just write Python. Don’t worry about complicated SQL, a custom new DSL or running docker containers. Just pip install vinyl and decorate a few functions and you’re off to the races.

&#x200B;

✨ Run queries anywhere. Run the same code to query and transform data in an S3 bucket, Snowflake, BigQuery or csv or json files. Vinyl is dialect agnostic. You can even join across data sources.

&#x200B;

📊 A built-in semantic and metrics layer. Auto generate dimensions, joins and time buckets without having to wrangle complex SQL. Define models and metrics and let Vinyl take care of the rest.

&#x200B;

💻 Embeddable. Embed models and metrics inside of BI tools, LLMs and dashboards using a vanilla Postgres connection or HTTP requestYou can try out Vinyl today here: [https://docs.turntable.so/quickstart](https://docs.turntable.so/quickstart)

I'd love to hear what you think!

https://reddit.com/link/1bkkm3f/video/kijekhsisrpc1/player

&#x200B;

&#x200B;",2024-03-21 23:15:46
1bjhrut,Unlock Career Success: Master the Art of Explaining Your Data Engineering Role,N/A,2024-03-20 16:34:07
1bfzjix,Question ,"How are bigdata and data engineering related, is a bigdata developer can be called a Data Engineer ",2024-03-16 06:12:10
1begno5,Google Cloud Composer or Apache Airflow,"From your experience, what are the biggest differences between both and have you encountered any challenges with Google Cloud Composer ?  

Thanks",2024-03-14 08:50:41
1b9ha96,"Is data project really difficult compared to normal software development project, please help","Is data project really difficult compared to normal software development, i recently moved to data domain , so this is enhanment project which was already build we need to fix the issues and bugs like why data is not populating , why this values are missing, we neeed to back track a lot through millions of data this is so much stress and difficult, I like analysing but here there are many different filter conditions used so if we remove one we may get answer , since our project does not have any product owners or why this filter condition is been used no one can answer 

I want to know is this how data projects are or is it because this project is enhancement kind of that why I am facing this much issue, i definitely know if this is from scratch i would be knowing in and out of every code and fetaures, kindly help",2024-03-08 06:24:51
1b7fi8b,A Community for Event-Driven Analytics,"Hey everyone, I've built a community for data people to discuss/discover the power of event data applied across multiple industries!

&#x200B;

Check it out, it's absolutely free:  
[https://join.slack.com/t/mavisaicommunity/shared\_invite/zt-2dglnb8if-H8YsIpK1feO81WV30JnmxQ](https://join.slack.com/t/mavisaicommunity/shared_invite/zt-2dglnb8if-H8YsIpK1feO81WV30JnmxQ) ",2024-03-05 20:23:59
1brjnlb,what is the difference between spark sql and pyspark?,title,2024-03-30 14:51:00
1bpalf6,Blockchain Decentralisation,"Hi folks,

So generally, on viewing a few youtube videos on blockchain, 
I got following insights

1. It is decentralised, That is, it's does not contain a single centralised system where all data is stored

2. Every user in a block chain network, has copy of all the blocks, which they can decipher if they have the right key

Questions:
1. Here Network is not a centralised Storage or Is it like a Kafka Topic from which the consumers(here users) consume

2. When users are decipher or accessing the blocks, where are they storing it? Like in their phones or other devices via  some network provided app/software which runs on the said device?

3. What happens if a network fails?

Thanks in Advance....",2024-03-27 19:28:34
1bp1kik,How to convert csv to ORC on Windows?,"Hi, so I'm doing benchmarking on a sample data in csv, and for that purpose I convert them into different file formats to see how much storage space they take up and how they behave. I managed to convert csv into Parquet and Avro using Python libraries, but I can't seem to make ORC conversion work :( I tried with pyarrows but apprantly it doesn't work on Windows. When I try with pyorc library I get the following error:

    Traceback (most recent call last):
      File ""c:\Projects\big-data-file-formats\scipt_orc.py"", line 11, in <module>
        with pyorc.Writer('dummy_data.orc', schema='struct<ID:int, Name:string, Age:int, Email:string, Address:string>') as writer:
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
      File ""C:\Users\szymk\AppData\Local\Programs\Python\Python312\Lib\site-packages\pyorc\writer.py"", line 37, in __init__
        schema = TypeDescription.from_string(schema)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
      File ""C:\Users\szymk\AppData\Local\Programs\Python\Python312\Lib\site-packages\pyorc\typedescription.py"", line 52, in from_string
        return _schema_from_string(schema)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
    ValueError: Missing field name.
    PS C:\Projects\big-data-file-formats

But my schema seems okay, here is my csv:

&#x200B;

|ID|Name|Age|Email|Address|
|:-|:-|:-|:-|:-|
|34389|David Black|74|chloe76@example.net|511 Gonzalez Shore|

And here is my code:

    import pandas as pd
    import pyorc
    
    # Load the CSV file using pandas
    df = pd.read_csv('dummy_data.csv')
    
    # Convert the DataFrame to a list of dictionaries
    data = df.to_dict(orient='records')
    
    # Write the data to the ORC file using pyorc
    with pyorc.Writer('dummy_data.orc', schema='struct<ID:int, Name:string, Age:int, Email:string, Address:string>') as writer:
        for row in data:
            writer.write(row)

How do you convert to ORC?",2024-03-27 13:13:52
1bif7cy,Need Help in choosing certification for DATA Engineering Role,"hi guys, need your help in choosing!   
1. [AI-102 Designing and Implementing a Microsoft Azure AI Solution](https://learn.microsoft.com/en-us/credentials/certifications/azure-ai-engineer/#certification-exams?ocid=aisc24_cloudskillschallenge_webpage_cnl)  
2. [AI-900 Microsoft Azure AI Fundamentals](https://learn.microsoft.com/en-us/credentials/certifications/exams/ai-900/?ocid=aisc24_cloudskillschallenge_webpage_cnl)  
3. [DP-600 Implementing Analytics Solutions Using Microsoft Fabric](https://learn.microsoft.com/en-us/credentials/certifications/exams/dp-600/?ocid=aisc24_cloudskillschallenge_webpage_cnl)  
4. [DP-100 Designing and Implementing a Data Science Soluition on Azure](https://learn.microsoft.com/en-us/credentials/certifications/exams/dp-100/?ocid=aisc24_cloudskillschallenge_webpage_cnl) certifications  
which one is more relevant to Data engineer role?

[View Poll](https://www.reddit.com/poll/1bif7cy)",2024-03-19 08:15:49
1bcwzob,The Inevitability of AI in Every Facet of Your Business: 11 Deep Thoughts With Lasting Impact,N/A,2024-03-12 13:15:46
1bchbvu,Want to get out of truck driving into DE Role! ,Is getting into DE role realistic without previous tech experiance! And if so do certs actually help on a application  when applying for jobs? Need real info ( nothing negetive),2024-03-11 23:01:31
1bcatsa,Which DE stack ?,"Hello guys,

I am planning to move to DE, in order to start learning, I’d like to have some advices. As I am currently working with Microsoft PowerBi, should I follow the Azure DE or go with the on premise stack, like the stacks that are in the Zoombootcamp ?

Thanks",2024-03-11 18:48:49
1bc3oqi,Clarity about interviews,"After 2.9 years of experience in data engineering at a firm, I sat for a coding round for senior data engineer.

To my surprise, I found out that coding rounds mean SQL rounds for data engineer profile. 
I was all prepared for coding questions and it all went under the drain.

I ended the call within 15 minutes as there was no point of continuing. Talked to HR as well about the misunderstanding. I don't have much hope for a rescheduled round though.

Is this a general norm or the company was special enough to test in this manner?
Ideally technical rounds can have SQL which is a basic requirement of a DE profile but coding rounds should have coding questions only.

I have a call with another firm tomorrow so going through all SQL concepts tonight. Wish me luck.",2024-03-11 13:51:55
1b93023,Why Starburst’s Icehouse Is A Bad Bet,N/A,2024-03-07 19:29:15
1bqgvk0,How to build my own data lake setup?,"Sorry if it’s been discussed already,

I think big data tools are there for a purpose, but they are not for many of us. And with recent innovations we have so many tools that can help us to build the same setup, without really needing to use the expensive and complex solutions, maybe, maybe not…

For an example,
- Apache Arrow : has awesome ecosystem to deal with data, whether it be in memory or thru wire
- duckdb : has better features to process big-enough data locally
- Apache iceberg : data table format for data lake
- delta lake : open-source storage format
- dagstet : orchestration tool which is highly customizable and also support these kind of pattern
- Kafka : streaming solution
- Python : to build everything (maybe rust or whatever)
- etc

It should be possible to build an ingestion layer that deals with data catalog and data lake. (Using streaming or orchestration tool with iceberg or delta lake)

The query layer - using Apache arrow to read only relevant data from data lake process them in duckdb, or maybe if it needs to process bigger data then using any other existing query engine.

I might be missing something, but feel free to share your ideas or if you’re doing anything like this. I know that these existing tools bring in so many features and stuffs, but should not we know how to do this from scratch and even a lite version of it. 

Because if you think about, in software engineering, we have frameworks to do stuff but sometimes we write code using just standard library, to keep it simple and do the job also keeping a space to make changes if needed. I think data engineering also should be like that, we should not use (I am not talking about big tech data engineering projects…) these tools 
straightaway, at least we should know how to do it manually…

I understand the complexity, of distributed systems and the infrastructure behind it. But my point is not to reimplement the weel, but also same time not depending too much on them.
",2024-03-29 04:52:35
1bp3am2,5 Exclusive Soft Skill Lessons You Can Learn From Senior Data Engineers,N/A,2024-03-27 14:31:11
1bp0w76,How I use Gen AI as a Data Engineer,N/A,2024-03-27 12:41:18
1bioqxr,Isn't data engineering job market tough right now?,I have 2 years of experiences in it and I find difficult to change a company within Europe. What are your experiences/opinions?,2024-03-19 16:41:04
1bcmrya,Airflow is so slow with one only dag!,"I don’t get it, I have a pretty good internet connection. I have one DAG in the AirFlow and it is so slow, what could it be??",2024-03-12 03:03:12
1bafeap,questions regarding problems faced by data engineers,"had a few questions pertaining to what problems have you encountered persistently ?  


Do you struggle with ensuring data accuracy across different projects?

&#x200B;

How often do you encounter issues with outdated or inconsistent data?

&#x200B;

Have data quality problems ever impacted your ability to deliver projects successfully?

&#x200B;

&#x200B;",2024-03-09 11:10:57
1bpfaox,need advice,"Hello, I have been an engineer responsible for machining for 2-3 years. I really enjoy making production analyzes and updating large Excel data with the help of Powerbi. That's why I received a 90-hour introductory training in data science and I want to improve myself in this field. Do the production field and this data science business intersect at some point? Or do I need to make a career change?",2024-03-27 22:37:41
1bhk49v,Jinbaflow: A New GPT-Empowered Data Analysis Tool,"Hi everyone, I’m the founder of Jinbaflow, a new low code workflow tool we’ve been working on with the goal of turning anyone into a data analyst. We want to allow anyone to use English instructions and AI in order to easily transform, analyze, and visualize data. The idea is to keep it super straightforward with a flow-style interface so you can see how the data connects and flows together.

We want to add more AI-empowered tools beyond the code generation to allow you to focus less on coding and more on the data. Currently, we're using GPT-4 and we're super happy with the results and how good it is at generating Pandas code for us.

We would love to get some early feedback, so if you’re interested please sign up here!

https://useflow.jinba.ai/",2024-03-18 06:25:51
1b84xj4,Do I need prior experience to be  a Data Engineer?,"Hello yall. I’m 16 years old I go to a Vocational High School. I’m currently in the I.T shop and I’ve been interested in Data Engineering. It seems pretty interesting and new. Data is very cool to me. I researched it some more that data engineer is a non entry level job. Is that true? Do you need to be a data analyst, data scientist prior to becoming a data engineer? Or can you get a degree in data and become one right out of college? ",2024-03-06 17:03:30
1bpjhi9,Looking for like minded people to Learn DE,"Hi, 
I am looking for like minded people who can start working on some personal DE Projects and build portfolio. 

Please DM me so we can collaborate and learn. 

Thank you. ",2024-03-28 01:41:10
1beqpsh,A poor attempt from Starburst to create a new category by merely re-naming existing concepts.,"**A poor attempt from Starburst to create a new category by merely re-naming existing concepts.**

* What is wrong with \`Data Lakehouse\` name? Why confuse people just making a new name for the 'Data Lakehouse' without offering clear, distinct value.
* Inventing a new name does not equate to creating a new category.
* Exclusively tying the 'Open Lakehouse' to Trino undermines its promise of openness.

https://www.starburst.io/blog/icehouse-open-lakehouse/",2024-03-14 17:27:23
1b7a0yx,Data teams should be the first to know about data issues. Metaplane raised $13.8M to automate that.,N/A,2024-03-05 16:50:11
1bawk3l,Roles in the modern data team,"We all know that good quality data is needed for analytics, machine learning and artificial intelligence to really be useful. 

I think the modern data team resembles a superhero team up of The Key Three (data owner, steward and architect) who implement governance and management with the analytics and engineering team known as The Core Four (data engineer, analytics engineer, data analyst, machine learning to engineer). 

Do you agree? 
Wha about data science? What about business intelligence? I wrote a post here... Check it out of interested 
https://medium.com/@robdoesdata/what-roles-exist-in-the-modern-data-team-571f934f78c1",2024-03-10 00:15:24
1b74vv1,"Azure, snowflake vs AWS","Hai, I am working on azure data engineer ,, data  bricks and one project on  snowflake.. now my company wants me to work on AWS .. I rejected the proposal one time but again they came with  new project and it's on AWS.. I am concentrating more on GEN AI so not willing to deviate from this .. any inputs .. is it really helps my career if I take this project or How can I. Say no because I have already informed them that I am looking for projects in azure and snowflake ..",2024-03-05 13:13:14
1b4kxs0,What data engineering content do you miss on social media?,I would love to create content about data engineering but do not know what you would like or miss. :) ,2024-03-02 10:06:06
1bjkinu,Will AI replace data engineering?,"Here’s my take
",2024-03-20 18:26:27
1bj3ae6,Real time analytics to give me data superpowers. Tool feasibility?,"Hey DE community, I'm weighing up the feasibility of a general purpose analytics tool to give its users data superpowers in real time. 

**I'm wondering Does a tool like this exist today? If yes then would love to see it! If not why not? What are the engineering blockers to building it?**

**Imagine this scenario:** I'm a sales rep pitching to a customer while getting a stream of real time data insights based on the context of the meeting, their profile, the questions asked, and the direction the meeting is going. I'm able to answer any question they have backed by data (e.g. niche competitors, related market sizes, performance benchmarks etc..) and I'm coming across like that dude in the move Limitless ;)",2024-03-20 02:50:44
18yq23g,Do we really need to perform dimensional modeling?,"Tasked with leading a BI/BW team for a mid sized organization.  Plenty of $$$, moderate amount of data.  Many data sources (100+). I am not an experienced data engineer, but I did stay at a Holiday Inn Express last night.  

Doing my research I have read up on Kimball and dimensional modeling and data normalization.  Sounds fucking painful.

Me and my team are Pretty good with Power BI.  Fucking love the Transform data within Power BI.  Using Azure Synapse/ Data Lake for central repository.

The dimensional modeling and normalization seems to be a massive lift.  Feels like we could spend years working on that - meanwhile the rest of the organization is wondering what the fuck we are doing?

I can take two dimensional tables from various sources and just model them in PowerBI.  Do it 1 project at a time.  Can turn a project around in just a few weeks.  People can see we are producing some dashboards in the near future.

With MPP processing power and columnar data bases - am I crazy to say fuck Kimball and fuck OLAP cubes?  I just want to store denormalized tables in the Datalake and model them in PowerBi as needed for individual projects.

Part of me thinks this might be short sighted, but I don’t feel like I have years available to work through fucking Kimball methods for 100+ data sources.

Feedback / suggestions are appreciated.",2024-01-04 22:31:50
ua4qe7,101 SQLFluff,Are you a data guy working with SQL and are you tired of people nitpicking when doing code reviews? Did your energy drain by those that had written comments in your Pull Request talking about comma positions? Take a look at my new post: [https://jungleboy.netlify.app/posts/sql\_fluff\_setup/](https://jungleboy.netlify.app/posts/sql_fluff_setup/),2022-04-23 12:49:36
162ig5h,Why I don't like SQL,"The reason I don't like SQL over something like Pandas is because in Pandas/Python I can break down the complex transformations in small simpler components and they are easy to interact and debug with. In SQL my only option is to write stored procedures and yes in that I can also do the same but I think the ecosystem of interacting and debugging with data step by step is not as good as python. And it is something that we should have. Recently I was exploring snowpark and I felt it is a step in a right direction. It allows to me to write my query in multiple python statements and then it will transpile the whole thing into an SQL query.  


Generally in my mind I assume the tables/dataframes as 2 dimensional structures and Pandas API give much more intuitive and wide variety of constructs to explore that structure rather than SQL. What do you guys think?",2023-08-27 06:12:38
13u24ha,Boto3 still relevant in 2023?,"I have been using Boto3 to orchestrate data pipelines, interacting with s3, glue, redshift etc. Have absolutely loved how robust it is.
I was thinking of expanding/depending my knowledge of boto3 however it seems as though the industry has other priorities.
Is boto3 going to be relevant 2023 and onwards or should I move on to other tools.
If so, which ones would you recommend?",2023-05-28 14:44:20
18lt3d0,DBT and ChatGPT,"As we’re starting to use DBT more, and are using it for codifying our business rules, I’m wondering if anyone’s had success training GPT on a DBT project. My initial attempts were lack luster. GPT seemed to know a fair amount about DBT, and of SQL, but struggled to be of much use when I trained it on a DBT project (zipped, uploaded to OpenAI GPT4).  My next step is to create a prep doc with some basic support/prompt engineering. For example, our mart “Legal First Name” and “Preferred First Name” fields come from different source tables, have different staging models. I asked GPT to tell me the difference between the two, as well as derive what SQL each would be off of source tables. Another approach I’m going to try is to do a DBT run materializing as much as I can ephemerally, and see if that helps get useful responses. If this marriage of well-designed DBT project/models and GPT/LLM can be cracked, I think (at least our org) will enter a new level of accessible data literacy.",2023-12-19 04:24:01
16jfsor,Are DEng Software engineers?,"Read someone’s post earlier talking about an aspect of data engineering being software engineering. This is the first time I’ve ever heard someone in data engineering consider themselves in the same group. 

Is this even a thing?

I just don’t see them as the same, mainly because most software engineers build applications/software. And most data engineers build data solutions (predominantly data pipelines).  Software Eng, front end, middleware development. Data - backend, and I wouldn’t class writing a power bi report as SE even if it is kind of front end. 

I get the whole following methodologies that software engineers have been following for decades. The whole DevOps thing, that has also been in data for a while just not wide spread (I first used CICD back in 2010). But because I know this doesn’t make me a software engineer. 

There was a distinction made that analyst weren’t considered software engineers. I think there was a thinking they don’t follow DevOps, but lots of them do. 

Very odd. My take is (and none are software engineers):

- The platform. Cloud engineer or a data platform engineer. Probably lots of terraform. 

- The pipelines. Data engineers. Sql, python…

- Data presentation. Data analytics engineer. DAX, modelling

But I’m old, been doing this 20 years, so feel free to educate me in my misunderstanding",2023-09-15 15:03:09
167lv2m,"Is the Mac worthless, or is it me? (Rant)","Hi, I was scouring this sub reddit to try and find  the best laptop for data engineering a few months ago. I use a point and click ETL program in my day to day, and wanted to learn how to build data pipelines by writing code. The top rated one I could find was the Macbook, so i brought the Mac air M2. I come from a windows only background and was excited to learn a new system.

&#x200B;

I've had the mac for three months and am struggling to get through basic SQL tutorials using python in that time. It seems like every library I have try to use isn't compatible with the Mac chips (Throws a random error). sqlalchemy/pyodbc just straight up won't work, and I can't find any other way to connect python to SQL. This thing has been an all around nightmare. I tried to stand up a local version of SSMS which I managed through docker, but when I tried to use a .bak file to load data into it, it doesn't have access to see my hardrive so that's another dead end.

I feel like every time I try to start a new tutorial, or do a basic function that i'd have no issue with on windows- I have to do some ridiculous workaround to get it running on Mac, and then apply fix after fix after fix as the ever-present error show themselves.

&#x200B;

I guess my question is, does anyone have any Mac specific, data engineering tutorials? I'm at a bit of a loss with this thing and it's starting to gather dust. If anyone can recommend material to set this thing up too. It's entirely I botched the setup when installing the interpreters.

&#x200B;

TIA

\-Definitely not a windows employee

\-Also definitely not someone that spent 30 minutes trying to find the /usr/local folder

&#x200B;

&#x200B;",2023-09-01 22:57:17
vn2qgi,"Drop the ""master"" in MDM? - W3C Inclusiveness","I'd like to get some feedback as to what others are or are not doing in regards to the political correctness occurring in terms used -- noted by the W3C - -  


  

**The World Wide Web Consortium on Inclusive Terminology**

The [World Wide Web Consortium](https://nam11.safelinks.protection.outlook.com/?url=https%3A%2F%2Fwww.w3.org%2FConsortium%2Fmission&data=05%7C01%7C%7Caca3b8299a684736c62b08da55f666a6%7Ccdb191c8fc034343aead2808b21fd513%7C0%7C0%7C637916814162886907%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C3000%7C%7C%7C&sdata=gBjYgQ48FSgVrBbMPtDQlGiXm%2BrZiuuCfcAsoTkGlj0%3D&reserved=0) (W3C) is the primary standards body for the web, currently led by none other than Tim Berners-Lee, creator of the web. The W3C have identified a number of American English terms often used in the global technology sector (I recall some of these from my college text books!) that derive from original uses which were explicitly not inclusive. That list includes some of the terms below, to which I’ve added several that BCD team members have flagged up to me in recent months. I ask you to join me in actively and consciously eliminating them from our vocabulary:

  

|**Term   to Avoid** |**Possible   Alternative(s)**   |
|:-|:-|
|master|main, primary|
|slave|replica|
|whitelist|allowlist|
|blacklist|dennylist|
|grandfather|legacy|
|off the reservation|counterproductive, outside parameters|
|manhours, manpower|FTE hours, level of effort|
|sanity check|coherence check|",2022-06-29 00:54:53
y81le9,Will Rust Take over Data Engineering? 🦀,N/A,2022-10-19 12:36:18
18nzg9f,Differences between just python and pySpark? What is pySpark even and do I need it?,"Hi guys, I know this question has been asked a ton of times and I have read them all but still struggling to understand. Maybe if I explain my situation it will help me more so I am making this. I “learned” pySpark as part of my masters‘ cloud computing class but I didn’t really. My instructor have us using it on Google Colab and it always take so much longer than just doing the same thing locally.  is this a Google Colab problem or Spark’s. I don’t know what pySpark is really. It isn’t the map reduce or the lambda functions either cuz I can do those on my Jupiter notebook with python too. I guess you can manipulate the data with SQL syntax on Spark but then I find it easier to just use panda. So what is pySpark? Am I having trouble differentiating python and pySpark because pyspark works behind the curtain only to supposedly makes data processing faster? And is it just that i haven’t use big enough data sets to really see this kick in? How big does data need to be? My course really did not do a good job of showing this. Like actually comparing two methods on the same data set and same task.",2023-12-21 22:40:36
17983kk,"Dbt is to SQL what Django is to python (Symfony to php, etc...)","I've been approaching, reading about, and trying to use dbt, and I came to the conclusion that it is a framework that eases the writing of sql statements which purpose is to make calculations (aka transformations) on relational tables. So I naturally did the analogy dbt<>sql, Django<>python, symfony<>php.

Do you agree with this analogy ?",2023-10-16 15:04:31
zlecvg,chatgpt anxiety,"Hey all,

I know there have already been a few posts on this- but for those of us just getting started in software/data engineering, the ability for chatgpt to write code is pretty scary...

What do you all think the impact to Data Engr will be?

I know it won't completely replace us, but do you foresee a big reduction in hiring?",2022-12-14 02:06:32
17vacxc,Why Cloud Data Warehouses Are Too Expensive For Emerging Data Requirements,N/A,2023-11-14 19:40:10
1atcwjw,Transitioning from Data Engineering in Mexico to the US Job Market,"Hi everyone,

I recently graduated with a degree in Software Engineering from a top university in Mexico, and I've been working as a data engineer for three years now at a transitional company in my city. My ultimate career goal is to break into the US job market, either by relocating to a city in the US or by working remotely for a US-based company from Mexico.

I'm eager to enhance my career prospects, and I'm considering pursuing a master's degree in either Data Science or Statistics in the US. However, I'm torn between whether this would be a worthwhile investment of both time and money, or if I could achieve my goals through self-study and professional development.

What are your thoughts on this? Do you believe pursuing a master's degree in the US would significantly improve my chances of entering the US job market, or would it be more beneficial to pursue other avenues of skill development independently?

I'd greatly appreciate any insights or advice you can offer. Thank you in advance for your help!",2024-02-17 21:31:17
phsexf,How to open a data with TB's of data,"I'm working with python, and predominantly pandas.

Let's say i have db/csv or other format with dozen/hundreds of TB's of data.

Let's say pandas cannot handle it, How to open it?",2021-09-04 14:21:10
12id86y,This is so far the best meme i have seen since i was born,https://preview.redd.it/meat8x5du7ta1.png?width=473&format=png&auto=webp&s=ed5b750c9fce2a21dde49c6c3fca36615f808194,2023-04-11 08:28:53
rc25d9,Can I be an extrovert and still be a data engineer?,"I am thinking about transitioning to a full on data engineering career and that is what eventually led me to this subreddit. In my current job (1 1/2 years) I am in a mixed role... analyst/scientist/engineer with some project management responsibilities as well. Before I was in a BI developer role for about 3 years. So as I work with data I find that I maybe take more pleasure in the data engineering tasks, such as DW design and ETL processes, as opposed to DS projects. I do enjoy doing analyst work, but comparing salaries this seems to be the least payed role of the three (DE, DS, DA). The plan is to do the Udacity nano-degree course to fill in the missing tools I need for a fully pledged DE, see if I really like this kind of work and then try to find a full on DE job. While I search I intend to ask for more DE tasks in my current position.  


This brings me to my question. I was looking at some videos from the Seattle Data Guy on YT and in one of the videos he mentions that a DE job might be a fit for you if you are naturally introverted. Personally I enjoy working with people a lot. So the occasional meet and presentation I do in my current role is very welcome. I fear that in the DE role that I am pursuing I will find myself staring at the screen all day and not really interacting with people.   


Is there anyone here that has transitioned from a Data Analyst or BI developer into a Data Engineer and can share their experiences from a social perspective. Do you work with people less then in your previous role? If yes, how does this affect you? Do you miss human interaction?  


Thanks and cheers  


PS: Really loving this subreddit. You guys are great! :)",2021-12-08 22:01:33
1b2a3xe,Is It Time To Move From dbt to SQLMesh?,N/A,2024-02-28 16:24:37
1b1r1ah,Importance of Tech Talks,"Boost your career growth by actively participating in Engineering Tech Talks. 

You will learn:

- How to make the most of it

- Benefits like Instance Recognition

- Growth opportunities 

- Bonus on how to keep up with it


I do think tech talks have helped me grow, learning new, building confidence and getting better feedback and reviews.


https://www.junaideffendi.com/p/why-tech-talks-are-important


Let me know, hows your tech talk looks like.",2024-02-27 23:45:53
1anrc6b,Is single-node data systems the future?,"I am looking at different systems and find that most of the popular data systems are single node: MySQL, Postgres, ClickHouse, DuckDB, etc. Distributed systems on the other hand can take some market shares but it seems that they are not as popular as those single node systems.

My assumption is that single node systems can satisfy most users' requirements, and distributed systems are for big companies and enterprises.

Is my understanding correct? Do you use single node systems or distributed systems? I'd love to hear your opinions. Thank you!",2024-02-10 21:48:16
ynkjno,Is DE one of those thankless jobs?,"I've been learning DE for a few weeks now hoping to eventually land a job but I still have some concerns.

My impression is that DE is similar to DevOps where you're only given attention when something breaks and your superior needs you to fix it ASAP. When everything is working as expected, you are rarely recognized for your good work.

When something goes wrong, you're the person that everyone points their finger at and blames. You're expected to be a jack of all trades developer while at the same time being a master DE developer. Basically, you need to know a lot of shit

While doing my research about the field, I came across another reddit post where people were mentioning that working as a DE was stressful AF.",2022-11-06 08:21:55
p8uu08,What can we do as a community to rebrand Data Engineering ?,"We all know how important our role is in the data lifecycle. We are providing and all. 

But DEs are often perceived as second class citizens to Software Engineers and Data Scientists.  I strongly disagree.

I think DE  undervalued, and we must do something to change this.

Questions :

1- What are your thoughts about this ?

2- How would you present your value as a DE ?

3- is there something we can do as DEs and as a community to promote DE as a ""SEXY"" / valuable profession ?",2021-08-21 16:48:32
17xhbfb,Need to Master Python,"I know there are lots of Python course available on the internet it teaches only the theoretical part of the python . However I have 15 days time out of work/family. Hence I would want something paid or free to start exercises along with theory part. Once I complete the exercise I should proceed further to next level . By end of this course , I would want to rate myself to create my own programs in data engineering. Is there any paid / free course available to start.

Currently bought Darshil Parmer course ( looks very basic )",2023-11-17 15:17:10
13yhf6e,Databricks users can now automatically correct data and improve ML models,"Hi Redditors!

I thought this community might find it very useful that Databricks has partnered with [Cleanlab](https://cleanlab.ai/) to bring automated data correction and ML model improvement for both structured and unstructured datasets to all Databricks users.

A big problem for companies on platforms like Databricks is underutilized data: data and label quality is often too poor to be useful input for reliable business intelligence, training of ML models, or fine-tuning of LLMs. Using the new partner integration for Databricks, users get more value out of their data with automated finding and fixing of outliers, label issues, and other data issues in image, text, and tabular datasets, enabling them to train more reliable models and derive more accurate analytics and insights.

To highlight what's possible with this new integration, their recent [blog](https://www.databricks.com/blog/better-llms-better-data-using-cleanlab-studio) shows how LLMs (Large Language Models) trained on Databricks data can be **boosted in test accuracy (by over 30%) using Cleanlab Studio** to train ML models on an improved text dataset. 

You only need a couple of lines of code too:

    cleanlab_studio.upload_dataset(dataset)
    dataset_fixed = cleanlab_studio.apply_corrections(id, dataset)",2023-06-02 16:30:24
12fj2we,Am I a fraud? Trying to find a job as Data Engineer,"I've decided to change career last Feb 2022.  


Since then I tried to learn all I could.  
Now, on April 2023 I can say I know **Python** only up to solve **4kyu problems** on Codewars, **SQL** up to **intermediate** problems on Hackerrank and I only have knowledge of how AWS works (but I never tried to use it), CI/CD (but never put into practice, I can use some functionalities of **PyCharm, GitHub and Visual Studio Code**, I know few keywords of **GIT**, and some knowledge (that it doesn't seem to be able to get stuck in my head) about databases, clouds, pipelines and ETL processes...  


I'm scared to apply cause I believe I will never pass a technical interview.  


I think that at least you gotta know how to build a proper **ETL pipeline** to get started as a Junior data engineer but I just can't remember anything I learn.",2023-04-08 11:27:08
y0h77w,Reverse ETL is a Passing Fad,[https://medium.com/castled/reverse-etl-is-a-passing-fad-4c07dc67d6f4](https://medium.com/castled/reverse-etl-is-a-passing-fad-4c07dc67d6f4),2022-10-10 15:10:53
14k0199,Let's install the databricks vs code extension,N/A,2023-06-27 01:48:35
17klp2v,We’ve made Data Quality an engineer’s problem. It’s actually a tooling issue,N/A,2023-10-31 13:59:36
18w3pvz,3 Mistakes Data Engineers Make When Consuming APIs,"Hey everybody, I started lurking in this community just a few months ago.

I just love how responsive the community is to questions that appear.
But I also think there are many skilled people who could spark discussions by sharing there knowledge.
My idea is to post often, maybe even daily and test what happens.
It doesn't look like it's against the rules, but I'll stop if that's not received well.

So, here we are:

🔸 Neglecting Documentation

❓ If you don’t read the API documentation, you might miss essential features and parameters and have a suboptimal or incorrect API response.

For instance, if you don’t specify the response format, you might get a default format that’s difficult to parse.

I’ve even seen one service promoting their “easier” SOAP API while having a much more flexible and modern RESTful or GraphQL API.

✅ Thoroughly review the API documentation to understand its terms of use and how to make requests.

Check links and standard parameters, and read plain English text. You never know what you’ll find.



🔸 Ignoring Error Handling

❓ Not handling errors properly can result in unexpected behaviour like loading insufficient data, breaking the pipeline, or fetching previously ingested data.

There are many reasons your requests may fail: Wrong parameters, changes in the API, or exceeded quotas.

Let’s be honest: computers can be carping. Your networking operations can fail even for no apparent reason, even if you do everything correctly.

✅ Build your pipelines assuming failure. Use defensive programming as a concept for every stage of your jobs.

Think of what can fail and handle those scenarios before the scenario where everything works.



🔸 Fetching Too Much Information

❓ Are your pipelines too slow? APIs return all data by default. That can cause problems like out-of-memory errors, slow data traversing, and duplication.

On top of that, you’ll need to pay more for computing costs and API calls if that’s how your provider works.

✅ Minimize unnecessary data transfer and implement caching mechanisms to improve performance.

Implement checkpoints and pull only new data. Embrace the YAGNI concept, and don’t fetch objects you don’t need.

—
What else?",2024-01-01 19:00:51
19bcijh,Do you data lakehouse?,"Do you…

- currently use a data lakehouse

- if so, do you like it?

- if not, do you want to?

- if not, why not?

(Data lakehouse = doing more analytics from your data lake using table formats like iceberg/delta/hudi to use the lake more like a traditional warehouse)",2024-01-20 14:07:03
15w66k0,A new database written in Rust that replaces your server entirely,N/A,2023-08-20 08:50:27
10kgn9l,Why does a faang badge mean anything when laid off.,"“They’ll be fine with a faang on their resume”.

Disclaimer: DE at faang currently.

Why does the badge matter aside from a recruiter reach out. You still need to pass technical loops. It’s not like they will be any easier for ex-google or the like. If anything they might be harder.

Then on the first point - if all big tech is slowing down/freezing hiring then most jobs are likely from startups, non tech or smaller companies. Most of these jobs will pay significantly less so the pool of jobs ex-faang employees would want is pretty small. Which likely means they will wait it out longer and not settle if they have the savings.

I can’t see how the badge means anything. Recruiter reach outs are overrated.",2023-01-24 21:22:05
1b2l5oe,Best open source databases?,"I want to compile a resource for the best open source databases. 

**Here is what I have so far:**

* [https://www.starrocks.io/](https://www.starrocks.io/)
* duckdb
* postgresql
* clickhouse

What are others that you would consider the best and why? 

Thanks!",2024-02-28 23:31:20
oqkhhl,Complete Data Engineering Guide,"Hey all,

Data engineers are some of the most [in-demand roles at companies](https://www.mihaileric.com/posts/we-need-data-engineers-not-data-scientists/),  but there aren't many resources out there to help you practice the  skills needed for these jobs. So I wrote a complete curriculum covering  concepts in database fundamentals, building data pipelines,  productionization, and engineering. [I hope you find it helpful](https://www.confetti.ai/curriculum/data-engineer?utm_source=reddit&utm_medium=post&utm_campaign=data-engineer-announcement)!",2021-07-24 06:20:30
17ws3d4,Does anyone here know people who are actually “over employed”?,"This is something I hear about very often in media but I know no one who has held down more than one job in this field personally. I only have two years of experience (been extremely fortunate they have been Data Engineer jobs) so maybe I simply haven’t seen enough yet. 

It just feels impossible to me, nor would I ever want to. I’m exploring doing freelancing down the line purely as a side gig and I recognize that as a big commitment. Even with the whole “you don’t really work 8 hours a day” there’s so much that can go logistically wrong with two jobs.

This feels like a lot like salary talk where some tech people are bragging about and exaggerating what they’re actually doing, and whole media narratives have been spun from what likely represents an extremely small minority of employees.",2023-11-16 17:11:14
1asa273,Is DE an actual position anymore?,"I have worked in the data space for 14 years now, started off in SEO, moved into personalisation and CRM management, went through the MQTT and IOT phase, did the ELT vs ETL shitshow, datalake vs data lakehouse, DBT, CI/CD. DE was an exciting space to be in, and to an extent, still is.

Do any other data professionals feel that we are now a one size fits all pair of trousers?

We are expected to do insights, analytics, pipeline builds, devops and stakeholder management?

It used to be that you had a sharp suit and pointy shoes and could deal with clients asking about ROI , then you worked in sales.

You wore a jumper your mum knitted you and knew how to write ""fuck you clive in sales you stole my wife"" in binary, you were a techy spodhead.

Now I am not sure.

DE was a space, now it is a commune with other great minds. is this a good thing or not. I think it is great from a personal point, but from a job perspective I am not so sure, especially when recruiters want 20 years experience with Flask.

&#x200B;",2024-02-16 14:35:49
12gbz1n,Pandas 2.0 vs Pandas 1.3 — Performance Comparison,N/A,2023-04-09 07:12:47
n4as1w,Why do software developers think they can just do data engineering?,Without any background in the field as if it's easy to design distributed data stores because you've written oop code before and memorized a bunch of Martin Fowler buzzwords,2021-05-03 23:42:29
134nien,Data Observability: The Next Frontier of Data Engineering,N/A,2023-05-01 14:09:40
19a70zm,Job Security Influencer debate,"Hi All,

So there is a growing debate on LinkedIn about becoming an influencer to ensure job security. One half believes you should be actively posting on LinkedIn about topics in your profession to increase your value. It is thought that companies will line up for you and you'll be less likely to face long bouts of unemployment. 

The other side believes we shouldn't have to become influencers to get a job. 

So my question to all of you seasoned DEs is, how do you feel about this? Are any of you posting on LinkedIn, blogging, making videos, etc and feel like this has secured you a place in the industry? 


For those of you that hire, are you looking for the candidate's LinkedIn profile to be full of content?",2024-01-19 01:33:33
1987pog,[Opinion] The Data Market is not consolidating. It is growing in Complexity like never before.,N/A,2024-01-16 16:59:24
18cvv43,What job keywords or names that you saw on LinkedIn and they are basically data engineering related?,So i noticed a lot of companies invent role names that have close requirements as data engineer,2023-12-07 13:43:05
yodbzr,"I need a mentor, Please Guide me I will take only some minutes?","

Whenever I try to post on this subreddit they are removing my post saying it's already asked. 
I have some doubts regarding Data analytics, Data engineering. 
I am a 20 year old Commerce student who self learnt Python and want to pursue a career in Data analytics and Data engineering. 
I am confused between Data analytics and Data engineering.
I tried wiki of this sub but it's overwhelving and confusing 
I need a guidance. 
If anybody experienced self taught, is there who can have a chat with me Please let me DM you or you can DM me. 
I doubt that this post can also get remove.",2022-11-07 05:04:48
wtzwv1,"CV Review!! Will start applying for FAANG mid/senior roles soon, am I qualified?",N/A,2022-08-21 13:42:06
14o0hg8,Does the world need a new SQL editor?,"Hi everyone,

I am a startup founder and building a AI powered modern SQL editor with following features -

1. Collaborate with your team mates
2. AI powered query generation from plain english
3. AI powered debugging
4. Schedule dashboard/query reruns at a click of a button
5. Integrate and get your data in Slack/Emails

The reason I am building this is I have found most editors like Dbeaver, MySQL workbench are very old fashioned and boring, so the world could use a modern SQL editor.

Let me know your thoughts on this, will anybody buy this product? Will it help increase efficiency of your data team? Would you move to a new editor? If no, then why?

Also would love to know what sort of problems are there with existing editors.

Thanks in Advanced. Go wild.",2023-07-01 17:21:06
u8ldsv,Data Engineering Movies,Does anyone know any Data Engineering Movies or documentary to watch? Please recommend if you know any thanks.,2022-04-21 11:27:08
oyd42e,Am I the only one?,N/A,2021-08-05 08:17:21
126cbku,Will Rust take over Data Engineering,N/A,2023-03-30 05:04:08
14joypx,Seeking Feedback on 'Data Engineering 101' eBook!,"Hi All,

I have mentored more than 200+ students and working professionals in the past 2 years. I've just released my latest ebook, **""Data Engineering 101: A Comprehensive Guide for Beginners and Career Transitioners.""**

Whether you're a beginner or transitioning careers, this guide covers all the essentials of data engineering. I'd love to hear your feedback and suggestions to make it even better. Please direct message me to receive a copy.

Description Of the ebook:

""Data Engineering 101"" is the ultimate resource for anyone interested in exploring the world of data engineering. Authored after having 200+ mentoring sessions and by a seasoned data engineering expert, this guide offers a structured and practical approach to mastering the essentials of data engineering.

Whether you are a beginner aiming to start a career in data engineering or a professional looking to transition into this field, this guide has been meticulously crafted to cater to your needs. It covers everything from the core concepts and responsibilities of a data engineer to the key distinctions between data engineering and other data roles. Additionally, it provides valuable insights into the crucial role of data engineering in today's data-driven organizations.

One of the standout features of this guide is its comprehensive framework, which breaks down data engineering into six pillars. Each pillar is explored in detail, providing you with a solid foundation and a clear understanding of the subject matter. To further enhance your learning journey, the guide includes a curated list of recommended resources for expanding your knowledge and skill set.

&#x200B;

Thank you in advance for your support and participation!

&#x200B;",2023-06-26 18:22:50
13mzoka,How to explain DE to a bunch of SAP boomers!,"I'm stuck in a new team full of SAP analysts and I've tried every possible way to explain what DE is and they think that DE is ML/Data Science. 

Whenever i try to explain to them the intricacies of ETL they behave like it's nothing significant to the business n just a tech spin off like ML or some short life spanned tech trend & tells me that SAP does this better. They don't even understand how fast spark is when compared to their Hana.
 
Some of these sap folks are too dumb i feel that they don't even know the importance of DE in 2023.

How can I help them understand what DE is and the value it adds to the business? 

I wanted to give them a 100ton heavy reply next time they daunt me.

(My stack is databricks, azure and SQL!)",2023-05-20 16:38:59
15r8qyx,Why there is not much people attend data engineer bootcamp got hire?,Many people try to change their field to IT from studying some full-stack bootcamp and now get stuck in between because of the weak demand in the market. But I rarely hear people study data engineer bootcamp get hire. Why is that? Is it much harder to be a DE than a full-stack? Or is it just not much people aware of this role?,2023-08-14 22:06:57
y6reqa,SQL,"Hi all, currently learning SQL. I intend to become a data engineer. Should I continue down this path or go for the  Aws cloud practioneer cert?",2022-10-18 00:20:00
uto7ql,data engineering in web3,"As the title says,

Have been lately seeing a lots of buzz around new ventures in building differebt types of products for web3. My knowledge and understanding of web3 is very minimal. Have you guys been following the trend in this world, like what would data engineering in web3 look like. Would the same toolset exist or there will be a bunch of new frameworks etc will be built. Just opening up this thread to share your thoughts and how we can equip ourselves for the need.",2022-05-20 07:55:18
11t3jz0,AI will replace data engineers?,"Hi, I am a newbie in this career, but I want to know in your opinion if data engineers are under threat to be replaced by AI. Ok, we know this is a tool but having into account that for the first time in human history a tool can learn from the user. Should data engineers be replaced? How long would it take,? Data engineering can be totally reshaped in something completely new to adapt into an AI world?",2023-03-16 19:21:08
1az2yy2,Why Apache Spark RDD is immutable?,N/A,2024-02-24 19:25:58
18szr7u,An article on adopting Databricks,https://eash98.medium.com/debunking-5-myths-on-adopting-databricks-18140cc3d183,2023-12-28 17:44:13
17qnsvo,"Why is it that there are no or almost every less hands on resources for GCP data engineer, whereas gcp data engineer certification has huge number of resources, but when it comes to azure , azure has more hands on resources and also for the certification .","A bit frustrated becuz of less resources to learn real time GCP data engineering, whereas Azure data engineering has lots of resources.",2023-11-08 15:15:37
16ka9bx,Anyone looking to learn Azure Data Factory? Here is my introduction video to Azure Data Factory.,N/A,2023-09-16 15:26:42
16bjqig,DE to the BE team after the big schema migration,N/A,2023-09-06 13:02:17
14t3jw5,How to Install Hadoop in Windows 10 & 11 | Data Engineering Tutorials,N/A,2023-07-07 10:39:10
xlpbq9,data warehouseing,"A pattern I am noticing..

I look at a job post for DE job -- ""strong knowledge of data warehousing, SCD etc.""

I have to then go look up SCD to remind myself. I've been a DE for 4 years and I just don't deal with data warehousing. Like not ever. I knew it better back when I was a DBA from working with the BI team. Ain't DW BI and/or Analytics job? I've been gravitating toward data pipelines and data lake infrastructure--using a DAG tool, python, terraforming AWS managed services. I much prefer this work and frankly I think it's of higher value. There's tones of people out there who know DW from ancient times. I came to DE from DBA to build actual software and get away from wallowing in SQL too much.

Just wondering what other's experience is. Is DW even our job? There's not even anybody on my team who does DWing--our Analysts have now evolved enough to handle all their own schemas. I'll give you a DW--a flat Redshift table with 500 columns LOL. It'll actually probably work just fine! 1990 called.. wants it's data warehouse back! Kidding a bit. I get they're still the right tool for a lot of Analytics use-cases but please, somebody else do it!",2022-09-23 06:10:23
w4c3yw,Really cool SQL learning method with Notion,N/A,2022-07-21 09:20:28
ut1lmg,Salary for AWS DE with 3 YOE,"Basically the title itself. I am a AWS Data Engineer with Python background residing in India and currently looking for a job change. 

My current CTC is 7 LPA. Depending on the market scenario, how much should I ask/expect for the DE Role considering the interview discussions went well.

Thanks in advance",2022-05-19 11:48:03
oujjuy,"Great Explanation of ""Data Engineer"" and ""Data Scientist""","One best Explanation of the Role  [Data Engineer and Data Scientist](https://www.youtube.com/watch?v=qWru-b6m030)  


All rights Reserve to Youtube Channel ""Altexsoft""",2021-07-30 12:21:47
fkr3ob,"I’m a Data Scientist, Not Just The Tiny Hands that Crunch your Data",N/A,2020-03-18 15:15:21
op55ro,Are data engineers second class citizen to software engineers?,"As topic. Are backend engineers more valuable (money wise) than data engineers? 

Anyone finds being a data engineer pretty boring?",2021-07-22 02:52:55
18gk992,Wtf,"Client gives some business rules to follow, me do that, boss revamps the requirements, me modify existing. Client screams, me wtf. ( caveman lang )",2023-12-12 11:56:47
1btd1ne,Batch pipeline cheat sheet,"I want to develop batch pipeline that illustrates all the steps and demonstrates how we can create OLAP /OLTP datasets using this pipeline.
 I am keen to hear everyone’s input on what works and what not works in this pipeline.

My aim is to create a cheat sheet for designing data pipelines in system design.
",2024-04-01 19:27:57
120s83g,How Kaufland E-Commerce automates data governance across over 15K tables,"[**Kaufland e-commerce**](https://www.linkedin.com/feed/?trk=guest_homepage-basic_nav-header-signin#), one of the fastest-growing online marketplaces in Germany, has implemented [**Secoda**](https://www.linkedin.com/feed/?trk=guest_homepage-basic_nav-header-signin#) to streamline its data ecosystem. With over 15,000 tables and triple digit growth in active data users, Kaufland E-Commerce needed a system to make data discoverable and efficiently used. 

[**Richard Hondrich**](https://www.linkedin.com/feed/?trk=guest_homepage-basic_nav-header-signin#), Head of Data and Analytics at Kaufland E-Commerce, created and maintained a consolidated view of all data assets with Secoda. The Secoda workspace is organized so each functional area and team is represented by a Collection, allowing for a single data repository for documents, questions, and knowledge. Every table across Kaufland E-Commerce's entire data stack maps to a specific Collection and has a dedicated owner. The Secoda platform also enables automated stakeholder communication, reducing downtime and increasing data accuracy.

Read more here:

[https://www.secoda.co/customers/kaufland-e-commerce-case-study](https://www.secoda.co/customers/kaufland-e-commerce-case-study)",2023-03-24 17:53:24
18g3xxk,"Struggling with title for ""data pipeline"" job position","My firm is looking to fill a new position. Because this person manipulates data without modeling it, the HIPPOS currently think the correct title is Data Engineer. I think this title is wrong and will make it harder to find the right person.

This job will: (1) gather requirements from the scientists (2) understand what's in the data stores (3) write study-specific Python/R pipelines to turn the data from the stores into the (generally highly specified) format the scientists need (4) make sure all the data from studies is put away safely unto future generations.

Seems to me the following titles might be better:

\- Research software engineer -- except there aren't releases really, just pipelines that are sort of project-specific

\- Algorithm engineer -- covers that they will be manipulating numeric data and such. But, this often means fancy stuff like ML algorithms which won't be happening

\- Data analyst -- except the final analysis is by the scientists

\- Python/R software engineer -- I am leaning to this one although again, software products won't be shipping

Thank you for any thoughts and have a pleasant day.  
",2023-12-11 20:55:01
1amr2yu,How do I learn pandas?,"This by far the hardest thing for me in de-zoomcamp.

I have WesMcKinneys book, doing a fcc course, doing a helsinki data analysis course.  
I also discovered Gorm analysis the next best tutorial next to Corey Schafer.

Still I haven't got a clue.  
I learned some linear algebra and Numpy to get the intuitive understanding, but that is not the problem but the unintuitive syntax.

If it weren't for gpt i would spend hours on docs.  
The naming scheme, the fact to think about is it an attribute or a method, should i use parens is just too much.

I actually struggled on Kaggle learn. I just can't remember it.  
things like cout, pipe and grep, mv for rename don't hold a candle to head, tail, describe in my opinion.

I just listed for reference I know that and other things by heart at this point even exclusive and inclusive indexing (and the reasons for it based on labels) but there is so much beneath it once you get past a certain roadblock and in the end you forget some things you know eventually.

Will this ever click for me as jobs basically demand this at this point.  
There is no open book either.  
Pandas is just so deceptive, it feels like it is easy and it pulls the rug under you.  
You don't go in expecting snafus like in some other languages, query languages.",2024-02-09 15:46:13
1af6ofm,Considering quitting job to go to data engineering bootcamp. Please advise," 

hey all  
I am considering quitting my job in April to focus on a data engineering bootcamp. Iunderstand that this is a risky play so I would like to offer first some bckground on my situation

PROS

* I have a good relationship with my boss and he said to me in the past that he would be happy to have me back if I change my mind
* My employer has offices around the country and very often there are people who come back for  a stint
* I have degree in math and I have been dabbling in stats more. The math behind machine learning is not complete gibberish to me. I can understand exactly how it works
* Getting in wouold allow me a greater degree of independence.  I can't afford to live on my own currently. I would like the ability to be in my own domain and go in and out as I please wothout having to answer to anyone, either out of respect or obligation.
* Making it into the field would allow me to support my parents. They got fucked in '08 and I can see them decline. I would be able to give them a nice place in a LCOL area to settle in. They never asked me now or ever to be their support in old age because ""we don't want to burden you son"" whcih is exactly hy i want to be ther for them

CONS

* I don't know the state of the data engineering market. I know Software engineering is currently a bloodbath due to companies restructuing as a reaction to lower interest rates.
* I would be a 31 y.o  novice. I hope to get into a field linked to mine so I have some ""domain knowledge"" but it's unlikely
* I plan to live off credit cards for the 16 weeks of the bootcamp. While I have no partner, I do have a car and might be fucked in case a major expense comes along
* AI has been leaping forward and the tools that are popular now may not be in use by the time I get in. Hell, I had been dabbling with python for a while now (making some mini prokects here and there) and already I see people asking ""why don't we use Rust"" instead
* I may not end up liking the job and be miserable wishing I did something more 'life-affirming'. Though while I can think of a few things like that, none seem to renumerate as well

That's my plan and goal for 2024. It's a leap of faith with one eye open. What do you guys advise?",2024-01-31 02:23:37
1ac9sco,Career in Data Engineering vs Cybersecurity which one is/will be more demanding,I know the above two fields are completely different… but would love to get your valuable inputs if i have to choose between this two which will be the safe bet for long run? Thanks,2024-01-27 12:27:46
14o8h6h,Data Engineer looking to study mechanical engineering,"Hello, I'm a 24 years old with 2 years of data engineering experience in industry (banking, ads companies). Currently, I'm interested in pursuing a mechanical engineering course online, and I would like to know how it is gonna help me and in what ways? I'm not asking as I'm doubting, but more like to have more ideas about what to do with the mechanical course I'm gonna study. I intend to keep working in data industry as I really like it, but I'm also interested in mechanical engineering :D. Also if you have any online course for a full time employee, I would highly appreciate it.

Edit: The reason why I want to study ME is that I intend to work on my personal projects in the near future (designing mechanical equipments) so I thought why not study mechanical engineering to have a good understanding of what I’m about to make and also about choosing the right material and the strength etc.. lot of stuff that go into the making of it

TL;DR I'm a data engineer looking to study mechanical engineer for fun, any course suggestions and advices?",2023-07-01 23:04:44
1agka5d,Should I pursue Data Engineering?,"Hello,

Before digging in let’s state my background:

1. I was Software Engineer for almost 2 years in an agile team where I contributed to analysis, development, reviewing and deployment.
2. The last year I am working as a Data Scientist but it’s more like AI Engineer where we use Azure and SQL server. However, the department is new thus, we did not really deployed something to production yet but we’re coming there. The thing is that currently I do not even think that I could use this experience for later, but it’s not a discussion for this post.

Being on both sides, I think that would suit me better to work as a Data Engineer as I think I’m better and more productive at giving technical solutions regarding databases etc than thinking of AI algorithms in terms of making our approach go that extra mile and I also see that for AI Tech Leads a PhD is necessary while in Data Engineering it’s not. Also AI Engineering in industry currently it’s just ChatGPT prompt engineering, thus I do not think it’s worth it much.

However, for some reason when I discuss with recruiters they are like I said something bad but it’s just my genuine opinion.

The question I want to ask is that provided that I’ll change jobs after at least 2-3 years, is it worth it to invest in courses, personal projects etc. in order to pursue a career in Data Emgineering or should I focus on my current position like MLOps? My main concern is whether I can find a job at Mid-Senior level as a Data Engineer without having any DE professional experience, but only my personal projects.",2024-02-01 20:09:20
17cxh8z,What do you recruiters expect for the role of Jr. Data Engineer from the candidate?,"Hi, thanks for stopping by.

I am currently working for a consulting company, the work I get here is none for now, but I used to work in ERP involving technical troubleshooting of Orcale products and making BI reports (SQL). 

Now I am trying to up-skill and get into data engineering, so far Im able to complete the GCP data engineer specialisation on coursera, also thinking about doing some side project like developing metric tracking dashboard or something similar. But that’s just my thought, can  you please validate/suggest something better? 

Also to understand your (recruiter/Sr. DE) perspective a bit, what do you look for in a candidate applying for entry level data engineer?

Also this journey for getting better alone is hard, any tips?

Thanks in advance.",2023-10-21 07:58:48
172bxkn,Snowflake best practices,"📬 Snowflake best practices

If you are a user or looking to onboard Snowflake, read this article for best practices covering Snowflake features.

If you already are aware kf Snowflake and its features then you can just skim through.


Read full story 👉 https://www.junaideffendi.com/blog/top-10-snowflake-best-practices/",2023-10-07 17:33:31
16jf6fn,What is Apache Airflow? (in case you don't know 😃),N/A,2023-09-15 14:39:03
14bvvhk,Spark is still a safe port when compared to DuckDB and Polars,You can find the post here: [https://mertkavi.com/spark-is-still-a-safe-port-when-compared-to-duckdb-and-polars/](https://mertkavi.com/spark-is-still-a-safe-port-when-compared-to-duckdb-and-polars/),2023-06-17 16:53:38
10xtpko,Big Data is Dead - blog Opinions!!,"Big Data is dead. Is the balloon deflated. 

[https://motherduck.com/blog/big-data-is-dead/](https://motherduck.com/blog/big-data-is-dead/)",2023-02-09 12:36:39
ydznnk,Do data engineers actually provide any business value to companies?,"It seems like 90% of the time, business executives make decisions and adopt strategy based on either (1) the output from some ad-hoc Excel report the finance team clobbered together in a day, or (2) their gut instinct. Even companies with a data warehouse seem to operate this way.

Are we actually accomplishing anything or is our entire field just spinning its wheels on stuff that doesn't actually matter at the end of the day?",2022-10-26 14:35:44
ydr5xy,What is a personality type of a Data Engineer?,"I'm curious if DEs are mostly analytical and detail-oriented people.

Asking, because I'm a community manager for a DE project and I'm starting to think that my personality type is impacting how the project is coming across and the fact that I am not as detail-focused in communication might not provide enough info for a data persons needs. So, it might be that I'm not the best person for the job because I don't want to change my communication style. I want project to be fun and attractive but it might be too much or irrelevant to some people in general.

I don't want to create or support stereotypes, I'm asking with curiosity. Also I don't want to impose any personality typing method.",2022-10-26 06:44:42
xx69mt,What are the highest paying companies?,"I’m currently working for FAANG and have am looking for change. I would like to try for pay bump but haven’t found a resource which makes my question easy to find. 

Some DE get paid SWE salaries in which case I can use levels.fyi, but even so I don’t know which companies these are. Anybody able to chime in?

TC: $250k as senior DE. 6yr experience in DE role",2022-10-06 14:12:02
xqcpz5,Does the future of Data Engineering lie with SQL solutions rather than Python?,or is it just the hype?,2022-09-28 13:26:17
wxgq24,"Reverse ETL Explained: Concepts, Use Cases & Where It Fits In Your Data Stack",N/A,2022-08-25 15:08:54
161f1bq,Stop using ADF as ETL,"Ok this is a rant, but please… it serves as an orchestrator. It can do transformation but gets expensive because is not an etl but an orchestrator. 
I’ve read too many posts about people deeming it or thinking to use it as an ETL. 

IT SERVES AS AN ORCHESTRATOR. Better if different systems are involved. 

You need an ETL? Look elsewhere or spend money and time and do not complain that does bad job if compared to other ETLs.",2023-08-25 23:39:06
14ef78b,"As DE, Do you agree?",N/A,2023-06-20 16:08:40
zyx034,What is Data Governance and why is it important?,"” In layman’s words, the absence of a Data Governance framework can lead to data inconsistencies and anomalies.",2022-12-30 10:09:49
yz3dhr,My Udemy Courses 😂 waiting for me to start them..,N/A,2022-11-19 04:22:17
yk7iec,Heads up- I’ve got 2 Healthcare Data Sr Data Engineers in Hyderabad likely looking for a new home soon. These guys are the best of the best! I led them for 8 years before leaving my company that just sold yesterday to another company last year.,"These guys are Commercial Health data SMEs.  They are experts in SQL server, they know SSIS/SSRS, some cloud tech, but can also serve as a full stack developer.   They are really good at what they do and have essentially been holding my old team and the entire department together since I left.   They are loyal and have both worked for their current company for over a decade but are open to trying something new.   They have integrated and supported almost every single commercial healthcare Payer data extract known to man.   Here in the US we call these people Unicorns.   The news of the sale happened yesterday at 5 PM and no one let them know all day today.   I whatsapp’ed them this morning to see how they were handing the news and they had no idea the sale happened..   Not surprising considering the dumpster fire I left last summer.   The new company has horrible Glassdoor reviews and it’s likely they will be eliminating their department all together.  

Anyone Have and HITEC City Sr Data Engineer Roles for them?  I am usually anti on recommending people for jobs but these two are the exception!",2022-11-02 14:59:58
17my21u,Future prospects of the DE role? Heard some opinions and want to hear you guys,"https://maximebeauchemin.medium.com/the-downfall-of-the-data-engineer-5bfb701e5d6b

I recently heard a local tech talk quoting this Article

The guy resposible for the talk didnt go all negative and just quoted this article about the part where it says DE work isnt as shiny as DS or Analysts. Kind of like front-end x back-end in webdev, still that doesnt mean Back-end is dying.....

But the speaker also talked about data bricks and how that tool, even if expensive, allows a significant reduction in needed workers to keep clusters going.

Then he finished talking about ""Analytics Engineer"" which is kind of a Mix between DE and Data Analysts, saying that position might grow in the future.


I want to hear your guys opinion on the article i linked and the things the speaker said. Do you think DE as a job role is declining? Will Databricks really reduce job openings? And what about the ""Analytics engineers""?",2023-11-03 15:48:22
1497d0r,Why Most Data Projects Fail & How to Avoid It,N/A,2023-06-14 13:03:50
13i0g1g,Graph Database vs Relational Database,N/A,2023-05-15 06:57:07
11okvkz,Apache Arrow Getting Started,"I recently started working with Apache Airflow.  (Not Arrow -- but I can't edit the title.  Doh!)

You know how that first couple of days of learning something, you are on the steepest part of the learning curve.  Well, rather than simply forgetting those things and moving on to being productive,  I decided to try to make the process a bit easier for the next person. Hope you like it.

[https://codesolid.com/airflow-python-etl/](https://codesolid.com/airflow-python-etl/)",2023-03-11 13:59:15
11bng86,I am more productive and happier going into an office as opposed to working remotely from home,"hi all, I work in a data engineering role for a large pharma company and i'm just trying to get an idea of where everyone stands on the whole work from home vs return to office mess that a lot of  us are in. Some of us are strictly at home, others are hybrid and others are full time in the office. Personally, I'm hybrid 3 days a week and its honestly pointless.  I'm stuck in an office that is big, loud and noisy and its hard to concentrate. Worst of all, my team is spread out through the country so nobody in this office is on my team.  its a 45 minute commute with traffic and it sucks. I'm 100x more productive at home. For some of my teammates, they work in big empty offices which I think is even worse. 

[View Poll](https://www.reddit.com/poll/11bng86)",2023-02-25 15:21:32
10lwt8d,booking.com offer: salary evaluation,"booking.com offer: salary evaluation

Experience: 5 yoe
Field: sr data engineer
Salary: 90 k eur
Bonus: 15%
Location: amsterdam

Hi everyone, this looks a bit low when I compare the people that share their salary in booking.com. what do you think?",2023-01-26 17:08:02
ywxzq6,An Introduction to Data Mesh,[https://medium.com/@memphis-dev/an-introduction-to-data-mesh-b885a92646a8](https://medium.com/@memphis-dev/an-introduction-to-data-mesh-b885a92646a8),2022-11-16 16:21:14
uau5v1,I don't understand the role of data ingestion.,"
Maybe i'm not understanding it correctly but why do we need to extract the data and then load it somewhere else and then process it? Why can't we just process it where it is, or create a copy of it? I am missing something",2022-04-24 12:53:21
t44vg7,What should be the learning path of Apache Spark?,"I have been trying to focus on Spark for a couple of years but getting distracted one way or other due to no specific target and roadmap. As a Python programmer, I can write code for Spark(beginner level) but do not know many concepts. My question is from data engineers: What should be my roadmap to cover most of the tasks in the next 3-4 months so that I could crack interviews? Where can I get bid datasets that can help to cover Spark's real use cases?

&#x200B;

Thanks",2022-03-01 10:24:43
r7s0yr,what would you do if you had some spare time in a day?," say you had 2-3 spare hours a day to spend doing anything that could contribute to your growth as a data scientist, possibly increase your income, increase job prospects, skills set, etc. What would that be?",2021-12-03 05:57:03
qcudu5,What it takes to be a Big Data Engineer? isn't it an overwhelming job?,"Money seems good (over 100 thousand dollars in the US per year)  but the list of skills...good grief... 

[https://www.imaginarycloud.com/blog/big-data-engineer/](https://www.imaginarycloud.com/blog/big-data-engineer/)",2021-10-21 15:29:59
n8i5v8,Tik-Tok(Byte Dance) Data Engineer 2nd Round.,"Hi Everyone, I have a 2nd round(1st Technical Round) for the Data Engineer position at Tik-Tok. I am working as a Data Engineer for the past 3 months and I wanted to know resources from where I can prepare Data Engineering System Design principles. I am leet coding as of now.",2021-05-09 16:49:40
hk4dle,"Koalas, or PySpark disguised as Pandas",N/A,2020-07-02 20:08:46
v83fo9,Job title for a Data Engineer?,"So we are changing the postings for our team to ""Software Engineer, Data Platform""

""Data Engineer"" is just proving to be a magnet for the wrong people who don't understand a Data Engineer IS A specialized Software Engineer. We get hordes of analyst-types and other lone wolfs who can write SQL, work with python notebooks and know a bit of cloud platform (all good) BUT have never used source control, followed coding standards, done a PR.. never worked on a dev team basically. I know the market is hot but we have to set the bar somewhere. Maybe if we were hiring for a Jr.

It's unanimous among us that we'd be much better off with someone who has this dev foundation (and maybe knows a bit of cloud) even if they've never moved a single byte of data.

I fear the title has become tainted before it really even got off the ground. Are we viewed as lightweights here or what? Excuse me while I go update my linkedin profile.",2022-06-08 23:50:56
1aen5t7,6 Things Data Consulting Clients Absolutely Hate,N/A,2024-01-30 12:21:36
11zdr4c,5 Best Data Engineering Projects & Ideas for Beginners,N/A,2023-03-23 08:29:43
st1ejn,"Why the f**"" do i fumble to answer even the simplest of the simplest questions in an de interview?","Why don't the panel understand an interviewee's curiosity or effort...why the heck the do they think the right answer is always the one to go.... Job interviews with the wrong panel is fkin hard!

I was interviewed by a bunch of novices who for sure were referring leetcode to test me... Though I was able to get 95% closer to the soln and also given the right approach... They rejected me! I feel very demotivated to make it interviews after this, as the interviewers vouch upon one's ego and being closed mindset",2022-02-15 12:00:52
jj34k9,🔧 Data Engineering Bootcamp Launch - Intro and Q&A 🔧,"We're launching the world's first coding bootcamp for data engineering!

Pipeline Academy is ready to roll, and your hosts (Daniel Molnar and Peter Fabian) are here to tell you all about it.  


[https://www.meetup.com/pipeline-data-engineering-academy-berlin/events/273973094/](https://www.meetup.com/pipeline-data-engineering-academy-berlin/events/273973094/)",2020-10-27 15:22:05
uml5k7,Any Job Opportunities??,I lost all my $ in stocks and I'm looking to get a better paying job because I learned the hard way that guaranteed income >> gambling.  My current TC = 🥜 and Meta canceled my interview.  And I've been applying to many companies and haven't heard back from any recruiters.  Is the job market trash or am I just undesirable?,2022-05-10 15:20:33
12jofkt,Why we dropped Docker for Python environments,"TL;DR Docker  is a great tool for managing software environments, but we found that it’s just too slow, especially for exploratory data workflows where users change their Python environments frequently.

We find that clusters depending on docker images often take 5+ minutes to launch. Ouch. In Coiled you can use a new system for creating software  environments on the fly using only mamba instead. We’re seeing start  times 3x faster, or about 1–2 minutes.

This article goes into the challenges we (Coiled) faced, the solution we chose, and the performance impacts of that choice.

[https://medium.com/coiled-hq/just-in-time-python-environments-ade108ec67b6](https://medium.com/coiled-hq/just-in-time-python-environments-ade108ec67b6)

&#x200B;

\-- Update

To clarify, our use case is that we have a user base of data scientists who frequently need to use extremely large data science packages and often end up with docker images of 15GB+ despite following best practices. Our users often can't just use a single very large VM or local machine due to the size of their datasets or the amount of processing required.

Few of our users enjoyed waiting 5 minutes for a cluster to boot to try their latest code out, or have to spend 10 minutes rebuilding a docker image to try out the latest packages, or add a new one.

That said, if you have a hot k8s cluster that can cache images, we're very jealous of your \~2s cluster boot times!

&#x200B;",2023-04-12 14:44:50
1abnkaz,The Only Free Course You Need To Become a Professional Data Engineer,N/A,2024-01-26 17:18:26
wobkvy,Am I expecting too much from a data engineer?,"We are in a client facing analytics team, and we hired a junior data engineer to flesh out our capabilities is data ingestion and cloud.

He has struggled somewhat with what we have put in front of him, which he puts down to being unfamiliar with the ""domain"".  By this he means he hasnt had to think about cashflow items, growth rates, averages, etc. For example he didnt pick up that the annual average was close to 12x the monthly average or that all the growth rates were falling between -0.2% to +0.2% (very unlikely that the every division our client would be so steady).

I understand that we have hired a data engineer and not a data analyst, but am I naive to expect a data engineer to have a decent understanding of the data they are working with?

Edit: The jury has spoken IATA. When I had junior engineer positions I was always expected to learn and understand the data at a high level, and I’m struggling to understand how I would have ever been able to check my own work if I didn’t understand what the data was a bit. I certainly wouldn’t have lasted if the data was coming out wrong every time. 

To answer one question asked, 6 months in role. I certainly didn’t expect them to know this stuff on day 1.

I guess I need to update our process that a data analyst checks the loaded data and not expect the engineer to check his work and catch errors.",2022-08-14 17:20:42
1augxjt,Why Data Engineering Is Key to Digital Transformation?,N/A,2024-02-19 06:49:23
1abjdyg,The Difficulties of Senior Engineer …. are not Engineering,N/A,2024-01-26 14:18:59
1817xn4,DELETING DATA DUPLICATES IN SQL TABLE WITHOUT ANY PRIMARY KEY,"So I have had this question asked of me in multiple interviews and my go to solution has been using a row function to assign row numbers to every record partitioned by all the duplicate columns in the table. 

Duplicates will have a row number of more than 1 and we can safety delete this entries from the cte like so:

Delete from cte where rn>1 

This will also remove the duplicates from the actual table as well but they all claim that this only deletes the record from the logical cte  not the actual physical table.

I have been using this every day on my job for the last couple of years without any issue.

Am I missing something? Any help making me understand this is greatly appreciated!

(I use MS SQL SERVER 2017)",2023-11-22 12:24:30
15m345i,Eczachly self paced bootcamp?,"I know there’s a thread about the bootcamp, but he recently launched a self paced bootcamp that is cheaper. I can afford the bootcamp, but it’s still very expensive (like 1month+ of my savings). Do you all think the self paced bootcamp is worth the price? Trying to get some opinions to make a rational decision.",2023-08-09 03:07:53
12zrw9p,How Apache Spark Works,Please check out Blog on [How Apache Spark Works Internally](https://www.sparkcodehub.com/spark-how-it-works) . It is one of the most detailed blog about the Internal working of Apache Spark,2023-04-26 17:56:58
10w6a1p,Revolutionize Your Data Processing with Deequ - The Ultimate Solution for Automated Data Quality Verification in Scala and Spark,"Hey Reddit Geeks! Have you ever faced data quality issues while working with large datasets in Scala and Spark? Say hello to Deequ, the ultimate solution for automated data quality verification! 

Deequ makes it easy to check for completeness, consistency, and conformity in your data, giving you peace of mind and accurate insights every time. 

Check out my latest blog for more information on how Deequ can revolutionize your data processing. 

[https://medium.com/@omarlaraqui/unleashing-the-power-of-deequ-for-efficient-spark-data-analysis-be0f490cce54](https://medium.com/@omarlaraqui/unleashing-the-power-of-deequ-for-efficient-spark-data-analysis-be0f490cce54)

\#Deequ #Scala #Spark #DataQuality #AutomatedDataVerification #GeekOut",2023-02-07 16:36:46
y38e6m,"As a Data Engineer, how much of my job can I automate away?",Given years of experience can you limit your workday to < 3 hours?,2022-10-13 20:00:45
x6p0ny,Entry Level Salary Expectations,"My school just started a DE BS and I'm really considering it. I have a natural knack for databases, stats, and programming. I actually overhauled a start-up's inventory and physically and virtually; allowing me to run some basic stat functions through excel for inventory predictions and product insights. That experience actually inspired me to go back to school. 

With that said what's a reasonable starting salary expectation when I graduate? I ask because I didn't know my worth and definitely got screw salary wise at the start up.

If it makes any difference, I'm attending one of the top engineering schools in Texas. We have a strong alum network and regularly have recruiters from top companies coming through.",2022-09-05 19:32:53
v328rq,Are DE vulnerable to recession layoffs?,"I am a full time DE and it came to my head when crypto and stock market “crashed” from all time high. We know that DE job market is hot right now but what if recession came? DE based on my experience is a non-product/cross-product team, similar to infra and security. We are not directing making profit, but more  on maintenance side. 

On contrast, SWE are in product team and since product is supposed to making money, so are they. Do you think DE will be the major victim of cost-saving compare to SWE?",2022-06-02 06:10:24
q81r8m,Spark VS Flink VS Quix benchmark,"At Quix we have just published our streaming libraries benchmark inspired by Matei Zaharia's methodology. We are very proud with the results (Flink and Quix outperform Spark consistently) and would love to know what other data engineers think:

\- [Benchmark results, details and analysis](https://quix.ai/compare-client-libraries-spark-flink-quix/)

\- Matei Zaharia's [paper](https://people.csail.mit.edu/matei/papers/2013/sosp_spark_streaming.pdf)",2021-10-14 15:14:16
q7ohqp,New Data Engineering position but paying low,"Hello!
I have this new offer from a really good company for a data engineering position. The salary is paying only $60K which is way below the bottom end of the salary spectrum. The role is relatively new, and has amazing prospects and I really really like the people working there. This position is also very open ended and let me explore different ideas with whatever I want to do with it.
I’ve been working as an analyst for a good amount of time, but I wanted to break into the data engineering side for quite sometime. Would it be a good idea to take the offer for 60K?",2021-10-14 00:33:21
puohbj,Laptop suggestion for data engineer,"Hi,

I have been working in Business intelligence field for many years. HR employees do not return me when I apply for junior data engineer roles.  When I apply for senior data engineer roles, HR professionals are inviting me for the interview. However, in the interview tüy find my experience not enough for a senior role.

I would like to improve myself in cloud(aws/gcp), containers and Spark for getting a data engineer job offer. I need to buy a good laptop. 

What are your suggestions?
 Do I need to buy an Ubuntu laptop? Or may I buy a Windows laptop and run Ubuntu virtual machine on Oracle virtual box? 
  Are some components(operating system, ram, CPU) are important for working properly with Kubernetes and Docker?

Thanks in advance:)",2021-09-24 17:25:38
nw8orz,Compilers can build column level lineage directly from code. Padme is in for a great surprise.,"[https://imgur.com/5v1al09](https://imgur.com/5v1al09)

https://preview.redd.it/x6ent25alb471.jpg?width=1140&format=pjpg&auto=webp&s=c3d1e1e2797d4b1258bd646a05652576456c8b9a",2021-06-09 22:54:09
ns5cng,Will DE dissapear with tools like kleene.ai?,This kind of tools claims to remove the DE team from companies.,2021-06-04 13:38:52
m20ib5,Is NoSQL irrelevant for data engineering? (article),"In this article, we’ll investigate use cases for which data engineers may need to interact with NoSQL data stores. 

Read more: [https://dashbird.io/blog/nosql-database-data-engineering/](https://dashbird.io/blog/nosql-database-data-engineering/?fbclid=IwAR0z7aA57uMFy2LQa3b74IaI1-DWmTVrlcc8P2AeIS1I8mvmvzmNiI3vLg0)",2021-03-10 15:49:46
l3lsqz,Python libraries for Data Science and machine learning you should know,N/A,2021-01-23 22:08:12
jiy242,"Do you think it is possible to have an interview with George Fraser, Fivetran CEO?",The question is how open is he for interviews and calls?,2020-10-27 09:37:34
dm3cg2,Is HAVING a redundant concept ?,"Is there a good argument to have `HAVING` as part of a SQL dialect ?
It seem to me that most SQL entities have the express role of:
a) Making an operation possible
b) Speeding up and operation

I don't see `HAVING` as having any of those roles.
a)
It seems to me like `HAVING` could in all cases be easily abstracted by two extra brackets and a select *
E.g. `SELECT COUNT(*) as cnt, entitiy FROM table GROUP BY entitiy HAVING cnt > x` translates painlessly into `SELECT * FROM (SELECT COUNT(*) as cnt, entitiy FROM table GROUP BY entitiy) WHERE cnt > x`.
Is there a situation where such a simplification would not work or would be to cumbersome to write.

b)
It seems like the optimizations that can be done via `HAVING` are minimal and would still be about as easy to implement in the case of only having `WHERE`.

Given a table that's something like:  `amount, sender, receiver` and a query of the form:
`SELECT count(distinct(receiver)) as unique_receiver, sum(amount) as total, COUNT(*) as cnt, sender FROM table GROUP BY sender HAVING unique_receiver > 5 AND total < 6000`
I can think of two possible optimization:
1. When we are using `>` on an expensive aggregated value (in this case `COUNT + DISTINCT`), we could use the already computed result of another `>=`  aggregated value (in this case `cnt`) and eliminate certain rows. So if we figure out `cnt` is, say, 3 for a specific `sender` in the above query, we know `unique_receiver` <= 3, so we can skip computing it for that `sender` since it would be filtered out by the `HAVING condition`
2. When using `<` the same logic could be used, but for a `<=` aggreagted value that's cheaper to compute (though I can't think of a practical case where this might happen)
3. When using `<`, given that any aggregated value has already reached a number bigger than what we are comparing it against, we can safely ignore the `GROUP BY` combination where this has happened, or finish the query and return nothing if no `GROUP BY` is present (in the case above, let's say `total` for an account reaches 6001 half way through iterating the rows for a given `sender`, when can now ignore any other rows with that specific value for `sender` and we can save memory by removing the existing row from the group by)
However, realistically speaking, the above optimizations require knowledge of aggregated metric complexity, when introducing check throughout the query code for certain conditions which would slow things down in most cases and in certain cases (e.g. for SUM or AVG) they only work if all the values in the column's being aggregated are unsigned, or maybe they could work otherwise, but in a reduced scope and with harder check or time spent constructing extra optimization metrics whilst computing the main metric.
Also, realistically speaking, I don't know of any database that actually implements something like this, I can't image that many queries benefiting from this optimization greatly and, if this optimization is being done, I see no reason for not implementing it in the multiple-SELECT scenario where `HAVING` is replaced by a `WHERE` on the query result.
So... how comes all SQL dialects seem to implement having ?
Is there some magic behind it that I am missing ?
Is it just legacy standards that kinda stuck  ? (It's not like SQL language that implement it are all ANSI SQL compatible)",2019-10-23 18:21:32
vyl19w,Building and maintaining a data stack through 8 company acquisitions 🤯,"I recently met with [Josh Richman](https://www.linkedin.com/in/joshcrichman/) ( Sr. Manager, Business Analytics @ [FLASH](https://www.linkedin.com/company/flashparking/)) about how he helped  plan and implement the data stack at his 500+ person company. Not an easy task! And to make it more difficult,  FLASH recently acquired [**8 Companies**](https://www.crunchbase.com/search/acquisitions/field/organizations/num_acquisitions/flashparking)  🤯, and he's had to find a way to bring all that data together.   


This Friday, Josh is holding an event + Q&A about his experience. In it, you'll learn:

  
🎯 The ins and outs of FLASH's data stack  
🎯  The planning and implementation process  
🎯  How he's been incorporating data form the 8 new companies  
🎯  Answers to your questions during the live Q&A.   


If you're interested, sign up 👉 [**here**](https://app.experiencewelcome.com/events/bVuGby/stages/B5fnr7) 👈   


And let me be clear: no product/service is being sold here!   


Hope to see you there!",2022-07-14 02:40:57
13qfean,Engineer | Scientist | ML | Analyst over 11 sub-domains,N/A,2023-05-24 08:14:25
1aqxenc,How's entry-level job market for DE right now?,"Hi, I'm a math graduate and looking into career options. I initially started with learning web dev but the job market is so saturated for entry-level and I decided to steer off from it. I love problem solving and working with data and I feel that I have the right aptitude for the field. What is your honest opinion on entry-level job market for data engineers? Thanks",2024-02-14 20:49:04
1ajsykm,AR/VR for DE?,"Okay, y'all... so the Apple Vision Pro apparently launched last week (on Friday it turns out). And every time something like this launches it creates a lot of buzz around AR for <insert any and every job discipline> work before it dies off a few weeks later. 

But since it's Apple, I can't help but wonder if people are actually going to jump on the hype train and start adopting the Vision Pro for data visualization. And if so, does it make things more impactful/easier to manipulate? How does it impact data modeling (if at all)?

I don't see myself using anything like this any time soon but maybe I'm old-fashioned.",2024-02-05 22:02:00
12y4g9n,Is Python a bottleneck?,"Relatively new to modern data engineering, having come from a software development background.

Python is notorious of its slowness in my background, with the exception of many well optimised lower level functions.

In DE, when talking about huge quantities of data, does python’s performance become a bottleneck? Is data serialisation or transformation…

a) even performed using python at large data scale?
b) hampered by reckless use of slower aspects of python?

DISCLAIMER: Still figuring out what python’s footprint in DE is (eg. Dagster), given my team has leant on low-code ETL tools before.

Thanks!",2023-04-25 01:54:46
12c396k,DuckDB doesn't fit all the scenarios,"Well, duckDB is a fantastic tool for many data engineering tasks, but like any other technology, it may not be the perfect fit for every use case. If you're dealing with massive amounts of data that just won't fit into memory, you might need a database system that can handle those ""big boy"" datasets. And if your data is more complicated than your ex's relationship status, then duckDB might not be up to the task.

If you're looking to process real-time data streams with the speed of a cheetah on espresso, then duckDB might not be the tool for you. You may want to consider a specialized stream processing engine, because duckDB is just a little duck trying to swim in a river of real-time data.

Finally, if you're trying to build an application that can handle massive traffic and provide high availability, then duckDB might not be the best choice. You might need a distributed database system that can scale like a hydra-headed monster.

All jokes aside, it's important to evaluate your specific use case and requirements before selecting a database solution. While duckDB is an impressive tool, it's not the right tool for every job.",2023-04-05 01:04:36
1162sjq,Can I call myself a data engineer if I don’t have an ‘engineering’ qualification?,"I’ve been told that I should be careful telling people that I’m a ‘data engineer’ because I don’t have an engineering degree, and only qualified engineers can call themselves engineers. I’m not trying to trick anyone into thinking that I have an electrical engineering degree or something, I just say that because it’s my job title. Is this something people care about?",2023-02-19 05:43:12
17i7niu,Hype of DE salary in India,"I think there was a hype of salary for DE roles, Currently i am taking interview and i can see candidate profiles and their packages. There salary is less than as compared to SDE.
People from top notch colleges like IIT Delhi,kgp and companies like IBM,Amazon.

Does anyone also feel like that?",2023-10-28 06:24:42
12t4hf0,The False Promise of dbt Contracts,N/A,2023-04-20 15:28:08
11brvxm,Apache Spark vs Databricks? Which one should I learn to expect more jobs and higher salaries?,Which one is better if I want to find a DE job asap? And which one pays higher?,2023-02-25 18:26:26
ya587h,Question to Snowflake haters,"There were quite a few posts and comments recently about Snowflake. Some folks compare Snowflake with evil companies like Oracle and IBM.

As a big fan of Snowflake (I do not work for them and have no interest in promoting them) and someone who was very skeptical about Snowflake hype, I am very very very curious there this hate is coming from and it is biased towards other products and vendors (and we know quite a lot of people here promote vendors they work for).

I would like to hear why you hate Snowflake so much and what product you love instead.

Here a few reasons why I felt in love with Snowflake and why I do not hesitate to recommend it to my piers. I do have extensive background working with traditional RDBMS, EDW platforms and Big Data/Hadoop/Spark/Kafka and all the zoo.

First off, Snowflake supports all 3 big cloud providers so you can move to another cloud and you cannot really do that with BigQuery or Redshift or Synapse. Yes, it is proprietary tech, and no, you cannot change their source code (but how often you have done it with other platforms??) but at least you are not locked on one cloud. A lot of companies who hire new CEOs/CTOs, love to start cloud migration projects and you never know which cloud you will end up using tomorrow.

Second, Snowflake does not keep your data hostage. They make it super easy to get data out of Snowflake. In fact, they help you do that by eating egress costs. You pay 0$ for outbound egress as long as you are moving data in the same region/cloud provider. Very easy to backup your snowflake tables to S3/Blob with literally one command and that command is very fast and efficient.

Third, performance is amazing. A lot of time you get sub-second response time - Presto, Athena, Hive, Databricks, Spark etc. can only dream about such performance. ANSI SQL compatibility helps a lot to port queries from other data platforms. Amazing query plan that helps you tweak performance of queries (good luck understanding Databricks execution profile!)

Fourth, the stupid thing just works out of the box. No indexes, no clustering or partitioning, no primary keys. No special-type tables or special-type data types. Just load data and enjoy.

Fifth, while real-time is tough with Snowflake and it is more like 5-10 second near real-time, they had UPSERT capabilities and Snowpipe long before Databricks had delta lake. A lot of distributed systems still to date do not have DELETE or UPDATE capabilities.

Last, but not least. People were building data lakes and data warehouses BOTH on Snowflake when data lakehouse (what a stupid term) was not coined by Databricks. It is very efficient as data lake because storage is dirt cheap and they support semi-structured data as well. With snowpark addition, this takes this to the next level but I am personally not sold on snowpark idea and I still love Spark. But if you look at what others do, they force you to build separate data lake and separate DW so you end up with two systems not one.

You get a host of countless other features that simple not possible in many other competing products. Dropped a table by accident and it was not on your daily backup? No problem, just run UNDROP TABLE command.

Want to go back in time to query your table as it was 30 days ago? no worries, use time-travel and point-in-time query feature.

Want to share your production environment data with non-production environment for development and testing? Just a few more commands to run and you get a virtual copy of your production databases in your non-prod snowflake account. You do not pay double price for storage since only metadata is shared.

Oh, and speaking of storage price - did you see how cheap it is?

Now one popular complaint - but Snowflake is $$$. Correct, that is if you are lazy #$% who does not read documentation and does not use all the great features that Snowflake gives YOU to help you spend less money.  VDW auto-suspend, caching, instant resize of compute cluster, automated multi-clustering to deal with concurrency during peak time, materialized views (very limited IMHO because they do not support joins but new dynamic tables feature should solve this problem nicely).

Now, it is not perfect by any means. I personally would love to see these features in future:

1. ability to enforce primary keys natively
2. simple visual UI to run, schedule and monitor SQL queries, with simple dependencies. Snowflake tasks are pretty bad and get really messy over time. I do not want to deal with external schedulers just to run simple Snowflake queries in sequence.
3. security model is confusing and can get quite messy if you do not think it through from beginning. I am not sure what they were thinking here by not implementing simple RBAC model. But on a bright side, they give all everything you need to build your own custom model / roles.
4. a lot of usability issues with UI though it is getting better. I mean common, no auto completion for SQL?? Fortunately, they have new UI Snowsight that has it but not all the features of the old UI are available there so depending on what you do you have to switch between old and new UI.

But as you can see these are pretty minor things.

Let's go - tell me why you hate it and what do you think works better in this world.

Thanks!",2022-10-21 21:25:54
14s3c1s,📣 Apache Iceberg has won the table format war,"📣Wait, what? Apache Iceberg won the table format war, apparently 😄  


**👉🏻** [**Iceberg won the table format war (But not in the way you thought it might)**](https://bitsondatadev.substack.com/p/iceberg-won-the-table-format-war)  


🙄 You might roll your eyes given that the author (Brian Оlsen) works for Tabular but IMHO it's a pretty even assessment of the current situation ✅  


I like Brian's optimism about the power of open standards to force the hand of vendors to do the right thing for users. I don't know if it will play out that way—and there are definitely some of the big vendors playing the ""open-source washing"" game ❄️  


But given three formats of approx the same capabilities, adopting based on the strength of its true openness seems like as good a basis as any other.  


**What do folk here think?** 

\- Will one of the formats ""win"" and others ""lose""?  
\- Will we just end up with a generic layer atop the others (UniForm etc) with a slight friction to bias the user to select the default (i.e. in the case of UniForm, Delta)?  


**obXKCD**

&#x200B;

https://preview.redd.it/8ryczu4b8bab1.png?width=500&format=png&auto=webp&s=10fcef5ef227d50ed07dcfdc589e6294664b2b09",2023-07-06 09:03:16
18y3bgk,"I am transitioning from Python/SQL Software Engineer and GCP Architect into Data Engineer in 2024 for a change of atmos and to level up my skills, I am a GCP ONLY kind of guy wanting to AVOID APACHE, how do you guys like or dislike the GCP DE tech suite and are the skills I list in my post enough?","My strengths are:

1. Python
2. SQL
3. GCP (solutions architect)

Things I am learning:

1. Yeet data = ETL
2. BigQuery
3. Pub/Sub
4. DataProc
5. Cloud Storage
6. DataFlow
7. DataPrep
8. DataFusion
9. Looker

Is this enough for GCP Data Engineer jobs?

 Can I straight avoid Databricks Apache certs and Snowflake certs?",2024-01-04 03:43:13
18i60w7,Data Engineer III Pay Rate,"I was approached by a recruiter for a Data Engineer III contract remote (USA) position at a FAANG company, who informed me that the pay rate was $75/hr. I would have expected a higher rate from a FAANG company, or am I being unrealistic? I've seen many posts where individuals mention working on contract for FAANG, and I wanted to understand the actual figures before I negotiate.

Additionally, they're asking me to agree to and acknowledge the pay rate via email before any process begins. Can I still negotiate the rate later if I acknowledge do the interviews and end up receiving an offer?

Edit: It's a shady contracting firm run from India that's employing me on their W2 ( No benefits, paid hourly) while I work for the end client.",2023-12-14 10:48:25
ywylfk,What is data engineering like,"I’m thinking of majoring in data engineering, but I don’t really get what data engineers do. Mainly the work-life balance, and the outlook. Because I don’t want to major in something that’ll just whittle away.",2022-11-16 16:41:38
ycnabu,Modern Data Stack is a joke,"A comical yet, very accurate take on the current state of the modern Data stack.  What a joke and a massive mess.  I feel like I will be making a lot of money cleaning up and shutting off a lot of garbage in the next 3 years.

https://medium.com/@laurengreerbalik/the-modern-data-stack-through-the-gervais-principle-bfd4b4e33ac7",2022-10-24 21:31:23
1ajkler,Are Data Contracts For Real? Or Just More Snake Oil?,N/A,2024-02-05 16:29:21
19anfkz,Fresh out of CS Bachelor,"Hi guys,

I've finally graduated from my Computer Science Bachelor course. Being way more interested in Data than pretty much anything else, especially AI (not interested), I'd like to not go into the CS Master, which is basically 3/4 AI and 1/4 anything else. Do you think I'm well positioned to go for a Data engineer carrier with my degree? 

If yes, where do I start learning the skills of a Data Engineer so that I can strike an entry level job in the data field? I read that there are usually other roles before one starts in before getting enough ""trust"" put in their ability to cover the DE role.",2024-01-19 16:44:34
196rt7n,What entry level roles to target to prepare for data engineering?,Hey all. I am coming up on the tail end of a CS degree and was wondering what would be roles a CS grad qualify for that would be best suited to transition into DE? I’m aware that certain jobs are taking much more of a hit in this market than others so does anyone have any suggestions for what roles would be the easiest for a fresh CS grad to get to eventually transition to a DE career? Software engineering seems like trying to win the lottery right now so any other suggestions would be great. I have a decent grasp on Python and SQL. Trying to squeeze an internship before I graduate and want to know what keywords to search for.,2024-01-14 22:10:40
17v6kpk,Should I suggest DBT to my company?,"Hello everyone,

I’m a very new data engineer at a company who’s wanting me to figure out if they should use DBT or not. Currently using GCP BigQuery but am trying to see if the built in data governance tools in Dataplex are sufficient or if DBT will be useful. 

What are the main differences in using Attribute Store, Profile, and Data Quality in Dataplex vs starting to use DBT? Mostly everything is coded in python.


Thanks",2023-11-14 16:55:35
17bb7m8,Why we built a Spark solution for Kubernetes,"Hey hey

We (Canonical) launched a solution for Apache Spark on Kubernetes this week. Read all about it [right over here](https://medium.com/p/4cfe17872eb6)

If you're a Spark user, feel free to check it out

/over",2023-10-19 05:03:25
16tx26a,Is this job offer OK?,"Hi,

I am currently trying to decide if I should accept an offer that I received from my summer internship. 

During my internship, I was hired as a data engineer intern (I was specifically looking for data engineering internship as it is a field that I'm passionate about want to get into). The company where I worked is a public, global software company. I have BS in CS degree in T20 CS schools in US, as well as 4 YOE software engineering experiences, and currently pursuing MS in Data Analysis in T5 CS school - with a goal of becoming a data engineer. 

Once I started my internship, I was assigned to a Business Intelligence team, and was assigned to work on dashboarding works (GCP, Looker etc). I didn't get a chance to work on data pipelinining or ingestion. I was confused but did my best to do the best I can - which I received good feedback and that turned into a full time offer to join the team. During my internship my manager also made sure with me to talk to some of the senior DEs in my team so that I can gain some insights, but the amount of technical work that I did was pretty minimal (intermediate-advanced level SQL for maintaining the data model). 

The offer turned out I'll be mostly continuing the dashboarding work and sounds like the team is right now in need of me to fully devote to the data visualization projects (and also added that the current DE team is very full). In addition, the location that they're able to offer is not ideal for me as the heat in TX caused me skin rashes and overall my mood was significantly affected. 

My manager left the team after my internship and the new manager seems like would prefer me to be a fully BI Developer for dashboards. I really liked working with my manager which was honestly a big factor for me to even consider BI role so I am pretty sad about it as well. 

I'm not sure if it is wise for me to take the offer given that 1) I'll be mostly working as a BI Developer (not able to actively develop my SWE skills / CS degree) / 2) at a location that I had such a hard time adjusting to. 

My career goal is to get into data engineering and to develop my technical skills. I don't mind getting more business related works, however, the team seems to be not willing to let me explore DE works at this point. 

Am I getting too picky on my choice given this market? However, I am afraid this will make it even harder to advance into DE roles in the future. 

Any suggestions or guidance would be really appreciated!",2023-09-27 21:43:54
15zfqyp,Should I pivot and become a Data Engineer?,"Is Data Engineering something worth pursuing? I've always done web analytics and technical SEO for ecommerce. 

I don't enjoy that industry so much (starting to find SEO very tiresome) but really love doing web scraping and dealing with databases. I'm also currently learning Python. 

Having researched careers and completed career tests/personality profiles etc, Data Engineering came up as a potential good fit. 

In previous jobs I've built tools such as competitor price comparison databases and simple stuff like that - is a Data Engineer career something I should be really putting a lot of effort into? 

I'm in my late 30s and have a lot of experience on big ecommerce projects. I'm in the UK.",2023-08-23 20:32:05
15nu3gd,Where can I find Fortune 500 companies' database design patters?,"Hi All,

I am looking to understand fortune companies' database design and architecture, specifically I am wanting to know how Spotify collects our data, uses it in AI through real stream technology. Where can I find this information? which websites will be helpful to learn them? I am preparing for system design interviews and would highly appreciate your help!",2023-08-11 00:56:47
14thvca,Will data engineering split as a profession?,"It feels like the field of data engineering is evolving - slowly the people doing end to end are disappearing, and being replaced by platform engineers, engineering analysts, etc.

it feels like ml pipeline engineers are another breed as well.

What is your experience around this? what roles do you see data engineering splitting into?",2023-07-07 20:06:34
14lc985,Would a data science masters degree be useful for DE?,"I've been studying and working toward a career transition in data engineering.  I'm not ready yet, but I have passed the Microsoft DE exam (and the DBA one), and I've been working in Azure to create demos.  

My mentor has described me as ""internship ready"" though I don't have access to internships, as all (so far) have required that only current CS-related college students apply.  I completed my bachelors years ago in social sciences, and I had lots of sporadic IT skills/training, but little of it is formal classes.

Next step is to accelerate/focus the process, which means some formal training.  My best options are these:

\- an offer of a data science masters degree (FREE) at an honors college, inc paid internship  
\- a bootcamp style 6 month graduate program ($10K) remote through a university/Springboard  


My question: If my goal remains DE, would be it normal (or not) to see a data science master's degree on a job application for a data engineer?",2023-06-28 15:17:21
14jqy4t,How I Got Into Data Engineering | Finding Success in Data Engineering Without the Textbook Route,N/A,2023-06-26 19:38:10
133kg14,Normalization and Data Engineering,N/A,2023-04-30 09:16:48
12pqntp,Webinar - Running dbt core on Airflow in production - learnings from 2 years of battle scars,N/A,2023-04-17 17:52:29
12bzubj,Run duckdb in the cloud with a few lines of code,N/A,2023-04-04 22:58:38
12b8wev,Improving SWE skills as a DE,"As DE, we should consider taking our SWE skills to next levels like Git. We all use Git daily but really most of us don't care.

I have seen most of the time squashing seems to be enough. Everyone most likely have seen or done tens of `fix xyz` commits and later squashed into one, right? It may work and no one cares in the team, but I encourage everyone to learn the better approach like the interactive rebase. 

The idea is to keep git history clean which helps not only in reviews but also when tracking back changes in the future.

It is a good idea to keep just a handful of commits like 4-5 per Pull Request and each commit should correspond to specific family of changes and definitely with a meaningful message.

I adopted this several years ago after learning from peers. The most common ones I use are:
- edit: instead of creating new changes, just go back and edit the previous relevant commit.
- drop: instead of squashing temporary changes, just drop them.

Do you guys use this approach, this would really make a difference and encouraging team for best SWE practices can really change game. 

DE is subset of SWE imo, and I do think to be top DE you have to bring top SWE practices.

👉 Learn more in detail from Gitlab: https://about.gitlab.com/blog/2020/11/23/keep-git-history-clean-with-interactive-rebase/",2023-04-04 04:26:41
122yczk,The Modern Data Stack (Demystified),N/A,2023-03-26 20:24:26
11znqc2,"Which has more jobs? ADF, AWS, or GCP.","For any aspiring data engineer, selecting a tech stack is really tough. Can someone help with it? Like which has more job prospects? I will be grateful.",2023-03-23 15:25:38
11rvszh,Am I a data engineer?,"I'm doing a MLOps internship where I'm preprocessing data, building models, evaluations and building a web app for monitoring and managepent. I also happen to use RabbitMQ+Celery to schedule model training and plan on using Airflow for data pipelining. Does that count data engineering knowledge? If not what technologies should I use to count as it?",2023-03-15 13:06:05
11ksc63,How to data model in Snowflake,"I learned that Snowflake is quite limited, when it comes to its data modeling capabilitiea. For example it does not enforce unique values for Primary Keys. How am I supposed to set up a data model without possibly breaking those constraints?

Thanks in advance for sharing your experiences.",2023-03-07 07:12:25
zh768a,Top 5 business intelligence tools (based on over 250 data teams),Ever wonder what BI tools other teams are using? We've gathered data from over 250 data teams and outlined the most popular BI tools used by Secoda customers in 2022: https://www.secoda.co/blog/top-5-business-intelligence-tools,2022-12-09 20:04:01
zfd4of,How to do Data wrangling in Azure Data Factory,[https://youtu.be/sPTERXmYnBs](https://youtu.be/sPTERXmYnBs),2022-12-07 20:42:33
ze3kgz,Book Review: Pythonic Programming,N/A,2022-12-06 11:04:07
zank10,Information vs. Data - Are you conscious?,N/A,2022-12-02 14:23:03
z9epqk,AWS zero ETL,"https://press.aboutamazon.com/2022/11/aws-announces-two-new-capabilities-to-move-toward-a-zero-etl-future-on-aws

Any thoughts? Trying to read between the lines here, it sounds a bit like a fancy federated querying but I’m just speculating. There’s not much to go off of detail wise and am hoping for a white paper soon. This would be huge if it worked well for my company! Would def change some upcoming plans to switch to another cloud dw.",2022-12-01 04:49:48
xzozy5,"PLEASE HELP: Applying for entry/associate level DA/DS jobs, how do I improve my resume?","&#x200B;

https://preview.redd.it/mk8qsfmayss91.png?width=696&format=png&auto=webp&s=30ce631c9f087abab71944b2b67414944785cdd9",2022-10-09 15:55:33
xodx3t,Please criticize my resume to your heart's content,"&#x200B;

https://preview.redd.it/0c1qstov36q91.png?width=640&format=png&auto=webp&s=c92e5ae70940a861433a56f6be6f6fa5ad637ee6

[View Poll](https://www.reddit.com/poll/xodx3t)",2022-09-26 09:01:41
utek1g,Buying vs. building a data discovery tool,We get a lot of questions about the tradeoffs of buying vs. building a data discovery tool and decided to outline our unbiased thoughts here. We hope this helps some of you considering the decision: [https://www.secoda.co/blog/build-vs-buy-data-discovery](https://www.secoda.co/blog/build-vs-buy-data-discovery),2022-05-19 22:16:39
u1vi3g,Best career trajectory for achieving equity/independence?,"I am a data engineer with some experience in analytics. Not a CS wizard, but practically very good at software and engineering. I started in a company 15 months ago and it's been going well. I got excellent feedback as well as a raise and senior title. 

However, I am not satisfied with being just an employee with no chance of getting equity. I know that I have to move to the next level at some point and the sooner the better. Especially because I'm at a point where I can invest in my career and take calculated risks (young, flexible, not a caretaker).

I want to get in a position where I either:

* own decent equity in the company that I'm working for, exposing me to big upside
* sell services as an independent professional and get to set my price

It seems that data engineering per se is not as well suited to solo freelancing the way other fields are. See this thread: [https://www.reddit.com/r/dataengineering/comments/g4uzjf/data\_engineering\_and\_freelancing/](https://www.reddit.com/r/dataengineering/comments/g4uzjf/data_engineering_and_freelancing/)

I am looking for opportunities to freelance on the side but I haven't found any decent ones yet. There's little room for part time. Most clients want you to be engaged full time for a period, which makes sense.

Anyway, given that it's an employee's market right now and I have plenty of interesting opportunities, I am trying to think of what the next best step could be.

One possibility is to join a small data consultancy that caters to lots of companies and build my skills there. This could be a stepping stone to becoming an independent professional. However, I am worried that it could offer limited possibilities of career growth and career capital compared to a startup/company. I suspect it only makes sense if it's a premise to going solo. If instead I will continue working for companies, it could be less helpful.

Another possibility is to join a new startup as an early employee with decent equity. Potentially risky and exhausting. However it could accelerate my learning and equip me to consult with startups in the future, get an executive role at a startup, or even start my own. And obviously the equity upside.

Finally, I could put a lot of effort into trying to join a FAANG. Not sure how this would help me except for the prestige and career capital.

Any good frameworks for thinking about this?",2022-04-12 10:05:53
tt9tu3,Moving away from the Data domain -," Hi, I'm a Data Analyst with 3 years of YOE and my skill set includes Tableau, PBI reporting, Pandas, SQL and Decent Machine Learning Knowledge. Recently I got interested in Data Engineering roles and gave some serious thought to it and read many articles, quietly following Data trends in various forums. I think DE as a field or profession moving towards SQL,SQL Heavy(Dbt, Snowflake) & some Cloud-based ETL tooling(Fivetran, ADF, Glue) and Even when I see Most DE roles at FANG most are SQL Heavy and have matured Infra setup. So, I don’t want to pigeon hole to some vendor tools and be SQL Monkey. 

Now I m giving serious thought to changing to Cloud / DevOps/SRE roles where I will be completely beginner despite that I see these roles are Linux, Vendor Cloud (AWS), CI/CD, and Scripting (python or Bash) and these pay above par DE roles and Equal to SWE and most importantly I enjoy this work.

Note: I know there are many roles in the DE space mostly SWE based DE roles that needed Python, Spark, DS &Algos and SQL  where DEs writes Pipeline and Maintain Infra as well but these are very small chunks in the DE space and I can say DE  is still fantastic career in Industry but hoping pay will be matching on par with SWE in future.",2022-03-31 21:09:33
su4q29,"Meltano founder featured on open-source podcast, Contributor","Hi all - hope this is not too self-promotional for this subreddit. But I produce a podcast called Contributor about various open-source communities and we have a new episode featuring Meltano founder Douwe Maan ([who has visited this subreddit!](https://www.reddit.com/r/dataengineering/comments/gj722d/why_gitlab_is_building_meltano_an_open_source/)). The project has pivoted from an ELT solution to more of a DataOps OS that glues together various open-source technologies like Singer, dbt and Airflow. The podcast gets quite technical and goes into some behind-the-scenes history at GitLab. If anyone is interested, you can find it wherever you get your podcasts, or by following this link: [https://www.contributor.fyi/meltano](https://www.contributor.fyi/meltano)

Thanks!",2022-02-16 19:54:31
slb9hy,How to choose a graduate program for a non-CS grad wanting to migrate to DE?,"
I basically have two options to compliment a BBA in accounting and several years in BI:

1) unranked MS in CS from small private university, focuses on SWE and enterprise computing

Pros: CS degree, includes leveling courses, big data courses, ML, OS, etc.

Cons: possibly a no name (Lewis University in Illinois)

2) MS in IT from UT Dallas with a focus on BI. 

Pros: more focused courses on SQL, data warehousing, Spark, hadoop

Cons: IT degree, no data structures, no java, some business courses

Both cost the same, ironically.",2022-02-05 16:45:34
rbz79j,Can Unethically Produced Data be Used Ethically? Survey. Please do fill it out,"*Can Unethically Produced Data be Used Ethically?* This survey serves as the backbone of my research paper which tries to examine the question of ethics in scientific research and its advancement. Please take a minute to fill it out.

[https://forms.gle/eKak1dpWGUbZ5bAZ7](https://forms.gle/eKak1dpWGUbZ5bAZ7)",2021-12-08 19:41:16
q86ub0,Net promoter score for data teams,"Data teams are starting to look and work like product teams. The role of a [data product manager](https://www.secoda.co/blog/data-product-manager-the-role-and-best-practices-for-beginners) is becoming more common and data teams are starting to specialize in specific parts of the data infrastructure. The data product now supports many types of stakeholders in the organization. Some of these stakeholders work closely with the data team on a weekly or even daily basis and some stakeholders rarely chat with the data team but are reliant on the data product to achieve their goals. Data teams have faced growing demands, making it harder to understand the different ways stakeholders interact with data.  

The product teams rely instead on survey tools such as Net Promoter Score to obtain an overall picture of customer sentiment. Given the shift towards data product management, we strongly believe that data teams can use a similar survey to measure their understanding of the data product.

We wrote this article about how we think data teams can improve their own understanding of the data product: [https://www.secoda.co/blog/net-promoter-score-for-data-teams](https://www.secoda.co/blog/net-promoter-score-for-data-teams)",2021-10-14 19:26:39
pda1hh,How to Become a Microsoft Certified Azure Data Engineer?,N/A,2021-08-28 13:45:39
p2jaza,"Hey Guys, i did a post that can be useful for some of you that are starting your jorney to become a Data Engineer.",[How I become a Data Engineer from Scratch]( https://link.medium.com/hSh4mxNPDib),2021-08-11 18:43:10
okwz2b,Get started with Airflow Sensors in 10mins and discover what are the best practices,N/A,2021-07-15 16:58:26
m10xn5,Is big data engineering dead?,"has the big been axed, and now just DE? I think that's my hiccup is understanding this aspect of the data industry.",2021-03-09 06:52:32
lkaizs,[OC] Get Your FREE Cleaning Data eBook,"Hello guys,

As someone who cleans data almost every day

I've created an ebook to make the command line as seamless and easy as possible to do that task.

Be one of the first 10 who gets this ebook for free: [How to Clean Data at the Command Line](https://gumroad.com/l/clean-data-cmd/free)

Would love to see your feedback, Thanks!",2021-02-15 09:56:12
l1co4w,The End of ETL As We Know It,N/A,2021-01-20 16:34:34
hrbk9f,Spark Vs Snowflake: The Cloud Data Engineering debate!,"For Enterprises, navigating the cloud transition can be tricky. With both **Spark** and **Snowflake** available for Cloud DataEngineering, how do you choose?

We've penned down our thoughts [here](https://www.prophecy.io/blogs/spark-vs-snowflake-the-cloud-data-engineering-etl-debate). I'm keen to know what you think!

&#x200B;",2020-07-14 22:31:23
hqgdsc,What's for Next 5 years,"Hi All,

I'm assuming no knowledge on the IT industry. I would like to learn the trending technologies which will definitely have good pay in the future + good scope in the future until 2025. 

1. Which programming lang (go/python) 
2. What DevOps learning path
3. AWS or Azure 
4. Conterinization is required or not? 
5. Is Streaming going to the future instead of ETL Batch",2020-07-13 14:37:33
t537cl,siliconANGLE: Dremio unveils 'forever free' tools for data lake analytics,N/A,2022-03-02 15:57:41
hnftuu,What every data engineer should know...,"Are you new to data  engineering and want to share some advice to other newcomers? Are you an  old hand and data wrangling and want to leave some pointers to the next  generation? I'm working with O'Reilly Media on 97 Things Every Data Engineer Should Know and we need your help to make it a reality.

If you have a blog post,  presentation, or white paper that is useful for data engineers, then  send them along. We can work it into shape for the book. Share your wisdom and help educate data engineers everywhere!

https://www.dataengineeringpodcast.com/97things",2020-07-08 12:03:31
zvnkaj,"I recently got laid off and still can't find a job, please critique my CV. Almost 2 years of experience in DE and 6 total years of experience in IT.",N/A,2022-12-26 13:52:28
1auuq18,Is this Bachelors Degree Worth It?,"I have the opportunity to specialise early university in a Bachelor of Electrical and Electronic Engineering specialising in Data Engineering. Would anybody be able to tell me whether this degree would be worth it in the long run for career opportunities. 
 
Moreover just would like to hear the thoughts and any input of those who are experienced.",2024-02-19 18:34:43
11a2wvf,What problems do you commonly see in cold messaging to data engineers?,"Data Engineers,

You might have gotten many cold emails from many different vendors. And as a new guy in this field, I don't want to send you another spam message. 

What problems do you commonly see in cold messaging? 

How would you write a cold email/message if you were in their shoe?  


Any feedback and/or example will be super helpful--thanks!",2023-02-23 17:10:38
181wqix,Dagster Tutorial: Building an Asset Graph,"👋 Hey, check out my latest dagster tutorial video of how to build you first asset graph 😜.",2023-11-23 08:36:56
174zktw,DE Bootcamp Recommendation,"Hello. 

I am looking for an accelerated bootcamp specifically on ETL, Snowflake (i.e. stored procedures, user-defined functions, data engineering concepts SCD I, II, etc.), SQL, Python for DE, Dimensional Modeling (Kimball, Data Vault, Inmon, OBT) and relevant course load in CI/CD or GitHub/DevOps. 

&#x200B;

Does anyone have a good recommendation for this type of bootcamp? I know there is a lot of ground to cover but I would appreciate any and all recommendations. 

&#x200B;

Thanks!",2023-10-10 23:32:12
16u8a37,Top 4 Azure Resources for Data Engineers to Master. Do you agree with my list or would you pick some other 4 resources?,N/A,2023-09-28 06:17:51
11t1uyj,Future skills for Data/Software Engineering,"There has been a ton of buzz around AI automation tools and how they will affect the job market. I would like to start a discussion around the inevitable change to skills and what direction they will take in the next 5-10 years...

* I believe that jobs will increasingly be focused on higher-level architectural decisions and not so much getting into the nitty gritty since code generation tools are getting increasingly sophisticated and there is no reason to think that won't continue...
   * This can essentially be boiled down to more Senior type roles, but at the same time-you need junior/mid people to get experience to replace the senior roles as they inevitably retire/quit
* I think it can be safe to say that the main limitation for any software/tech to be created is time/budget...
   * Would we then say that if efficiency is dramatically increased, then the # of projects would increase dramatically as well? 
   * And therefore the jobs available would shift to a more administrative/supervisory role, but there would be just as many or more jobs?

This video below (at the linked timestamp, only about the last 6 mins of the video) explains well what I foresee as the future:

[https://youtu.be/brAwZ5l\_fuQ?t=626](https://youtu.be/brAwZ5l_fuQ?t=626)

Essentially, I see over time, all programming jobs be **decreasingly technical** as code generation tools become inevitably more sophisticated, but **increasingly soft skill and communication oriented**\--however, still at similar or more # of jobs...

What do you all think of my prognosis?",2023-03-16 18:16:53
1113mtq,How can I promote my skills to build my own company.,"I’ve been in the field of data analytics and data engineering / BI for 10 years now. If I were to start my own firm with this skill set, how do I go about it? How can I look for projects that need help with data?",2023-02-13 08:48:52
10uz4c4,Looking to Hire a Mentor 🙏,"Hi, I'm looking to switch into a career in data engineering, but I know myself well enough to know that self-study is not effective for me.

* I will definitely constantly doubt whether I am studying the most optimal thing

* When I get frustrated/confused, I will constantly wish I just had an experienced human being to answer my questions, instead of googling and drowning in terminology and opinions

* I think I will become a generic data engineer, instead of becoming really really good at a single hyper-specific tech stack.

* I personally get motivated 100x more when I have 1-on-1 accountability, whether it be fitness, study, religion, a hobby, really anything.

So I am looking for a senior-level data engineer who can give me advice. **I want someone who will essentially teach me how to become a clone of themselves.** I want someone who will tell me the exact steps to get a job similar to theirs.

For structure, I was thinking maybe 1 hour per week on Zoom to critique my weekly study progress, and maybe 3-5 text messages per week for quick questions. We can discuss money privately, but I am definitely willing to compensate you fairly for your time!",2023-02-06 05:54:33
10hlbb8,Does anyone know what an Excel file looks like outside of opening it in Excel?,"Recently I’ve been working with .xslx files and I renamed the file in excel and I tried opening it after that and Excel told me it was corrupted.  I realized I have to rename the file inside excel.  Also, I am not able to open the excel file in another program or my IDE and it can’t seem to display in git either.  I mean I haven’t like opened one with vim or nano yet, and I’m sure it will just output like cryptic binary or maybe nothing. I am curious does anyone know what like the underlying file is? 

Is it like a .csv file or parquet file at all? I’m sort of assuming Microsoft does things like this on purpose so people need to use excel, and I’m guessing somewhere in the .xslx file is the name it was “saved as” in Excel itself, and excel does some sort of validation that the file name in the OS matches the name it was saved as in excel, but like what is the underlying file actually? Is it like a csv file that can have excel functions converted to binary or something? And how do they make it so hard to view outside of excel?",2023-01-21 07:07:17
1044ef5,The Top Online Resources for Learning Data Engineering,"Hello everyone!

If you're interested in learning data engineering, there are some excellent online resources available to you. Here are my top picks:

1. Coursera: Coursera offers a variety of data engineering courses from top universities and companies, such as Johns Hopkins University and Google.
2. edX: edX is a non-profit MOOC provider founded by Harvard and MIT, and it offers a range of data engineering courses as well.
3. Dataquest: Dataquest is an online platform that teaches data science and data engineering through interactive coding challenges and projects.
4. Data Engineering Bootcamp: This bootcamp, offered by Trilogy Education Services in partnership with top universities, is a full-time, immersive program that teaches students the skills they need to become data engineers.
5. Kaggle: Kaggle is a popular platform for data science and machine learning competitions, and it also has a number of resources and tutorials for learning data engineering.

I hope these resources are helpful as you start your journey in data engineering! If you have any other recommendations, please feel free to share them in the comments.

For a more detail insight, check [https://medium.com/p/720cb18cc2c6](https://medium.com/p/720cb18cc2c6)",2023-01-05 17:17:38
zgz4ni,12 Things You Need to Know to Become a Better Data Engineer in 2023,N/A,2022-12-09 14:44:34
yh6ofu,A polite request to review my resume and suggest edits. Thank you!,N/A,2022-10-30 06:33:24
y87kap,Why There's Never Been a Better Time to Adopt Apache Iceberg as Your Data Lakehouse Table Format | Dremio,N/A,2022-10-19 16:41:00
tcirxt,Real image of Bill Inmon's house.,N/A,2022-03-12 15:31:25
rpcqe2,Which one of these might correspond to software/data engineering? I'm on O*NET.,N/A,2021-12-27 02:48:37
pkxfd4,Rethinking the data catalog,"Sturgeon's law states that 95% of everything is crap. 

Nowhere is that more true than the data world. Most dashboards are wrong, most tables are trash, most ETL is broken, and most metrics are incorrect. 

For some reason, most metadata management tools choose to solve this problem by trying to catalog this encyclopedia; they're building a library. But in reality, most of the books are just bound-up junk mail and shouldn't be cataloged in the first place. To solve this problem, opinionated data discovery powered by search is the key.

Over the last six months, our team at Secoda has been making data accessible to people using the modern data stack. We believe that every company will need a data discovery tool in the future. Existing tools have been able to deliver value to data teams, but have not cracked the elusive data discovery problem because of their focus on technical cataloging, instead of cleaning the overwhelming mess and fragmentation in our current data stack. After speaking with tons of data teams, a few things became clear to us: 

1. Existing tools are not comprehensive in the data knowledge that they capture. They traditionally focus on only one area of data knowledge, which is technical metadata. 
2. Data discovery tools should reduce the number of questions data teams get from people trying to use company data
3. Existing data discovery tools are built for technical users, but aren’t intuitive for non-technical users.
4. Existing tools combine curation and consumption into one view.

We found that teams are looking for a tool that captures all data knowledge in one place and reduces the number of questions data teams get from non-technical teams. The tool should focus on the business users because they are the source of many discovery questions and have few options to find the answer for themselves. 

Since coming to this understanding, we’ve made some significant changes to our product. The first significant change is that we don't think of Secoda as a data catalog. Instead, we think of Secoda as a knowledge management tool that helps data teams share metadata, charts, queries, and documentation with any employee. Data teams are creating more data at an earlier stage, and are looking for a way to organize this knowledge and share it with employees. If you want to read about how we think about the ideal data discovery tool and how Secoda will continue to try and address the insights we’ve gathered from data teams, here's the full length article outlining how we think the data discovery layer should fit into your data stack: [https://www.secoda.co/blog/rethinking-the-data-catalog](https://www.secoda.co/blog/rethinking-the-data-catalog)",2021-09-09 13:05:25
p77ib0,Career change,"So, I’m wondering if data engineering is for me. Long story short I’ve been a full stack developer with 12 years of experience total, and working in B2C and B2B platforms for the past six. The last two years have been working on Salesforce as a developer / development manager. 

Honestly, I’m burnt out. I love my team but I hate Salesforce, and I’m sure others might agree. I’m ready to try something new career-wise

I had an interest in data science for years, but can’t back it up with the schooling or degree. I’ve been learning Python, Jupyter, Pythony based data science imports, etc. for about a year. I’ve got a decent background in SQL and some learnings in NoSQL.

Based on experience, I think I would be a better fit for a Data Engineering role, but I don’t know if I have the knowledge, and really don’t know if I have the experience.

Long story short, could anyone help in pointing me in a possible right direction if I were to pursue this path?",2021-08-19 04:05:27
myanhk,Are Apache Spark developers in high demand? Is it easy to get DE a job for juniors who know Spark?,"So my point is because there are a lot of programmers out there who know Java, PHP, JS etc. there is a fierce competition to get a job with these languages. Is the situation same for Apache Spark and general DE job market? Is it easy for a junior to find a decent job with Spark?",2021-04-25 15:33:18
m4bxxc,Is there a shortage of Spark/Scala developers? What is the average salary?,Will it be hard for a beginner to get a Spark/Scala job? What salay can a beginner expect?,2021-03-13 18:18:52
i3udh6,Lazy Data Scientists,N/A,2020-08-04 23:31:51
hulhhd,what core skill i should upgrade when working in data engineer jobs ?,N/A,2020-07-20 13:44:24
14hi03a,Coworker described as DE but nvr seemed to code anything?,"EDIT

Thanks so much for all of the comments so far. Learned sth new today that DEs have the option to use a lot of GUI-driven tools to maintain their workflows & effectively doing less coding as a result. Tbh no shame in that as long as things work as intended, but, still, just color me surprised. 

Anws, more feedback/comment is always welcome!



—————————————————————————————



TLDR

I'm a DA/DS first & foremost w/ limited knowledge in DE. 

A coworker who previously worked in a global consulting firm *as a DE*, who's supposed to be a DE seems to not have any working knowledge coding skills. Only seems to know basic SQL, close to no Python but tbf to the guy, does have mad Power BI visualization & dashboarding skills... 

Just found the whole thing very strange. I kinda get not being a Python wiz cos I can kinda justify doing DE w/ only SQL but the DEs I've worked w/ apparently can code in Python pretty well... Given Airflow, PySpark etc. as pretty popular tools for DE needs.

Y'all finding it sus too right? Or am I too harsh on him?





—————————————————————————————

Recently got a new gig at a small shop as a DA but implied that the role will involve some DE responsibilities. Tbf I kinda set myself up here cos I've been mainly DA/DS but rly wanted to get up close & personal w/ data at a lower level so this role is perfect.

The company apparently also recently hired another person (let's call him Joe) w/ the same kind of work setup/expectations but Joe has DE experience. Joe previously worked at a global consulting firm but he just immigrated & that's why he needed to start small so to speak. If I namedrop the consulting firm name, chances are u know. When I learned of this, I was ecstatic. I'll be working closely w/ Joe anws & I can't wait to learn DE stuffs from him.

At first things went well. He appeared to know his stuff cos he had a way of explaining them all conceptually. The technical terms & buzzwords, even introduced me to some of the tools/platforms I wasn't aware of or expanded the usages I wasn't aware of for some tools I know previously.

But that's the extent of that... Just in words. So far what Joe has been doing are writing some very basic SQL queries & mucking around in PowerBI almost all the time. And no, Joe didn't even seem to understand Power Query M & DAX as he just connected his data model to prod server, query a bunch of data as is & display them. But I gotta give it to him, the visuals are pretty to look at.

Can't overstate how dissapointed I was... Not to mention I walked over to Joe's desk one time to discuss abt sth & on his 2nd screen there's a YouTube tutorial that displays glaringly ""What is Matplotlib"", which prolly means he doesn't know Python as well. But props to Joe he's actually doing sth to expand his skillset.

I didn't mean to be derisive, Joe's a cool dude as a person but I'm genuinely confused as to how a DE in a global consulting firm appeared to be so detached from actual code. Did he just get lucky/exaggerate his technical skills? 

On another note, I think I can imagine a world where DEs don't rly need Python & can get by just w/ SQL but given the increasing usage of tools on the cloud and/or as a service for DE needs, hiring someone w/ very limitied coding skills seems to only make life difficult.

Any comment/feedback is greatly appreciated. I just feel off throughout the whole thing so far.",2023-06-24 03:19:59
ldv48w,How I Learned Data Science (resources to get a job) in 2021,N/A,2021-02-06 10:21:36
14dzv4e,"John Oliver, Data Engineer.","IYKYK

https://preview.redd.it/6oyozg5yf37b1.png?width=1024&format=png&auto=webp&s=969cd6952fb41ae577b139f70b95c96b60a928f4",2023-06-20 03:38:45
12lobgb,Data Engineer is terrified of programming,"Hello:  
My employer has hired a data engineer/architect. We went over his grand design to replace the firms litany of legacy applications that handle everything from real-time transaction data to automated report-making, to email notifications, to FTP, to orchestration. Overall probably 10-14 different applications. Our architect's design is as follows:  


* Azure Data Factory to handle literally everything.
* Logic Apps to handle email notifications, since that's the one thing ADF can't do.
* A SQL database. 
* Power BI Paginated for reporting.

That's it. That's all the tools we shall ever require. I think this stems from a phobia of coding, some of his behavior and opinions corroborate with this.   


I'm writing to ask you all: is Azure Data Factory really that good? Is it typical ""best practice"" ""industry standard"" to not involve any amount of code? What's your thoughts on low-code? Personally, I think there's some glaring issues with the architecture, but I want to see if I'm missing something not-obvious before opening my mouth.

Thanks!",2023-04-14 07:31:22
12hvsjf,Which organizations have the strongest developers?,"This is a bit of a career question that turned into an argument.  Which organization would you say have the most skilled/advanced developers (not DEs exclusively, but developers in general)?

[View Poll](https://www.reddit.com/poll/12hvsjf)",2023-04-10 20:51:03
11e2rqj,ETL vs ELT: Check out the major differences,"ETL (Extract, Transform, Load) and ELT (Extract, Load, Transform) are two different approaches for data integration in data warehousing.

In ETL, data is extracted from various sources, transformed to fit the target schema, and then loaded into the data warehouse. In contrast, ELT loads the raw data into the data warehouse and then applies transformations as needed.

The main advantage of ETL is that it can handle complex transformations and can be more efficient when dealing with large datasets. ELT, on the other hand, allows for faster loading of data and greater flexibility in performing transformations.

Ultimately, the choice between ETL and ELT will depend on the specific requirements and constraints of each data integration project.

&#x200B;

https://preview.redd.it/tm49toucpwka1.png?width=1466&format=png&auto=webp&s=bed1ec98649c37be3071b485fd8d82868de9ef57",2023-02-28 10:28:23
10xw9f0,Querying 1 Billion Rows of AWS Cost Data 100X Faster with DuckDB,N/A,2023-02-09 14:36:12
z0gic9,How can I become a data engineer now?,I learned sql and python from multiple udemy courses. My hands on work comes from tests and projects in those courses. Is there anything else I should learn before applying to jobs? For context - I have 5 years software sales experience looking to get out of sales and into a technical role,2022-11-20 21:53:54
yhrydo,Where to store data for long periods of time? (100+ years),"I want to store my data for long periods of time. I have my flash drive but from what I've heard the data could be corrupted in 2 years. I could store it in google drive, but I have only 15 GB max storage. All help is helpful, thanks!",2022-10-30 22:24:14
y85exo,Entering DE,N/A,2022-10-19 15:16:35
muukee,Top 50 Data Engineer Interview Questions And Answers,"Being prepared for a Data Engineer interview with a question type that the interviewee might ask is an excellent way to acing the interview. No matter how experienced the candidate is because the interviewer’s questions usually target multiple areas such as compatibility of the interviewee, leadership skills, project management skills, knowledge of technical tools, and so on. 

Here Wissenhive has collected the top 50 [data engineer interview questions](https://www.wissenhive.com/blogs/top-50-data-engineer-interview-questions-and-answers) to help you in preparing for the data engineer field role.",2021-04-20 16:43:07
j9iwv4,"10 Key tech skills you need, to become a competent Data Engineer",N/A,2020-10-12 02:50:00
1634jq2,Successor of SQL,"What are your thoughts about EdgeQL? Do you think it can be the successor of SQL  
[https://www.edgedb.com/docs/edgeql/index](https://www.edgedb.com/docs/edgeql/index)  


Do you think it is better and more intutive than SQL?  
EdgeDB is started by creator of async/await, uvloop and asyncpg : Yury Selivanov",2023-08-27 22:41:30
1955990,Is Databricks a niche enterprise platform?,"I might be shortsighted about this topic and I wouldn't have any problem in admitting it. However, I've never talked to a DE that has worked with Databricks, ever. I've worked in mid-sized companies and Databricks has never been a topic discussed.  
Most positions I see don't ask for Databricks knowledge or experience, at least in Brazil, where I'm from, or Portugal, where I'm looking some opportunities recently. Looking at their website, it seems that only very large companies use their services. 

From a management point of view, why would you use another platform instead of using the cloud that your company already uses? Wouldn't it be cheaper and easier to negotiate some discounts (like reserved instances) and keep everything in 'one stack'?

I want to emphasize that I'm not saying the Databricks is useless or bad. I only wants to understand what companies use it and why.  
",2024-01-12 20:42:52
11ik4bm,Levels.fyi - why?,Any time I try to use it it's a miserable experience. Doesn't load/takes forever. Errors every other click. Why do people use it?,2023-03-05 01:51:22
15ucsst,Comparison: Snowpark vs Spark,"Thought some people might find this comparison interesting. 

Should you migrate your big data workflows from Spark to Snowpark? Are you wondering what all the fuss is about? You’ve come to the right place.

In this article, Snowpark and Spark go head-to-head as we compare their crucial features. We’ll discuss the tradeoffs between the two tools, backing our claims with evidence from a benchmarking analysis.

[https://www.keboola.com/blog/snowpark-vs-spark](https://www.keboola.com/blog/snowpark-vs-spark)",2023-08-18 06:47:10
14p0s2f,Data engineering salaries,Why are data engineering salaries bad when compared to software engineers for example ? I’m sure we have seen many software engineers with Magen 3 or 5 years of experience making above 200k while data engineers who have experience for 10 or 15 years make 140k or 150k if lucky,2023-07-02 22:09:34
12pyojl,I started a newsletter about DE and AE,"Hey folks!

I started a newsletter called Simetrique about Data Engineering and Analytics Engineering. I am trying to make it not about latest news and trends (however it's impossible to avoid), but rather to write about topics that are interesting to myself. For example, interesting tools, approaches or articles. Also, sometimes I'm going to write about my personal experience with data and analytics.

I'm currently an Analytics Engineer, and held a lot of analytical titles (such as data analyst, BI engineer and even a head of BI) for the past 10 year. So the data is my passion, especially its engineering part.

First 4 issues are already published so you can go and check the type of content I'm going to post. Planning to make about 1 newsletter a week, so not going to spam you.

[https://simetrique.substack.com/](https://simetrique.substack.com/)",2023-04-17 22:01:44
ztbebo,Scala or Rust? which one will rule in future?,"Hi everyone,

In recent days, I heard a lot about Rust for data engineering. What are your thoughts on this?  
What do you think, will Rust suppress Scala? Will Rust rule over?

\#dataengineering",2022-12-23 09:42:57
y6pypz,Migrating SQL-based dbt models to python,"I have been using dbt heavily in my Snowflake Data Warehouse. My dbt models are all SQL-based but I would like them to move them python due to modularity and hiring python devs is easier than SQL.

I know dbt released support for python but the docs are almost non-existent and very few community articles. Also, Snowflake announced Snowpark for letting users write python. 

Can I just use Snowpark and eliminate dbt from my stack altogether? What would be the best approach here?",2022-10-17 23:16:05
1aytrbb,Is a position as a Junior Data Engineer a good entry point towards working with data?,"Hi, I’ve been working as a Web Developer for 2 years and I would like to change my career in the direction of Data Science/Data Engineering. For now I try to understand in depth what is the difference between those fields and if Data Engineering would be the right choice for me. 

Could some data engineers tell me how your working day looks like? Is it more about writing a code or about designing databases? 

I like python and have some experience with web development with Django as well as with pandas, numpy and PySpark. Besides I like theoretical mathematics. After reading some short descriptions of Data Science/Machine Learning/AI I feel like I would be the most interested in ML, but maybe Data Engineering would be a good entry point in this direction. ",2024-02-24 12:48:40
ohkbuj,Big Data Pipelines - Real-life examples,"I'm looking for examples of real-life processing of data for later downstream use for Data Science and Machine Learning.

The case study should include:

(A) What the raw form of the data looks like: (1) What is captured (2) Frequency (3) Typical data sizes of incoming raw data.

(B) How the data is extracted, transformed, and loaded.  What were the key decisions and inputs received in order to determine what to extract, transform and load?

(C) Where is the clean data stored. What governance looks like for the clean data. How is this data maintained incrementally?

(D) Any examples where a Data Scientist or Analyst would come back and ask for some change to be done in the transformation pipeline that would help their work.",2021-07-10 15:04:11
1aithj0,How far can Excel take you in Data Engineering?,"In any of the following roles :

1. Data engineer
2. Data Analyst
3. Data Scientist

Which of the 3 roles uses excel the most (Generally)?",2024-02-04 17:39:28
17wzxr0,Why the Modern Datalake is Being Built Privately,"The modern datalake (or perhaps ""lakehouse,"" as you prefer) is remaking the enterprise data architecture. These modern datalakes have expanded to unprecedented sizes, scaling from petabytes to exabytes, while becoming more performant and multi-engine capable through the adoption of cloud-native principles and open standards. These datalakes don't fit in the public cloud for several reasons, the biggest being economics. This mismatch has led to a surge in the construction and migration of datalakes to private cloud environments and colocation data centers.

[https://blog.min.io/why-the-modern-datalake-is-being-built-privately/?utm\_source=reddit&utm\_medium=organic-social+&utm\_campaign=modern\_datalake\_built\_privately+](https://blog.min.io/why-the-modern-datalake-is-being-built-privately/?utm_source=reddit&utm_medium=organic-social+&utm_campaign=modern_datalake_built_privately+)",2023-11-16 22:50:38
qzht31,The US Dream,I am from India. I want to move to US and settle there. Can I get a International job offer in Data Engineer role from India. Has anyone done that?,2021-11-22 09:39:50
1ajj95k,How much data is too much data?,"I’m building a data tool to help you collect and analyze data from multiple sources. Some more key features include pre-built and custom metrics, AI-assisted querying of DB, alerts, built-in, and bring your own data sources.

What am I missing? Need help 🙏",2024-02-05 15:33:09
123wdcc,"The Data Engineer is dead, long live the (Data) Platform Engineer",N/A,2023-03-27 18:49:31
xyqi3i,"Guys, what tech in your opinion has the biggest impact on data engineering?","Hi people!  


We at CodeSquire, are wondering what tech has contributed the most to data engineering. Our team believes it is Airflow. What do you think?

[View Poll](https://www.reddit.com/poll/xyqi3i)",2022-10-08 11:32:46
hgmgzz,Data Engineers Fear to Data Scientist lol,"1.Does Apacha Spark big data Developer will be merged with Data scientists roles I heard in future many companies will.merge DS +DE as some does now .I'm interested into DE roles but verey bad at Stats and Maths .I don't want DS roles Can I get DE based roles alone without DS in future ?

2.Why are very few Projects on Apache spark does that make my growth as Big data Developer limited in future.

3.How would typical Data Engineer need Devops tools like Docker , Kubernets,etc Do He need manage AWS services ? 

Could some one give me some answers regarding above questions .",2020-06-27 03:44:03
1ay7kx7,HELP ME!,"Hi Everyone!

I need you help. I am working for a startup (let's call it *Darth Vader*) as intern (I have a background in Economics and Finance). One of my main tasks is to implement a process that has different steps:

1. Download data from Darth Vader's social networks accounts (Meta, TikTok, YT, LinkedIn): performance for each channel and for each post.
2. Copy all these data in a Google Sheets (*Dashbord*).
3. Compute some analytics: Engagement Rate, Followers Growth, etc.

Now, let's talk about the big issue that I have been having: since this task has started I have taken these data exporting all the CSVs for each social and importing them into the Dashboard. Now, my chief (CFO) wants to have the Dashboard updated 2 times a day. It is a really hard work and it makes me to waste a lot of my time. So, I would like to create an automatic ETL process to update that file. I have tried with Google APPs Script, using the API of each Social. However, the algorithm that I have written is seen as dangerous by Google and it does not work. Moreover, I have tried with some extensions of Google Sheets (like YT Metrics), but they are too expensive. Last, but not least, I have access to Zapier, within I have tried to create workflows to obtain data. Unfortunately, it has some negative points:

* It does not work for all the social networks.
* It does not provide all the data that I need.
* Bad connection with Google Sheets.

Extra: we have a free account on [Later](https://later.com/). But, I have never used it because I have no access.

Disclaimer: there is no employee that has competence in data engineering.

May you suggest some ETL processes, Tools, and/or Documentations for API? Unfortunately, I have skills in Data Analysis and Data Science and they are not useful for this task. I am in front of an Ocean and my Head is not able to give me a direction.

Thank You so much",2024-02-23 18:18:20
19fgizn,Is this for real? $1090 per month as a Senior Data Analyst?,"For context the salary converts to $1090 (4000 Dirhams) per month. For the pros here, what is an ideal salary for data engineers with the skills listed in the job description? Assume the area of work is HCOL.

https://preview.redd.it/gzfbr2sqsmec1.png?width=500&format=png&auto=webp&s=9c2842dc454f443c8c7df8aded9261b362cf9919",2024-01-25 18:45:28
195l6dn,How hard is it for a Data Engineer to move into ML/Data Science?,"I have completed a few projects on data engineering (real world projects and not demo projects). I know how and where to use the common tools. I have mostly been doing this on Azure. So I am familiar with services like synapse analytics, data factory, databricks (pyspark) , SQL databases. And PowerBI for visualization purposes. I had done the basics of ML(classification, clustering, some simple Deep learning, etc...) a few years ago. But that's not enough. How much time would it take to get decent at ML and and probably be amongst the top 15% in this industry globally?",2024-01-13 10:31:52
17xj3we,Am I qualified to jump to a mid/senior data engineer position from a business data analyst role?,"I'll try to keep this short.  I work at a Fortune 100 company with a ""senior engineer consultant"" title, although that doesn't really describe what I do.  I'm a business employee (not IT) and my day-to-day is:

* Run validations to ensure IT data replication worked (Python & SQL) 100+ tables, ~30M daily records.  Hadoop Hive, Oracle, Teradata.
* Pull tables from disparate data sources and compare to find differences (MS Power Query)
* Tell IT what changes they need to make to the tables 
* Write adhoc SQL reports

I spend most of my day in Toad writing SQL.  I make about $130k.  I want to move to an even more tech role (DE) but am concerned I lack qualifications.  My resume looks more like a business user and not a tech employee.  I hate Tableau and creating visualizations - I want to be more back end.  Writing new SQL is the best part of my job.

* 10 years strong SQL (CTE, multi joins, window/analytic functions)
* 5 years moderate Python, Java, AppsScript
* 5 years Tableau, Cognos
* 5 years Talend (ETL)

Am I qualified to apply for a mid or senior data engineer position? I can tolerate a modest pay cut but going back to something like $70k would hurt.  What do I need to highlight on my resume to get noticed outside of SQL and Python? A coworker suggested I should be focusing on dbt. Certifications worth obtaining?",2023-11-17 16:39:06
16xhh96,How do you teach yourself sql and python? I am confused.,"The question is pretty simple and straightforward but I don’t understand how people teach themselves anything. I guess I am just confused how to study/learn something on your own without any true guidance? I am trying to learn sql and eventually want to move onto learning tableau/power bi and python by myself. I don’t understand how to start and what to do. Can someone help here? In general, I guess how do people teach themselves and become proficient in anything?",2023-10-02 00:05:43
z8mcm6,Why the Fortune 500 is (Just) Finally Dumping Hadoop,N/A,2022-11-30 08:45:21
tttdoa,"Monte Carlo announces small-batch, locally sourced Data Observability Platform","Tired of mass-marketed, over-hyped tooling for the modern data stack? 

Rejecting synthetic data meshes in favor of authentic data fabrics, hand-woven by local craftspeople? This just might be the data quality tool for you.   

We’re excited to announce our latest release: **Data Observability Small Batch**, the world’s first platform for locally-sourced data!

To celebrate, we’ve put together the 5 pillars for small batch data observability.

&#x200B;

https://preview.redd.it/bmkmlixnqxq81.png?width=574&format=png&auto=webp&s=c5abdc8e197cf75e1af0b0b1344b44d24f4ed36d

Check out the full release: https://www.montecarlodata.com/blog-monte-carlo-announces-release-of-observability-platform-for-locally-sourced-small-batch-data/

**Oh, and Happy April Fools’ Day.**  

Disclaimer: I very obviously work for Monte Carlo. :D",2022-04-01 15:24:45
se0quu,"My co-worker explored low-code tools for building internal metrics dashboards, here’s what he wrote...",N/A,2022-01-27 15:18:40
r2rjmo,Any DE's from India?,"Any fellow Indians here? I wanted to know what is the tech stack you are working on and what is the compensation scene here.

Edit :

To elaborate further, we can cover the below questions I think :

What tools you use mainly , is it sql and python mainly or it's a mix of some other tools

Are you working as specialized DE or you are expected to some DS/DA work

Do you undertake DevOps part in your project as well?

What is the compensation like for YOE",2021-11-26 16:49:47
pcokrh,is dbt can be used at the enterprise level?,"I just start evaluating the dbt (data build tool) product for our analytics team to perform analytical transformations with dbt. But I am seeing some limitations with my testing. Can some please comment is below items achievable with dbt?

1. Can dbt support multi db connections in one model query? (like DB Link)
2. Can dbt read one snowflake and save the results into postgre? 
3. Can dbt read the data from s3 without EMR cluster (Spark cluster)?
4. Can dbt store the output to s3 or onedrive storage?",2021-08-27 14:35:18
oxvc3u,Spark Help,"Hello,

I've just started learning Apache spark. I have it installed on my system and when I type PySpark, I see the word ""Spark"" in characters. Next it takes me to a prompt >>>

How do I start my session and setup my first job from?



Update:  Udemy has a great course for freshers trying to understand Spark.
https://www.udemy.com/course/sparkstarterkit",2021-08-04 15:40:06
onr0t0,MacBook pro M1 for data engineering developer,"Hi, there is there anyone who has a MacBook pro M1 and using tools data engineering and rocker or Kubernetess? What is the impression",2021-07-20 00:30:35
cq607n,"Coursera Specialization Data Engineering on Google Cloud Platform has 30 days of free trial. Launched in July 2018 along with 11 other new specializations from Wharton, Johns Hopkins, Duke, Rice University...",N/A,2019-08-14 06:50:14
1b1ci1n,Senior vs Staff vs principal,"See lots of confusion behind how the 3 levels differ during the process of getting a job. 

What do the rounds look like for each? 

Best way to study? ",2024-02-27 13:59:51
1al7lro,Databricks vs Snowflake: A Complete 2024 Comparison,N/A,2024-02-07 16:56:43
1akxlj3,Synthetic Data In A Nutshell — Why Founding Went 10x In This Market,N/A,2024-02-07 07:35:37
1akgys6,Azure DevOps CICD + Databricks,"Hi DEs,

I am in my final semester in my graduate study. I am planning to build a project with Azure stack. I am planning to create a CICD pipeline with DevOps with 2 or 3 environments like (DEV, UAT, PROD) or (DEV and PROD) for Databricks notebooks. Is it a good project to showcase in my resume for job hunt ? What more can i add ?  


Your suggestions will significantly help me.",2024-02-06 18:38:57
1afxacb,Senior data engineer interview test,"Hi all,

I'll be recruiting a senior data engineer in a couple of months and need to design a test for the interview.

Did anyone have any examples of what they have been through during the interview process or something they have set themselves?  Just looking for a bit of inspiration!

Thanks",2024-02-01 00:17:33
18efcs2,How should I choose the macbook pro version?,"Hello everyone, I am currently aiming to become a data engineer. (currently I am a 4th year student and interning at a DE position). I'm wondering whether I should buy the 8gb/512 16gb/256 16gb/512 m2 series or should I buy the m3. Am I really wondering? Can you explain it to me?",2023-12-09 15:01:16
18bb4le,Databricks Certification,Any Dumps for Databricks certification exams - Databricks certified data engineer associate?,2023-12-05 12:41:32
181iebj,Looking for DE remote jobs,"Hey everyone,

I am looking for remote data engineering jobs. I have 5 years of experience in systems/data migration to Google Cloud and I am certified too.

I tried searching remote jobs in the US on LinkedIn but no results.

Are there any specific good platforms to find remote jobs or is anyone looking to hire here

PS: I also have few months of experience on GenAI, LLMs, Langchain and RAG.",2023-11-22 20:09:29
17x6t1p,ETL Hello World!,"Hi all, it's been a while.

We've been hard at work on the Pansynchro framework over the past several months, and recently got around to something we'd been putting off for quite a while: running benchmarks of Pansynchro against some other data synchronization products.  The results were a bit surprising: we knew Pansynchro was fast, but we didn't realize it would even beat SSIS, with its reputation for high performance, by such a wide margin!

Results: [ETL Hello World!](https://pansynchro.tech/etl-hello-world/)",2023-11-17 04:20:24
17wzniy,"How to ""Break-In"" to the Industry When I Already Have Solid Academic/Work Experience?","Hello all,

Preface:

- Bachelors double major in Data Analytics & Finance, minor in I.T (2019 grad)
- Master of Science in Data Science (Machine Learning) (2023 grad)
- 3 Years of relevant experience doing data migrations (lots of SQL, Excel, VBA, Azure/AWS, Shell scripting)

I know I have a lot of the credentials to get into the industry - but I'm still having a hard time landing a role. Whether that role is analytics, DE, BI, DS or anything in that vein. I'm not necessarily discouraged since I love what I do currently - I just feel like a lot of my academic background is being completely wasted. 

Are projects still relevant to someone like me given my background? I have a small portfolio with 3 projects on it, but I don't know what else I can do to stand out and get one of those roles. 

Is it possible that my home country (Canada) doesn't have enough opportunities? Is my resume not strong enough to even get an entry-level role based on everything I've already laid out? 

Also, would any of the AWS/Google/Azure certifications/exams be worth my while? I know they hold some value in the industry.",2023-11-16 22:38:32
17qg6u5,How would you compare the recently released mage ai when compared to Orchestration tools like airflow etc?,Here’s the link to mage live tool : https://demo.mage.ai/overview,2023-11-08 07:08:15
17jubdy,We added REST support for our open-source streaming platform,"Now, you can streamline data effortlessly and quickly regardless of your programming language by simply using REST for producing and consuming messages, including straight from your front-end with memphis.dev.

If you are not familiar with us - Memphis.dev is a highly scalable event streaming and processing engine. Before Memphis came along, handling ingestion and processing of events on a large scale took months to adopt and was a capability reserved for the top 20% of mega-companies. Now, Memphis opens the door for the other 80% to unleash their event and data streaming superpowers quickly, easily, and without breaking the bank.",2023-10-30 14:11:54
17hrlkt,Understanding Databricks Unity Catalog for Unified Data Governance," The [Databricks](https://www.linkedin.com/company/databricks/) Unity Catalog is a centralized managed metadata solution provided in the Databricks workspace with unified access control, auditing, lineage, and data discovery features. They are built on Delta Lake and offer a centralized location to manage all organization data assets.  


🔗 Databricks Unity Catalog:  [Databricks Unity Catalog for Unified Data Governance (mssqltips.com)](https://www.mssqltips.com/sqlservertip/7827/databricks-unity-catalog-for-unified-data-governance/?utm_content=head) ",2023-10-27 16:35:44
17ahjq0,AWS Certified Data Engineer - Associate Beta Exam for 75usd between 27th Nov 2023 to 12 Jan 2024,"[Link](https://aws.amazon.com/certification/certified-data-engineer-associate/?mkt_tok=MzAyLUNKSi03NDYAAAGO3xAzPFpa6UKE2O0Ei_onjQWL1_C2lc-fcUbOAaGuj7V88Pb-EQPamfiYugDCUTjylDD65FA87AUDQvIxOdbiuVuky64LJwlyGdFGN_D7MBCM) to the source . 

   

This is my first AWS exam and i am willing to take this opportunity to expand myself . is there anyway to get some familiar material even though its in a beta exam . ",2023-10-18 03:56:33
177orgc,12 Best Data Engineering Online Courses,N/A,2023-10-14 12:46:06
170vu6a,Any Data Engineers available for a quick interview?,I'm taking an organization development course for my undergrad which requires me to ask a professional who currently works in your dream role about organization development at your company and your perspective. Preferably hop on a Zoom call so I can acquire a text transcript for notes. Please let me know!,2023-10-05 22:43:19
16y6c83,Can't find a job (switched from Backend to Data),"I started my career in 2017 working as Backend Developer, however, two years ago I decided to move to a Data Engineer role in my previous company. I requested the switch to the company because I wanted to try something new and I'm passionate about the data world and everything its involved. 

However, two years later I was layoff with some other team members. Now, we just two years of experience as Data Engineer it's getting difficult to find a new job remotely or even in my country (I'm from Latin America, I worked for US based companies though staff company). 

These days I've been study and getting certified in some tools like Apache Kafka, dbt and Azure, maybe this would help me to find something. I got the Databricks Apache Spark recently, but I worked with Spark for two years. 

Anyway, I've been thinking to switch back to backend. 

What would you recommend me, what your thoughts? 

Just want to read some comments or maybe similar experiences. ",2023-10-02 19:48:03
16ltlsm,Curious if other people experienced this data accuracy problem and if you think this could help fix it,"During my last 6 years leading data science teams at Stripe we faced a challenge to represent consistent and accurate data across teams, which sometimes led to stakeholders distrusting the data. Especially as the number of users and tables grew, it took a lot of asking around and ad hoc querying to understand how to use tables.

With the recent advancements, LLM's could potentially help solve this problem. I started working on a SQL tool that finds similar, verified, SQL code snippets in your code base and then provides suggestions using a LLM, such as on which columns to join or what values to filter. Which results in quicker and more accurate SQL development.

Here’s a short demo video that shows what it looks like: [Demo Video](https://youtu.be/JP5iA-guf-Q)

Would love to get some feedback from this group! Even though this is an early version, is this something you'd find useful? Any features you'd love to see or concerns you might have?

Also, lmk if you are interested in trying it out!",2023-09-18 12:00:23
169o3x4,Semi-structured data modeling,N/A,2023-09-04 10:09:05
1665pmu,Dask vs Spark,"People often ask how Spark compares to Dask. This is a hard question to answer  well, since It largely depends on the type of work you're doing. There are a lot of different areas to consider on a case-by-case basis. We try to answer that in an unbiased way in [this blog post](https://medium.com/coiled-hq/spark-vs-dask-27216502b129), but that's hard. Interested to hear what you think.",2023-08-31 08:55:09
15z0etr,How to Install Hadoop in latest Macbook?,"For those who are looking for step by step guide on installing Hadoop onto Macbook M1 and M2, this is the blog I highly refer to. 

&#x200B;

[https://pub.towardsai.net/how-to-install-hadoop-on-macbook-m1-or-m2-without-homebrew-or-virtual-machine-ac7c3c5a6ac9](https://pub.towardsai.net/how-to-install-hadoop-on-macbook-m1-or-m2-without-homebrew-or-virtual-machine-ac7c3c5a6ac9)

&#x200B;

&#x200B;",2023-08-23 10:55:02
15uoxjp,NEW VIDEO 🥳 What's new in Apache Airflow 2.7?,N/A,2023-08-18 16:16:34
15rndo6,Polars vs Sql Query Performance,"I’ve been designing various ETL processes within pandas for some time now. One of the use cases I come across frequently, particularly within data migrations, is to read data in from a sq query, run some complex manipulations using Pandas, otherwise unachievable (or at least very complex) using SQL.

In some cases, particularly for large transactional tables of 50mil plus rows, we’ve had memory issues which we have resolved via chunking, or by resorting to constructing very complex stored procedures in SQL where chunking is not ideal (often where a series of GROUP BYs are required). Another issue is that these processes never beat the performance we could get if we constructed the queries within SQL, even though they are much easier to maintain and create. We have had issues where a data migration would simply take too long for certain tables for a go-live script.

I’ve been reading into Polars, and am going to start structuring some reports to get a feel for it - ultimately, I would like to replace stored procedures where possible.

One thing to note here, reading through the docs, is that I usually use SQLAlchemy for the data download - but I notice polars explicitly stated the benefits of using Connectorx which might solve some of this issue for me.

Has anyone done any benchmarking to compare native SQL queries to the same query performed in Polars? Particularly interested in processes that drop duplicates rows etc - queries that are not easily made within SQL once you start having multiple queries, temp tables and ctes (even with window functions).

Equally, if these types of benchmarks have not been made, I might do some benchmarking of my own - I would be interested in some example functions that are much easier to construct in polars vs sql that anyone would like to be included.",2023-08-15 09:10:50
15l3qqx,Why people uses celery with Django?,"I have been messing around with celery for a month now, and most of the resources online must mention Django somehow. I just want to use it for task consumer on `AWS` `ECS` or `fargate` (have not figured it out)  


# Questions
1. Why Django + celery?
2. Why people don't use standalone celery and communicate with other application through task broker
# Bonus Questions
- How do I develop with SQS locally? Should I do it online instead?",2023-08-08 01:34:28
15gkxb6,Send data to Salesforce from BigQuery,"New data engineer here. Our analytics / reporting views are on BigQuery and there is a need to upsert this data daily into Salesforce objects. Our business users use Salesforce. 
Any inputs on what the best approach could be. Volume is in 10-15GB range.",2023-08-02 21:44:47
15g6t8n,Job Demand in the time of Gen AI?,"I'd like to know what your take of DE Demand in the time of Gen AI.

I'm taking degree to become a Big DE in 2 years' time. But I'm concerned about the future demand of this job. 

What're you thinking",2023-08-02 11:49:20
15g1rc5,Should I switch from Data Science to Data Engineering?,"Hello all, I am an undergraduate student pursuing a bachelor's degree in computer science (halfway through it). I have been learning Data Science and Machine Learning for two years now and have also worked on many portfolio projects. Lately, I am getting concerned about the job market in data science and am considering switching my career to Data Engineering. I am here to seek advice from experienced professionals who have been in the industry for some time on whether I should change my career to Data Engineering or stay focused on Data Science.

The transition to Data Engineering might not be too challenging for me, as I am already familiar with data concepts and techniques. If I start learning Data Engineering now, by the time I graduate, I will have the necessary skills to pursue a career in this field.

I would truly appreciate any insights, suggestions, or experiences you could share with me. Thank you in advance!",2023-08-02 07:12:29
15e17ma,Non-proprietary data architecture,"I’m currently identifying on which platform I’ll build my analytics/ml platform and I’m looking for second opinion.

Requirements:

-Non-proprietary solutions. I don’t want anything that is exclusive to Azure/AWS/GCP. After seeing what Microsoft is doing with Intune I can’t allow this to happen (changing price, features and else on the fly)
-Easily scalable. From a few gb per customer to petabyte class customer.
-Real-time analytics , efficient and fast data ingestion, delivery
-Full support and optimization of Nvidia rapids library/Intel AMX
-Designed and optimized for kubernetes

So far it seem like only Apache solutions (sparks, kafka, nifi, hbase and else) is making the cut. Am I wrong?",2023-07-31 00:26:03
14sjfg6,7/24 ETL Job Monitoring,My company wants me and my friends not to sleep on a regular basis and monitor ETL jobs all the time. Is this normal for companies or I am in a dead-end position?,2023-07-06 19:37:35
14sf3co,DE new on the job,"Hi guys,

Just started working as a DE at a startup company. I was wondering, how long did it take for you guys get familiar with the data model of the company you're working for? 

I'm pretty proficient in SQL and Python, but I feel like I'm unable to help with the more complex tasks because I'm not too familiar with the data model yet. I often use the information_schema tables to find the tables I'm looking for. Should I be able to memorize the tables at some point? 

Any tips on how I can speed up the process of getting the know the data structures?",2023-07-06 16:57:39
14iy7nv,Data engineering framework,"For data science , there is a framework called crisp dm that is used to build scale able datascience projects 

Anything like that for building large data pipelines?",2023-06-25 21:41:27
13l0o5d,"Will exploring research papers from google, meta or microsoft make me a better data engineer?",I mean are those papers just blahblahblah or they contain a very much practical information which will help me become a better data engineer and make me more marketable and valuable?,2023-05-18 14:28:42
13c8a6g,Will I have fun pursuing a data engineering job?,"Short bio for myself. I am an electrical engineering grad who has worked as a power distribution project manager for two years. I honestly do not love my job and recently, as a hobby, I've been enjoying programming. I'm currently taking the CS50 Harvard class for computer science. When I was in school, I really loved MATLAB and modifying arrays in Python (for example, making a program where the compiler tells what is or is not a prime number). I'm sorry if this is a stupid question, but is data engineering right for me?",2023-05-08 23:15:57
12xhg6o,"Visualizing Bitcoin to USD Exchange Rates using FastAPI, Prometheus, Grafana, Deploy with jenkins On Localhost Ubuntu Server 20.04"," In this article, we’ll explore how to visualize the exchange rate of Bitcoin to USD using FastAPI, Prometheus, Grafana, and Docker. We will create a simple FastAPI application to import exchange rate data from an API, store it in a database, and expose it as metrics using Prometheus. Then, we’ll use Grafana to create dashboards that visualize the data, and deploy the whole setup using Docker and Jenkins 

&#x200B;

https://preview.redd.it/4ly1yiz9duva1.png?width=1908&format=png&auto=webp&s=1e083f76d0c2d8430ae068f20a6067bd7fbf0b1e

[https://medium.com/@stefentaime\_10958/visualizing-bitcoin-to-usd-exchange-rates-using-fastapi-prometheus-grafana-deploy-on-jenkins-76c7e3aa30e1](https://medium.com/@stefentaime_10958/visualizing-bitcoin-to-usd-exchange-rates-using-fastapi-prometheus-grafana-deploy-on-jenkins-76c7e3aa30e1)",2023-04-24 14:22:32
12pb72z,Finding mentors for data engineering,"Hi Everybody,

Im here to express my  thoughts that are currently going on working as Data Engineer.  
Im in the journey of promotion ladder process in my company. I feel Im stuck as Data Engineer, I need some tips to be a Senior Data Engineer.   


Im looking for some mentorship programs online, I don't find anything suitable ( have you heard of [https://www.jointaro.com/](https://www.jointaro.com/) , any reviews? )

&#x200B;

Backgroud: Im Data Engineer, working in DE-MLOps team. Work is more oriented to building the data pipeline for ML model, but most of my tasks would be on terraform. The queries, code would be written by Data scientist, All we have to do is take the code and build pipeline. It was interesting before, coz I was new to IaaC and cloud. Now I feel it as bit boring. How do I create an impact on projects like these which can help me to climd my career ladder.",2023-04-17 10:36:34
12e5kv9,Given the choice: Snowflake or Databricks to work for?,"Both are pushing hard on their own product lines.

Neither can say they are true market leaders.

But, if you had the choice to work at one which one would it be and why?

To the Snowflake and Databricks employees in the sub, what would you say is good about your respective company?",2023-04-07 01:47:50
123oh8h,DSA For The Rest Of Us - Part 2 Introduction to Binary Search with Rust.,N/A,2023-03-27 14:28:04
123h5li,What do you think about the Lakehouse concept?,"Databricks first bring out this concept. However, I think they described a big dream but hasn't made it come true. The performance of querying data in a data lake still cannot be compared with querying data in a data warehouse. Are you using data lakes now? Do you think Lakehouse can be a promising direction?",2023-03-27 09:07:48
11tta5t,We just launched the ability for companies to import data from their customers' data warehouse or database,"Hey r/dataengineering! Charles here from [Prequel](https://prequel.co). We just launched the ability for companies to import data from their customer’s data warehouse or database, and we wanted to share a little bit more about it with the community.

If you just want to see how it works, here is [a demo of the product we recorded.](https://www.loom.com/share/4724fb62583e41a9ba1a636fc8ea92f1)

# Quick background on us:

We help companies integrate with their customer’s data warehouse or database. We’ve been busy helping companies export data to their customers – we’re currently syncing over 40bn rows per month on behalf of companies. But folks kept on asking us if we could help them import data from their customers too. They wanted the ability to offer a 1st-party reverse ETL to their customers, similar to the 1st-party ETL capability we already helped them offer. So we built that product, and here we are.

# Why would people want to import data? 

There are actually plenty of use-cases here. Imagine a usage-based billing company that needs to get a daily pull from its customers of all the billing events that happened, so that they can generate relevant invoices. Or a fraud detection company who needs to get the latest transaction data from its customers so it can appropriately mark fraudulent ones.

There’s no great way to import customer data currently. Typically, people solve this one of two ways today. One is they import data via CSV. This works well enough, but it requires ongoing work on the part of the customer: they need to put a CSV together, and upload it to the right place on a daily/weekly/monthly basis. This is painful and time-consuming, especially for data that needs to be continuously imported. Another one is companies make the customer write custom code to feed data to their API. This requires the customer to do a bunch of solutions engineering work just to get started using the product – which is a suboptimal onboarding experience.

So instead, we let the customer connect their database or data warehouse and we pull data directly from there, on an ongoing basis. They select which tables to import (and potentially map some columns to required fields), and that’s it. The setup for your customer only takes 5 minutes, and requires no ongoing work. We feel like that’s the kind of experience every company should provide when onboarding a new customer.

# How do we do it?

Importing all this data continuously is non-trivial, but thankfully we can actually reuse 95% of the infrastructure we built for data exports. It turns out our core transfer logic remains pretty much exactly the same, and all we had to do was ship new CRUD endpoints in our API layer to let users configure their source/destination. For those interested in our stack, we run a GoLang backend and Typescript/React frontend on k8s. We are also huge fans of [DuckDB](https://duckdb.org/) and use it heavily.

In terms of technical design, the most challenging decisions we have to make are around making database’s type-systems play nicely with each other (kind of an evergreen problem really). For imports, we allow the data recipient to specify whether they want to receive this data as JSON blob, or as a nicely typed table. If they choose the latter, they specify exactly which columns they’re expecting, as well as what type guarantees those should uphold. We’re also working on the ability to feed that data directly into an API endpoint, and adding post-ingestion validation logic.

We know that security and privacy are paramount here. We're SOC 2 Type II certified, and we go through annual white-box pentests to make sure that all our code is up to snuff. We never store any of the data anywhere on our servers. Finally, we offer on-prem deployments, so data never even has to touch our servers if our customers don't want it to.

We’re really stoked to be sharing this with the community. We’ll be hanging out here for most of the day, but you can also reach us at hn (at) prequel.co if you have any questions!",2023-03-17 14:50:18
11ts4oz,How to migrate data from one software to another?,"Hello, 

I am working on building a property management software, and I wanted to know how could I migrate data from one software to another to make customer onboarding easier. 

Is there a third party tool to help with this process? If so how would it work? 

Thank you.",2023-03-17 14:08:26
11r8x0u,How is the job marketing right now?,"Hi,

Recently, I have been applying the DE in US for a short while. But the response rate is a little low <3%.

It is frustrating and I'm not sure it is my problem or the job marketing itself right now.I'm kind of the new grad in the DE role since I was DS before but having some experience with ETL and cloud.",2023-03-14 15:17:52
11qn7sy,DE Design Interviews,What are some good resources to prepare for the pipeline/ETL design questions for a DE interview? I am looking for some books/guides/articles/blogs with a lot of practical examples to master the concepts needed for these rounds.,2023-03-13 21:58:19
11ksnif,Starting a career in tech with Data Engineering?,"Introduction, I'm a sophomore(Bachelor) in College who wants to be a Data Engineer. I'm currently in the process of learning Python Frameworks and SQL.

From what I've seen in this subreddit and Job listings is that DE jobs are more suited to people who switch from Data Science/ Data Analysis and the crowd is mostly based on that niche. Now, from what I've heard switching from core Data Science has the advantage of having learned Machine Learning and hardcore Data Science. Moreover, a lot of job listings that I've seen actually specify that the Data Engineer has to implement Data Science and Machine Learning. I guess this is required so that DEs have better communication with Data Scientists, who are the end user.

Now, the major problem I have, related to this, is that the Data Engineering Roadmaps and Bootcamps don't ask you to learn Data Science or Machine Learning. Also, my college degree doesn't let me access the Linear Algebra course(they replaced it with ODE/PDE), also I don't quite like Math. The Thing is that learning DE tools(Like Spark and Hadoop) alongside Data Science(which is more mathematical in nature) would be quite hectic for me as I aspire to crack a Remote Paid Internship within an Year. For now, this is my primary goal. I am willing to learn another stack if need be.

Please advice me on what my course of action should be. Should I change domain? Or should I learn Data Science?",2023-03-07 07:31:16
11ijb0o,studying for dp-900,"hey there I am studying for the DP-900

&#x200B;

anyone want to jump on discord or msteams and do some screenshare for this practice test I have?

&#x200B;

Figure we can explain the answers to eachother 

&#x200B;

my skill level is beginner

&#x200B;

thanks let me know!""",2023-03-05 01:14:54
118a91y,Interview Goes live February 22nd on your favorite podcast app.,N/A,2023-02-21 17:08:23
10yxr90,Where do Collibra developers find jobs?,I am trying to engage some Collibra developers but the Collibra subreddit is gone now - what is the right place to find smart and interesting Collibra people? Thank you!,2023-02-10 17:32:57
10xfhxz,To FAANG or not to FAANG,"Hey everyone, I would like to ask for some career advice and other perspectives. I'll try to give a clear idea of what is at stake but without being too specific for privacy reasons.

Background is I'm a data engineer with solid exp in Backend + Devops (and some analytics) and around 10 years experience spread out between big and small companies but lately more startups. I live in a ""poor country"" and I work for a small startup in the US. Which means I get a great salary in USD without any benefits, but it's really great specially considering cost of living here.

Now to the latest events: I have been applying for a FAANG job that was always kind of a dream job to me and I finally got the job, it is definitely one of my favorite FAANGs and it's to work from my country but alongside the US team on cool projects, my manager would be in the US. All looks great from the work perspective. Now when it comes to compensation and place of work is when things get complicated. They would require me to move to another state of my country because that's where the countries' HQ are and I would be required to work around 3 days a week from the office.

So my first consideration is moving to another state. I wasn't willing to do that because family and friends are where I currently live, which impacts on life quality, but anyway I could probably do it for a dream job.

Second, compensation, this one is more complex because some parts of it are subjective and I have to do some more maths still, but roughly, nowadays I am technically a contractor on the startup so I have to pay for a lot of stuff (accountant, health insurance, have no legal work protection, no benefits such as gym allowance etc.), but still the compensation at my current job is around 40-50% higher than the offer from the FAANG (that's why they hire ppl in poor countries lol). I'm not super greedy and have a simple life but that ought to be considered.

Other considerations which are minor but added up should be considered are: health insurance at the FAANG is the best in country, not to mention all the other perks these FAANGs usually have. It is also one of the FAANGs that are very well in the market rn even in this turbulent times. Now about my current job, I am currently working a lot and it has been very stressful at the startup (12h+ are more common than 8h a day), the startup I am working on now is not on a bad situation but there's a lot of instability in the market overall, we were expecting to get investment earlier but with the current market we're waiting a bit longer to get a better evaluation. Both positions are for data engineer that match with my skills and both have nice tech stacks that I'm interested in working with, maybe I would give some points to the startup because we have more freedom to experiment with new tech, I imagine at the FAANG that should be more rare, but still, the overwork kinda compensates for that.  
Please AMA I might have missed that can be answered in a not-too-specific way. POVs are very welcome.",2023-02-09 00:07:35
10vzi9w,How will ChatGPT and generative AI change data analytics?,N/A,2023-02-07 11:24:42
10m5e3f,Data Teams and DuckDB,[https://www.youtube.com/watch?v=8VfPX06bbK0](https://www.youtube.com/watch?v=8VfPX06bbK0),2023-01-26 23:05:25
10f8o4k,I'm having around 2 YoE. Thought of making a switch within a year. Looking to get into top product companies. Any advice to optimise my resume would be appreciated. Thanks in advance. Also I'm confused about which resume to use. Single column or Two columns.,N/A,2023-01-18 14:27:39
10ashfn,PySpark — Load BigQuery table into PySpark dataframe," 

It becomes very important if you are working as a data scientist / data engineer or doing data analytics and have to write a lot of PySpark code mainly for data science, data engineering or analysis-based projects, such as data modeling, data processing, and data analytical algorithms, then it is your responsibility to improve the efficiency of PySpark code when you are doing anything within PySpark.

[https://macxima.medium.com/pyspark-load-bigquery-table-into-pyspark-dataframe-e660fc9d9b0](https://macxima.medium.com/pyspark-load-bigquery-table-into-pyspark-dataframe-e660fc9d9b0)",2023-01-13 11:41:34
zorwxh,Any EU D.E's?,"I'm looking at moving to the EU and wondering if anyone has any experience in working D.E roles with English only.

In Australia I've been on 98k€ for permanent positions or 579€ per day for contract and I'd like to receive a similar amount.

I have an EU passport.",2022-12-18 06:23:53
yyw5gz,Why Data Engineer Salary Levels are on the Growth Path Globally,N/A,2022-11-18 22:14:07
y8v78p,Building An Open Data Lakehouse With Dremio - Episode 333,N/A,2022-10-20 11:04:37
x6d02q,Emerging Trends of Big Data Technology in China and the U.S. in 2022,"2022 is a year of full of global turmoil while witnessing the rapid development of global data technology. During the year, the new generation of Chinese Internet companies such as TikTok, SheIn, Shopee, etc. has obtained partial results in globalization. The new generation of data technology stack MDS (Modern Data Stack) in Silicon Valley in the US has flourished. And the cloud-native data tech companies have attracted the attention of global capital in OLAP engines and the DataOps engines space. All the changes foster the trend of data technology globalization.

Making a general observation, from Hadoop, Spark, and the data platform (Data Middle Office) to the new generation of the data technology stack, we found that big data has also entered a new stage actually.

The early stage of big data technology (2005–2015)

The big data technology stack dominated by Apache Hadoop, Apache Spark, and Apache Oozie, has gradually replaced the commercial data warehouse technology stack, which was dominated by Teradata and Greenplum in the past, and quickly took over most of the market by its features of distributed, high-performance, open source, and free charge mode.

At that time in China, it was a time that Baidu, Alibaba, and Tencent underwent an entrepreneurial rise. Encouraged by the successful testimony of a large number of open source software by Google, Amazon, and AOL in the U.S., domestic Internet companies began their big data journeys. As a result, the rapid growth of open source users of big data in China speeded up the construction of big data platforms in traditional industries, which mainly aimed at replacing the ODS layer and unstructured data storage and processing based on Hadoop and Spark.

Open code, low-cost X86 hardware support, and an easy-to-use SQL ecosystem allow most of the medium-sized Internet companies in China to continue to use this system for big data management and mining in the Hadoop/Spark ecosystem.

📷

For details, please refer to [https://medium.com/codex/emerging-trends-of-big-data-technology-in-china-and-the-u-s-in-2022-1e372e630e57](https://medium.com/codex/emerging-trends-of-big-data-technology-in-china-and-the-u-s-in-2022-1e372e630e57)",2022-09-05 10:37:48
x1kuhw,How Carguero Kickstarted Their Data Quality Revolution,N/A,2022-08-30 14:40:28
wgb4bt,Execute a notebook from another notebook in Databricks,N/A,2022-08-04 20:17:46
uzf166,Going back to pursue a STEM degree worth it?,"I'm sure by now this sort of question has been asked a million times, but I've noticed that when applying for jobs a lot specify the following requirement:  


* Bachelor or advanced degree in Mathematics, Computer Science, Engineering, Science, Econometrics, or one from a quantitative discipline""

I currently have a Bachelor of Accounting (graduated 2015) and Master of Commerce (Finance and Statistics, graduated 2018) and have been toying with the idea of completing a Bachelor of Comp Sci or Data Science whilst working full-time - also considering Grad Certificate or Grad Diploma, but post-graduate courses are significantly more expensive than full Bachelor degrees. (One unit in a post-graduate equals about 4-6 undergraduate units - we have Commonwealth Support for Bachelor degrees in Australia, whereas Post-graduate degrees are full-fee). In saying that a full 3-year Bachelors Degree will still cost around $AUD20-25k (USD$14-18k).

I've recently moved into a Financial Data Analyst position as of November 2021 where all I do is automate accounting processes within Excel i.e. OAuth API integrations from our accounting package, Xero, using VBA and Python, and eventually will build dashboards using industry data in Power BI.

I understand the basics of SQL but haven't used it in a commercial setting and the company I work at doesn't have a database I can work with. Our technology stack is Microsoft, so if I did want to incorporate a cloud platform in the future, it would be Azure.

But I digress, I currently have no student debt and want to know if it's worth getting the elusive STEM degree. On one hand I'd be interested in completing one, however, all signs are telling me it's a waste of time and money as industry experience is more important. As a person who likes a structured and certified approach, my mind always just thinks going back to university is the way to go (I've clearly been brainwashed).

The alternative is to effectively turn my current position into a Data Analyst/Data Engineering role, however I won't have any mentors and will be doing everything self-taught. I work with 10 other people all of whom are investment analysts and accountants and my boss have given me free-reign to adopt anything that will improve productivity.

&#x200B;

tl:dr Has getting a Comp Sci or STEM related degree given you any more of an edge in the data engineering field compared to someone who has self-taught everything and gained industry experience? I'm approaching 30 this year and the thought of going back to university makes me feel like I'm regressing 10 years lol As I'm typing this, I think I've answered my own question, but curious to hear others thoughts.",2022-05-28 03:45:25
ur2tgo,"What's a good tech stack for an (aspiring) data journalist who wants to take datasets and slice and dice them, pull meaningful information and create interesting tools and graphics that help tell a story?","For example, I know how to program a simple Jupyter Notebook that pulls Covid-19 statistics from an API and create a graph in plotly.

But how do I deploy this so it can be shared with peers, or create graphics that add to a story?

Or like I can create a program that pulls Tweets from a given user, but then from that large database I'd like to query the information to analyze maybe how often they said keywords and then plot them.

So there's a couple different levels.

1) I need to pull information from a source and store it, maybe in an online database that can be queried by a team. It may be a bunch of CSV sheets that I need to upload into a single repository that we can use to efficiently pull subsets of info.

2) From this big data set, we can pull subsets to analyze and create graphs and interactives from.

3) These graphs and interactives need to be part of a story, so accessible online on a website. So a Covid-19 tracker would need to update once a day to grab the latest numbers.

I have skill working as a reporter at a newspaper, and I have a degree in math and physics and skill in python and SQL.

The challenge I think for me right now is thinking in terms of creating an end product, and using my programming skills and reporting experience towards creating something definite.",2022-05-16 18:39:26
tsbnjo,Best Tech Stack for future FAANG/unicorn job,"I'm currently on Azure ADL gen2, Databricks and Spark. I've interviewed for 6 different companies in which 4 uses AWS and out of those 4 one uses Snowflake+dbt. The other 2 companies are on GCP and Azure (same stack as my current company). 

From what I'm seeing in the marketplace, few companies are on Snowflake or dbt, which is supposed to be part of the ""modern data stack"". So my question is: does taking a job at the company with GCP will reduce my chances on moving on after? I see almost everyone on AWS and very few who uses databricks are 100% on Azure. GCP seems like the outlier, but the company is the better brand out of all the 6 I've talked to. 

Companies: 1 Consumer Goods on Azure, 3 startups on AWS, 1 gaming on GCP and 1 gaming on AWS+Snowflake+dbt",2022-03-30 17:28:38
tjykn0,What is the most painful part of the queries in big data?,"I'm trying to figure out where sampling strategies can result more attractive to use. Either in joining big tables, approximating results, ETL...
It would be cool to have some feedback first, thank you!",2022-03-22 08:52:25
tfhydt,"Can Lakesoul surpass Iceberg, Hudi and deltalake in the future?","I have seen a new unified streaming and batch table storage solution named [Lakesoul](https://github.com/meta-soul/LakeSoul) on Github, which is similar to Iceberg, Hudi and Detalake, but with several new functions, such as *upsert,* meta-data Management and so on. However, there are also some shortcomings, such as Flink is not supported. But its roadmap shows Flink integration in progress.  This is very interesting.  According to the official comparative test results of Lakesoul, Lakesoul has the strongest comprehensive strength. But Iceberg, Hudi and Deltalake all have huge user bases. Can Lakesoul take their share? 

I tried *upsert*, which can be interpreted as a combination of *update* and *insert*, which is a great time saver. Lakesoul supports range and Hash partitions, which can be used to add, delete, and modify rows and columns simultaneously using *upsert*.

Can't but admit that using Lakesoul is more efficient , but there are so few developers and it doesn't get much attention on GitHub, which is a big problem. 

***Here's an official description of Upsert from*** [***Lakesoul***](https://github.com/meta-soul/LakeSoul/wiki)***：***

Multi-level partitioning and efficient upsert: LakeSoul supports range and hash partitioning, and a flexible upsert operation at row and column level. The upsert data are stored as delta files, which greatly improves the efficiency and concurrency of writing data, and the optimized merge scan provides efficient MergeOnRead performance.

***Code Examples***

    import com.dmetasoul.lakesoul.tables.LakeSoulTable
    import org.apache.spark.sql._
    val spark = SparkSession.builder.master(""local"")
      .config(""spark.dmetasoul.lakesoul.meta.host"", ""cassandra_host"")
      .config(""spark.sql.extensions"", ""com.dmetasoul.lakesoul.sql.LakeSoulSparkSessionExtension"")
      .getOrCreate()
    import spark.implicits._
    
    val tablePath = ""s3a://bucket-name/table/path/is/also/table/name""
    
    val lakeSoulTable = LakeSoulTable.forPath(tablePath)
    val extraDF = Seq((""2021-01-01"",3,""chicken"")).toDF(""date"",""id"",""name"")
    
    lakeSoulTable.upsert(extraDF)

[The official of Lakesoul, DMetaSoul](https://github.com/meta-soul/LakeSoul/wiki), introduces that LakeSoul is a unified streaming and batch table storage solution built on top of the Apache Spark engine by the DMetaSoul team, and supports scalable metadata management, ACID transactions, efficient and flexible upsert operation, schema evolution, and streaming & batch unification.",2022-03-16 13:42:03
tdtxq8,Common Mistake I See Engineers Make,N/A,2022-03-14 09:57:52
stims2,Learn Data Engineering,N/A,2022-02-16 01:00:02
spbunr,IS DATABRICKS COMMUNITY EDITION DOWN?,"Hello guys, 

I've been trying to set up community edition on databricks and all goes well until I try to log in

 The connection times out and still goes back to the log in page,

Inspected element and figured out that when u click on login it gives a 500 (server unexpected error ) then a 404 not found when it times out and the login reloads,

No Interface is shown could they have taken it down for good?

Can someone pls check if the edition is still working?

&#x200B;

Would be gratefull for any kind of feedback",2022-02-10 17:25:49
snn05g,Auto-labeling: Is it real or a pipedream?,"Auto labeling can seem like a pipedream to many but Tyler Mckean, Head of Customer Success at Superb AI claims their Auto-Label tool with Uncertainty Estimation for rapid active learning is the real deal - thoughts?

Link to his upcoming presentation: [https://events.cognilytica.com/CLNjMyMXwyNA](https://events.cognilytica.com/CLNjMyMXwyNA)",2022-02-08 15:34:02
sm3way,"As a Data Professional, how can I make a big impact in South Asian countries?","I  belong to a South Asian country, and one of my biggest problems here is  that there just isn't good data infrastructure. How does one do data  science when there isn't even any data?

Could  anyone tell me how a data professional could go about making more impact in a South Asian country where good quality data is sparse?

Should I  become a Business Analytics expert and help companies set up ETL? Or should I be a data engineer?",2022-02-06 18:36:04
rys9ff,Looking for an easy setup for blockchain data,"Hey everyone. I am not a DE so I am a little unsure of what I need to do. I am comfortable with learning it all on my own mostly, but would like some general steps.

I'd like to access blockchain data (BTC or ETH for now) and load this into a warehouse. What options do I have to do this for free? I'd ultimately like to model some stuff using dbt and create an output into a self-hosted BI tool.",2022-01-08 05:20:56
rrbor4,Starting my job search. Critique my Resume?,"Problem I have always had making a resume is I don't have a degree or anything that feels able to fill out a resume with. I know I can do the work, but I have a hard time selling that on paper. Any help would be greatly appreciated. 

Anon

480-555-1234 ● [anon@gmail.com](mailto:anon@gmail.com)

Data Engineer

Ambitious, Data enthusiast seeking opportunities to further my experience in the world of data engineering and analytics. I am Interested in using skills in computer programming and data engineering to improve productivity by automating ETL Pipelines, and providing data driven business insight.

**Technical Skill Set**

**Snowflake:**  Experience developing and implementing data warehouse solutions and data pipelines for large data sets and business analytics. Comfortability managing relational data models to provide efficient access to business intelligence.

* Snowflake Zero to Hero Certificate (2020)

**Python:** On the job experience using the python programming language to automate ELT processes for large datasets.

* Short list of library knowledge
   * Pandas
   * Apache Spark
   * Numpy / Scipy
   * Selenium
   * Pyautogui
   * Matplotlib 

**SQL:**  Robust knowledge of database querying language SQL. Daily use of this language to process data in preparation for visualization.

* Ultimate MySQL Bootcamp Certification (2022)

&#x200B;

**Work Experience**

**Endurance International Group ●** Phoenix, AZ (November 2017 - Current)

**Sales Data Analyst** (August 2019 - Current)**:** Provide Business intelligence to the stake holders through data Analytics. 

**Achievements**: 

* Revolutionized the daily reporting process.
   * As the key man on the team I used my knowledge of VBA, and Python programming to automate the process of reporting out daily analytics to the stake holders. Saving on average 2.5 to 3 hours of work each morning. 
* Lead the transition from Excel to Snowflake.
   * I took a leading role in transitioning my departments data environment from Excel sheets to a relational data model in the Snowflake warehouse. 
   * Enabled the scaling of data ETL from 100 agents locally to tracking the sales of 1500 agents in multiple calls centers globally. 
* Detailed analytics of data
   * Using my experience in data analytics and data processing it was my responsibility to identify irregularities and opportunities in enterprise data.",2021-12-29 16:11:33
roh6z4,Save NumPy Arrays to CSV Files,N/A,2021-12-25 20:50:43
rj9w9l,Data architect with snowflake,How essential is a data architect in a snowflake environment with complex relationships? Does snowflake have data model deployment and management options?,2021-12-18 15:36:15
qq6qbq,Seeking advice to become a data engineer/scientist,"Hi everyone,

I'm thinking on changing careers for a while, my current role was developed within my company, it's been 10 years here already, my current role is a mixture of technical project engineer and data analyst, I always worked with facilities expansion, so layout, continuous improvement, renovations, RFQ, qualification, 3 years ago the company implemented BI systems to merge systems DBs and start using it to make business decisions, part of this was to have business units superusers with access to main DB and give them a tool to handle data and bring insights related to their areas, we previously were using excel and VB, we moved to Alteryx designed (block based BI tool).

During last three years I have been working in both directions, my previous function and this expanded role, lately I have seen that there is no much change of career progression at the facility side, while the analytics side is full of opportunities. 

My question is, I am 30yo, have some experience with data handling, visualization and reporting during last three years, but not industry standard (very little coding knowledge), as I see it would need to be really good at Python and SQL at basic aside from understand data structures and algorithms, is that something feasible, where should I start? I am a bit afraid of the algorithm bit as it seems that you need to have a Pos Doc to understand some crazy concepts (specially with ML) lol",2021-11-09 15:58:38
qi0rbj,The Modern Data Stack Ecosystem – Fall 2021 Edition,N/A,2021-10-29 00:43:33
pc4b9b,Why Every Data Scientist Wants a Data Engineer,N/A,2021-08-26 17:31:20
ouog61,"‎For folks wanting to branch over to analytics engineering (or just interested in a wider look at the data ecosystem), check this out: Adventures in analytics engineering education and community w/ Claire Carroll, co-founder of analyticsengineers.club",N/A,2021-07-30 16:54:13
oknuhr,Running Apache Spark on Kubernetes,N/A,2021-07-15 07:29:07
obhovi,SQL Interview AIRBNB,N/A,2021-07-01 08:55:04
oa7rve,Top 9 Feature Engineering Techniques with Python,N/A,2021-06-29 12:59:12
o2k3ju,Virtual machine requirements for data engineeering,"Hi guys. I hope you are doing fine.

So I am learning big data and starting to get the hold of it.

I have a 8GB RAM laptop which is Windows.

Questions:

1. Do I need a Virtual machine with linux while doing all big data operations ?
2. If I do need how much RAM and hardisk space should I allot ?
3. I am having 8 GB ram. Should I add more ram?
4. Say for example I am gonna work with all types of big data tools ,softwares and technologies , will it be possible for me to do in windows??",2021-06-18 08:17:17
nthgyy,Case Study about Big Data And Visualization?,"In our Big Data and Visualization case study, we deploy a web app using Machine Learning (ML) to predict travel delays given flight delay data and weather conditions.

We build a complete Azure Machine Learning (ML) model, Integrate an Azure ML web service into a Web App, and visualize batch predictions on a map using Power BI.

Want to know more? Check out this blog post on [copy data with azure data factory](https://k21academy.com/microsoft-azure/data-engineer/case-study-big-data-and-visualization/?utm_source=reddit&utm_medium=referral&utm_campaign=dp20313_june21_rdataengineering) to learn more",2021-06-06 08:55:32
n8jtdz,.NET ETL - MySQL Bulk Insert and Expanded Transaction Support in new actionETL release," [actionETL](https://envobi.com/) is a high performance, code-first .NET ETL library. The [latest release](https://docs.envobi.com/articles/release-notes.html#version-0400) adds:

* [Bulk Insert](https://docs.envobi.com/articles/adb/sql-database-access.html#bulk-insert) for MySQL-compatible databases
* Transaction support for bulk inserts
* Minor additions and bug fixes

Apart from the [release notes](https://docs.envobi.com/articles/release-notes.html), check out actionETL [features](https://envobi.com/) and the [extensive documentation](https://docs.envobi.com/), and get the above Community edition or use the [free 30-day trial](https://envobi.com/trial/) to try it out.

Cheers!  
Kristian Wedberg  
Disclaimer: I'm the architect of the framework.",2021-05-09 18:08:42
mfh91l,Where to find remote data engineering role?,Do you also find it hard to find a remote data engineering jobs?,2021-03-29 04:01:17
m6xc7w,Best laptop for data engineer,"Gaming laptop like   Lenovo Legion 5 - 2XID | Ryzen 7 4800H | 16GB | SSD 512GB | GTX1650Ti 4GB or 

Acer nitro 5 2021 11th Gen Intel® Core™ i5-11300H @ 3.10GHz, NVIDIA® GeForce® GTX 1650 with 4GB of dedicated GDDR 6 1x8GB DDRIV, upgradeable up to 32GB, 512 GB SSD.

Btw i used for machine kearning and deep learning also.

Thank you",2021-03-17 10:25:08
m0b1k3,Tutorial on how to rescale numerical data to a range between two values,"Hey, I've created a tutorial on how to rescale numerical data to a range between two values using the R programming language. The tutorial explains how to standardize data to a 0/1 range or to a range between two other values: [https://statisticsglobe.com/scale-data-to-range-between-two-values-in-r](https://statisticsglobe.com/scale-data-to-range-between-two-values-in-r)",2021-03-08 08:09:19
kkersa,Are you ready for CCPA?,N/A,2020-12-26 07:30:44
kgwoe0,"Is ""97 Things Every Data Engineer Should Know"" a real thing?","Who knows what is this book ""97 Things Every Data Engineer Should Know""? 

It appears on [amazon](https://www.amazon.de/Things-Every-Data-Engineer-Should/dp/109811504X)/[goodreads](https://www.goodreads.com/book/show/50025727-97-things-every-data-engineer-should-know)/google books, but rather than title and cover there is no any additional information... is it a real thing??

Is it somehow related to [Tobias Macey's in-progress book](https://docs.google.com/forms/d/e/1FAIpQLSdAsJjYx6zU8SdajqKv3YiT78kS-JloDxaIopNsBtbHW36xOg/viewform) with the same name?",2020-12-20 16:13:22
k67u5d,"Not getting Slack Notifications when dags fail, tried SlackWebhookOperator and SlackAPIPostOperator","Hello!  jr. data engineer here, and a bit stuck!

This is my code using SlackWebhookOperator, I used the same formatting when trying to use SlackAPIPostOperator except I  had 'token' not  webhook\_token'

    from airflow.models import Variable

from airflow.contrib.operators.slack\_webhook\_operator import SlackWebhookOperator import os

    def get_channel_name():
        channel = '#airflow_alerts_local'
        env = Variable.get('env', None)
        if env == 'prod':
            channel = '#airflow_alerts'
         elif env == 'dev':
            channel = '#airflow_alerts_dev'
         return channel
    
    def task_fail_slack_alert(context):
         failed_alert = SlackWebhookOperator(
             task_id=context.get('task_instance').task_id,
             webhook_token=os.environ.get('SLACK_URL'),
             channel=get_channel_name(),
             text=""""""
                    TEST
                  """"""
        failed_alert.execute(context=context)

&#x200B;",2020-12-03 22:28:44
jxbo2c,"How AI, ML, and Big Data Analytics Fit Into a Non-Tech Company",N/A,2020-11-19 21:26:23
jr14tq,Airflow managed solution - workflow deployment,"I’m trying to understand which is the best way (best practices) to deploy and execute the workflows using an airflow managed solution (Astronomer, Cloud composer, etc ).

With a self airflow deployment using Docker containers I created a volume with the following structure:

* Volume:

- dags
- scripts (Python scripts )

And according to what I have read, in a managed solution the only thing you upload are the dags and not the scripts. Am I wrong ? If I’m right, where should the scripts reside and be executed ?

Right now for example, I have a workflow which scrapes certain data (using scrapy) and then, this data is used to generate some PDF files (using weasyprint).
What I did was installing scrapy and weasyprint libraries in the same airflow container, and the scripts with the logic to do this are also in the same container.
To execute those scripts I’m using a bash operator from my airflow workflow. On this case, where should I install scrapy and weasyprint libraries, and where should the scripts be stored at using a managed solution ?",2020-11-09 16:47:26
iiav7m,"Predict ""Wine Quality"" using ""Machine Learning"" |Real world problem solve| python| Data Science",https://youtu.be/qakGunkQ9AU,2020-08-28 17:00:10
hwrg60,How to Fix Your Data Quality Problem,N/A,2020-07-24 00:17:08
g3ffpp,$60 incentive offered for feedback from data engineers," Hey everybody, I've been using a research site called Respondent for the past few weeks to make a few extra bucks during these tough times. Some of studies on Respondent do not apply to me so I have been paying it forward and sharing the opportunities with people who might actually use them. This research group is offering a $60 incentive to hear from data engineers. If this is you, and you could use a few extra bucks, follow [this](https://app.respondent.io/respondents/projects/view/5e9509c7d850fe00389a31f6/seeking-data-engineers-for-a-quick-30-minute-remote-activity?referralCode=joshuaallessio-ff8154cce629) link and sign up!",2020-04-18 01:59:35
fxe5bi,SQL and Pandas,"As i progress in my Data Engineer career, i only continue to see competition between SQL and Pandas rather than being complementary. What i imply is that they both seems to do exactly the same thing. I am yet to discover any major difference. I checked google but did not really find anything. 

1. What exactly is the difference between the two?
2. As a Data Engineer, when exactly do you use one instead of the other and why?

I will be glad if anyone can point out how they complement or why i actually need both",2020-04-08 20:27:15
fea5r6,Big Data Analytics with PySpark + Tableau Desktop + MongoDB,"If you want to, check it out below:

[https://www.udemy.com/course/big-data-analytics-with-pyspark-tableau-desktop-mongodb/?referralCode=348A25E57F2654D3F0DA](https://www.udemy.com/course/big-data-analytics-with-pyspark-tableau-desktop-mongodb/?referralCode=348A25E57F2654D3F0DA)

https://preview.redd.it/4769we4ta0l41.png?width=2559&format=png&auto=webp&s=69d740fbbc307382ec260d47c0cab7e53c2bee06",2020-03-06 07:40:55
fdhq8s,Hiding database credentials in python script,"Hello,

Please your advise on how to hide database credentials  when pulling data out of a sql database using python?",2020-03-04 18:53:57
19bhkx4,Urgent: Taking 2 extra semesters for a CS Minor as a Stats major?,"(The reason I put urgent is because the deadline for me to choose courses is in 2 days 🥲 i know not the best)

Hello, I’m a fourth year undergrad student majoring in Statistics and minoring in GIS and Human Geography at a university in Canada and I’m looking to become a data engineer after graduating but I’ll likely start as a data analyst first before transitioning into a DE. 

I’m wondering if it would be worth it to take CS courses for an extra 2 semesters to change my Human Geography minor into a CS minor. 

The thing is, CS at my school can be pretty theoretical, and 2 required courses focus on making projects in Java and C which I’m not sure how relevant those languages are for DEs. However, I do have the option of choosing an SQL course and machine learning courses after I complete those required courses so it’s something I’m considering as I assume it is relevant to the data science industry. 

However, if I’m not gonna really use a lot of things I learn such as math proofs, Java and C  during work then I honestly think those courses might be overkill, but if CS will overall teach me skills that are fundamental to the engineering aspect of DE then I’m willing to just push through and finish the CS minor. I know for a fact that I learn better in classroom settings than on my own so that is also something to consider. 

For those who work as a data engineer, what are the most important languages/technical skills to learn in order to start a career as a DE? I know Python is very important and I’m currently using it a lot in one of my upper year stats courses, but how about languages such as Java or C? I’m already familiar with R and will learn SQL on my own. 

If anyone can provide me insight on the current industry and tips on how to get hired in Canada or the US it would be much appreciated!",2024-01-20 17:59:11
18dt300,Quantum Computing, IBM recently announced a break through in quantum computing. That got me thinking - even though normalized quantum computing is while out - what does data engineering look like in that new age? What are yall's predictions? ,2023-12-08 18:23:35
157mn03,Rust for data engineering?,found this on substack and reposting here for more reach.,2023-07-23 18:43:55
150dzb6,Unit Testing for data engineers,N/A,2023-07-15 14:51:10
149oqub,What sort of Broadband speed would I need as a Data Engineer working from Home?,I create quite large complex datasets and Dashboards from multiple databases using SQL and then often download them to web based Excel.  I also present them on Teams calls.,2023-06-15 01:11:04
141r6s9,Paid user testing: $20 Amazon gift card for 45 min zoom,"Looking for testers for our open source data product

We just want to watch people install and use it for the first time.

Requirements: 

* Know SQL
* Available at some time 10am-4pm EST

If you are interested, dm me",2023-06-05 20:43:34
12q9zaj,One step closer to the orchestrator-less data stack - dbt cloud webhooks,"Last month, dbt somewhat quietly released a pretty important feature for dbt Cloud: Webhooks. Although seemingly small, this is a big deal for data teams for simplifying their stack. The number of tools you need to maintain is dropping slowly and things are consolidating under dbt Cloud.  


here is an introduction how to make use of webhooks using fal-serverless - [https://blog.fal.ai/dbt-cloud-webhooks/](https://blog.fal.ai/dbt-cloud-webhooks/)",2023-04-18 04:14:18
12lx0mt,ETL 1 Billion rows for less than $1 with Delta Lives Tables on Databricks,"ETL can be one of the most expensive costs of data engineering for data warehousing.  Today, Databricks announced they were able to perform the typical ETL of an EDW, with all the transformations and rules, at breakneck speeds, and cheap cost.  Would love your thoughts on this, and can you try it out for yourselves and let us know what you think!  

&#x200B;

[https://www.databricks.com/blog/2023/04/14/how-we-performed-etl-one-billion-records-under-1-delta-live-tables.html](https://www.databricks.com/blog/2023/04/14/how-we-performed-etl-one-billion-records-under-1-delta-live-tables.html)

&#x200B;

Direct link to Repo to Repro: [https://github.com/shannon-barrow/databricks-tpc-di](https://github.com/shannon-barrow/databricks-tpc-di)",2023-04-14 13:18:15
11rd3ti,Will Automation can make me layoff quickly?,"I want to post here because I am concerned about the jobs marketing today (2023). I hope this is the right place to ask questions about automation and  ETL. Someone told me once that automation can make me lay off quickly. –I was unsure if it was true. I do not mind hearing the reality with the job market....

To tell you more about myself, I am currently job searching and a pro bono data analyst. Plus, I specialize in Tableau, Python, and ETL. –I hesitate to re-study Airflow and SQL servers to automate my ETL. As you can see, I NEVER have had a paid job, and I have concerns about what the companies are trying to achieve with ETL automation and publishing Tableau simultaneously.

Once you finish their projects with automation, will you get laid off quickly? If not, I would like to know how the company works with ETL automation and keeps employees longer.

Also, I would be grateful for an alternative suggestion of what to do next for my future career.

Thank you so much.",2023-03-14 17:49:35
10trqas,What pandas alternative (excl. Spark) you use the most? Comment/Reason appreciated.,"What of the pandas alternatives (excl. Spark) do you use the most? Reason appreciated.

[View Poll](https://www.reddit.com/poll/10trqas)",2023-02-04 21:24:20
yvafx5,We are doing an on site team building activity for our tech department. Sure you can build robust data systems but can you engineer a structurally sound tower made out of spaghetti and marshmallows? Share your best team building stories.,N/A,2022-11-14 19:48:40
yuths5,AWS CloudFormation Introduction,"Have you ever heard of infrastructure as code? This will tell you what it is about and why it is so useful:

[https://erwinschleier.medium.com/aws-cloudformation-introduction-e6d6f3fe89d2](https://erwinschleier.medium.com/aws-cloudformation-introduction-e6d6f3fe89d2)",2022-11-14 09:05:14
vx85tz,AWS or Azure,Which should i learn more about in terms of features and career opportunities wise? Which would you suggest guys?,2022-07-12 10:41:02
u6b9yj,Average Data Engineering pay for 2 YoE,"Hey guys

I am not sure if this is the most fitting sub to ask this but what are the average pays for data engineers with 2 YoE remotely ?",2022-04-18 11:50:47
n879mx,What is apache beam vs airflow vs spark?,When to use each,2021-05-09 05:16:50
hy9sru,AutoViz: A new tool for Automated Visualization in Data Science.,https://youtu.be/aiKa6QCV0qQ,2020-07-26 16:26:47
18q3t82,SparkSQL is Destroying your Pipelines,N/A,2023-12-24 21:02:29
rxuoyl,How hard is it to get a job in Tech?,"I’m going to be joining a consulting firm as a junior data engineer this year when I graduate from college. How difficult is it to break into tech? Also, how many YOE should you have when applying? Would i have a good chance of getting an interview with only 1 YOE at this job?",2022-01-07 01:20:43
v0w5ye,What about the Modern Data Stack as a Service ?,"This series of articles is a pedagogical experiment to help apprehend the data world as it is and its evolution. it’s dedicated to non-data-experts.  


In this first article, we’ll explore the transition in the way how data-driven companies handle medium-to-large data before 2015, up to how any company can do it in 2022.  


See the article here: [https://link.medium.com/wjuuD1NJrqb](https://link.medium.com/wjuuD1NJrqb)

[\#bigdatatechnologies](https://www.linkedin.com/feed/hashtag/?keywords=bigdatatechnologies&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A6936962025352781824) [\#moderndatastack](https://www.linkedin.com/feed/hashtag/?keywords=moderndatastack&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A6936962025352781824) [\#MDSaaS](https://www.linkedin.com/feed/hashtag/?keywords=mdsaas&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A6936962025352781824) [\#dataanalysis](https://www.linkedin.com/feed/hashtag/?keywords=dataanalysis&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A6936962025352781824) [\#businessintelligence](https://www.linkedin.com/feed/hashtag/?keywords=businessintelligence&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A6936962025352781824) [\#bi](https://www.linkedin.com/feed/hashtag/?keywords=bi&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A6936962025352781824) [\#datagovernance](https://www.linkedin.com/feed/hashtag/?keywords=datagovernance&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A6936962025352781824)  
[\#data](https://www.linkedin.com/feed/hashtag/?keywords=data&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A6936962025352781824) [\#Innovation](https://www.linkedin.com/feed/hashtag/?keywords=innovation&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A6936962025352781824) [\#OpenData](https://www.linkedin.com/feed/hashtag/?keywords=opendata&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A6936962025352781824) [\#Cloud](https://www.linkedin.com/feed/hashtag/?keywords=cloud&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A6936962025352781824) [\#SaaS](https://www.linkedin.com/feed/hashtag/?keywords=saas&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A6936962025352781824)

&#x200B;

[credit: selfr.io](https://preview.redd.it/0bykwfixxk291.png?width=1816&format=png&auto=webp&s=220ea0816e5f1ec88fa640ac012e7cbe3a14a08e)",2022-05-30 09:12:57
uppme1,GOT THE JOBBBBBB...KINDA,"So I graduated from University with first class hons in I.T. last year 2021. I have been doing some light technical support work for the past year at this job I got following an internship.

Over a month ago I was interviewed for a job for the role of Data Engineer at this big bank in my country and tbh I didn't think much of it because I know these posts can be so competitive and I didn't think I did the very best on the practical assessment. But to my surprise I was told that I would be getting a second interview.

So I was so excited and hopeful and nervous about the opportunity. The interview went and I thought it went really well, one of the best I think I did. 
They said they would get back to me the following week regarding the decision but they were a lot of delays during that time and I was told to be patient they just need a little bit more time.

When they called again recently I was told that the post had actually been filled but that they were so impressed and really liked me so much that they wanted to make a new post available to me for the role of Data Engineer. 

It was honestly so crazy for me to think that they did all of that for me and that was the reason in the delays in letting me know their decision. I became emotional man because I was rejected a few times before and to think they did all of that just for me....I plan to go and do my very best and show them they made the right decision. 

Any tips on how to prepare myself so when I start in a few weeks time I hit the ball rolling? YouTube series to watch etc. I really want to do well.",2022-05-14 20:24:50
1avii6q,New Video 🥳 Send Airflow Notifications to MS Teams with Notifiers,N/A,2024-02-20 14:09:17
1ak90hf,"The Essential ""Personality Traits"" You Need in Your Data Platform",N/A,2024-02-06 12:50:44
1950lxe,Great video on Spark internal workings,"Hi, I'm preparing myself for a interview for a data egeneer role next week, and I'm asking you for a good video material on Spark internal workings.
It should cover some of the following topics:
1. Partitioning
2. Shuffling
3. Persistence and Caching
4. Broadcasting
5. Catalist optimiser
6. Sort merge join

Reading materials would also be fine but I prefer video materials with good explanation of those topics. 

Thanks in advance.",2024-01-12 17:29:42
18z9ku4,Any recommendations for people on Linkedin to follow to stay up to date with the hardware side of technology?,"Title\^

But to be more specific, I'm trying to keep up with how hardware tech development that affects software tech development. 

Things like breakthoughs in CPU or storage, etc. 

Also if you know of any educational people to follow that talk about hardware? I'm quite a novice. ",2024-01-05 15:32:53
18quryl,Azure Synapse Analytics: A Step-by-Step Guide,"Azure Synapse Analytics: A Step-by-Step Guide."" Get ready to explore this comprehensive resource! 💡  


Are you curious about Azure Synapse Analytics and how it can transform your data analysis efforts? This article has got you covered! We'll walk you through what Azure Synapse Analytics is and how it works, providing you with a clear understanding of its capabilities and benefits. 🚀  


Don't miss out on this valuable resource! Read the full article here:  
[https://devblogit.com/azure-synapse-analytics-a-step-by-step-guide-for-data-analytics-beginners/](https://devblogit.com/azure-synapse-analytics-a-step-by-step-guide-for-data-analytics-beginners/)  
[\#AzureSynapseAnalytics](https://www.linkedin.com/feed/hashtag/?keywords=azuresynapseanalytics&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7145203826956914688) [\#DataAnalytics](https://www.linkedin.com/feed/hashtag/?keywords=dataanalytics&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7145203826956914688) [\#StepByStepGuide](https://www.linkedin.com/feed/hashtag/?keywords=stepbystepguide&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7145203826956914688) [\#DataInsights](https://www.linkedin.com/feed/hashtag/?keywords=datainsights&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7145203826956914688) [\#LinkedInArticle](https://www.linkedin.com/feed/hashtag/?keywords=linkedinarticle&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7145203826956914688) [\#dataanalytics](https://www.linkedin.com/feed/hashtag/?keywords=dataanalytics&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7145203826956914688) [\#data](https://www.linkedin.com/feed/hashtag/?keywords=data&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7145203826956914688)",2023-12-26 00:11:43
18fxkru,Why We Built a Streaming SQL Engine,N/A,2023-12-11 15:50:13
18aqvuk,What is the difference between spark and SQL? by first principle thinking,Title,2023-12-04 18:40:14
1873xp3,What ETL tool would you recommend for a non profit organization?,"My company, a non profit organization, has given me permission to research and use an ETL tool for a couple projects we have. So far, I’ve done all of it in Python. Which ETL tool would you recommend in terms of affordability and usability? It would also be nice to have more value to my resume.",2023-11-29 23:51:41
17rl6j7,Iceberg vs Hudi,"What’s the big takeaway from all this?  Seems like things are heating up between these two formats. 

[https://tabular.io/blog/iceberg-hudi-acid-guarantees/](https://tabular.io/blog/iceberg-hudi-acid-guarantees/)

2 days later….

[https://www.onehouse.ai/blog/on-iceberg-and-hudi-acid-guarantees](https://www.onehouse.ai/blog/on-iceberg-and-hudi-acid-guarantees)",2023-11-09 19:41:31
17ns54u,"Learn PySpark for Free, better than Paid courses",N/A,2023-11-04 18:39:29
17dkzqr,SSIS alternate for data pipeline?,"I am looking for an alternate to our current ETL setup that includes SSIS.

The new system should allow for unit testing the tasks, transformations and what have you, as well as having the capability to lock down executions in production environment. 

Suggestions?",2023-10-22 04:19:46
17cjqsr,Does anyone have a really good system they would like to share for aliasing tables inquiries?,"I work with a lot of old legacy queries written by people that no longer work for the company, and one of the frustrating things about these legacy queries is that the aliases are just stupid. Like, no thought was put into them whatsoever. They are just completely random characters, for example a Salesforce table is aliased as SFDC, and another table that is joined to it is 'u', and in a subquery that is joined to all of this, it is 'ae'. There is literally no naming convention at all. It's just a bunch of completely random crap, and half of the queries follow this naming convention of just having random stuff thrown into it, several other queries actually do have a naming convention like a,b,c. Then, there are other tables that don't use letters as the alias, they just use the shortened table name like 'as superstore_east, as superstore_west'. It's going to drive me up the wall if I don't figure out a system....",2023-10-20 19:48:15
175g3wn,Is it scam or valid ?,"I had interviewed for data scientist role at msd pharma ; later he asked for  purchasing certification course costing rs.20000 , I haven’t yet replied in fear of scam , is it valid ?",2023-10-11 15:01:22
16gzg4h,Where should I move to from databricks?,"I moved from a big company which used Hadoop and afterwards to Snowflake, Kafka, Airflow and several DBs. 
I am now in a small startup full of SWE which doesn't know what data company means and reject the exiting solution. I'm replacing a DE which didn't have any prior knowledge how big data company should look and integrated Kafka to pull all the data from all the microservices DBs to Databricks. 

I started to investigate the animal called Databricks and this is my conclusion :

Databricks castrate 90% of Airflows ability, integrate some data engines such as hive and spark, adds file explorer and ml flow (this part is nice because I am not familiar with the framework and the open source Alternative) and offer it for the naive begginer De. All tasks spin up a pod for work flow task or an EC2 for queries (on AWS). 
Please correct me in the comments if I missed something out. 
If you solely whish that in a Data company an ML engineer should have all the data and have fun with it, this should be the solution imo.. 
But I aim to educate my software engineers provide them data services with all the capabilities and together with them to choose the right services form their problems.

We have raw data of weight, GPS and images from which we collect metrics and extract various insights about usage of the tool that we monitor. Ai models are needed here no doubt. 

1. Given that all the data in S3 buckets, which datalake (or simply a query layer+ data catalog?)  should I replace to? I kind of like the idea of separating the storage and computing layers costs , and I prefer not to maintain the layers, I can live with query optimizations. 

2. I do think it's a great time to integrate a catalog and lineage services (before hiring the ml engineers) so suggestions for that are also welcomed 


Please don't hate me all of you Databricks fans 😅",2023-09-12 18:46:20
16bh90c,Should I get my masters in Data Analytics Engineering?,"I graduated with my BS in information systems, and I would say i have a lot of experience with SQL and Python, a little tableau and power bi. I currently work a software engineer 1 role which I started about 4 months ago. My daily work is not too difficult(not just for me but other CS majors on my team) even without a CS background, everything i do at work I pretty much learned online (udemy) but thats to show my company does not require extensive knowledge on programming and CS topics so l'm in no way shape or form a CS genius. But I use python everyday to automate task and build small applications.

I've been thinking about what I want to do in the future and came across Data engineering, i like it but i know i would have a lot to learn.

My question is, to become a Data Engineer should I go for my masters (Data analytics Engineering) or should go a more self-taught route.

Btw company will help pay for masters. Not in full but will help every year.",2023-09-06 11:03:40
15p7leh,Which ETL solution do you prefer in a professional environment?,"Hey guys,

I'd like to know which ETL tool / stack do you prefer in a business context / professional environment (not hobby projects)? Why do you prefer that solution?

Best regards

[View Poll](https://www.reddit.com/poll/15p7leh)",2023-08-12 15:32:46
15fghp0,y'all keep asking for book suggestions,N/A,2023-08-01 15:55:02
1520p8f,snowflake employment help for my wife,"I am looking to help my wife switch her domains. she is currently a FTE at infosys with \~6 years experience in the IT Services domain. She spent 5 years at IBM before joining Infy in same project.

She has extensive onsite experience where she worked with IBM Australia with Westpac back.

As expected, she has stagnated massively and will become completely disconnected with tech if she continues in these companies. One rather seamless switch that we feel she can pursue is moving to Snowflake platform and pursuing a career in either dev or as an analyst.

She has been following tuts from multiple resources and aiming a certification as well, but lack of enough hands-on experience is making her nervous and preventing her from focussing on studies.

Given the current market situation, job opportunities are not easy either.

How can she explore internal switch within Infy to pursue Snowflake? Otherwise, how can she overall pursue this switch? We are f9 with her taking up contract positions as well as long as she gets to work on Snowflake.",2023-07-17 12:33:28
14g9fwd,Planning to start a Data Engineering Newsletter: Seeking Community Input!,"Hello,

Fellow members, I am excited to announce that I will be starting a newsletter dedicated to helping individuals navigate the field of data engineering. In this newsletter, I will cover various topics such as how to break into the field of data engineering, transitioning from different roles to data engineering positions, job search tips and tricks, resume-building strategies, learning resources, sharing personal experiences, and many more.

I would love to hear from all of you about what specific topics you would like me to address in my newsletters. Your input will guide the direction of the content and ensure that it is tailored to meet your needs. Whether you have specific questions, areas of interest, or challenges you'd like me to tackle, please share them in the comments below. Your valuable feedback will help shape the content and make it more relevant to our community.

Thank you all for your support, and I look forward to embarking on this exciting journey together!",2023-06-22 17:30:04
12kult0,The state of data engineering 2023,"tl:dr; 

* **Status:** Ongoing survey, [participate anonymously here](https://rudderstack.com/survey)
* **Results:** will update this post with results once it is finished

Hey folks, I see many useful surveys here on r/dataengineering. Although they covered specific topics such as database, I didn't find anything in broader context relvant for every data emgineer covering overall modern data stack and customer data management. So this anonymous survey should be useful for everyone here. Organized by Data Engineering Weekly newsletter and RudderStack. I will share the survey results soon on this post. Here's the [survey link](https://rudderstack.com/survey). Appreciate your help and expertise.",2023-04-13 16:24:11
10tlk3j,Statistical programmer vs Data engineer. Which one is a better job(salary/demand/work-life balance/career growth)?," 

Please give your insights by the metrics bellow:

Statistical programmer vs Data engineer. Who will win?

1. Salary
2. Supply-demand curve. Which one has more jobs (i.e. better burgaining power)?
3. Work-life balance
4. Future career growth (i.e. more recession-proof)",2023-02-04 17:13:25
10ngrdv,Meetups,"Hey Everyone,  are there any meetups for Data 
professionals in NJ area?",2023-01-28 14:48:17
10f85jc,The modern data stack and why the struggle of enterprise adoption,"Building an enterprise-sized data stack is no piece of cake. Usually, in larger companies, everything is more complex. You have more people (possibly independent teams working with the same data), the whole setup is vastly more intertwined, and the code and programming language are enormous across the organization. There's a lot of management with different opinions and strategies, and data engineering tools are changing faster than any other domain.

The struggle starts with a management strategy to be a more data-driven and self-serving organization. The struggle expands around eliminating data silos (Excel is your enemy), creating more transparency, faster decision cycles based on facts, and empowering people from the business to be more data savvy.

A typical environment that wants a data-driven strategy most commonly has large ERP, CRM, and other internal systems that hold most of the business data. The beginning of the data journey is primarily standard among the more prominent companies, such as correlating the demand with orders and processes from the ERP with the customer from the CRM, connecting it with the user data from your website(s) with Google Analytics.

The sales team might build a different system, including many financial business rules resulting in a pixel-perfect report sent out by e-mail. The product team pulls all data into an MS Excel sheet, directly adding custom transformation. Marketing uses the third tool and cleans data with their custom web app. Usually, you have more internal data locked into a legacy software or format.

Working with data within a big organization is complicated and unpredictable. On the surface, it might look relatively easy. But everyone in the domain knows that all the different solutions, custom scripts, and code add technical debt, especially if the people who built it leave.

What options can enterprises choose from and what opportunities the Modern Data Stack can give this and more I'll explore in the article [«Modern Data Stack: The Struggle of Enterprise Adoption»](https://airbyte.com/blog/modern-data-stack-struggle-of-enterprise-adoption).

What are your experiences working at a larger enterprise, and why wasn't or isn't it easy to adopt tools from the Modern Data Stack? I would appreciate any feedback.",2023-01-18 14:03:57
107bel7,"Someone from work is showing me how to use FTP, does this have any relevance in data engineering?",My thoughts are it could be useful to receive files to put into S3 or alike?,2023-01-09 10:45:41
1023gha,My personal project: Automated logging for code changes - Cluster-level distributed tracing with personalization. If you guys can tell me your thoughts,The Sprkl Operator is an improvement of the OpenTelemetry Operator. The OpenTelemetry Operator applies automatic library and infrastructure-level instrumentation. The Sprkl Operator provides the same functionality and integrates Sprkl instrumentation with it.  Join our beta - tell us your thoughts: [https://sprkl.dev/](https://sprkl.dev/),2023-01-03 09:22:31
zq9hyk,Open-sourcing text-to-SQL for blockchain data!,"We used text to SQL to build a ""Webflow for Web3 Dashboards"". We discovered loads of problems in the entire blockchain data space outlined in the thread.

Demo: Design your dashboard, type your query in English.

We're open-sourcing everything.

[https://twitter.com/vatsal\_aggarwal/status/1604995355468173312?s=20&t=DJ-zmyaV7g5jgbUGpsssmQ](https://twitter.com/vatsal_aggarwal/status/1604995355468173312?s=20&t=DJ-zmyaV7g5jgbUGpsssmQ)",2022-12-20 01:00:14
zpfidd,Common Pitfalls For Beginners,"What are some common pitfalls or mistakes that beginners in data engineering might encounter, and how can they be avoided?",2022-12-19 02:32:18
zjrtrb,DataBricks Certified Data Engineer Associate - Earn it for FREE!,N/A,2022-12-12 07:53:58
zfa4uz,latest projects in Python relevant for advancement into Data science in the near future.," Any latest project in python would be great. I request you to support me by suggesting latest python projects. I did my search in the internet and most of the projects are pretty much out dated.

Background: I have completed basics of python and yet to start NumPy-pandas. Meanwhile I would like to start with a project for my current level in python and later extend it into a bigger project using NumPy-pandas and visualization",2022-12-07 19:00:07
yu86mc,Should I switch jobs / be patient / work a side project ?,"Hello, I am a data engineer on my third year. I need perspective about my career progression, because I feel like I am not making any.

My experience: 

- 1st company ( 2 years 4months ) : building ETLs in Python and developing dashboards in Power Bi 

- 2nd company ( 5months and still ): Talend ETLs and Power Bi dashboards 

The tech stack I covered is very basic and still at the surface of DE imo. I don't even think dashboard development is DE related. 


I am now looking up job offers on the market and I feel like all the high paying jobs are out of my reach. I am constantly trying to learn new tech on my own but it can hardly be as relevant. Unlike software dev, DE requires a bit of existing context and infrastructure.

At work, they really don't ask for much innovation or problem solving, it's just the same tasks all the time. And everyone around me is content with this so I feel kinda overqualified for what I am asked to do.

What I want :
- become a very good data engineer in the next two years, industry wise.

What's on my mind :
- find another job somewhere else
- be patient and wait
- work on a side project
- freelance


Any suggestions?",2022-11-13 17:32:45
y4qkmx,Wayfair Data Engineer Co-op 2023 OA,"Hi 
Yesterday I received OA for wayfair data engineering co-op and it says there is going to be 3 sql questions that needs to be solved in 130 mins. 
If anyone here received OA for 2023 please do share your experience!",2022-10-15 15:18:47
xve45n,How Big Data Analytics Can Help Airlines Better Understand Their Customers,"&#x200B;

https://preview.redd.it/jqnggn3dasr91.jpg?width=1920&format=pjpg&auto=webp&s=2f3ac85bdbb7c8d1cedd94a49b1691fe5906198b

 Source: [https://uk.sganalytics.com/blog/how-big-data-analytics-can-help-airlines-better-understand-their-customers/](https://uk.sganalytics.com/blog/how-big-data-analytics-can-help-airlines-better-understand-their-customers/)

Airport congestion is on the rise as new aircraft routes, and flight operators are added on a daily basis. Airport officials throughout the world face an operational headache when trying to keep track of so many distinct flights while also dealing with a limited number of airports and bendable runaways. This is the first big use of [Big Data.](https://uk.sganalytics.com/blog/how-big-data-analytics-can-help-airlines-better-understand-their-customers/) Airport capacity, runway bandwidth, number of passengers, and route options are all used by data specialists to detect trends in data and suggest the most efficient operational models .",2022-10-04 12:42:13
xc8a8f,Looking for testers and feedback for a new data labeling platform,"Hey r/dataengineering, you may not have heard of us but we’ve been labeling data for almost a decade now, and we’ve recently rebranded ourselves from Supahands to SUPA (you can read more about us here: [https://www.supa.so/post/supahands-is-now-supa](https://www.supa.so/post/supahands-is-now-supa)),

Anyways...

We launched our new product [SUPA BOLT](https://www.supa.so/) and would like to invite y’all to test it out and share with us your feedback ($50 free, no credit card required). Our goal with the product is to make it easy for anyone to get their data labeled, reviewed, and improved so they can trust the quality of their data for model training.

If you work with computer vision teams, what are some of the biggest problems you all face when trying to prepare labeled datasets for model training/testing?",2022-09-12 09:32:02
w9o3vk,Version Control for Data Documentation,"The problem with existing data discovery tools is that they don't focus on publishing, approving, reproducing, and iterating on data knowledge. Today, we are excited to announce [Secoda](https://www.linkedin.com/company/secodahq/)'s new publishing and change management workflow to solve this.  


Using this change, teams can asynchronously submit changes for review & publish a company wide data discovery portal that contains all information about your data. When a new version of the data portal is published, a version of the data discovery tool is created in Git.  


This change is built to give data teams a way to approach updating data documentation like software engineers. With this new change, data teams will be able to manage their data discovery tool like a product. More on this change here: [https://www.secoda.co/blog/version-control](https://www.secoda.co/blog/version-control)",2022-07-27 19:48:34
vtqok1,OLAP DB is warehouse?,Is OLAP db the data warehouse? Or both are different?,2022-07-07 19:22:25
v7ii43,Does anyone also think it is ridiculous like me? SQL is No coding skills needed but with a Difficult Interface. According to some articles introducing the advantages and disadvantages of SQL.,"I search ""SQL advantages and disadvantages"" on google. The top results claim that no coding skills need is an advantage of SQL, and the difficult interface is a disadvantage. 

I'm wondering wheter the dilemma belongs to SQL or these articles.",2022-06-08 06:08:59
v70rgi,Castor raises $23.5m to build a data catalog designed for viral adoption,N/A,2022-06-07 16:38:09
ty5bma,Is data engineering a complicated field?,"Hi all, just dropping in from SREland. I heard that DE is as complicated in scope as SRE. 

Here are my key ingredients for defining complicatedness:

1. the path to success is open to interpretation 
2. there is a lot of debate about the role
3. multiple capabilities contribute to the work e.g. in SRE - incident response, DevOps, tooling, observability, perf engg etc.

So is it true that data engineering is complicated? 

Or is it a highly focused role like, for example, front-end development work?",2022-04-07 04:52:22
szitip,DEPLOYING ML PIPELINES ON AWS EC2 Vs DEPLOYING ON SERVERLESS INFRASTRUCTURE LIKE AWS FARGATE,"I have just written an article in Medium about deploying ML apps/pipelines on AWS EC2 using Flask, Docker, Kubernetes, GUnicorn, and Nginx.

[Click here to read article](https://medium.com/@jkimera5/build-and-deploy-machine-learning-pipelines-on-aws-ec2-using-flask-docker-kubernetes-guicorn-9cd2720496bf)

I know serverless services like AWS Fargate are easier to deploy to and manage. So, I am wondering under what circumstances would someone opt for Server (AWS EC2) over Serverless (Fargate)?",2022-02-23 14:30:21
szgsfa,"Cookies Are Out, Conversions APIs are in: How to improve ROI on FB, Google & TikTok Ads",N/A,2022-02-23 12:54:30
st6bkt,How to Query SQL Databases Without Writing SQL Queries,N/A,2022-02-15 16:05:06
snnimi,Losing directions with Data engineering..,I beggined with python and sql.....sql is still going on fine but python sucks.. I'm not finding any data relevant projects to learn python at that front. I'm tired doing same tic tac toe and rock papers scissors project ideas from internet. I don't want to waste time watching YouTube tuts and not learn it by doing it in real.... I tried everything to set up spark locally..damn thing doesn't works! My biggest problem now is AWS/cloud... I keep watching vids but I'm clueless to think of doing a personal projects on AWS..does any yt guy makes AWS free tier based projects? Totally everything sucks! I've no idea how to fit in... I'm loosing my confidence cos of the mix up and my inability to get the right distributed lab to practise it...,2022-02-08 15:56:42
qpgu08,Is your data quality suffering because of a first-mile reliability problem?,"If your data product is fueled by tens to hundreds of external data sources, then this may be relevant to you. When schema changes, volume anomalies, late-deliveries plague the first mile, they go on to infect your downstream warehouse tables and business processes. When the reliability of all those data sources are questionable, they cascade into points of failure that are out of your data team’s control & awareness.  

If you'd like to learn how to improve your data's first-mile reliability, check out [**our latest blog post here**](https://databand.ai/blog/data-supply-chain/?utm_source=forum&utm_medium=r&utm_group=de)**.**",2021-11-08 16:10:41
qmnoll,Achy Breaky Chart (Parody),N/A,2021-11-04 15:55:29
o8ogre,DE algorithms and data structures to know,Interview coming up they test algorithms and data structures for python any I should focus on ? I can learn them all but wondering if there are some main ones. I always struggled with DFS and BFS but for DE what would be best to prioritize ?,2021-06-27 02:38:23
nwkej9,Is real-time processing worth it for your analytical use cases?,"Real-time technologies are powerful but add significant complexity to your data architecture.

Find out how to reap the benefits of real-time processing with the least architectural changes and maintenance effort: https://dashbird.io/blog/real-time-processing-analytical/",2021-06-10 10:38:38
nco48m,AZ 900 - How I Cleared Successfully AZ 900 + Tip & Tricks | Exam Practice Questions,N/A,2021-05-15 01:39:41
nahbe2,Want to become a data engineer in a FAANG-like company - what do I need?,"Hi there!

I live in Canada, and want to become a data engineer at a faang-like company. I'm a business intelligence analyst in a microsoft stack, on-prem shop (familiar w/ SSRS, SSIS, some SSAS). I had some programming backgroud, and am brushing up on python. From some of the local data enginner positions listings, I see a lot of big data related products like spark, nosql, hadoop and other stuff.

If I need to learn one of each product type and to be semi-proficient, what would those products be? I'm not even sure what I need besides learning a general programming language, nosql and some distributed system..",2021-05-12 05:30:16
n7j1z2,What's the difference between Data engineering and Data Science? Are the job roles entirely seperate?,"I am a complete beginner so I am not exactly sure how different they are in terms of the kind of job roles and responsibilities. 

What kind of skilling is needed to become a Data Engineer? 

What kind of skill set is good start to cover a particular component in a data engineering pipeline ? 

What libraries should I learn in Python to get started with DE?

Is data modelling a part of DE or DSc? 

Are there overlaps in the job roles of a DSc and DE ?",2021-05-08 06:26:35
n5dchq,Is it easier to enter to the DS field than the DE field? Which one requires more skills and which one is a better career choice?,I don't understand why so many people want to enter DS. Is it really that easy for one to become a data scientist? If so is the job market really ready for such an influx of so many new learners?,2021-05-05 11:07:32
mh4s4x,Data Scientist in need of Data Engineering advice. Possibly need to hire someone for temp work,"Hello,

I am an Engineer / Data Scientist. I am working on a  project that I believe to be very important and I need to complete ASAP.  As of next month, I really won't have the time to complete it, and I don't want to let it go. I think this would take me 6 months to get off the ground and someone with the right skills could probably get this going in about a week.  My choke point at the moment is efficiently bringing the data I need into a platform that would allow ML Integration  +  web app. The data is all publicly available from a state gov website and the data formats are all over the map. I am thinking I want to host everything w/ AWS, but could be convinced otherwise. 

**I am willing to hire someone for some very short term work to help accelerate the project. What's the best place to find some quick consultation / very short term help? I don't have a huge budget, but willing to fairly incentivize the project w/ CA$H or crypto.**",2021-03-31 12:13:40
m0mr2u,In Celebration of International Women’s Day!,Data Engineers: Powerful Allies in Achieving an Equal Future: [https://streamsets.com/blog/data-engineers-powerful-allies-in-achieving-an-equal-future/](https://streamsets.com/blog/data-engineers-powerful-allies-in-achieving-an-equal-future/),2021-03-08 18:45:47
lmbz4n,More depressed as an employed data engineer than when I was unemployed,"First off, I have no dependent, I'm working from my parent's house. Maybe that is why.

They said data engineering grew by 40% while data science only grew by 10%. It wasn't like I had a choice anyway, so I took a data engineering job.

I'm basically doing a bitchwork for people who are less qualified than me. They are nice, but it is also quite depressing, when none of them has a graduate degree while I have a master's and have been trying to get into data science so hard. 

I don't think it gets better, and manager won't change my job because data engineers are harder to find. Can anyone help me and to be blunt so that I can change my attitude?",2021-02-18 02:55:51
hpb9uc,"Data-related careers that don’t involve cleaning, organizing, and collecting data","Hi, I’m a university student taking data analytics, but I really hate cleaning, organizing, and collecting data. What are some data-related jobs that don’t involve doing these? 

Thank you.",2020-07-11 14:53:57
ho0feu,ENTRY LEVEL/FRESHER,"hello guys , i want to find  data engineer jobs , i know about mysql, basic machine learning algorithms, worked in some deep learning projects (object detections.).But i realize i like data engineer more than data scientists ,  so what skills do i need to improve and learn to work in data engineering field.",2020-07-09 09:57:38
gdz0nl,What IDE are you using ?,"I'm curious to know what IDE's / Text Editors you use?

&#x200B;

Basic SQL - Python - HTML and JavaScript i use VSCode

Data Warehouse Development I use Visual Studio

Edit sorry I made the poll single choice only (fail) please let me know why you use this IDE and how it improves your workflow.

&#x200B;

for me I use VSCode mainly with python to test and write OOP code, I use a gist in Github to host my settings and extensions and move around different client environments. Also love the fact you can choose different environments to use for Python

[View Poll](https://www.reddit.com/poll/gdz0nl)",2020-05-05 14:44:56
16uhm2t,Would you class yourself as working “In Tech” or working “with Tech”?,"Dealing with a troll that doesn’t want to post the question because he thinks he’s ultimately right.

I worked in IT for 5 years, during that time I’d class myself as working in tech because I worked in the information technology department. Dealing with computers, mobile devices, laptops and monitors.

However, I class myself as a Data Engineer that works with tech not in it. After all, majority of Data Teams (personal experience) don’t sit in the IT department. 

I 100% utilise tech, but I work in retail. 

I don’t fix technology, I don’t create software, I write code using tech but I don’t work in the tech industry or I’d be working for companies like Apple, AWS, Google etc. 

Happy to go with the majority but my opinion is that we don’t work in tech because it fully depends on the position of your data team and the industry that you are in.",2023-09-28 14:31:40
15shn2j,I'm feeling overwhelmed with Data Engineering (DE),Please help how can i  continue the consistency?,2023-08-16 06:27:06
1ausru6,Isn't Snowflake expensive when your other infrastructure is on AWS?,"When your data sources and ETL pipelines live in AWS for examples as S3, Glue Jobs and RDS, then when you want to intagre it with a data warehouse, isn't the choice of Snowflake a bad decision cost-wise? Does it not cost extra to move data between AWS Cloud and Snowflake Cloud? Does it not make more sense to use Redshift then to avoid moving data via public internet?",2024-02-19 17:19:45
16ijykz,Is data analytics part of data engineering ? Can data analytics be in the scope of a data engineer,"Hello,
Context : im a 4y xp data engineer. In my last job i enjoyed building pipelines, configuring the infrastructure (iac) , programming spark jobs (dataproc) optimising queries and jobs ..etc. i have passed also GCPDE certification lately. 
I recently started with a consulting company that pays well and seems to have a great reputation being expert in the cloud fields.
As soon as i started my contract with them i received my first “mission” (that i can refuse, the company seems cool with it as long as it doesn’t match your profile). The thing is, its a data analytics engineer role working mostly with dbt, sql/bigquery and airflow for the orchestration. At first i said to my self dbt seems to gain a lot of popularity and it’ll be good to master sql once and for all. But then i felt like i could miss the dataops part of data engineering.. so do you think it’s a good idea to accept the mission. Is data analytics  part of a data architect path? Your thoughts are welcome!",2023-09-14 14:29:40
137bmjg,Snowflake Certifications—Which One is Best to Pursue in 2023?,N/A,2023-05-04 05:57:57
12cehg3,"Alternative to Excel for large data processing, please?","Essentially I run a potentiostat where I get a piece of current/density data every 30 seconds. These run for a 10-day cycle so I save the file as an Excel spreadsheet from time 0s to 999990s in 30-second increments with 66668 different data points. This has been running for 3 months so I have hundreds of thousands of time-stamped data points.

I need to put this info into a nice graph to collate my work but Excel constantly crashes, is very laggy and makes the fan on my laptop sound like an aeroplane is about to take off. 

Does anyone have a suggestion for another programme I can use, please?",2023-04-05 09:26:13
19f9s1r,"Why are foreign keys ""facts"" rather than ""dims""?","Filtering, grouping etc by foreign key is extremely common, which really suggests to me that they make more sense as dimensions for a particular object rather than 'facts', which generally have unique, high-info (a la shanon) data - that don't refer to other objects or tables.

Is there any good reason that in star/snowflake data warehouses foreign keys get put into fact tables instead? It seems illogical - but I'm probably just missing some crucial intuition here.",2024-01-25 13:50:28
10n5b6o,Google is fumbling. Is Microsoft the new king of AI?,N/A,2023-01-28 03:49:03
yp5mbh,Discussion: Databricks vs. Snowflake - Who wins?,N/A,2022-11-08 00:12:44
18x6y0i,What is your reverse ETL tool of choice?,"We often discuss about ETL, rarely about its reverse counterpart (i.e getting data from your warehouse into various destinations). What is your tool of choice for the job, if you do rely on this mechanism?",2024-01-03 01:42:00
1auwziw,How do you describe your job to normies?,"""Data Engineer"" is a horribly esoteric job title which tends to result in a lot of blank stares and confused looks from all the ""normal"" people in my life. How do you describe data engineering to normal folk in as few sentences as possible?

My favourite: ""I'm like a plumber for data, i.e. I do for number and computers what a plumber does for water and toilets.""",2024-02-19 20:01:41
15rkval,I'm going back to data migration (DE is boring),"Hi guys,

A month ago I started working as a DE at a scale up, expecting it to be super fun and exciting. I've never done something this boring in my life. 

I notice that all the tasks are very ""superficial"" (if that makes any sense). Rather then delving deep into the data and finding the root cause of data quality issues, we just do basic checks in SQL and run some automated script to do the rest. The python stuff we do is not that special either, we just open a connection to a database and execute queries in a notebook. Building ""pipelines"" is nothing more than syncing existing scripts from other database to a new environment. 

I thought data engineering would be more fun (or at least more tedious). So I've decided to go back to my previous employer, where I was part of the data migration unit (I liked the complexity of data migration but was very curious about DE). 

What do you guys think, should I give DE another chance? I think I'm better suited for data migration, as it requires a totally different repetoir of skills in addition to hardcore SQL.",2023-08-15 06:50:28
12g8i52,How should you enter into a data engineer role?,"So to put it short, I've recently been researching a lot about data engineering and I think it might be the career of choice for me. Till now, I was basically prepping myself to land a data analyst/data science job but tbh I'm a very introverted person who don't exactly enjoy meetings and presentations nor do I particularly enjoy trying to answer business questions and that's the primary reason why I think data analytics isn't for me. I haven't yet acquired any sort of professional experience yet and I'm about to pass out from college this year. I'm aware of the fact that most of the peeps who end up as data engineers start out in different roles mostly either data analysts or software engineers. I'm confused as to which starting role I should be aiming for to get my first job so that I can transition into the data engineering career in the future.",2023-04-09 04:16:21
inghfi,British Columbia split into 3 areas of equal population,N/A,2020-09-06 05:23:01
18jusv1,Seeking Guidance on Career Transition and Salary Expectations for Indian Data Engineers.," 

Greetings fellow data enthusiasts! I'm a Data Analyst with 2 years of experience, specializing in ETL tools such as Abinitio and Informatica. Currently, I'm diving into the exciting realm of the modern data stack and seeking advice on the skills I should acquire in the next six months.

**Current Skillset:**

* ETL tools: Abinitio, Informatica
* Languages: SQL, Java, UNIX
* Basic understanding of cloud and data warehousing

**Career Transition Plan:** In the upcoming six months, I plan to make a switch in the modern data stack. Given my quick learning ability, I'm eager to know:

**Python vs. Scala:**  


* Is Scala still relevant, or is it considered outdated? (Referencing a post by Zach Wilson)
* Recommendations on whether to focus on Python or Scala for the modern data stack.

**SQL Proficiency:**  


* What level of SQL knowledge is typically expected for someone transitioning into the modern data stack?

**Software Engineering Knowledge:**  


* Essential software engineering concepts and practices relevant to a data engineer.

**Data Structures and Algorithms (DSA):**  


* The importance of DSA in data engineering roles.
* A list of DSA topics commonly asked during interviews.

**Additional Tech Stack:**  


* Other technologies or tools that complement a data engineer's skill set.

**Cloud Knowledge:**  


* The level of proficiency in cloud technologies (e.g., AWS, Azure, GCP) required for a data engineer in the current job market.

Indian Data engineer folks need your help also here for salary expectation (😅).

**Salary Expectations:** Currently earning 7 LPA, I'm curious about the salary expectations in the market for someone with +2 years of experience in data engineering. Is aiming for 15 LPA too ambitious?

**Closing Thoughts:** If you have additional insights or points to consider in my career transition journey, please share your suggestions. Your help is greatly appreciated!

&#x200B;

&#x200B;

&#x200B;",2023-12-16 16:30:17
18hdjry,Is MotherDuck ProDUCKtion-Ready?,"The article shows you all the important differences between DuckDB/MotherDuck and other databases, including results from the first iteration of performance tests.  


[https://medium.com/gooddata-developers/is-motherduck-producktion-ready-a3a0347715c5](https://medium.com/gooddata-developers/is-motherduck-producktion-ready-a3a0347715c5)",2023-12-13 10:05:03
14n38qz,What's Your Data Strategy? Help us take it to the next level! :),"I'm here seeking your valuable insights and opinions on data strategies, particularly in the context of my organization's current situation. We have made some progress so far, but we're eager to explore what more we can do to enhance our data practices.

Here's a summary of our current data setup: We have sourced data from multiple channels, applied cleansing techniques (such as duplicate removal), and implemented SCD2 (Slowly Changing Dimensions) in our data layer. Our tech stack includes AWS, Glue, DBT, and Redshift, where the final cleansed data resides.

&#x200B;

1. What additional steps can my organization take to improve our data strategy?

&#x200B;

2. How can we maximize the value of our cleansed data in Redshift?

&#x200B;

3. What are open sources available to add more value to the data, infra , DevOps, etc..

&#x200B;

4. Are there any cost-effective solutions or techniques that we should consider implementing to improve data quality, data governance, or data management? I.e Open Source

&#x200B;

5. Data observability, Precise Alerts, and notifications.  


6. AI on Data and analytics. 

I'm good to hear about your experiences, suggestions, and success stories related to data strategies.

&#x200B;",2023-06-30 15:17:36
d0zcqz,Master student here who wanna be a Bigdata Engineer!,"Hi guys

First of all, sorry for my broken English.

I'm a Master student, joined this community today to see how people in the DE field think and live in the world. and also I'd like to get some energy from you guys to keep studying without being tired.

I'm studying Data Engineering only for a year. A year ago, I didn't know about DE (cuz I was an Android Developer) and I just started studying DE because I was attracted to Bigdata at a data conference. Doing my research in my lab, I've built Hadoop cluster, Spark cluster with Docker and made them connect to MongoDB. I studied HDFS, Distributed system concepts, and Spark streaming as well.

DE is new to me still, hope I get some advice about what I should study and prepare to be a Bigdata Engineer. 

am also wondering how you guys studied in the past and how to stack your career.

It's too general question, sorry but please share your experience and some helpful advice to me. I'd also like to meet a friend or mentor to talk about engineering, school, career, experience, and personal opinion.

I read some recruiting sites to check what kind of data engineers companies want, they want Hadoop, Spark experience with real 'big data'.

My plan is, for now, starting a personal Spark project on Docker and push into my git.

God bless you guys and thanks!!

&#x200B;

p.s.

I read [this post](https://www.reddit.com/r/dataengineering/comments/d0pcmq/master_student_looking_for_personal_projects/), the answers helped me a lot. I think I should read [awesome data engineering](https://github.com/igorbarinov/awesome-data-engineering) and [road map](https://github.com/hasbrain/data-engineer-roadmap) first!",2019-09-07 18:05:03
1af1fn9,Experience with Snowflake?,"Hi everyone. I run a staffing agency and have a client who is looking to bring on a Data Engineer with Snowflake experience. I haven't heard of Snowflake before, so I am diving in more and it seems like it is pretty popular in healthcare, and people enjoy how intuitive & easy to use it is. 

I'm hoping ya'll care share your insight into working with Snowflake and how it compares to other databases? 

Hoping to avoid the recruiter witch burning (LOL), because I'm looking to be more informed. ",2024-01-30 22:29:50
16a5pyb,The Best Way to Optimize Keywords on your LinkedIn Profile,"If you are looking for an open role and are refining your LinkedIn, don's miss out on the ""Skills associated with the job post"" feature on LinkedIn job postings. It can be a great resource to tweak your profile, or identify new skills to build!  


[https://medium.com/@seancoyne/linkedin-profile-tips-for-data-engineers-part-2-886467ba4b2d](https://medium.com/@seancoyne/linkedin-profile-tips-for-data-engineers-part-2-886467ba4b2d)",2023-09-04 22:08:59
11smj3u,Beginners guide using Rust for Data Pipeline,[https://www.decube.io/post/beginner-guide-for-building-data-pipeline-with-rust](https://www.decube.io/post/beginner-guide-for-building-data-pipeline-with-rust),2023-03-16 06:46:27
zmu0bk,Re-evaluating Kafka: issues and alternatives for real-time,N/A,2022-12-15 19:40:34
yv7j8l,Pyspark,N/A,2022-11-14 18:08:48
ewxa41,Open Data Engineering positions,"We still have two open DE positions open for a growing team.  Team will soon be moving from an on-prem stack.

Looking for experience with AWS native, Spark/Python/Scala, Talend and/or Streamsets would be nice and building pipelines end-to-end

Must have strong SQL background and preferably legacy ETL experience.

Must be willing to relocate to Roanoke VA

Salary range is in the 85-120k range depending on experience

DM me if you’re interested in talking more.",2020-01-31 23:46:16
97w4ip,Why I’m sick of ETL tools and what I use instead (hint: it’s a different ETL tool),N/A,2018-08-16 21:00:13
pb8pk5,How to build your analytics team?,"I spent a lot of time trying to build an analytics team in my past job. I wish I'd read this article earlier. I couldn't recommend a better resource for someone getting started with data teams.

[https://towardsdatascience.com/how-to-build-your-data-analytics-team-1276d6729ac4](https://towardsdatascience.com/how-to-build-your-data-analytics-team-1276d6729ac4)",2021-08-25 10:46:03
x17dwy,"+160 Million rows processed in 47 minutes (spark, dataproc, py, airflow). How would you optomize?"," 

1. Don't Collect Data.
2. Persistence is the Key.
3. Avoid Groupbykey.
4. Aggregate with Accumulators.
5. Broadcast Large Variables.
6. Be Shrewd with Partitioning.
7. Repartition your data.
8. Don't Repartition your data – Coalesce it.
9. Use parquet
10. Maximise parallelism in spark
11. Beware of shuffle operations
12. Use Broadcast Hash Join
13. Cache intermediate results
14. Manage the memory of the executor nodes
15. Use delta lake or hudi with hive",2022-08-30 02:36:13
gnsm39,"For those who ask ""What should be my project to find a job in data engineering/science""?","Hey all,

&#x200B;

From time to time, I see people in this sub asking for some ideas on what their project should be.

I'm going to give you 1 idea and describe why it is complex enough. However, one thing before we start.

Which is, **you have to be curious about what you are doing**. Plus, your work has to bring some value. **You are not coding for coding when you are employed.** You have to bring value to your employer.

[Newscatcher](https://github.com/kotartemiy/newscatcher) is a Python package that allows you to collect the latest news articles (already normalized). *Disclaimer: I did it.* 

It has thousands of news sources. You can filter out by topic (economics, business, science, etc), language, and country. 

You can use is as your data generator. Then, you can build a pipeline to collect and store this data. How and where is up to you. 

&#x200B;

Why it is an interesting task for a data engineer:

1. Some fields will be missing from one source from another. Though it is normalized, sometimes \`published\` field will not be present, for example
2. Deduplication. Newscatcher returns the latest news only. So, most likely every time you check the source ([nytimes.com](https://nytimes.com), for example) the vast majority of news articles will be the same
3. Concurrent/parallel processing. Try not to do it with a loop
4. Time zone normalization. The different news feed has different time zones. If you store all the data in one place, you have to normalize the timezone 
5. Full-text search. If you have to search through the collected data, SQL will not be enough (SQL is horrible for the full text search)

And finally. If you go for it, write a concise and clear README. I have been hiring myself. You cannot even imagine how few people who search for their first dev job has a github with clean repositories. **Noone will dig into your code if you do not provide a basic README where you explain what you did.**

Moreover, having a good README will show that you can explain yourself well (that is a big part of real work, trust me). 

&#x200B;

Good luck",2020-05-21 07:54:30
1ay6i2r,From data engineer to data scientist?,"Good evening everyone, I am currently pursuing a master's degree in Computer Science, with a focus Artificial Intelligence, I've done subjects such as machine and deep learning which I really enjoyed.
I am at my last year, and I've been contact for an internship as a data engineer, which eventually i accepted as I am a bit more free with my exams in this period.
The internship lasts for 6 months and by that time I should have completed my degree.
I like my data engineer role, however after my degree I would like to pursue a career as a data scientist/ML engineer, as I see that world way more fascinating.
Would these 6 moths as a data engineer be wasted doing this? 
Is there a career that brings the world of data scientist and data engineering together?


TLDR

I am doing an internship as data engineer, would it be wasted later if i ""change career"" to data scientist? is there a career that brings these two worlds together?
Thank you very much!",2024-02-23 17:36:28
191vw1r,"Engineering manager in 6 years, 3 at an early stage startup. AMA","* started working in 2018
* switched a few times
* currently working at an early stage startup, here since last 3 years. Built the product from scratch
* Responsible for data and infrastructure charter",2024-01-08 21:00:42
11ca12v,What motivates a data engineer?,"I am a PM and we are hiring a data engineer for our team. I would like to ""classify"" different motivational type of candidates.

I imagine that there are the ones which are very technology driven who like to learn and work with different and new technologies.

Then, maybe other who want to have end-to-end responsibility from source to even building DWH or even Dashboards.

Does this make sense ? What kind of ""types"" exists?",2023-02-26 09:25:15
shgvr7,Women in data engineering,"Hello, you mystical beasts.

I am currently employed and hold a masters degree in an unrelated field. I want to earn a bachelors and/or masters in the field of data engineering. My hope is to find scholarships and grants that can help make my dream a reality. 

Sorting through the internet for scholarships has been a daunting task. I want to find real opportunities to apply for grants or scholarships specific to the field for women.

Please help with any information you may have. If you were awarded any scholarships, I'd love to know which! Also, any websites or collective of where to best find and apply would be greatly appreciated.

Thank you!",2022-02-01 00:22:48
1ankv2w,Expectations of employers from employees after AI Gen AI boom,I would like to know that the time since Chat Gpt has become famous and helping employees write code faster for DE. Seeing this has the employers expectations gone up ?. Are they expecting that  for example:- prior to GPT if an employee completed 2 tasks in a week now they are expecting 3 or 4 tasks for the same time ? has this changed the salaries as well ?,2024-02-10 17:02:27
13gb4g2,Why become a data engineer in 2023?,I am really confused if I should become a DE or there are better career options?,2023-05-13 07:56:52
1azs3w5,Was your first job as a junior remote?,"I know it's pretty hard to find an entry level job as a data engineer especially lately but I would love to know if there's hope in the dark tunnel and people who managed to get that


Thanks in advance ",2024-02-25 16:33:58
19572tl,"""Entry Level"" Salary","Hello,

Apologies if I shouldn't post this here. Long story short, I have been interning at a tiny startup (10 employees total) as the ""data guy"". My responsibilities have so far been a sort of data full stack: tons of web scraping, data cleaning, applying off the shelf algorithms, dashboarding, etc. They want to hire me on fulltime. Most of my time will be spent building data pipelines, building out a data infrastructure, building multiple front end for internal tools, etc. What's a reasonable salary expectation?

\- This is my first role in a data position/tech

\- Currently getting my masters in Data Science

\- Self-taught for 2 and a half years

\- Location is in a USA tech hub (Not CA WA or NY)

Thank you!

\*edited for location",2024-01-12 22:00:10
17lufwx,I asked ChatGPT to draft exercises that take me from beginner w Python and Pandas to expert,"Certainly! Here's a Python project involving pandas and data analysis that progresses from beginner to advanced levels:

**Project Title: Python Data Analysis Project**

**Level 1: Beginner**
*Objective: Get familiar with Python, pandas, and basic data manipulation.*

1. **Setup**
   - Install Python and the pandas library using a package manager like pip.

2. **Data Loading**
   - Use pandas to load a simple dataset (e.g., CSV or Excel file) into a DataFrame.

3. **Data Exploration**
   - Perform basic data exploration tasks like displaying the first few rows, summary statistics, and data types.

4. **Data Filtering and Selection**
   - Learn how to filter and select specific rows and columns from the DataFrame.

5. **Data Cleaning**
   - Clean the dataset by handling missing values, duplicates, and outliers.

**Level 2: Intermediate**
*Objective: Dive deeper into data manipulation and visualization.*

6. **Data Transformation**
   - Perform more advanced data transformations, such as adding new columns, merging datasets, and reshaping data.

7. **Data Visualization**
   - Use libraries like Matplotlib or Seaborn to create basic visualizations (e.g., bar charts, scatter plots) to understand the data.

8. **Data Analysis**
   - Begin analyzing the dataset by asking specific questions and using pandas to find answers.

**Level 3: Advanced**
*Objective: Apply advanced data analysis techniques and machine learning.*

9. **Feature Engineering**
   - Create new features from existing ones to improve model performance.

10. **Statistical Analysis**
    - Conduct statistical tests and hypothesis testing to derive insights from the data.

11. **Machine Learning**
    - Explore machine learning with Python by using libraries like scikit-learn to build predictive models.

**Level 4: Expert (Optional)**
*Objective: Develop data pipelines, optimize performance, and collaborate on data projects.*

12. **Data Pipelines**
    - Build data pipelines using libraries like Apache Airflow to automate data processing tasks.

13. **Performance Optimization**
    - Optimize your code for performance, especially when dealing with large datasets, by leveraging parallel processing and efficient algorithms.

14. **Collaboration and Reporting**
    - Collaborate with others on data projects using tools like Jupyter notebooks and Git. Create professional reports and dashboards to communicate findings.

15. **Big Data and Cloud Integration**
    - Explore integration with big data platforms (e.g., Apache Spark) and cloud services (e.g., AWS, Google Cloud) for scalable data analysis.

16. **Advanced Visualization**
    - Master advanced data visualization techniques, such as interactive dashboards with Plotly or Tableau.

17. **Natural Language Processing (NLP)**
    - Apply NLP techniques to analyze and extract insights from text data.

18. **Time Series Analysis**
    - Learn time series analysis and forecasting methods for temporal data.

This project will help you progress from a beginner to an advanced data analyst in Python using pandas. Feel free to adapt and expand upon each level based on your specific interests and goals in the field of data analysis.
----


Thoughts?",2023-11-02 03:23:49
11ttur3,6 Effective Tips to Hire Data Engineers Remotely: Best Practices for Data Engineer Interviews,N/A,2023-03-17 15:10:59
11a588a,Those who use dbt Cloud...,"What are some things that would make it better?  
What would be nice to have? What's missing?",2023-02-23 18:43:07
114e77k,Looking for your suggestion.,"A European knocked me on messenger saying he is looking for a data entry specialist for his company and will pay me $10,000 monthly salary which is ridiculous. But before he hires me I have to do some sample work and the security fee is $80. Now the real thing is, the sample works is extra work for me and he should pay me extra for it but why is he asking me to pay $80 instead? And after giving the sample, he will hire me, how much is the guarantee? If any of you have done something like this and got good feedback please let me know. Thanks in advance.",2023-02-17 08:09:03
zgs1g8,What monthly salary $ should I quote for remote position (US Company),"What's the salary I can quote for a remote position in the US (im not in us). Please share the figure. Thanks



Yoe: 5+ 
Tech stack: Data engineering 
Location: Remote

[View Poll](https://www.reddit.com/poll/zgs1g8)",2022-12-09 09:07:53
ywhs4u,Data Engineer Camp bootcamp.,"I was looking for some courses online about data engineering, and I came across this bootcamp from Australia ([Data Engineer Camp](https://dataengineercamp.com/). Has someone ever heard of it or taken it? The curriculum looks solid and the structure seems to be fine. The price is around usd$2,900, not that bad in comparison with other bootcamps,  and has payment options.

I would like to read some opinions or thoughts on it before going any further.

I will appreciate any feedback.
Thanks.",2022-11-16 02:59:27
yutut4,data engineering job,"I am a data engineer having 12 years of experience in various tools and technologies ranging from Oracle, Teradata, bigdata, Hadoop, spark, python mostly in the financial industry, what's the way forward someone like me going forward should I pursue data architect role or consulting? Or keep doing what I am doing?",2022-11-14 09:23:05
wtze8w,Extract dark data safely and efficiently using SQL Server Command Line Utilities,N/A,2022-08-21 13:17:01
tq56yz,Mastering SQL Query Optimization,N/A,2022-03-28 09:10:14
qrmeyl,Are you a recovering data scientist? User testing for new open-source end-to-end data tool!,N/A,2021-11-11 14:25:13
q6mu6b,Any Indian DE here??,I am going to pass out in '23 and was looking for a DE for career tips as there are very limited opportunities for freshers inDE,2021-10-12 14:00:41
peek26,Future of snowflake,"In my org , we are moving from big data aws tech stack to snowflake based stack
Just wanted to.know hows the future for snowflake 
Will it fetch high paying jobs ? How many it will have charm",2021-08-30 09:22:57
oi5fu6,How to use Pyspark in Pycharm and Command Line with Installation in Windows 10 | Apache Spark 2021,N/A,2021-07-11 14:29:01
je1l2o,Building up production ready architecture with Apache Airflow on AWK EKS,N/A,2020-10-19 13:25:25
9mz3eq,Flink Vs Spark | Apache Flink is successor to Hadoop and Spark,N/A,2018-10-10 12:41:12
1ahck2r,Data Architect without working as a Data Engineer,Can I become a Data Architect without working as a Data Engineer?,2024-02-02 19:53:52
pnldw2,Roast My Resume,N/A,2021-09-13 18:30:59
17lzwkm,Which is the best data pipeline software at the moment? Airflow vs Dagster vs NiFi vs Prefect vs Keboola,"Hello Guys,

My name is Alessio and I am a student at the University of Turin. I am currently conducting a research for my thesis based on the evaluation of data pipeline software with the focus in the enterprise field.

I understand that your time is valuable, but I would be extremly grateful if you could take a few minutes to respond to a short survey that will help me make informed decisions. Your opinions are crucial for my research and to identify the best software among those listed.

You can find the survey here, it is a google form: [https://forms.gle/APcom1Eizp3Js34z5](https://forms.gle/APcom1Eizp3Js34z5)

If you don't know all the five software, just evaluate the ones you're familiar with.

To show my gratitude to the community, I would like to offer you the survey results as a token of appreciation for your participation and find out what the community thinks about these software.

Your contribution will be completely anonymous and will not take more than 15 minutes.

Thank you in advance for your participation!",2023-11-02 09:53:11
12zztbq,Palantir Foundry — The Data Operating System That is Not Talked About Enough,N/A,2023-04-26 21:12:02
12yljn3,TLDR: we launched ChatGPT for your metadata and it’s free to use,"It lets data engineers automate documentation, find what they’re looking for, and answer any data question in plain english. Take the grunt work out of your day-to-day.

You can have it:

* Tell you what things mean across your database, dashboards, metrics, queries etc.
* Ask it to write queries to certain tables and explain queries
* Tell you what are relationships between tables for natural language lineage analysis
* Write dbt / airflow / sql / LookML code with context about your data stack
* Tell you additional context about your data (ie. what has PII, what is owned by who etc.)
* Summarize docs, questions and dictionary terms across your assets.
* Helping you find the right asset through any search method:
* “Help me find the best table to calculate MRR”
* “Help me find the best dashboard to view customer churn”
* Summarize docs / tables / dictionary terms / tables

More videos and example use cases here [https://www.secoda.co/blog/transforming-data-discovery-using-secoda-ai](https://www.secoda.co/blog/transforming-data-discovery-using-secoda-ai)

[ChatGPT for your data stack](https://reddit.com/link/12yljn3/video/j89dl181m1wa1/player)",2023-04-25 14:46:44
1an131t,What to do when you get professionally ignored and humiliated,"Note: An attempt to relieve frustration constructively and as a way to find solution for current state of mind.

The issue is that the architect, who manages team made me work ( core components and end to end for months) and got awards for all 7 of team excluding me and a new joinee. I didn't get anything, zero credits. This has led to  restlessness, constant worrying on how to deal, sleep trouble, unable to peacefully do anything for last 5 days.

Background - DE in mid size service based org with 1.7 YoE.

How not to deal:
Not complain which i was attempting.
Not talk rude like I do to relieve frustration.
Not entertain related conversation with anybody.

Radical but Clarity stuff...
Talk him to face and ask ....
-----work stuff ------
I just wanted to ask if you aware of this ?
Answer could be anything. I just want to unload.

Current plan is just work and work. I wouldn't know what exactly, the approaches that would work.


Alternative approach:
Study and revise skills and develop new skills, in well rounded manner. Shift to another organisation. All done in duration of month or two. 

Complaining is bad move. Team is managed by arch mf,  not manager. At the end of complaint scenario i don't know who is winner.

After putting this down frustration is little relieved but nothing has happened yet so head is still pricking.

Well you see, when I saw awards mail notification, I thought there might be chance my name will be there ( may not be too since awards are business unit wide).

But what happened was exactly opposite, everybody's name is there and not mine. This has some degree of impact on me. Parents and a friend asked me why my face was like that (probably dull or something).

[View Poll](https://www.reddit.com/poll/1an131t)",2024-02-09 22:51:44
1alcsiq,Considering quitting current DA role in order to full send DE path,"First thing first I have enough in my SECOND savings (yes second) to last me 6 months, and obv longer if I cut down on discretionary expenses and even longer if I had to then tap into my primary savings, and even then that’s after a month worth of PTO I would be paid out. So just to get it out the way, financially I’d would not be suicide.

Secondly, I am a DA who is basically an excel monkey, I have been up skilling the last 8 months, I have personal projects and have cloud certs to match. 

The way my resume is worded, recruiters have been interested and are reaching out on a weekly basis for DE/BI/Analytics Engineer roles, I have already had one interview last week, have another scheduled for tomorrow and have a call with another recruiter today which could turn into another interview.

My day to day in my current job is a shi*t show, we are using excel well past its limits and the powers that be don’t care and want the show to run on. The tool we use, aka excel, always crashes and works against us heavily, deadlines aren’t realistic because again, excel. Zero automation, the pipeline report(again excel) is pulling from a DW SAP/Oracle source is never updated on time, which means our deadlines thus become fire drills every week.

 It’s really weighing on my mental health at this point because:
 1) I know there is a better way to do this
 2) I’m stuck using an absolute sh*t work flow
 3) manager and higher ups don’t care and don’t want change. 
4) and I can’t do anything about it

It’s very frustrating 

So part rant/ part question: Am I using sensible judgment considering this “break” in order to learn more DE concepts and put more effort into getting into the DE field? Or should I just stay in this position with trash tools to atleast keep a paycheck coming in while constantly fighting my tools? It’s one thing if I’m spending hours trying to get a code to compile, it’s another story fighting excel from crashing for 4 hours a day to just get the macros and pivots to work…

(I am by no means a DE super user, I say that to say I know my stuff but I’m not like a 10-20 year vet of this stuff, I know how make things work in the cloud/ETL/Python/SQL/DataBricks/SSIS/PowerBi/DB’s ect but majority of my day to day is just DA stuff (again with excel) but I have experience with SSIS and PowerBI.


Thanks for listening to my Ted talk.",2024-02-07 20:29:31
197i22i,Any Hiring Manager here that could shed some light what do you check for culture fit in a candidate?,"Pretty much the title. However, here is some background. 
I am a senior level Data Engineer skilled in pyspark, sql and azure cloud. Know in and out of Databricks too. I can write complex etl code in both functional and OOP style and create end-to-end orchestrated ETL with minimal supervision. I also work well with business teams and strive to create the utility that they are looking for. My past experiences have taught me to he humble no matter what. 
However, I recently i have been failing at behavioral rounds even though I exceed expectations at tech rounds. 
So, for all the Data Engineering Hiring Managers out there, what is it that you look for in a candidate beyond their technical competency. 
Your inputs here would benefit me immensely in improving myself.",2024-01-15 19:53:22
18xmqus,What are some good UDEMY courses for data engineering skills ?,"What Udemy courses would you recommend   
Possible topics:   
\- ETL/ELT

\- Python for data engineering

\- Designing data warehouses and datamodeling

\- Use of cloud tech (AWS, Azure)

\- Optimize data models and structures for maximum - performance and scalability.

\- integrating new database technologies.

\- Optimizing query performance and data access for users.  


I am currently pursuing a degree in Big Data, but it lacks hands-on projects in data engineering technologies. ",2024-01-03 16:01:08
18m1v88,LLM-powered web scraping with 1000s of residential gaming PCs on a distributed cloud [Tutorial with Workflow included],"In this tutorial, we share an approach to web scraping that leverages a distributed cloud network with thousands of residential gaming PCs. 

You can read the tutorial here: [**https://substack.thewebscraping.club/p/web-dragon-llm-powered-web-scraping**](https://substack.thewebscraping.club/p/web-dragon-llm-powered-web-scraping) 

But why do this? 

The AI boom and the resulting increase & importance of web scraping has two challenges: 

\- Website technology preventing automatic extraction

\- Website changes breaking manually coded screapers

The nature of a distributed network offers solutions to both of the main challenges faced by scrapers. 

In this tutorial, we use SaladCloud but there are also other distributed cloud networks for you to choose from.

Since every Salad node is an actual residential computer, loading a page from these nodes ***looks*** like legitimate human traffic to most websites, and is much less likely to get blocked. 

Additionally, these gaming PCs have GPUs capable of running modern Large Language Models(LLMs), providing a potentially much more flexible and resilient option for parsing website content. 

## Key Takeaways 

* Actually accessing page content was extremely easy on a distributed cloud, with more than 99% of pages viewed successfully within 2 tries. Just as we humans are occasionally presented with CAPTCHAs, our crawlers also ran into CAPTCHAs from time to time. We dealt with this by retrying the URL with a different queue worker (and therefore a different IP in a different location). 
* Once the boilerplate is set up, the process of writing the data extraction layer is a lot simpler and more intuitive than traditional web-scraping techniques, consisting mostly of asking natural language questions. It’s easy to imagine giving this framework a simple web interface that would allow the creation of custom web scrapers with no coding knowledge required at all.
* LLM’s are not a magic bullet for the problems faced by Web Scraping. Prompts that work reliably with one website may not work very well at all with another site. You’re still going to do some per-site customization.
* Setting up a proof-of-concept RAG application is pretty straightforward with off-the-shelf open-source tools. As is the case for most software, developing that proof-of-concept into something production-ready is non-trivial.
* There are a lot of choices to make, knobs to adjust, variables to tweak in using the LLM approach to data extraction. There’s going to be a significant amount of experimentation involved in reliably getting the results you want. Everything from what models to use, how to split your webpage into chunks, and how you write your prompts can have a major impact on both the efficiency and accuracy of data extraction.
* Using LLMs and Vector Databases is much more computationally intensive than traditional methods of web scraping, which rely on highly efficient HTML selectors and other low-cost text processing techniques. In our test run with [Harrods.com](http://harrods.com/), each page took about 9s to process on an RTX 3080 Ti, yielding a JSON object like this:

&#8203;

    {   ""name"": ""Moncler 999 x adidas Originals NMD Padded Boots"",   ""manufacturer"": ""Moncler, adidas Originals"",   ""price"": 637,   ""currency"": ""USD"",   ""description"": ""The Moncler x adidas Originals NMD Padded Boots are a collaboration between the two renowned brands. These boots are designed with a focus on functionality and style, featuring a unique combination of materials. The lining is made of other materials, providing added comfort and warmth, while the sole is also crafted from other materials. These boots are currently available for personal shopping and in-store purchase only, as they are not currently available for online purchase."",   ""is_available_online"": false }

* Whether this technique is worth the extra computational cost will depend a lot on your particular use case. At scale, it’s likely that your savings in engineering time would significantly outweigh the additional costs from computing, especially given Salad’s low costs compared to traditional clouds.
* The LLM space is developing at a breakneck pace. It’s possible (likely even) that by the time you’re reading this article, none of the models I’ve chosen are still considered state-of-the-art. The good news is that open-source inference servers like [🤗Text Embeddings Inference](https://huggingface.co/docs/text-embeddings-inference/index) and [🤗Text Generation Inference](https://huggingface.co/docs/text-generation-inference/index) make it very easy to try out new models without changing the rest of your code, each requiring only a single environment variable (MODEL\_ID  
) to configure models from Huggingface Hub.

You can try it yourself here: [www.salad.com](https://www.salad.com)",2023-12-19 13:27:42
18aczp8,Salary of a Data Engineer in Cambodia,...,2023-12-04 05:09:02
189p5b8,[Research] Tell me about your experience with Monte Carlo (Data Observability)," Hello,

I'm a copywriter/technical content writer. I have to write an article about Monte Carlo and I'd love to get some insights from people who use this platform on a daily basis. Could you please help me? :)

1. What does Monte Carlo help you accomplish?
2. What are its most remarkable features?
3. Where does it fall flat or not deliver as expected?
4. Could you let me know why you chose Monte Carlo over some other competitor?

Or, feel free to write anything about your experience with Monte Carlo.

Thank you!

P.S. I'm an independent freelancer. I am not affiliated with Monte Carlo.",2023-12-03 08:22:17
17uz7yw,A Personal Odyssey: “Transforming Data Analytics with AI”,N/A,2023-11-14 10:13:55
17usm4e,Major contributor paid fresher salary,"Background - I have a background in data analytics with 4 years of experience. After my post graduate certificate I moved into data engineering. And, I am currently working in my first role as a dara engineer as a contractor with bank.

Location - Toronto
Salary - CAD 70,000 per annum 

Situation
When I joined, there were two more people who joined with me. Me, having done some courses and a portfolio project, had the most experience and ended up contributing upto 90 percent of the code written for my spark project, without 10,000+ lines of code.

I am being paid the same as other two, and I am the one making most contribution as well as have activate participation in discussions.

How should I deal with it. Being on contract means I cannot negotiate much of salary (from what I have heard about the team's structure).

I applied to other opportunities but with major experience in data analytics, I am just getting rejections.

What to do. Please advise?",2023-11-14 03:00:20
17d8lwm,Need Advice! Worth it to Switch Roles? Data Engineering to SRE,"I got laid off a few months ago, and have been incessantly interviewing since then. Went from no calls, to a few calls, to a few on-site interviews, to three offers.

Now, I have a quandary. Here are the three offers:

1. Entry-level data/analytics engineer at a major retailer. Remote, but don’t think I will gel with the team, based on interviews. Also, very low pay

2. Senior analytics engineer at a semi well-known finance company, based out of Washington. Pay is good, but will have to relocate

3. Entry-level database/infra/site-reliability engineer at dream company (Apple). The team seemed wonderful! I know I will have no WLB, but working at Apple is worth it, in my eyes

I’m leaning towards option 3, but am afraid it’s got nothing to do with my previous experience. I know I’ll learn a lot, and even enjoy my work. But, am I doing the right thing? I don’t think I wanna do SRE/infra forever, but would love to explore it for now. It doesn’t fully align with my long-term goals, but I think it might be worth pursuing. Am I committing career suicide? Pls halp 🥺

I have two years of experience after a master’s degree. And am afraid I’m “running out of time” to settle into my career.

Edit: SRE/Database Engineering, not just SRE like the title says",2023-10-21 18:08:53
1770yer,Data & AI conference happening in San Francisco: 100% off on the ticket price!,"I work for this database company SingleStore and we are hosting a Data and AI conference in San Francisco on 17th of October, 2023.

It is an in-person conference with amazing speakers line-up like Harrison Chase, co-founder and CEO of LangChain and many more. We will have hands-on workshops, swags giveaway and much more.

I don't know if it makes sense to share this but I believe it might help some of you near San Francisco to go and meet the industry leaders and network with other data engineering folks.

Use my discount coupon code 'PAVAN100OFF' to avail 100% off on the ticket price. (the original ticket price is $199)

[Get your tickets now!](https://singlestore.com/now)",2023-10-13 15:10:30
16xqihp,Java still a requirement in DE?,"I’m an expert in python, SQL and Bash. Do data engineers still used Java? Since MapReduce is kinda phased out by Spark/PySpark.",2023-10-02 07:48:18
16ljox2,Job prospects compared to SWE?,"In the computer science careers Reddit group, I hear a lot about SWE layoffs and SWEs having a tough time finding a job. Same post, different person.

Curious, what has been the experience of Data Engineers during this time? Curious about the comparison.",2023-09-18 02:43:59
164k97b,Why Airflow? The Top 5 reasons to use it!,N/A,2023-08-29 14:38:50
15fd3i8,Seattle Data Guy - What is Apache Iceberg?,N/A,2023-08-01 13:46:43
1586jvw,Tableau for automated analyzation and dumping that into some database.,"So I want to know if I can achieve this or not?

I wanna get data from some database, analyze it (should be automated) and dump that automated output data to some other database.Does Tableau allow me to do this? If so, how?

So that I can take that dumped data into my database and visualize it using my own React view.

Update : How about Tableau PREP do this work? I'm seeing some promising expectation from that.

PS : I'm a newbie, so please consider helping me out before attacking I guess!",2023-07-24 10:18:03
14iwmnp,Meerschaum in 100 Seconds (à la Fireship),N/A,2023-06-25 20:38:44
14htepc,How to become a valuable data engineer · Start Data Engineering,N/A,2023-06-24 13:42:09
14e6et1,"In your opinion which are the best alternatives to Pandas,Polars and PySpark to work with dataframes in Python?","I wrote this article:  
[https://medium.com/@lgsoliveira/three-alternatives-to-pandas-polars-and-pyspark-to-work-with-data-in-python-153f7cb0c3e5](https://medium.com/@lgsoliveira/three-alternatives-to-pandas-polars-and-pyspark-to-work-with-data-in-python-153f7cb0c3e5)

WHAT IS IT? An article showing 3 alternatives to work with dataframes in Python

WHO IS IT FOR? For all data professionals working with Python

WHY IS IT RELEVANT? Because it is important to be on top of the programming benchmark  


What it is your opinion? Do you just work with Pandas?   
Do you use all the 6 libraries? ",2023-06-20 09:34:11
13r091r,Introducing Microsoft Fabric: The data platform for the era of AI,N/A,2023-05-24 22:40:04
11f6sxk,Breaking down microservices silos: Building real-time cohesive APIs,"As a developer, you're no stranger to the challenges of creating a cohesive API from multiple microservices. Here is our approach: [https://getdozer.io/blog/microservices-unified-apis](https://getdozer.io/blog/microservices-unified-apis)",2023-03-01 15:35:03
11bztrq,Do you think SWE with 5yeo will still hold an edge as now in 2027 ? Is it better to switch to DE ?," 

Hi everyone,

I   am fairly new to the tech industry and have been working for a startup   for six months in the support team. My day-to-day tasks involve   identifying and fixing bugs in the company's products. However, I feel   like I am not making much progress in my career in this role and would   like to explore new fields where I can advance and become a senior  professional in a few years as possible as I could.

I   have observed that software engineers are the majority in the field,  and  with more profiles with over five years of experience set to enter  the  job market between now and 2027, I'm concerned that I might not  have a  competitive edge as a software engineer. Therefore, I am  exploring  alternative career paths that are new, in-demand, and have a  high  potential for growth.

I'm   considering pursuing a career in data engineering, DevOps, BI, or   product niche such as SAP. What do you think about these career paths?   Are there any other alternative paths you can suggest? I would   appreciate your advice and insights on this matter.

Thank you in advance for your help!",2023-02-25 23:58:49
zprh9o,Dimensions and fact tables,"Hey guys, I recently starter as  a data engineer and already struggling with the job. not the technical part but mostly business logic. We are developing a solution for D365 tool where we ingest the data and process it throughout the bronze silver and gold layers. Most difficult part is creating dimension tables as I have no clue what they should include, which columns and so on. I could not find a proper source where I could see all the dimensions tables per module (finance, sales, production etc) as template to use for my project. any tips? sources?",2022-12-19 13:23:39
x7cdri,Why every SaaS company will offer native data pipelines to customers,N/A,2022-09-06 14:43:20
wmm8nt,Is reading SQL or writing SQL more important?,"Lately it feels like the data community has come to a consensus that SQL is a fundamental skill, after we strayed from the path for a decade.

But in my experience across data and software engineering, I've spent a surprising amount of time reading other people's code vs. writing my own. See [https://news.ycombinator.com/item?id=28598528](https://news.ycombinator.com/item?id=28598528) for a relevant HN discussion.

I'm wondering, which is more important in your book? Why? Can the two be truly separated?

[View Poll](https://www.reddit.com/poll/wmm8nt)",2022-08-12 14:10:24
vz5u24,Future job opportunities for a data analyst,"Data analysts are often asked to move around within an organization. Some companies will hire you directly out of college while others may ask you to start at entry-level positions and then train you to become a senior analyst. If you're lucky enough to get hired straight out of school, your salary will likely increase over time. You'll also gain experience working with different types of data sets and technologies. Here are some options beyond data that might make sense to a data analyst:

* **Business-Ops**: If you love working with numbers and data, then you might consider looking at jobs that require a strong quantitative skillset. Examples of jobs that require a strong quant background include Finance, Quantitative Research, Data Science, or Business Analytics. These jobs often require a bachelor's degree in math or statistics, along with several years of experience.
* **Data Science**: Data science is not just about statistics. Data scientists need to understand the business context and goals of the problem at hand. They also need to understand the technology stack and tools available to solve the problem. If you are looking for someone who knows how to write code, then this isn’t the role for you. A data scientist needs to be able to think through the problem and come up with the best solution.
* **Analytics or Data Engineer**: Data engineers are responsible for creating and maintaining databases, data warehouses, and other systems that store and analyze large amounts of data. These systems are often used to help companies make strategic decisions about their products and services. A data engineer may also be called an information architect, database administrator, or data scientist.
* **Product Manager**: Product management is a job that requires collaboration across multiple functions. It involves working closely with engineers, designers, marketers, salespeople, and other stakeholders to create products that solve problems and delight customers. As a product manager, you will be responsible for creating and maintaining a roadmap for your team. You will also be responsible for defining features and requirements, ensuring that the product meets those requirements, and managing the development lifecycle.
* **Data Leader**: Data leaders will be responsible for leading teams of data scientists, data engineers and other data members. They will also oversee the development and deployment of data products and services across the enterprise.

We wrote this blog as a deeper dive into data analysis future opportunities: [https://www.secoda.co/blog/what-does-a-data-analyst-do](https://www.secoda.co/blog/what-does-a-data-analyst-do)",2022-07-14 20:41:32
vxl1qs,Python certification,"Does it add value? If so, which certification is best for python?",2022-07-12 20:40:16
v8ae6h,Interview tomorrow RE why Snowflake is such a big deal,"(Mods please feel free to delete if this is too self-promotional!). Hi everyone, I have a weekly show that dives into modern data stack & data engineering topics. This week I'm focusing on why Snowflake is such a big deal, interviewing a current colleague who's a Snowflake alum who's very good at explaining the history of Snowflake's technology and unique business model. I picked this topic since the big Snowflake conference in Vegas is happening next week. I'm not really into conferences but I'm pretty fascinated by how Snowflake revolutionized data infrastructure.Live Thursday 10am PST [https://www.linkedin.com/feed/update/urn:li:activity:6940462384562008064](https://www.linkedin.com/feed/update/urn:li:activity:6940462384562008064)  


EDIT: Forgot to mention that the show is live on Linkedin, so you have to be logged in there to watch... OR you can watch the livestream over on the Hightouch channel on YouTube (simulcasting on both platforms for the first time tomorrow)",2022-06-09 06:24:50
uur7cy,"What is more common in the industry? Software Engineering style DE jobs, or Analyst style DE jobs?","title.

Analyst style DE job = Use GUI tools, use pre-made tools that do a lot of things for you, focus is on SQL and building dashboards

Software Engineering style DE job = Lots of programming in Python, Scala, SQL, using custom solutions that your team built, focus is on writing clean code and architecture, distributed systems, performance, scalability, maintainability, need knowledge of computer science

[View Poll](https://www.reddit.com/poll/uur7cy)",2022-05-21 16:58:13
rusk9k,Rough estimate for total compensation as Data Engineering manager,"Looking for insight into what to expect in this type of role. I’m not building the DBs or the pipelines, but due to my business knowledge regarding the data and industry knowledge in this specific field I have the ability to deliver insights and roadmap items in addition to building the SQL processes to transform the data ingested from our inbound pipelines. Initially managing a small team with the expectation to grow fairly rapidly. 

Based in major west coast metro (non Bay Area) but company is remote first. What compensation packages would you expect here?",2022-01-03 04:10:18
r6krnt,Saw this on the r/funny which makes us data engineering get a good laugh,N/A,2021-12-01 17:58:03
osmkn5,How to Data Engineer,"Hi,

Wanted to share my experience and knowledge on Data Engineering. Please check out my articles:

[How to Data Engineer (Part 1)](https://medium.com/codex/how-to-data-engineer-part-1-c5aec20e1e2d) \- intro to how I switched careers and became a Data Engineer.

[How to Data Engineer (Part 2)](https://medium.com/codex/how-to-data-engineer-part-2-538d79678d9a) \- what does Data Engineer stand for?

[How to Data Engineer (Part 3)](https://medium.com/codex/how-to-data-engineer-part-3-98ed68a77f1f) \- technologies and tools a Data Engineer uses in his daily tasks.",2021-07-27 13:12:59
obr216,Starting a career in DE as an Undergraduate,"**TL:dr:** what are the actual expectations of companies when hiring an entry-level DE? What questions do they ask in an interview? Also, working remotely in another country? Also, any ideas/collabs for summer GitHub showoff projects?

*Skip the first two paragraphs if you are only interested in the actual question.*

I've just finished the third out of four years of my uni degree in data science and engineering. I've been looking into part-time job offers lately, as some of my classmates have already been hired somewhere. Here in Barcelona, it seems that there is a great demand for data engineers, more than for data scientists (which tells you that many businesses are not yet mature enough wrt. their data to take advantage of it). For this reason, I want to specialize in the engineering side of data.

To make the most out of my time, I am searching for an internship at some company... but I am quite disoriented. Not only because my degree has focused a lot more on Data science than on engineering, but also because companies seem to want you to be familiar with a huge amount of technologies, some of which I'd never heard of.

Thankfully, [this awesome post](https://www.reddit.com/r/dataengineering/comments/ob355w/data_engineer_roadmap_2021all_tools_and/) has helped me to find a direction. However, I still have some doubts regarding the process. I wanted to ask you for tips to help me in my job searching: what managers *actually* expect from entry-level data engineers (technological abilities, conceptual competencies, attitudes...), what would they consider a plus, what is a reasonable pay... Especially, what does a typical entry-level/internship interview look like (which questions, which answers...)? This is my main concern.

Just in passing, I also wanted to ask your opinion on these two courses I am taking. One is a preparation for the [Azure Data Engineer](https://docs.microsoft.com/en-us/learn/certifications/azure-data-engineer/) certificate and the other is for QlikSense's [Business Analyst](https://learning.qlik.com/mod/page/view.php?id=24700) certificate. Are they worth it or should I be looking into something else? (will companies find these courses valuable?)

Also, I am also curious to know if it is a thing to work remotely in another country where they pay you higher, and how one would go about searching in this direction.

Lastly, if someone here has any ideas for a collab in a small summer practice project to show off on GitHub, please DM me.

Thanks!! :D",2021-07-01 17:44:43
o9qm2h,Python vs Scala – Concurrency,N/A,2021-06-28 18:45:00
m7pdn6,Engineer vs Data Engineer what’s the difference,N/A,2021-03-18 12:08:07
lb6iow,Why Data Engineers Should Care about DataBricks IPO,N/A,2021-02-02 21:10:38
lb4hs3,Is Databricks still relevant,"My boss wants me to start developping with Databricks but is asking is it still a good tool?  
Or is it like Hadoop, a tool that still has it's uses, but no longer leading the pack.

And if it isn't the best tool, what is?",2021-02-02 19:44:12
kky6zd,Data engineer career path,"Hello, 

I have a background in networking and I've dabbled in python a bit. I'm very interested in data engineering and want to focus my attention on the steps necessary to get a data engineering job. So I guess my questions are, what exactly do I need to know (I've seen python, SQL and apache spark seem to come up a lot) to get my first job and what resources do you use to learn?

Also any idea on projects I could make to have my CV taken more seriously?

&#x200B;

Thank you",2020-12-27 05:40:19
k6qqrg,Data is overwhelmed,"Internet is now filled with all types of data and If at all I wanted to learn Data Engineering. It is really confusing me how to excel in this field. I'm stuck with a regular day job along with trying to pursue the right path, rather than learning some random youtube videos, courses, etc.. Is there anyone like me trying to learn everything but learning nothing. 2020 is gone like this. I learned nothing. At least 2-year rock-solid plan I need to make a good switch for a high-paying job. I don't have any urgency for any job change.

I don't even know - version control, Jenkins, SQL(very basic only), not even wrote single line of code, etc.. so I feel like surviving is getting a tougher daily basis.

my daily job goes like managing the people and do organizing between teams and handling client calls. Even If I learn I can't implement it anyway, So learning becomes useless. I have my own server Dell T30, bought 2020 to learn and implement. But nothing happened so far. at least i wanted a good change.",2020-12-04 18:39:55
io0c40,"Trying to break into the data analytics field, please advise [26/M]"," **Background**

I am a 26 year old college undergrad with a bachelor’s in Psychology and I currently work in a customer service job in South Florida. I lost my interest in pursuing a master’s in neuropsych after coming across the subjects of machine learning and data science. I was enamored by the fact that the data science field took what I loved about my undergrad research (data collection, and analyzing tests and trials) and the possibility of training a computer to perform and automate tasks, as well as understand and process large amounts of data to make informed decisions.

**My Goal**

My goal is to break into this field to obtain an entry level position in a data science related field, such as analytics.

**Steps I Have Taken**

Here are the steps I’ve currently taken:

· Starting Back in 2017 I researched the field and learned about MOOCs.

· I took a handful of classes, and learned as much as I could using Udemy, YouTube etc.

· I learned some Python with no prior knowledge about programming.

· Took a class about machine learning, it was a bit too hard to understand but I took what I could from it and enjoyed it.

· Its 2018, I learn more about the field and I also I start looking at job postings and looking at the skills required for data analytics.

· I develop a handful of projects which are reports on things I find on Kaggle and built a portfolio website which can be viewed here: [http://asketez.com/](http://asketez.com/)

· After developing a couple of small projects, I feel confident and start applying to jobs while keeping track of them in a Google Sheet.

After applying to around 150 jobs, I get 0 calls back or responses. I understand I have no experience, or a related degree. But what can I do, to show employer’s that I could be a good entry level employee? I have a portfolio with some projects. Should I create more? Should I intern instead of looking for something entry level? Should I go back to school?

Please advise. Thanks for any assistance.",2020-09-07 03:48:59
fv75za,My Journey Towards Data Engineering,"Dear all,

I am a newbie without Data or IT related background. I suddenly fell in love with Data and decided to kick start a career in Data Engineering and progress from there. So far, i have been learning python and SQL (since couple of months ago) on my own. While i still feel very shaky with what i have learnt so far, the only way to progress is by doing, breaking, making things. I am ready to push myself. I really want to learn.

I hereby seek opportunities to engage in projects for knowledge purpose. I am available 100% for free. I need you to support my career by engaging me. Plus, i will be more than happy to hook with someone who can also mentor me along the way.

Thank you",2020-04-05 03:33:51
y3aayl,Anyone hold down two DE positions at once ?,"Ever since WFH has become the norm there seems to be a new craze going on called over employment. Essentially people work 2-3 work from home jobs concurrently either with or without the permission of their employers.

This ranges a whole spectrum of industries and there is a whole sub reddit dedicated to it but I wanted to bring the discussion here because Data Engineering and other data related positions seem like they would be ideal for this type of set up. Specifically because of the automation scripting that are used in so many data related task.

What I am seeing is people are able to automate the most basic job functions and only require manual intervention in the event that something breaks or when making upgrades or enhancements. It seems there is quite a bit of controversy around this and if it's ethical and obviously it's frowned upon by many employees.

If I were to do this I would prefer to do it the legit way as I would like to keep the great relationship I have with my employer. My current data engineering position usually involves at least 2 zoom meetings a day and the times aren't consistent so if I considered taking a second role it would have to be something with no or minimal meetings. Also my primary position I would keep salaried with the benefits and anything else I would only consider doing on a contractual / hourly basis. 

Do such jobs exist where basically you are given a project and a time frame to compete it within but besides that you don't need to be available for calls or meetings at any set hours ? That would be the ideal situation for me if it was something I could work on outside the hours of my primary job and on the weekends.

Obviously the money is an important reason to do this but my primary motivation is also keeping a diverse skill set sharp. My job right now is almost entirely based in SQL and PowerShell and the only platforms we are using is Azure, on prem SQL Server and the ETL tools are SSIS or Data Factory. I put a lot of effort into learning Python for Data Science and Data Analyst task and I'm kinda bummed I'm not using it. It would also be nice to take on a second gig that uses a different platform like AWS so I can keep myself relevant and up to date with all the main cloud environments.

Is anyone here doing this successfully ?",2022-10-13 21:14:02
18cae7h,"I've definitely never received a snapchat from a girl, but I can auto-format my SQL queries to TitleCase!","&#x200B;

https://reddit.com/link/18cae7h/video/s3e6yr3jvp4c1/player",2023-12-06 18:21:56
1568qwa,Is a MacBook m1 air with 16gb of memory good enough to learn data engineering?,"I am about to embark on learning the command line, and python (already know SQL) before looking at data talks zoomcamp. 

I haven’t my own personal computer in years, and I’m leaning towards M1 Mac air but I heard it has some issues with python libraries, docker (and other virtualization softwares) - self-teaching is hard enough (while working full-time) will this derail my learning experience?",2023-07-22 03:21:37
1888y99,What Frustrates You in Work as a Data Engineer?,"Hello Reddit! I'm Deepan Ignaatious, Senior Product Manager at DoubleCloud, an end-to-end analytics platform based on open-source technologies and eliminating routine tasks that data engineers don't enjoy. Like building of pipelines or maintaining open-sourse solutions. However, I feel that every one has his or her own specific points, that you loathe while being a data engineer.

What truly frustrates you in your role? Is it the late-night calls to fix a pipeline failure? Or maybe it's when business requirements change last minute, upending your carefully structured data models? What are the day-to-day irritations or major hurdles that you encounter in your work?",2023-12-01 10:46:00
ths51b,Merge between the data science and data engineering,"'''According to Kim, the high demand for data science and data engineering skills has lead to the merging of the disciplines.'''

what are your thoughts about this point of view which was mentioned in this interview

https://www.itnews.com.au/digitalnation/video/data-and-analytics-no-longer-optional-for-business-deloitte-577169",2022-03-19 10:07:29
rhb0g2,Scrapping Reddit?,"Let me run an idea by you Data Engineers.  Let's say I wanted to pull data daily from Reddit, specifically from a subreddit.  For this example let's use Cats.  I want to pull data on the latest cat videos, specifically how to train cats, how to take care of cats, etc.

I would use the data for a website that tells people how to become better cat owners.  Basically a blog site with new cat ""tech"" everything day.

Is the data scrapping feasible by one person?  Could I include twitter too?

I just started a MS in data science, so I'm pretty new to APIs and data mining.",2021-12-15 22:18:38
i97t1m,How to deploy data analytics (ETL/BI/ML/AI/...),N/A,2020-08-13 20:39:15
11a3vga,If Fivetran and Kafka had a baby...,"….it’d be a free platform named Fivka (Kaftran?) that provides a cloud-based, no-code way to create \*actual\* streaming pipelines. 

But no romantic table for 2 in the back corner of Ruth’s Chris is scheduled for them… so we got to babymaking :)

As such, we’d love to have both your **warm** and **snarky** feedback on our baby (beta platform).

The most upvoted comment will receive 4 free pints of homemade, super premium, 14% butterfat ice cream shipped to their home. (actually)

**The Pitch:**  
Our goal with the Estuary Flow platform is to enable building no-code reliable pipes that don’t require scheduling, and support batch/streaming and materialized views in milliseconds. 

A free account up to 25gb/mo in data movement can be had here: [www.estuary.dev](https://www.estuary.dev/?utm_source=social&utm_medium=reddit&utm_campaign=reddit_feedback&utm_id=18681982783)

**The Details:**

Estuary Flow is built on top of an open-source streaming framework ([Gazette](http://gazette.dev/)) that combines millisecond-latency pub/sub with native persistence to cloud storage. Basically, it’s a real-time data lake.

Beyond being able to sync data continuously between sources/destinations without configuring, say, Kafka, there are a few benefits to a UI built on top of this streaming framework, specifically:

**\*Collections instead of Buffers.** When a data source is captured – like Postgres CDC, or Kinesis, or streaming Salesforce – the data is stored in your cloud storage as regular JSON files. Later, you can materialize all of that juicy history *and* ongoing updates into a variety of different data systems. Create identical, up-to-date views of your data in multiple places, now or in the future.

**\*Continuous Views instead of Sinks.** Materialized views update *in-place.* Go beyond append-only sinks to build real-time fact tables that update with your captured data – even in systems not designed for it, like PostgreSQL or Google Sheets. Make *any* database a “real time” database.  


**\*Completely Incremental, Exactly-Once.** Flow uses a continuous processing model, which propagates transactional data *changes* through your processing graph. This helps keep costs low while maintaining exact copies across different systems.

\***Turnkey batch and streaming connectors.** Both real-time data as well as historical data supported through one tool and access to pre-built connectors to \~50 endpoints.  For example, you can capture from the batch Stripe API, join it with data from Kafka and push that all to Google Sheets – all without building a custom integration. Or if you want, plug in your own connector through Flow’s open protocol.

**\*Transformations.** We have a nascent transformation product via TypeScript or SQLite which is quite powerful, with a lot more planned. Flow also offers schema validation and first-class support for testing transformations, with continuous integration whenever you make changes.

**Managed CDC.**  Simple, efficient change data capture from databases with minimal impact and latency.  Seamless backfills – even over your very large tables that Debezium tends to choke on – and real-time streaming out of the box.  

We have thick skin and welcome all feedback on our newborn.

So thick a phlebotomist uses a hammer and nail to take our blood :)  
But we also love hugs if that is what you have for us!  


[a quick video of our baby, Fivka \(Estuary Flow\)](https://reddit.com/link/11a3vga/video/t566u66qlyja1/player)",2023-02-23 17:49:21
z6i7nf,Data engineer position at TicToc,I’m going to be applying to a DE position for recent graduates at TikTok. Any chance anyone here works there and is willing to give some advice?,2022-11-28 00:54:14
1avew2o,Multiwoven - Open-source reverse ETL,"Hello folks!   
[https://github.com/Multiwoven/multiwoven](https://github.com/Multiwoven/multiwoven)  


 I'm Subin, co-founder at Multiwoven .Multiwoven is a OSS reverse ETL platform that helps dev & data teams to sync data from databases to business tools. Multiwoven is built using Ruby on Rails . Our data sync orchestration is built on top of Temporal using temporal-ruby SDK.I would greatly appreciate any feedback. Our codebase is available at Github. Please star us to get updates.",2024-02-20 10:52:12
1anyxbc,Tips for student passionate about DE,"Hello to all the BOYS. I am living in Germany and studying there, i got a working student position as a document management at a really nice company . All my passion is to become DE  and start on the right track, i am working currently as a project supporter dealing with sql scripts, verifying documents and mainly working on the data processing system of the company now. I feel i am not learning more. My interview in 2 days any tips ?? from you brothers, i am feeling confident and there is highly possibilities to get a job offer in future",2024-02-11 03:58:07
1allnm9,Thoughts on Mage AI,"Hello Guys  
i work as a Data Engineer and i've been exposed to using DBT Heavily Latly   


and i like it ! it's a fine tool with a fine mindset behind it  


but honestly would kill to find a decent way to orchestrate it in a neat usable way  
espcially with alot of Piplines depending on it

&#x200B;

so far i've tried all the Big Titles

airflow

dagsters

Even SSSIS from the Past out of curiosity

but none of them feels convient enough

and the i ran into Mage AI as a forth solution

it looks nice so far ...

but also it looks very new 

so any ideas on it ? a feedback Maybe ?? and Most importantly... is it a production-ready solution to go for ??",2024-02-08 03:04:20
14htd1x,Build a Star Schema with dbt," **Build Datawarehouse with dbt using Kimball dimensional model | dbt models | Custom Schema | Macros** 

📷[**Blog**](https://www.reddit.com/r/dataengineering/search?q=flair_name%3A%22Blog%22&restrict_sr=1)

**Transform and materialize tables with dbt**

[**https://www.youtube.com/watch?v=cK617PcokS0**](https://www.youtube.com/watch?v=cK617PcokS0)

Topics covered:

* dbt
* Transform and materialize tables
* ETL
* [dbt setup](https://www.youtube.com/watch?v=gH1w4OIgXj4&t)

Tech Stack: **Airbyte**, **dbt, Postgres, SQL, SQL Server**",2023-06-24 13:40:16
149qjuf,Mage.ai - Appropriate First Pipeline Tool to Learn?,"As the title suggests, I’ve been working on a personal project recently and decided to use Mage.ai as the base of my pipeline. I have some experience with the tools involved in Data Engineering (ie. SQL, Azure, data manipulation/cleaning, visualization) but I have no experience in building pipelines. 

My educational background is an MS in Data Science and Bachelors in Data Analytics & Finance. I enjoy analytical work and what not, but the allure of maintaining a pipeline and continuously learning new tools sounds much more appealing. Furthermore, my current employer (tech startup) doesn’t have enough data for my analysis to be incredibly valuable. It’s actually more beneficial to them if I handle DE work as well. 

That being said - is it a good idea to use Mage.ai to automate my pipelines? Is there another tool you’d consider picking up first?

Thanks everyone!",2023-06-15 02:40:07
10v9zsv,Do you find any use of AI (artificial intelligence) in your daily tasks ?,"I'm not talking to ML engineers and Data Scientists. It's more for the data engineering part. 

Did you witness any use of AI in ETL pipelines for example or any other data engineering tasks ?",2023-02-06 15:53:35
zwd9rt,From the chat with colleagues:,"\- It's barely a solution, it's a walkaround

\- It's data engineering, dude!",2022-12-27 11:11:09
nti8g8,How to start with database engineer?,I am 1st year student after trying my hands on  some technology i have decided that i want to become data engineer but the problem is how to start?? i have watched many videos but i am really confused . is there any book or resource which is like step by step guide ? and also please list some good resources about data engineering.,2021-06-06 09:50:01
1av6zs7,Career change worth it?,"Hey all, software engineer background here with ~2 years of experience and extensive experience with AWS struggling to find a job. Is making the transition to data worth it? With the explosion of AI,  DE/ML is more relevant than ever. Thanks!",2024-02-20 02:57:11
1aqfxq5,15 Best FREE SQL Courses and Certifications Online in 2024,N/A,2024-02-14 06:03:01
1aocg8e,The 2024 Data Engineer: Trends You Need To Know,N/A,2024-02-11 16:51:25
1ai7q0c,Career Advice: SWE or Data Engineer," I recently completed my master's in Computer Science with 4 years of work experience at a Fortune 100 company(Financial Domain). I worked as a Backend Developer and Data Engineer(Data Warehousing).

I am currently looking for jobs but am confused about the roles I should target.

On one hand, I have backend experience and I feel I could leverage that better than the Data Engineering experience(because I haven't worked on fancy technologies like Informatica, Kafka, etc).

I have also done a few projects in AI/ML and my long-term plan is to work in the ML/AI domain so Data Engineering makes more sense.

Any advice on what would be the right choice in the current job market? (I am looking for jobs in the US).",2024-02-03 22:12:44
1abrv0q,Looking for product feedback on newly developed tool for data teams,"Hello everyone, I am the Head of Growth at a Silicon Valley startup and we've pivoted to build a new product and I would love to demo it to as many data engineers or consultants as I can. The tool we are building is an AI chat interface powered by our customers event data. The goal is to goal is to reduce ad-hoc data requests by 80% while also efficiently managing our customers data. We are in the phases of product development so it is not live just yet.

Please let me know your thoughts and let me know if I can demo it for you.",2024-01-26 20:20:51
199ghsm,How do you think the job market will be this year (2024)?,Title,2024-01-18 03:35:07
197ydsd,Career advice on salary,"Hello all I'm currently a Junior Data Engineer at a company based in California. I work remotely from Oregon. I'm currently on track for a promotion to simply Data Engineer. When I first joined the team most of my work consisted of simple database migrations, running/monitioring jobs in Kubernetes and simple Pandas transformations.

&#x200B;

Now I've developed whole repo's worth of ETL for reporting and analytics to our data warehouse. Ensuring almost all our namespaces are running at high performance. I've deployed multiple machine learning models developed from the data science team. Assisted in our transition to utilizing DBT across our entire codebase. My boss' long term goal for me is to implement AirFlow in the future to make our downstream jobs more fault tolerant.

&#x200B;

I currently make $95,000, what is a reasonable salary I can negotiate for?",2024-01-16 08:51:27
194cjs0,What I would do differently getting into Data Engineering in 2024,"After nearly 10 years in the industry I collected some of my thoughts about what I would do differently if I started out today.

Would love to hear similar retrospective insights from ya’ll!",2024-01-11 21:08:41
18we8em,DE skill for LLM project,"I will join an start up LLM project as a Data engineer. Which skills/technologies/... I should prepare?

Edit 0:

Clarify, I am working in a AI research team. And our next goal is develop a LLM project. I cannot share more details but there are some keywords such as: *building foundation model, autonomous AI agent*. Besides, the data source will very diversified (internet, book, paper report, ...).

I am using MongoDB, Elasticsearch as data lake for my AI colleagues. Beside, my project is small and very started but has a big vision :))

I want to build a data platform which in charge of ETL jobs, storing, .... Of course there will be more  requirements in future. Which best suitable technologies  I should learn?",2024-01-02 02:35:49
18p127w,Should I use Talend or native software for a new project?,"We're planning to start an ETL project that involves many business logic as well based on our existing java application.
So should I create ETL pipelines with Talend and integrate with my application or create the pipeline in java in my application directly?
We have mostly Java developers and some Talend developers in other teams.
My main concern is that does Talend offer quick development and also does it offer the level of control that code development offers like exception handling, complex logic, etc.",2023-12-23 08:10:42
187gb42,BIG DATA WAREHOUSING,"Hey Data engineers, 

I am building a project in big geo-data warehousing in iot system, i need to build a system that ingest, store, analyze ..., data in real time form many iot devices. i am a little bit stuck on the tools that i need to use. i read about Apache Kafka but i didn't understand the role of it in my solution, can someone help me with what are all the component needed in the solution and how they work together (like databases,data warehouses,....). thanks",2023-11-30 11:22:43
181bgav,Kappa vs Lambda guide,"Is Kappa’s simplicity the key to your real-time processing needs, or does Lambda’s hybrid versatility suit your batch and streaming scenarios better? Discover the differences, use cases, and benefits that make each framework unique in this guide 

[https://memphis.dev/kappa-vs-lambda/](https://memphis.dev/kappa-vs-lambda/)",2023-11-22 15:18:32
17u9zad,"Delta Lake - Partitioning, Z-Order and Liquid Clustering","Hey folks! I wrote a new post that explains the details of different ways of organizing your data in Delta Lake.

I explained Hive Style partitioning, Z-Order curves, as well as the latest Liquid Clustering feature that makes use of Hilbert curves with ZCubes for incremental clustering.  
Have in mind that this post is highly technical as I go into detail about the implementation of each, including the different space-filling curves 🙂

[https://medium.com/towards-data-science/delta-lake-partitioning-z-order-and-liquid-clustering-944030ff1828](https://medium.com/towards-data-science/delta-lake-partitioning-z-order-and-liquid-clustering-944030ff1828)

If you don't have an active Medium account and want to read this piece drop me a message and I'll sort you out",2023-11-13 12:53:05
17t6l0u,Seeking advice for carrer transition coming from Mechanical Engineering," I'm 27, working in the automotive industry in Brazil since 2018, when I joined a major company as an intern. While I  enjoy the engineering challenges, it's not where I see my future. The  salary progression in my field is slow, and growth opportunities are  scarce. 

I've been studying to transition into a technical role in data,  working on projects with Python and SQL. However, considering the skills  expected from a data engineer, it seems I have a long way to go. 

Should  I bridge the gap by transitioning to data analysis before becoming a  data engineer? Is it possible to jump straight ahead in data engineering? ",2023-11-11 23:07:17
17gumky,Help create a datalake house.,"I am currently exploring the concept of a data lake house, and I'm eager to embark on the journey of constructing a modest data lake house to gain a comprehensive understanding of the process. My intention is to create one from the ground up, without using platforms like AWS, Azure, or GCS. I would greatly appreciate any guidance or recommendations on how to get started with building a small-scale data lake house.  


I hope to use opensource applications.

&#x200B;",2023-10-26 11:47:27
17feagl,Creating a data 'warehouse' of various publicly available data (no in-house data)? best approach,"Hello, first off I know very little about data engineering so please bear with me. I work for non-profit that does not have any in-house data, instead we rely heavily on publicly available macroeconomic data from various national and international statistical agencies (e.g. World Bank, IMF, OECD, US/Canada govt, Eurostat, etc.) as well as some third party subscription data. Most of these aforementioned institutions provide API service, and I've been asked to create a data 'warehouse' / repository of some sort where every data we rely on can be automatically updated via API calls and they can easily be accessed by analysts from one central platform as opposed to individual analyst going to World Bank/etc. and downloading excel files individually then creating analysis/power BI/tableau dashboards etc. with them. We want to get rid of existing redundancy as some projects rely on similar data but everyone is pulling this data manually on their own. I unfortunately have no data engineering background and am a bit lost on where to begin, what tool to learn, what i should read up on, etc. Any tips appreciated, thanks so much.",2023-10-24 14:35:08
176ck92,"""Meet a Data Engineer"" interview questions","No, this isn't a post about job interviews. This is related to some research I'm doing on data practitioners and their career journeys. 

Full disclosure: I work for a vendor (I know, ick).

We're doing some interviews of our users to feature on our community. Not about how they use our product, but about their personal story, We can talk about data stacks all day, but this isn't really about that! 

If you were getting interviewed for an article or podcast, what questions would you WANT to be asked? What do you want people to know about you, your career journey, and professional accomplishments? What fun facts or personal stories would you be comfortable chatting about?

And from the reader's perspective -- what would you want to know about your peers? What's interesting to you? 

I'd love to learn more. Thanks for entertaining my question. ",2023-10-12 17:32:07
171nqtn,Fluvio Open Source Repo getting some upgrades that the community seems to appreciate,"Seeing signs of the hockey stick 🏒 traffic on the Fluvio Open Source repo: https://github.com/infinyon/fluvio

Stopped the mobile notifications for the stargazer bot on discord and slack to stop the dopamine rush!!!

But thank you for checking us out. 🙏

The readme.md needs an update. But, the first priority is to release the single binary independent of Kubernetes and Etcd for you to try locally on your own machine.

Coming soon!",2023-10-06 21:05:51
16zoe98,"SWE or SWE, Data","I recently got a job in Data Engineering.... my responsibilities are 80% DE 20% BIE. However, my official title in the HR system is ""Sr Software Engineer"". Not sSWE, Data. Not sSWE, BI. Just sSWE. The company does not have a Data Engineer job title so all data engineers are called software engineers. There is a dedicated ETL team of around 8 data engineers (all called SWE) but I am just dedicated to the BIE team because that team has enough ETL needs that they need a DE full-time instead of the ""get in line"" mentality. I report to the BI Director, not the ETL Director.

I am given the opportunity to get business cards (and can list whatever title I want). I also wonder what to put on my resume down the road. What do you think I should put? Should I simply list Sr Software Engineer even though it's a little inaccurate or go with something like Sr Software Engineer, Data ? For ""official"" purposes, my title is sSWE, but it's well known internally that I focus on bi/data because I'm officially a part of the BIE team.

&#x200B;

&#x200B;",2023-10-04 14:16:19
16v2eaf,Does using DBT Core on Airflow cause performance issues?,"For example, if it's being run with the BashOperator. Is it resource-intensive? Trying to figure out if a DBT deployment on Airflow is expected to be resource intensive and will require scaling of Airflow infrastructure even though there aren't that many DAGS, or if there is something unusual going wrong that needs to be investigated.",2023-09-29 04:54:38
16g79i3,Manager ask everyone take part in sprint planning,"As title says. Manager asks everyone in the team to come up with sprint planning for the part they work on. Discuss with all stakeholders including skip levels and PMs.

Saya it's good for visibility. While this is sort of valid, but I wonder isn't this part of manager's job. I'm curious is this common, and / or good experience to have for maybe 3 to 6 months.

Appreciate any thoughts!",2023-09-11 21:02:40
16bo4e8,Career Advice PLZ. Should I continue interviewing?,"Hello everyone,

This is a throwaway from the Midwest USA.

I would like you opinions on what my next move should be. I am looking for a position as a data engineer primarily to improve on my skills in Python and because I like to build systems. I have an engineering degree but not in data or computer engineering so I'm self taught in those regards. These past few months I've been interviewing with a few companies and one of them has recently sent me an offer. I cannot give the name of the company but it is a world leader in the manufacturing sector and I was offered roughly $80k in the midwest. There are three more companies that I am interviewing for and two of them have given me coding tests that should be completed this week. One of those companies is a smaller company working in clean energy and the other is a fortune 500 also in manufacturing. I know that the other two companies would pay about $15k - $25k more per year. That is mostly because I was unprepared in salary negotiations and said the number I was comfortable with instead of fair market value for my area and because the fortune 500 had their salary band posted.

My question is should I accept my first and only offer while finishing the interviews for the other three companies or just inform them that I have accepted an offer elsewhere and thank them for their time? All these positions are remote and two have headquarters within 2hrs drive of me. Also, and this is huge, I have a felony that is over 9 yrs old but I used to have problems with my background check. My records used to get mixed up because some companies only cross referenced first and last names. I was recently let go from my previous employer and I think the BG check was why, I was a data engineer for them. It was a non-violent offence. As you can imagine getting a job has been hellish for me so please do not judge me on my previous offense.

&#x200B;

Thanks",2023-09-06 16:03:48
1663txm,Cyber security v/s Data engineer which one would you choose as a career if you are starting out now in 2023 and why?,Cyber security  v/s Data engineer which one would you choose as a career if you are starting out now.,2023-08-31 07:06:31
15w4612,Does data engineer need to know DSA / Competitive coding,"
In usa Does all data engineering interview have dsa or competitve programming as a part of interview Like do they asked linkedlist,stack,queue,dynamic programing questions in the first or second round for a data engineering position ? 

I am not talking about pre assement sort of thing I am asking for actual interview",2023-08-20 06:58:07
15m6clz,Why not SharePoint?,"I know, I know I'm poking at the very fabric of a ""data lake""

But really. If my data can't be expressed in a tabular format. Like for example a rosbag file, or some random binary format what are some reasons to use S3/Cloud Object Storage, instead of SharePoint if the user already knows and feels comfortable using SharePoint?

To be clear I don't think SharePoint should be used. But I sometimes struggle to sufficiently justify it to myself even.",2023-08-09 05:50:04
15lpwvl,"Kafka is dead, long live Kafka",N/A,2023-08-08 18:18:17
15llojl,What is your preferred ETL method ?,N/A,2023-08-08 15:39:28
15fchuj,What are the best programs in top colleges to get a master's in Data Engineering?,"I did my bachelor's in India and have a 3 year work experience in a consulting firm where I developed interest in data engineering. I feel I have a high level knowledge in this field and I want to learn more (I know online courses are best to get me up to speed) but i am looking for employment opportunities abroad as well.

Please suggest and thanks in advance.",2023-08-01 13:22:36
1592ku0,Remember that database migration making your revenue metric swing by +5%?,"Hello people!

Me and a friend have been working on an open source tool to improve data quality issue, related to non-moving data that moves 😬 We call it a drift  
✅ Configure your smart drift alerting as code  
✅ Get instant ChangeLog of your metric  
✅ Audit changes and tag your drift patterns to drive continuous data quality improvement

If you are interested, star us and join the waiting list :D [https://www.data-drift.io/join-the-waitlist](https://www.data-drift.io/join-the-waitlist)  


https://preview.redd.it/to1o65guo2eb1.png?width=5760&format=png&auto=webp&s=c6112ba0e13a71c9e37d8cf3326cd0cca7a90f7b",2023-07-25 08:48:01
158ly9a,How to scrape an e-commerce site in an uninterrupted manner using JavaScript,N/A,2023-07-24 20:24:47
15082zu,M1 vs M2 MBA for tasks relating to PySpark or Apache Airflow?,"I want to purchase a MacBook for my basic tasks relating to data engineering. For instance, running Apache Spark Server and executing PySpark jobs and/or running Apache Airflow Server and executing the code for workflow orchestration.

Currently, I am using a virtual machine inside my Windows Laptop. So, I am sure that any MacBook Air will outperform deadly with respect to the setup that I am currently using. Plus, I have had enough of reading reviews and watching comparisons between M1 Air and M2 Air.

This I am posting because I want to get to know the experience from the personal user instead.

I am confused about purchasing which MacBook Air only for personal use considering I will not be replacing it in a year or two.",2023-07-15 10:07:10
14quusz,Data engineer vs Fullstack as a job and why should I choose Data engineer?,"Hello redditors of r/dataengineer I feel like i walked in to a party I was  never invited to but here I am, asking a question.I was recently accepted to get educated as an Data engineer or a Fullstack .net developer.Now I am going to be honest, I an a geotech engineer and love tech but looking to challenge myself abit intellectually workwise but dont really know either of the occupations well enough to decide, I am not necissarly trying to combine my previous work with a new one, More of a fresh start and I wanted to know if you guys think i should choose Data engineer over Fullstack .net developer and why.",2023-07-04 23:56:16
14q59ct,Hello DE people Need your help to solve a simple problem as part of the assessment.,"Given a data set A for training an AI model, we would like to have a function/method that subsamples A and yields a subset B in a random manner, which will be used for mini-batch selection. Please write a function in pseudo-code which receives a set A and outputs a set B.”  
For the completion of the task, please consider the following:  


1. edge cases,
2. how the function can be unit tested,
3. responsibility of the function,

&#8203;

    Ans:
## I thought of one approach:
    random_A = random.shuffle(A)
    B = random_A[0:len(B)]    

Need your help to get an idea to different approach, unit testing this.",2023-07-04 04:54:19
14ji86t,The easiest and quickest way to manage and run Airflow in the cloud with an end-to-end project,N/A,2023-06-26 14:03:54
14ilkoj,Heard of Streaming Sublinear Stochastic Algorithms?,"Also called sketches - they’re a must in every data engineers toolbox. Widely used across the industry, they offer incredible efficiency gains in less disk/memory used as well as CPU. The only cost is that the result is not 100% accurate.

LinkedIn, Datadog and Reddit have all written interesting blog pieces about how they use such sketch algorithms. The savings were up to 88% disk in LinkedIn’s case and in Reddit - their HyperLogLog solution used just 0.15% of the memory of the naive set solution.

I recently learned about them and wrote a good summary here: https://twitter.com/bdkozlovski/status/1672622173369008128?s=46",2023-06-25 12:53:31
14f9uef,OpenAPI: The pathway to 100.000s of pipelines,"Hey guys, i'm the data engineer building the open source python library that will revolutionise data ingestion.   


Here's the latest: Generate a pipeline from Openapi spec - complete with scalability, schema inference and evolution [https://dlthub.com/docs/blog/open-api-spec-for-dlt-init](https://dlthub.com/docs/blog/open-api-spec-for-dlt-init)  


With this and possibly a little help from LLMs, it should be possible to generate pipelines for half the apis out there.

If you wanna check out the schema evolution, try the colab demo [https://dlthub.com/docs/getting-started/try-in-colab](https://dlthub.com/docs/getting-started/try-in-colab)  
",2023-06-21 14:52:31
14eqed6,Ebooks order to read,"Hello folks.
I pretend to learn data engineering. I’m already in Data Analytics field. Which is the order I should learn those books:

* The data Warehouse toolkit
* The kimball group reader
* designing data intensive applications 
* Fundamentals of data engineering 
* T-SQL fundamentals 
* T-SQL Querying

Thanks in advanced",2023-06-20 23:07:04
14ctv72,Breaking Analysis: Snowflake Summit will reveal the future of data apps...here's our take - Wikibon Research,N/A,2023-06-18 20:12:10
14bufkt,"Explain me how websites like Dall-E, chatgpt, thispersondoesntexit process the user data so quickly",What architecture are used by these websites to process such huge datas,2023-06-17 15:52:07
13q904b,Build a datawarehouse,"Im planning to build a datawarehouse 
Information will come from two sqlservers and some excelfiles 
which tools do i need ? 
do you suggest using dbt ? databricks ?
is there any tutorials ?

Please advice and thanks",2023-05-24 02:52:43
13hpkiz,How to automate DE for data products?,"I'm a product manager of a corporate data product that reports sales data for multiple drug stores across the US. It usually takes a sprint (2 weeks in our case) to integrate a new pharmacy chain or add new columns/measures or even enable existing columns for newly onboarded chains. Our stack is mostly msft - adf/synapse/aas/pbi + airflow. My team consists of 4 senior DEs (all full FTE) and adding another team members probably won't increase our throughput due to possible admin issues.

**What can I do to automate engineering to get the work done faster?** I was thinking about running a POC to figure out how to employ more parametrization so they don't hardcode everything, doing some sensible defaults when onboarding new pharmacy chains or finding a way to go low code/no code so our support can do most of the easy repetitive work instead of DEs. 

I know I'm not providing too much info here and I'm also not the most technical PM but I was hoping some of you went through this journey already and got some tips?",2023-05-14 22:34:00
12sev02,Career progression help - Dev Engineer,"I've been in business operations for 5 years applying a project management and technology solution mindset. The automation space I work in (RPA) is heavily nuanced. I don't see it going far as an industry, although I know I can knock it out of the ballpark if I stick with it.

I've got a degree in development as well as an MBA. Experience with programming languages, SQL, etc.

I make a little over 6 figures. How realistic is it for me to get into data engineering / analytics making $140-160K within this year working full remote? What path would you recommend to someone?

I'm starting to pick up Python this week after having put it off for ages.",2023-04-19 23:49:52
12l8ryw,T-Sql Query Tuning,"How to use Chat GPT or suggest any other online free tool for query tuning.

I have data analysts writing a bunch of queries that uses same set tables of tables over and over again in left joins and CTEs with full table scans 😵‍💫🥴",2023-04-13 22:24:07
12iz03x,Technical Questions for Data Engineer Position,"This is an article that I made while looking for a new job opportunity. I hope you like it and feel useful. 🙂

https://link.medium.com/DlqgDCJaVyb",2023-04-11 21:50:32
12bspck,Freelancing DE - Rate / Possible pitfalls,"I was recently approached by a small organization that was looking to improve their analytics capabilities. Right now, they're just using an excel sheet that's not even shared between users. 

One of my old coworkers currently works at this org and recommended to the CEO to have a discussion with me. We had just a preliminary/casual discussion about what it would take and it's well within my skill set to give them what they want. It's a fairly basic set up with a self-service analytics tool like Tableau or Power BI at the end. 

My questions are, what should I be looking to charge? Has anyone done freelancing DE work and have any words of wisdom or pitfalls they'd like to share? This would be my first freelancing job and I'd be doing it on top of my 9-5.",2023-04-04 18:56:03
12ac3qm,Data Analytics Lifecycle,"
The analyst comes to you (the data engineer) saying I need this data, and you say OK. You give them the data and then the analyst comes to you again saying they actually need to change something in data. How can you avoid this back & forth and work better together as a team? By getting on the same page about the complete analytics lifecycle.

Following this structure helps data engineers better understand their data and improve the effectiveness of the data analytics work of their team.

Data Analytics Lifecycle, here it goes

### 1. Discovery

What problem you are solving and what business outcomes you wish to see.

### 2. Data preparation

Decide which data sources will be useful for the analysis, collect the data from all these disparate sources, and load it into a data analytics sandbox so it can be used for prototyping.

### 3. Model planning

A model of relationship between variables that can help you understand the probability of an event happening. It could be a SQL model, statistical model, or machine learning model.

### 4. Building and executing the model

Once you know what your models should look like, you can build them and begin to draw inferences from your modeled data. 

For the sake of brevity, let's skip it in this short post (I'll write another post about it or you can read about [building different types of models on this post](https://www.rudderstack.com/learn/data-analytics/data-analytics-lifecycle/))

### 6. Operationalizing

Once the stakeholders are happy with the analysis, you can execute the same model outside of the analytics sandbox on a production dataset.

Questions/thoughts? Share your challenges/success-stories following this lifecycle?

P.S. If you like this, I will write one post every week to share similar primers on different data engineering topics. Do share the topics and support my open-source work at [RudderStack](https://github.com/rudderlabs/rudder-server)",2023-04-03 06:49:24
125ezxs,Transition to Big Data Engineering......Let me know your thoughts,"Hi Guys,

 I am from India. I have 3.5 years of experience in ETL/ BI development/Data Analytics.

* ***Skills:*** *Alteryx(It is a low code tool), SQL, PowerBI, Python(Average)*
* ***CTC:*** *18.5 L  ( including bonus)*
* ***Background:*** *Mechanical Engineering from T2 college.*
* ***Strength:*** *Good problem solving skills, Self learning*

Now I am thinking about switching in to Big data engineering side. I have joined a course to learn DataBricks, Pyspark, Kafka, Azure etc. I have listed my thoughts below. Let me know your suggestions.

* *Salary won't go beyond(30L) in my field. But the upper band in big data is 30-50L.*
* *If I can find good remote job(US/UK) , I can make up to 30-70L.*
* *I can find freelance works or start a small company in the future.*
* *I am good at ETL & Visualization part. If I learn Data engineering and eventually Data science(after some years), I can get good opportunities*",2023-03-29 06:12:26
11x47ox,How hard is it to get a data eng. Job remotely overseas?,"I am from Latin America and i am in CS uni and having to decide between following a data eng or soft. Eng path.

A huge factor in my decision is how easy it would be to get remote work for companies in North america/Europe, i heard Data is a harder field tondo that  due to safety concerns, local laws and in some cases requiring more academic formation.... is that true?

For my goals which one would be the best?",2023-03-21 02:28:58
11u5fhx,Metadata Management: Key to Data Engineering Success or Overhyped Trend?,"Let's talk metadata management. Some folks say it's the unsung hero of data engineering, while others think it's a waste of time. So, what's the deal?

1. **Finding data:** Metadata makes it a breeze to find the right stuff.
2. **Teamwork:** Helps peeps from different teams understand each other's data.
3. **Tracking data's journey:** Know where your data's been and what's happened to it.
4. **Keeping data on point:** Makes sure your data's reliable and accurate.
5. **Ready for the future:** Helps you stay flexible and adapt to new tech.

But hey, there's another side to the story. Some say metadata management just adds headaches, takes time, and makes us rely on specific tools. They think we should focus on building a solid data architecture instead.

So, let's get real: is metadata management a game changer or just another hype train we should skip? Drop your thoughts and experiences below, and let's hash it out",2023-03-17 22:06:39
11tttqg,DSA For The Rest Of Us - Into to Linked Lists,N/A,2023-03-17 15:10:01
11mon3m,Code with ChatGPT - SQL and Python examples,N/A,2023-03-09 10:16:33
11euy09,Which is the best data bricks certification to begin with?,I am interested in becoming certified in data bricks and am looking for feedback on the best resources to learn and obtain certification.,2023-03-01 05:08:05
10s153q,Resume review! I am an international grad student in California. Applying for a summer internship but haven't gotten any calls yet. Wondering where am I going wrong.,N/A,2023-02-02 21:16:32
10r3rrr,I need to become an expert on enterprise databricks adoption but nothing to do with code...,"...what would you suggest?

Focussing on:

* Data Engineering processes (Databricks + AWS)
* Adoption step by step
* Best practice",2023-02-01 19:35:11
10n3w1q,Data engineering and sports (soccer especially),"Hi everyone!

I´m about to finish my ds degree and i´m kinda worried about my futures jobs. I´m not talking about if i´m gonna get a job soon (i´m working in an argentinian company as data analist part-time and in a govermental agency as data engineer in a short term proyect), but if i will be able to work in a proyect that i like. I really like the stuff i do, but make dashboards to a company to see the revenue or the  absenteeism is not the kind of work that makes you be in love. I thought that working for something that has impact in my country will be great ( the agency takes care of the health of  ), but the state bureaucracy makes the work very slow and usually my team cant get the resources to make a diference with our job (i.e. cant get daily data because the staff are in holidays or cant get a dev server in a razonable time).

&#x200B;

All of this makes me wanna get a job in an area that i really like, and feel that my work makes something for the world. I´m kinda young yet (will be 26 in april), so i think the best way to guide my carrer to something diferent is studing something related to the thinks i like. 

So, do you know something about big data and sports (i would really like soccer stuff) or machine learning and sports? A book will be very usefull, but i really want to gain experience on the field, so maybe a course or degree will make me the thinks easier.

Also, any storie about a search similar to this (with a happy end will be perfect) it ll be great.

Thanks!",2023-01-28 02:38:07
10l58ez,EP 3 - Migrating from Delta Lake to Iceberg,N/A,2023-01-25 18:09:38
108mihu,Why use Spark?,"I am new to data engineering and one of the popular software you hear used in this field is Apache Spark, so I am spending some time to understand what exactly it does.

The Spark websites describes it as _Unified engine for large-scale data analytics_. So okay, I interpret that to mean it enables you to crunch and analyze data very fast.

The next question I now had was, okay, if it helps in data analytics, where does it get the data it analyzes from?

So I discover it could be various sources even including relational databases! I found this video [here](https://www.youtube.com/watch?v=rAeQ2k-C00o) that shows how to connect it to a Postgres database in Scala.

But I can connect to Postgress with something like [Quill](https://getquill.io/) and run sophisticated queries to fetch data. Which then got me thinking, what is the difference between using Spark to connect to the database and using something like Quill or your normal pure JDBC driver?

So in this particular case of using Spark to fetch data from a relational database, what is the advantage? Does Spark magically enable me to perform queries and analytics faster?",2023-01-10 22:05:04
100pfqa,Best college majors for data engineering?,"It seems like CS is the obvious major but are any other majors that are just as, if not more, relevant?",2023-01-01 18:13:09
zzfr0s,Pyspark Test if file exists,"Hello 
I need your help. 
I'm new to Pyspark and I need to write a script that moves several files.
 I would like to test if a file exists before moving it. 
I'm working on Azure and the files are in S2 storage. 
I saw that there was an OS module that allows you to do this but it seems that it only works if the file is local. 
Do you have a solution to perform this existence test of a file in S2?

Thks",2022-12-31 00:03:08
zvuepy,Is DE a good startup area?,What do you think? Are there good ways to create a startup under the topic data engineering? Or is it merely a craft which doesn’t suit for running a startup?,2022-12-26 19:14:55
zhpeit,Loosely coupled monoliths and where to find them,How we guarantee ACID integration over arbitrary SQL databases without a central monolith,2022-12-10 10:41:21
zg3711,Salary in France,"Hi all,

The following is a question about salary in France as a junior data engineer.
It’s written in French for simplicity as it is targeted towards French speakers :

Salut à tous,

Je suis étudiant en 5eme année d’école d’ingénieur en informatique et j’ai un stage dans une grosse ESN française (qui me proposera probablement un CDI par la suite) en tant que data engineer.

Étant contacté par pas mal d’entreprises sur LinkedIn par rapport à un CDI suite au stage, je me pose la question du salaire et de la négociation. 

Si je me trompe de subreddit, n’hésitez pas à me le dire et éventuellement me rediriger !

Ce qui est difficile pour moi, c’est de savoir à quel salaire je peux prétendre pour un premier emploi dans la data, spécifiquement en tant que data engineer (ingénieur de données).

Informations sur ma situation : 

-stage development web (2,5 mois)

-stage recherche opérationnelle (2 mois)

-stage data engineer (6 mois en 2023)

-école plutôt bas du classement car réorientation en informatique, mais dans les meilleurs (voir major selon le semestre) de promo

Sachant que je cherche hors région parisienne, avez-vous des idées du salaire auquel je peux prétendre ou des conseils pour la négociation ?

Quand je cherche sur internet (Glassdoor, par exemple) je trouve des estimations de moyennes autour de 40k-41k/an, voire 45k, pour 0 à 1 an d’expérience. Toutefois, je doute de la fiabilité de ce que je trouve sachant que j’entends pas mal d’offres pour d’autres étudiants en informatique de 32-35k pour un premier CDI. 

41k me paraît un peut-être haut pour un salaire hors région parisienne mais 32-35k me paraît quand même bas pour un métier qui est censé être parmi les plus rémunérés dans la tech en France.

En tout cas, j’apprécie toute aide, notamment si vous êtes dans le milieu de la data.",2022-12-08 16:07:09
ze7red,Discover the amazing features of Apache Airflow 2.5 😍,N/A,2022-12-06 14:10:18
z8cdfu,QDF - Data Factory ITS NEW!!,N/A,2022-11-30 01:06:07
yymrkk,Why you may prefer an ELT pipeline over an ETL pipeline,"The ELT (Extract, Transform, and Load) approach matches the reality of data needs that rapidly change. Because the ELT process transforms data after it has been loaded, it is not necessary to know in-advance exactly how the data will be used – new transformations can be performed on the raw data as the need arises. Furthermore, analysts can always access the original raw data, as subsequent transformations will not have compromised its integrity. This gives analysts autonomy from data engineers, since it is not necessary to modify ingest pipelines in order to transform the data in a new way. If a raw and unmodified version of the data exists in the destination, it can be re-transformed in the future without the need for a resync of data from source systems.   


For the full post see: [https://airbyte.com/blog/elt-pipeline](https://airbyte.com/blog/elt-pipeline)",2022-11-18 15:27:40
ywixcg,What happened at Coalesce,"together with Eric we are experimenting on new formats for the Data Stack Show.  


Shop talks are small conversations between me and Eric where we discuss a topic. We try to keep it short and fun.  


the idea is to use these as conversation starters with a broader audience. It would be amazing to hear from you and open a public discussion on the topic!

&#x200B;

Check the Shop talk here:

[https://datastackshow.com/podcast/shop-talk-what-coalesced-at-coalesce/](https://datastackshow.com/podcast/shop-talk-what-coalesced-at-coalesce/)",2022-11-16 03:51:53
ype6gu,What is Palantirs USP or differentiator?,"What is Palantir doing differently compared to e.g.Tableau, Power BI.
Are there any comparable solutions?",2022-11-08 07:16:46
y8dt52,Job interview questions.,"As DEs, what questions would you like to answer during an interview to standout from the crowd?",2022-10-19 20:44:25
y6imum,What’s your Favourite Cloud Provider for Data Engineering and why?,"Vote :)

[View Poll](https://www.reddit.com/poll/y6imum)",2022-10-17 18:28:55
y403hr,"16GB macbook enough for development? (Docker, Airflow, etc)","I am currently running off an M1 air with 8GB ram. So I can't really use the Airflow docker local. By the time I get the webserver, scheduler, etc... up, I'm swapping memory hard. With OS and browser I'm sitting at around 11GB

As I am getting more and more into back-end development I'd like to know if you think 16GB is good for now. I think it's okay, anything that requires more will be moved up to prod / cloud anyway. But just wanted to verify before I make the $2k commitment.

The reason for this is that I pretty much only buy hardware from costco. They have better deals than the apple student store, and an incredibly good 2 year warranty included.",2022-10-14 17:32:24
xytfc3,MBA for Data Engineers,"I've been seen a lot of discussions lately, about certificates and stuffs for those that are in the area. What about an equivalent of a MBA for devs/DEs? Is there something related in US universities, for those who have a bachelor (therefore, it's more like a complement). That could, of course, add a lot of value to a CV?",2022-10-08 13:55:32
xva9q8,A quick question around insights from Data Warehouse,"Hello Folks,
How are your biz ops teams (everyone from Sales, Marketing, Product, Finance, Operations and Support) drawing insights from data warehouse.
Do you have mechanism to deploy actionable insights to their individual software stack?

How effective has it been?",2022-10-04 09:18:22
xo1pcg,Advice to your little You?,"Hello :-)

I have been a software engineer for about three years now, both front, and backend. I would love to study data and shift to this field, I searched a lot and found that making pipelines and storing, organizing and cleaning data is the best for me. what would you advice me to do to become a better data engineer from scratch.

Thanks for all your help and responses <3",2022-09-25 22:54:36
xeywfk,[Azure] Does anyone have good tips or recommendations for ingesting data from Azure Event Hubs?,"Currently I'm trying to stream event data into Event Hubs by connecting it up and reading the stream in DataBricks.   


However, I don't feel like it's the best way to process the data as it's a lot of coding to split all the data out. Decoding then Splitting the attributes in a notebook. Plus i keep getting errors around using readStream in Databricks. 

Is there a better way of doing this or any other tools within Azure? I've seen Data Explorer but I'm not sure if that would stretch a budget or it's just a waste of resources.",2022-09-15 14:56:18
x6zz88,Switching from Big Data Engineer to Bigdata Platform Support,"Hi all, is it a good Idea of switching from service based company's Bigdata Engineer role (1 year experienced)  to Investment banking's Bigdata Administration role (200+ % hike) . (Although I can opt for change of role after 1 year being in same role)
All suggestions are welcomed...",2022-09-06 03:37:02
wyewvp,Does what kind of degree matters? Do I need MS CS?,"Hello!

I am current grad student in biostatistics (first year), hoping to enter data science by the time I finish.

I know that data engineering heavily favors computer science degree. I did have a cs certificate in my undergrad( kinda like minor/ bachelors was in math), and did contract work for a company in software engineering (didn't go too deep / 3 months).

I will plan to join statistical consulting and data science groups in grad school to hopefully practice my programming and statistical analysis skills.

I am just wondering, if I don't like being data scientist or being a biostatician, is being a data engineer feasible without going through masters in computer science or bootcamp?

what do companies looking for data engineers think of biostatistics degree?

I currently know python, R, and SQL quite a bit if that helps",2022-08-26 17:21:34
wnbk17,A Cost-Effective Way of using The Modern Data Stack (II),N/A,2022-08-13 10:43:10
wlusyu,Databricks DBU Azure,"Just a quick question, does someone know whether a similar table for Azure exists ([https://www.databricks.com/product/aws-pricing/instance-types](https://www.databricks.com/product/aws-pricing/instance-types)).

And to any Databricks employee, this pricing structure is horrible. It's impossible to estimate or compare anything as a **DBU** is simply an undefined entity, it could just as well be grams of Unicron crap...

EDIT: Nevermind, found it on Azure itself. But still, this could really be made a bit more transparent and easier to find...",2022-08-11 15:53:46
vaaiyd,Is Data Engineering my next logical step,"Hello All,

I am thinking data engineering should be my next professional step.  I have been doing Sql Development for over 15 years, ETL via SSIS and some IICS for about 8 years and Reporting OBIEE BIP, SSRS, Power BI for about 7 years.  These were overlapping in a lot of my positions.  I have also done Python, Scala, JavaScript, and .net programming when needed to support ETL or in house systems.  Lately I have been doing Business Intelligence and I want to switch gears to another role.  My youngest is on his way to college and I will have a lot of free time, no more running him to practices and games.  I just think its the correct time to buckle down and learn data engineering.  I have spoke with my current boss and he is supporting me moving to another team, but not until we finish these last few projects.  Any way my question is outside of just moving to the data engineering team, what is the best way for me to ramp up on data engineering skills?  Any suggestions would be appreciated.",2022-06-12 00:38:39
v3mf9n,Enabling a Data Mesh with an Open Lakehouse,N/A,2022-06-02 23:55:23
uvssgg,What’s the typical salary for Sr. DE?,I am Sr.DE with a TC of 180k. With 4 Yoe of exp. Is that a normal? Or am I being underpaid. Location BayArea,2022-05-23 05:09:07
tuad69,Demand by Country by tech stack?,"Hi,

Was wondering if there was any specific data that showed the demand of DE by country by tech stack?Is there any preferred tech within specific regions/sub-regions?

&#x200B;

Was chatting with some international friends and each one kind of had a different experience of tech stack. The biggest was AWS vs GCP, looked like people who generally use GCP were mostly from northern places. This was a supersmall sample size of like 6 people. It was an interesting thought to see if there was a trend somewhere.",2022-04-02 04:25:41
tp0x2j,Work Culture,"Hi, I am interested in becoming a Data Engineering and I want to know how is a regular day in your job? how os the work Culture? How is the work/ life balance?",2022-03-26 19:29:58
taifz2,Personal Traits of Data Engineers vs. Data Scientists,"As the title indicates, I’d love to hear everyone’s thoughts on how the personal traits of effective data engineers and scientists differ. Thank you!",2022-03-09 21:44:35
szj41q,Orchestrate Spark pipelines with Airflow on Ocean for Apache Spark - The Spot by NetApp Blog,N/A,2022-02-23 14:43:51
sul1xo,Mongodb vs Postgresql,"Which database to choose for user management system like user profile, remarks, comments and so on. Also, need to run few analytical queries.",2022-02-17 09:46:06
sdb5y9,Will Julia become the main language for data processing in the next 2-3 years?,"why / why not in the comments

[View Poll](https://www.reddit.com/poll/sdb5y9)",2022-01-26 17:22:11
sc5f7d,Need help evaluating a job offer,"About me: 
- 4 years in academic-style roles in economics. Experience using R and Python to build datasets for econometrics. 

- Enrolled in Masters of Data Science degree part-time. Most of the classes I’ve taken are DE courses (Scala, Data Engineering, Databricks).

- Looking to move into more more DE-esque roles but worried that I don’t have the correct “formal” training. 

About the Role
- Mid-size firm specializing in financial analysis for small financial firms.role is on a small team of 3-6 data analysts.

- Job title is “data analyst” but tasks mainly involve moving data submitted by clients into their on-prem data warehouse. “Side-projects” to automate this system and eventually move everything to cloud in a year or two. 

- job would be to bring “technical skills” to the team and help them make this transition.

My thoughts/ concerns
- I don’t have any “formal” DE experience, what I do have is mostly self-taught. How should I think about continuing this self-education in a role like this? 

- Is it more important to get “hands-on” experience, even though it’s outside of a modern DE solution? Or, should I focus more on trying to get into a role with more established De practices?

- Pay seems mediocre for this type of task (80-90k).",2022-01-25 04:47:45
s51d6t,DE at Big 4,"I recently got an interview with one of the Big 4 (D) for a data engineer job. Just curious if anyone worked for them before?

What was the work like and how was your interview? 

Thanks in advance.",2022-01-16 02:40:06
rfldjv,Show your love for SQL!,"Me and my team at [Census](https://www.getcensus.com/) have build a free feature called [sync-a-swag](https://blog.getcensus.com/sync-a-swag-use-operational-analytics-to-receive-free-census-clothing-gifts-and-more/). In a nutshell, you can write a SQL query that will send a free t-shirt right to your doorstep.   


No gimmicks. This is totally legitimate. Please give it a try!    


This [link](https://www.getcensus.com/sync-a-swag) provides step-by-step video tutorials. If you prefer documentation, you can complete sync-a-swag [here](https://docs.getcensus.com/destinations/sync-a-swag).",2021-12-13 17:43:17
qs6292,Would you say ETL and data architecture are difficult to learn?,I was wondering if someone could answer my question on ETL and data architecture. Could anybody tell me if it's difficult to learn and how long would it take to learn both of them? Would you say it's easier or harder to learn either of them compared to learning python?,2021-11-12 07:28:55
qpzo5q,Top 10 Data Engineering Courses in India to Take Up in 2022,N/A,2021-11-09 09:01:26
qe8o1e,Does anyone has AWS Data Analytics exam notes to refer?,"I am in middle of my study and also referring **internet and aws documentation, one Udemy course**. It will be great help if anyone can provide some docs prepared by them for their own understanding.",2021-10-23 16:56:00
qcu1f5,Gamification: Learning Python & Data Camp,"So, I'm well aware that data camp isn't necessarily the best way of becoming a data engineer, but my question comes from the perspective of learning Python as a programming language. What I'm looking for is a platform where I can veg out on the sofa with my gf watching Netflix or whatever, pick up my phone and bash out some Python challenges or problems. Is data camp the best platform for that sort of requirement. I already have access to a good Python course, but just want something that has its own mobile app that I can just pick up whenever I feel like it.",2021-10-21 15:13:37
pydj1p,What is the future of ETL Data Tools Market in Between 2021-2027?,N/A,2021-09-30 05:49:22
purthj,Twitter community list. These people are creating or promoting the engineering practices that we are using now or we'll use in the future.,N/A,2021-09-24 20:23:41
ptazl0,Co:Rise Learning Platform & dbt Analytics Engineering Course,"Does anybody have any experience with the Co:Rise learning platform? I've heard of the usual Udacity, edx, etc. but not Co:Rise.

They recently announced an Analytics Engineering course that looks to use dbt for various projects over a few weeks. Cost is $400.

Anybody have insight into the topics covered by the course structure?
 
Any thoughts on if this would be worth it?

[Here's a link to the course in quesiton](https://corise.com/course/analytics-engineering-with-dbt?utm_source=emilyh)",2021-09-22 16:18:01
preqiw,CDC implementation in Airflow.,"How can we implement CDC in Airflow using Mysql or Python Operator. :thinking_face:


Can anyone share helping source or thoughts. :smiley:",2021-09-19 19:32:44
pacy4t,Anyone work with a ‘Technical PM’ before?,"My company hired a technical PM to help me manage the Azure data migration that I’m working on, as well as other things.

Title is “Technical PM”, and he was introduced to me as such by my boss(she said he’s best of both worlds and should be able to help me). He hasn’t really helped out in any of the coding, or documenting work for the migration(requirements,etc.) that I’ve been working on. He hasn’t helped in making the architecture diagrams or set up key milestones for the project.

Although he does seem to be making lots of suggestions about what tech to use or where to partition data. Kinda strikes me as odd as he does not even have his access to any database upstream figured out yet. Thus far, I’ve not seen any pulls or commits to the git I’ve set up for the work.

Anyone have experience with this kind of PM? I don’t want to be mean, but it seems weird to me to recommend anything without actually getting your hands dirty with the data. At the very least, shouldn’t tech recommendations be followed by  proof of concepts? I usually build out POCs and present data on why it will/will not work for the particular thing in question.

I got a red flag when I heard him say that Azure Synapse is solely a data warehouse solution. Which seems odd because their info page suggests it’s rebranded to be able to do more than just be a data warehouse. He instead suggested Data Factory because it doesn’t only use Apache Spark for its transformations. Yet I read on the page and have seen it offloading the work to Apache Spark cluster. Just odd, I guess, but how do you usually break this kind of behavior without coming off as arrogant. 

I mean I don’t know *everything* about what Azure offers, but I do know how to rtfm pretty well. I also utilize our sandbox environment to test out POCs once I have a coherently mapped out plan.",2021-08-24 00:56:35
p82rem,Which offer to choose ?,"I have 1 YOE as a jr Data Engineer at a Fortune top 5 company.

Recieved two offers(India): 

One from a silicon valley startup in Preffered location. 
Current salary + 60%. Will be setting up the entire data infra there.

Other from Walmart (Good brand name here) tech stack is java(not Preffered). Location - not Preffered and current salary +50%



Currently inclined towards startup offer. Challenging work plus I have already experienced a huge company, setting things on my own would be great !

Only will miss out on brand name. But I guess that I can get later also + it is not FAANG+MULA anyway.

Does it get harder or easier to get into FAANG once you become more senior ? 

Looking for what fellow DEs think of this.",2021-08-20 10:47:53
p6zl9k,"Podcast - Building Data Engineering Pipelines at Scale (with Data Warehouse, Spark and Airflow)",N/A,2021-08-18 20:32:52
p1ojoi,Step by Step How to Clean data with Python BeautifulSoup (13 Code walkhroughs),"Hi Guys, I thought I would share some useful ways to clean data from the web

[https://webautomation.io/blog/how-to-clean-web-scraping-data-using-python-beautifulsoup/](https://webautomation.io/blog/how-to-clean-web-scraping-data-using-python-beautifulsoup/)",2021-08-10 12:51:31
ox3fa9,MLOPS > ML,N/A,2021-08-03 13:25:03
oiyell,Make me learn Deltalake.,"Hi, I've joined a company very recently 2-3 weeks back and there are implementing a deltalake. Can you guys recommend me some good resources on learning deltalake? ALSO some good Databricks learning material?",2021-07-12 19:25:18
ohwiwy,The key step of data engineering for ML project,[https://hungngph.medium.com/storage-feature-engineering-for-real-time-prediction-in-momo-500b0db18aa2](https://hungngph.medium.com/storage-feature-engineering-for-real-time-prediction-in-momo-500b0db18aa2),2021-07-11 02:58:34
obmssw,HDFS - Hadoop Distributed Filesystem,N/A,2021-07-01 14:18:42
nrnhrd,Google Sheets row limit,N/A,2021-06-03 20:43:45
ndo69d,Anyone did SAP HANA to S3 or Snowflake or Redshift data migration,"1. Can you please suggest the best way to handle the SAP HANA historical data migration and CDC in AWS? 
2. To decommission SAP HANA - What would be the best data warehouse available in the market.",2021-05-16 13:18:58
n78k1j,Skills needed to become a data engineer,N/A,2021-05-07 20:47:18
myufij,State TTL for Apache Flink: How to Limit the Lifetime of State,N/A,2021-04-26 10:16:45
mp4j48,"Do you have ""big data"" problems at work?","I work with relatively large datasets, often in the 10s of terabytes, sometimes even 100s.  However, I haven't really noticed any problems working with large datasets apart from pipelines and queries taking longer to run. Tools like Dataflow and BigQuery can easily handle data of this size. 

Yet, it seems that there is this notion that big data is challenging to work with. From my perspective it seems large datasets are similar to working with smaller datasets. Do you agree?",2021-04-12 02:00:21
mot8be,Am I a Data Engineer?,"Hi all,

in the last 10 years my work was about:

\- desing and develop ETL using SSIS and (above all) Talend Open Studio for DI

\- analyze data sources in order to integrate them (using ETL processes developed using ETL tools, SQL script and, recently, Python script) in a single relational data warehouse (based above all on MS SQL Server)

In the last years I used MongoDB in order to gather sensors data (very simple and little projects) using Node Red in order to control data flow

About data visualization, I used above all  IBM products (IBM Cognos BI or , in the recent years, IBM Cognos Analytics)

So, I have a strong knowledge about ETL, SQL and data warehouse (following Kimball best practices) but a superficial knowledge about Python (I know only what I need to know) and Hadoop and Spark (is Hadoop yet used in real projects?)

Moreover, I haven't knowledge about Apache Kafka, Flink and other realtime stream processing tools

 In order to be critical and constructive about my level of knowledge, as your opinion, am I data engineer and what do I need  to go in depth in order to solve my knowledge bugs in this scenario?

Finally, do you advice me to go in depth about  any cloud platform as GCP, AWS, Azure, etc. about ""data engineering world""? 

Thanks a lot for your hints!",2021-04-11 15:54:22
molna1,Data engineers vs. tools,"I have read the article ([https://towardsdatascience.com/we-dont-need-data-engineers-we-need-better-tools-for-data-scientists-84a06e6f3f7f](https://towardsdatascience.com/we-dont-need-data-engineers-we-need-better-tools-for-data-scientists-84a06e6f3f7f)) that says that ""**a large portion of the Data Engineer’s role could be replaced with better tooling for Data Scientists"".**

Honestly, agree.  It is a pain to find/hire an experienced data engineer for an affordable price. Instead, my company would rather hire a middle-level engineer but provide them with the right tooling to scale up their job. What do you think?",2021-04-11 06:12:52
mlzl6d,Data Engineer Mentor Needed/Newbie,"Hello all,

I've decided that I want to pursue data engineering. I've learned basic python, sql, data structures, and algorithms.  I also have my AZ-900 certification.  I love working with databases, SQL and really enjoying the newly added language, Python.  I need to know if I'm on the right path and what do I need to focus on next.  I guess I'm just looking for some direction.",2021-04-07 11:02:45
m7vxpi,7 Reasons Why You Should Consider a Data Lake (if you don't have one yet),"Data lakes are an essential component in building any future-proof data platform. In this article, we round up 7 reasons why you need a data lake. Including a demo on serverless Event-driven ETL with Data Lake on AWS. 

Read more: [https://dashbird.io/blog/7-reasons-why-you-should-consider-a-data-lake/](https://dashbird.io/blog/7-reasons-why-you-should-consider-a-data-lake/)",2021-03-18 17:24:16
lzi3zq,Data Engineering - Open Data Lake Platform,"[Data engineering](https://www.qubole.com/platform/open-data-lake-platform/data-engineering/) is the aspect of data science that focuses on practical applications of data collection and analysis. For all the work that data scientists do to answer questions using large sets of information, there have to be mechanisms for collecting and validating that information.",2021-03-07 03:27:26
ks28cz,BI Engineering vs DE (AMAZON) Career progression,"Hello guys, I have just received an internship offer for a **BI Engineering position at Amazon**, with a project shifted towards **DE (some web-scraping, building pipelines and probably some visualization tools on top of that)**. I think the proiect is interesting and the team is small enough for me to learn a lot of techniques and develop, however my **ideal position would be to shift towards the DE role, either at Amazon or at another tech giant** (or FinTech etc). From my limited understanding and reading some job descriptors, the 2 positions are really similar, depending on the company obv.

Do you guys have any idea is **this position (BIE) can be a good starting point for a career in DE**, and how easy would be to change roles, assuming my project is mostly on the DE part already? I would massively appreciate it if any of you guys had any inputs or advice!

Thanks in advance and all the best!",2021-01-07 01:22:01
joppi3,What kind of courses should I be taking in university to become a data engineer upon graduation?,N/A,2020-11-05 19:52:57
jj13f6,Help me understand it.,"Hello everyone,

i just wanted to ask few questions so that maybe you guys will HELP ME understand better.

so i recently got an internship in a company as a data scientist/data engineer. i was given a task where i have to store a video, an audio, an image and a text into a single numpy array combining them all into one. It could also be a series of videos into one numpy array or images or audios or texts.

I could not understand it and asked them why would they want to do that and in answer they told me that it was to make a training set to train the model.

does that make any sense? or i am not understanding it properly. As far as i understand i believe that we just store data lets say videos or images or a text files from different sources into our storage and when we need them for training purpose we extract the files. i still cannot understand why they asked me to convert a video into a numpy array.

I was told that thats what data engineers do, that they extract all these different data files from different sources and make them into one so that we can have a training set.

It would really be great if you guys could help me understand it because i am just starting my data science career and i don’t understand fully how these things work. I was also expecting a lot of data cleaning and SQL queries and maybe work on data pipelines but rather i was told to do this task.

Thanks everyone!",2020-10-27 13:27:49
j6dlg9,Kafka Connect in a nutshell,N/A,2020-10-06 21:05:01
iree0u,Airflow operators,Which are the most important/intresting operators in Apache Airflow that you have come across and why?,2020-09-12 15:16:39
i8v5c6,Amazon or Facebook Data Engineer Interview process. Easier one?,"Which of the two DE interviews is easier to crack(at a junior level) and why? Would love to hear personal experiences in comparison if any.

[View Poll](https://www.reddit.com/poll/i8v5c6)",2020-08-13 06:55:24
hjhty0,Unemployment rate for different race (1948-2020),N/A,2020-07-01 20:01:21
hhhoe8,Spark architecture confusion (If not for storage ),"Why spark Developer are called as DE . Amazon Redshift ,DWH,ETL,elt come under data Engineering. Spark now is used for Analysis of large Data spread over clusters in Ram and analyse data and tell answers related to data. isn't this Data scientist work for Analysis .as DE build things and see proper flow of data and pipleines . Am I missing something ? 

If u just want on building things and pipelines  .Do I need spark ?  Or Spark is used for Spread data over multiple clusters and storing on hdfs  and later used for Analysis

 If majority of data is migrated to redshift which distributed cluster based data warehouse does spark will be relevant 

Note : I'm super confused and not interested in Data scientist role.Guide me usage of spark ,interms of DE",2020-06-28 16:46:25
g8dma1,A hypothesis that the Federal Reserve can set interest rates based on the movements of the planet Mars. Here I have data going back to 1896 that shows how the Dow Jones performed when Mars was within 30 degrees of the lunar node. (- from appendix of Ares Le Mandat 4th ed),N/A,2020-04-26 12:36:21
g112ky,Switch to Data Engineering Role,"Hi all, I'm trying to make a switch to Data engineering and would love any advice on this. Currently I'm a Data Analyst, working in reporting. I know SQL quite well and python at a functional proficiency. I build reports in ssrs, dashboards on tableau, write python programs to run and automate data exports , and build python ETL code for some of the smaller transactional processes in my company. I'm no expert in programming,but I know what tools to use to get a job done. I have a master's in engineering and have been working for a year now. I feel I'm being underpaid for my skillset and also not very satisfied with my job (internal politics and company culture in general).
What can I do to get into Data Engineering?. I've heard bi engineer role is kind of a gateway into Data engineering. Is this true, or am I qualified enough to get into Data engineering? I see that distributed computing (spark,Hadoop etc) and DAG(airflow etc) tools are in high demand these days. I am still learning these online, but is it necessary to know these ? 
What salary can I expect for a data engineering role with my level of experience? (Los Angeles Area).
Any advice is appreciated.",2020-04-14 07:25:52
dwnb6c,With the advent of telepathy nearing how do data engineers support these solutions. Can a data engineering (reasonably) provide value with existing skills for current demand to support this potential future demand ?,N/A,2019-11-15 07:28:14
bz0fkh,"Wrote an Article on Apache Kafka VS AWS Kinesis, Looking for Feedback/Suggestions. Thanks!",N/A,2019-06-10 17:10:37
bivyhf,Get started with Pyspark on Mac using an IDE-PyCharm,[https://medium.com/@achilleus/get-started-with-pyspark-on-mac-using-an-ide-pycharm-b8cbad7d516f?source=friends\_link&sk=d4f25f2069d8602286f279d02026ffcc](https://medium.com/@achilleus/get-started-with-pyspark-on-mac-using-an-ide-pycharm-b8cbad7d516f?source=friends_link&sk=d4f25f2069d8602286f279d02026ffcc),2019-04-29 22:55:14
9r3z8y,Why You Can’t Do All of Your Data Engineering with SQL,N/A,2018-10-24 21:35:49
xsofgw,Why is NoSQL data so ignored?,"Why is NoSQL data so much ignored by tools and service creators?

Most of the datasets I am working with are JSON/NDJSON or other types of data that fit ideally in the NoSQL world. For example, I have millions of procurement notices, and if I decompose them to flat tables, it's about a hundred tables. 

So I put it into MongoDB or as JSON objects into Postgres. But most tools and service creators so much ignore NoSQL data processing. ETL, ELT, and other tools like dbt, Meltano, or data cleaning tools like OpenRefine do not natively support NoSQL data sources and destinations. 

Why so? Is it so complex task? Or NoSQL market is too small, and nobody needs it?",2022-10-01 06:49:01
qjzanp,Confused,"Hi,

currently working as DE at a top Analytics firm. There are two parts to my confusion, I like the job but 

1. Not sure if the Industry will help me survive after 10 years, most of the things are getting automated anyway. I agree we need to keep learning new every day.
2. I studied electronics as a major, but I joined an Analytics firm, but now I'm not sure if it was a mistake, not trying enough in the electronics(VLSI) domain.",2021-10-31 21:34:36
1aq0u4f,Job Market for Foreigners,"Hello everyone!

I'm a data scientist transitioning into data engineering. I was wondering how is the job market in 2024 for foreigners in your country.

Are there jobs that I can work home office from my country? Would I need to realocate? I'm wondering about this mostly for the US, Canada or Europe. How hard would that be? 

I'm brazilian, if that helps.",2024-02-13 18:37:12
12fo8lc,Software engineer/developer or Data Analyst? Which one is more similar.,"After 1 one of studying for data engineering, my personal opinion is that data engineering seems more similar to a software engineer/developer role than a data analyst role as I had believed before I started my course of study.  


Any thought?",2023-04-08 14:46:15
ioq48j,An overview of Flink's Table API & SQL for unified stream and batch processing,N/A,2020-09-08 08:47:16
htxsod,Anyone interested in online coding class?,"Led by college Ph.D. professor (**of Mount Saint Mary’s University; PhD from UCI; more than 10 years experiences in data science field**), suitable for 11th+12th graders, college freshmen+sophomores, and people simply interested in comsci/coding. You can receive **rec letters, app background enhancement, and appear at press release**. DM me for more information! :)",2020-07-19 09:25:41
e2cstw,Concepts of Data Preprocessing,"Data is truly considered a resource in today’s world. As per the World Economic Forum, by 2025 we will be generating about 463 exabytes of data globally per day! But is all this data fit enough to be used by machine learning algorithms? How do we decide that?

Read this article to find out :  [https://towardsdatascience.com/data-preprocessing-concepts-fa946d11c825](https://towardsdatascience.com/data-preprocessing-concepts-fa946d11c825)",2019-11-27 07:54:14
yr9m1j,ScrapeIN’ - new scraping API is looking for beta-testers!,"Hey all, 

Are there any experienced scraping API’s tech-users (the tools like ScraperAPI, ScrapingBee, ScrapingBot, Zenrows, etc.)? Or just web scraping enthusiasts? I really need your help! 

My name is Alex, I am a scraping developer with a mission to build the best Proxy API tool out there (humble is not my way.)  So here is my project - [ScrapeIN’](https://scrapein.app/)  where I am trying to combine and automate the best practices for bypassing site protection and create all-in-one scraping infrastructure for any data engineer. 

I released the first MVP version of my Proxy API and want to make sure that it works as planned, so it would be awesome if you could help me out and test it for any issues and bugs. 

So to test my ScrapeIn you need to

1. Go [here](https://dashboard.scrapein.app/)
2. Register - it will allow you to use scraper for 14 days with 1000 credits. I can extend access on request if needed, just ping me here or in dms or by email. I don’t request credit card upon registration or anything, so don’t worry about the payment that supposedly should follow the trial😅
3. Look through our [API docs](https://dashboard.scrapein.app/docs) 
4. [Use ](https://dashboard.scrapein.app/)the API key given to you for scraping any public data from the web.  
5. [Use](https://dashboard.scrapein.app/query-builder) visual CSS selectors mode in order to extract the necessary data from a site accurately. 
6. Take and submit a short questionnaire Google [form](https://forms.gle/vbEaerevcoDjFNNc6).  
7. Enjoy increased ScrapeIN’ account balance by 1000 free credits! 

I really appreciate any of your feedback and thoughts about ScrapeIN’. Don’t hesitate to share with me any of your feedback in DMs or at support@scrapein.app.",2022-11-10 08:54:41
ypsx1u,How can someone without IT background could work as Data Engineer ?,"
I'm applying to jobs that offer you a training, normally the interviews to get to know me, go great, they love me, they seems to be willing to hire me. However when I have to do this assessment and cognitive tests, I fail miserably. My score is less the average. So as you can realise I feel very bad, I don't know what my problem is... lately myself steam is on the floor and I'm considering myself very dumb. I tried practicing this tests online for free, but either they are not as complicated as the ones I get for the jobs or I  can't access to my results cause you have to pay. Anyway... I've applied to a new company where they also offer a training so to get taken in account they ask for the following requirement: 

●  Derive functions from mathematical problems

●  Pattern recognition

●  Learning ability of a new (technical) language

●  Interpretation of graphs

Since you have experience as Data Engineers I'd like to know how do you interprete this requirements, specially if you could help me with examples or links where I can experience already some of  this points. 

Thank you all for your time! 

📷",2022-11-08 17:32:24
199dfy5,How can I get into data engineering?,"Hi. I’m a mechanical engineer working currently as a mechanical designer. I do HVAC/plumbing system design and analysis. My company is a consulting firm and everyone here has architectural engineering degrees, so I feel out of place. It’s also a really small office and I don’t like it nor do I feel comfortable here. I’ve been working there for 7 months. It is my first job out of college and I’ve realized I want to do something more concentrated in data/analysis. My favorite classes as an engineering student were linear algebra, differential equations, statistics etc. Are there online certifications/programs out there that will help expand my skills and are worth the money and time so I can get a new job? I’d like to avoid going back to school because I don’t want more debt.",2024-01-18 01:08:07
sw3ds7,Python one liners for seemingly complex tasks, https://link.medium.com/lENWsa2DLnb,2022-02-19 06:25:31
183qx2q,I shared a Python Pandas course (1.5 Hrs) on YouTube,"Hello, I uploaded a Python Pandas course on YouTube. I covered the introduction and installation of pandas, series and series operations, dataframes and basic dataframe creation, creating dataframes from various file formats, dataframe operations, identifying and handling missing data, data manipulation using loc and iloc, sorting and ranking data, combining and merging dataframes, data cleaning techniques, handling categorical data, data transformation techniques, handling date and time data, group by operations, aggregating data using functions, time series data visualization, advanced data manipulation techniques (apply, map, and apply map), data visualization with pandas tools, working with multi-index dataframes and text manipulation methods topics. I am leaving the course link below, have a great day!

[https://www.youtube.com/watch?v=KvFZf3cL\_IY](https://www.youtube.com/watch?v=KvFZf3cL_IY)",2023-11-25 19:06:08
13l14gz,Build custom ELT connectors in no-code within 10 minutes,"Hi everyone!  
With the Airbyte team, we’ve been working on improving the way we could create new ELT connectors without code or local dev. The result of all this effort is what we’re releasing today: a [no-code connector builder](https://airbyte.com/blog/launching-the-no-code-connector-builder-build-custom-connectors-in-minutes). 

The best to showcase it is through a [preview video](https://www.youtube.com/watch?v=mE655VHbP-c&t=1s). It handles authentication, pagination, rate limiting, schema handling, transformation, response decoding...actually everything except which data to pull and how. 

Today, the connector builder is best suited for synchronous HTTP API connectors. We’ve published a [compatibility guide](https://docs.airbyte.com/connector-development/connector-builder-ui/connector-builder-compatibility) which can help you identify if a specific API is a fit for the connector builder. As we support more components, we expect that the vast majority of API connector needs will be supported. 

It's available on both Airbyte Open Source and Cloud. Would love to hear any thoughts about it!",2023-05-18 14:47:53
11nbk7a,ChatGPT Is Now Finding Bugs in Databases,N/A,2023-03-10 02:11:09
11cm4ax,"Is it me, or has the recession especially affected the data engineering job market?","For example, go on Indeed, and look at data engineering jobs posted recently in big tech cities like San Francisco.  There are very few data engineering jobs being posted (about 1 page of recent jobs), even a couple of months after the new year.




If a data engineer was laid off, what kind of job would they then look for?  Would they try to become software engineers, go into devops, or look for data analyst openings?  I've only worked in data, so I feel like I would have to learn an entirely new domain.",2023-02-26 16:53:11
15q740g,"Scheduled for a 1st round technical DE interview w/ Meta tomorrow w/ little experience, think I'm screwed.","I graduated just over a year ago with my B.S. in CS and have been with a consulting company since then doing some full-stack and random stuff like technical writing.

I got scheduled for a 1st round technical with Meta for a DE role after filling out a questionnaire from a hiring manager asking how much SQL and Python I had used before.

I've only started using Python within the last month since I'm started doing a little bit of Leetcoding again to prep for interviews as I'm trying to leave consulting, and I've used SQL in school and when I was on a project for a few months as a dev doing JOINS and just querying stuff when I needed to. Nothing complex.

My interview is tomorrow. After reading [this article](https://medium.com/p/345235afaac0), where the guy basically states Meta DE roles look for people with advanced skills in both Python and SQL and 4+ years experience, I'm pretty sure I'm just wasting my own and their time by doing the interview.

If they ask me anything like this: "" *Here’s an S3 bucket and here’s a data warehouse, how do you get data from one to the other and why? Streaming or batch? Unit testing? Logging? Data quality? Be able to talk about these points and answer how and why..."",* I'll have no clue how to answer.

I really want to do SE and not DE, I really just agreed to the interview so that it looks like I'm actually trying to be productive and find a new role, when what I really want to do is leave and join a non-consulting company as an SE.",2023-08-13 19:00:58
15nk88r,How to get hired to Databricks in NL,"Hi, does anyone knows the process? How much algo/fundamentals knowledge do I need? Let's say algo in terms of codeforces rating or how much time on leetcode easy/medium/hard and fundamentals in terms of questions that might be asked and areas. Thanks for all the answers. Intersted because they pay good and it's EU + NL has 30% tax ruling.",2023-08-10 18:26:08
xh4zk0,Do you have to be very good at math and Python for data engineering?,"Hello everyone,

I’m interested in Data engineering, but I struggle with math, and my Python skills are ok.

I know these skills:
-Excel
-Python: general, NumPy, Pandas, Seaborn, Matplotlib, Plotly
-SQL (PostgreSQL)
-Machine Learning: Scikit-Learn and practice with Kaggle datasets.

I really enjoy cleaning, organizing, and extracting data with Python and SQL.

Thank you in advance!",2022-09-18 02:44:40
pvnq06,What is Big data and where is it actually used ?,"So I've been listening about Big data everywhere , what is this field ? Is it related to data engg. ? And how can a beginner get started in this field ??",2021-09-26 05:38:16
1b3irll,CMS for LLM content,"We want to generate a substantial amount of LLM content for our website. We setup an OpenAI account but their user interface is quite terrible. We  want to generate some product descriptions using our custom data and prompts and are looking for a CMS tailored for this purpose. We would want features like being able to generate content on a schedule, validate content based on our validation logic and regenerate it if fails validation, automatically generate content for newly added products and regenerate content when the corresponding data changes. Some testing features would be nice too, we want to first test our prompts on 10-20 outputs and then push the prompt to production. Our product team is not familiar with databases and we want an easy solution for them. 

Is there any service that provides some or all of these features?",2024-03-01 02:17:48
1as1vt2,Code equivalent of SAS in Databricks SQL,"Hi, We have to convert a SAS code to Databricks SQL and we are finding it difficult with one of the query. Kindly provide your inputs on what will be equivalent code in Databricks SQL.

I understand that it's joining two datasets to create a new table.

DATA NEW TABLE;

SET
 
SOURCE_TABLE_1

SOURCE_TABLE_2

;

RUN;",2024-02-16 06:23:37
1am0hgt,Do you create tables for a data cube or calculate them on demand?,"In my data I have three dimensions: customer, product and year. The business has 5 customers, 10 unique products and it has been operating for 11 years. When I multiply it out: 5\*10\*11 = 550 tabels. Should I create these tables physically and persist them from a SELECT statament or should I write some kind of parametrized SQL group by statament that users will interact to calculate tables when they need them? My solution should lvie ina data warehouse like Snowflake or Redshift.",2024-02-08 17:04:16
1aezqz8,The data engineering zeitgeist: Microsoft Fabric vs Data Contracts,N/A,2024-01-30 21:22:06
1adt90e,Understanding the Clear Bounds for Data Products in the Organizational Data Mesh Journey,N/A,2024-01-29 11:34:04
1acwv3f,Spark Your Data-Driven Adventures: A Beginner’s Guide to PySpark,"Forget boring textbooks and stuffy seminars! Welcome to the data jungle, where mountains of information await your exploration. 
You, armed with the mighty PySpark, are no ordinary tourist. You’re a data explorer, a fearless decoder of secrets hidden within numbers. 
Brace yourself for twists and turns, thrilling transformations, and jaw-dropping insights as you conquer the ever-expanding horizons of big data.",2024-01-28 07:08:05
1aciszo,Singlestore Zoom call reviews,"

I’m sorry to be mean , but offlate some of the weekly zoom calls have really bad speakers . There is a guy who wears Blippi glasses and when asked questions , he says the tutorial is in progress and asks the audience to follow his LinkedIn profile . It sounds more like he is interested in getting followers rather than he knowing the subject . I doubt he knows things about langchain, llama index , etc , when asked about it , he gives a nod and lame smile . These zoom calls were excellent when Akmal ran the show . I never missed Akmal’s calls. Nowadays I get out of the call when that blippi joins . 
#singlestore #weeklycalls",2024-01-27 19:29:53
19feora,TPC-H Benchmark: Databend Cloud vs. Snowflake,"Databend achieves a 67% cost reduction in data loading and is  approximately 60% less expensive for query execution compared to  Snowflake.

[https://medium.com/@databend/tpc-h-benchmark-databend-cloud-vs-snowflake-aa97971c32a0](https://medium.com/@databend/tpc-h-benchmark-databend-cloud-vs-snowflake-aa97971c32a0)

*For this benchmark, no special tuning was applied. Both Snowflake and Databend Cloud were used with their default settings. And remember,* **don't just take our word for it - you're encouraged to run and verify these results yourself.**",2024-01-25 17:28:41
19f8qkh,Data engineering job transition...need advice,"Hi guys,

Currently I am working as an  automation test engineer  at a well reputed product company for 2.11 yes 
But I want to shift into Data Engineering.
I also cracked some interviews for automation engineer ......a company approached me with 80  per hike for Automation test engineer role
Should I take the job or upskill into a data engineer which might take more 2 to 3 months.
The new job needs me to reallocate to a different location.
So reallocating and again preparing for data engineering will take a much longer time than I estimated.",2024-01-25 12:57:46
19eiqz6,Why does Redshift's SUM() evaluate differently for VARCHAR vs NUMERIC?,"I have a column that contains a bunch of decimal numbers (USD$), but the datatype is currently VARCHAR(50).  When SUM() is run over this column, I get the incorrect sum.  If I first CAST(.. AS DECIMAL()) though, it evaluates the correct sum.

Anybody know what might be causing the difference in the two SUMs?

And yes, I'm altering the column to be a numeric datatype now!  Just curious what's happening under the hood.

Thanks!",2024-01-24 14:47:24
19a8h1q,Worth quitting full time DE job for FAANG Contract? (not Amazon),"Hi everyone, I work as a data engineer at a large entertainment company. I recently received a 12-month contract offer from a FAANG company, offering almost double my current salary. I’m wondering if it’s a smart idea to quit a fulltime position for a FAANG title on my resume? I’m more inclined towards saying no, but I also wanted to know your opinions.",2024-01-19 02:43:51
199mah5,LSTMs according to their inventor Jürgen Schmidhuber,N/A,2024-01-18 09:30:52
196ogxq,Salary progression?,"I work in insurnace in London and this is my first job out of uni. Currently earning 38k.

Salary review is in April which I will have a little over 1 year experience. What would be a realistic salary increase in my current role?

I know by looking on LinkedIn I should be on average 45-50k with 1 year experience but would like to know from you guys what I could expect. Last year my company gave a 5% increase to my team but the company performed about 3 times worse last year compared to this year.

Many thanks looking forward to reading your responses",2024-01-14 19:52:45
194plfy,Professional Certificate: Google IT Automation with Python,N/A,2024-01-12 07:46:56
18vzsfe,How to tackle inconsistency in schemas?,"Hello,   
I've been doing a fairly small project for the client, where I was assured that each country consist of the same amount of columns with the same names and business context. We've ingested the data into one dataset and now I want to enrich it but found out, that the assumption was not true. What do I mean is:

Lets pretend we have few countries though lets take into consideration - GB and US and each have 20 columns. Most of those columns have the same meaning, but there are pairs which not, like:

SB1 for US is Strength Evaluation

SB2 for US is Power Evaluation

while 

SB1 for GB is Power Evaluation

SB2 for GB is Strength Evaluation  


and its case in whole dataset that one country SB1 is Power SB2 strength, for another its reversed and so on. My silver layer looks like that

  
| ID | Market | CK  | SB1 | SB2 | SbX | ColX |
|----|--------|-----|-----|-----|-----|------|
| 1  | US     | 1US | 2   | 1   | 9   | 9    |
| 2  | US     | 2US | 2   | 2   | 9   | 9    |
| 3  | US     | 3US | 1   | 1   | 9   | 9    |
| 1  | GB     | 1GB | 3   | 5   | 9   | 9    |
| 2  | GB     | 2GB | 4   | 4   | 9   | 9    |
| 3  | GB     | 3GB | 5   | 3   | 9   | 9    |

What is expected output in that scenario I guess is (look at SB1 SB2 cols)

| ID | Market | CK  | SB1 | SB2 | SbX | ColX |
|----|--------|-----|-----|-----|-----|------|
| 1  | US     | 1US | 2   | 1   | 9   | 9    |
| 2  | US     | 2US | 2   | 2   | 9   | 9    |
| 3  | US     | 3US | 1   | 1   | 9   | 9    |
| 1  | GB     | 1GB | 5   | 3   | 9   | 9    |
| 2  | GB     | 2GB | 4   | 4   | 9   | 9    |
| 3  | GB     | 3GB | 3   | 5   | 9   | 9    |

and I have to keep it in mind that its for multiple pairs and multiple countries across. Any protips, ideas how to handle that? I guess it can't be solved on ingestion level, so raw and curated zone is not the place to make it happen, the silver dataset has to be transformed and values filled accordingly",2024-01-01 16:08:02
18vyiap,Reflecting on the Year 2023,"A small blog post on open-source, data engineering, and blogging",2024-01-01 15:05:34
18pciap,Databricks Delta Live Tables pipeline,"I have use case where I would like create a DLT pipeline from existing source Delta table. To run this pipeline incrementally, would like to read CDC from the source delta table. Documentation I found uses file system as a source for triggering DLT pipeline.

Thanks in advance!",2023-12-23 19:03:51
18m7hzm,Is this a good 6 Months Internship ?,"Hi, I was just wondering what do you think of this DE internship :   

&#x200B;

What you will do:

* Work closely with several stakeholders such as CRM, and Customer Support to understand their needs and challenges.
* Integrate relevant data sources that contain user information and campaign data
* Design ETL data pipeline for cleansing, enriching, and structuring the data for analysis
* Develop a user-friendly reporting tool for accessing and interpreting the reason behind the non-delivery of CRM campaigns
* Improve the code base to optimize the cost of our cloud computing resources
* Ensure that all the code written is high quality, scalable, and testable
* Develop, test, and maintain big-data scripts that generate the datasets used by Customer and Monetization tools
* Work closely with the whole team to achieve company objectives

**Qualifications**

This role is excellent for a person with:

* You are currently enrolled in your last year or have just graduated with a Master’s Degree in Software Engineering, Data Science, or a related technical field
* Passion for Software Engineering and/or data analysis
* Experience in SQL and Python programming
* Experience using a visualization tool (Looker Studio, Tableau, Power BI, or similar)
* Curious: Being ready to tackle complex problems with insights and analyses at the crossroads of tech, product, data, and business
* Fluent working in French & English

Skills that we value:

* Eager to learn new programming languages and Big data frameworks
* Experience with one of the following Big data tools is a plus: BigQuery, Scala, Hadoop, Spark, Elasticsearch
* Good communication skills and the capacity to adapt to non-technical interlocutors
* At ease in teamwork, familiar with agile methodology and pair-programming ",2023-12-19 17:34:59
18lnyj1,Data Engineering Camp - Boot Camp Question,"Hi - I'm completely new to data engineering and am hoping to take a boot camp through Data Engineering Camp ([https://dataengineercamp.com/](https://dataengineercamp.com/)). To get in, it looks like I need to take a short 15 min quiz for SQL and Python. I've been learning through online videos, but for anyone who took the course, can you give me advice on what I would need to know/focus on in order to pass this quiz and get in? Any additional advice would be greatly appreciated, too. Thanks!   ",2023-12-19 00:13:12
18jsem5,A soon-to-be DE asking for suggestions and resources!,"Hey everyone!

So, in about two months I'm expected to start a 6 month internship followed by fulltime contract from a Business Consulting company in EU full remote.

I do have a bachelor degree in computer science
and been working for a year as a full-stack dev junior using Python's micro framework Flask, Jinja2, MySQL, PostgreSql, gitlab, html,css,jscript,JSON,bootstrap, ect to build CRM platforms for our clients from scratch.

I've also used in the past Datacamp's Data Scientist path so have a basic understanding of libraries like pandas,numpy,matplot.

Also do have Azure AZ900 certificate if it's worth anything.

Now, the company has told me that I'm going to work with (at least):

1. Pyspark
2. Elastic search
3. Amazon AWS Athena
4. Git

Therefore I was hoping someone could/would recommend me any resource or course that brings these arguments altogether. Obviously I do understand that in a 2 month timeframe it's impossible to reach a good level but it's still worth than nothing.

Any tips,hints,suggestions,opinions will be very much accepted and definitely help me a lot!

P.S: To spare you some time, Have I ever worked with Big data or huge datasets? No, but I'm a database collector. I do have my own rack at home and a total of more than 2 TB of data. Doni know other programming languages? Yes, C and a little PHP and Java. Have I ever used Unix environments? Yes, d'Istria like Fedora, Manjaro and Ubuntu for person fun or school projects.",2023-12-16 14:31:03
18i1zoe,How much do you pay for data traffic?,"Hey Reddit,

Quick question, what is your average cloud bill for data traffic around your streaming use cases + which data format are you using?",2023-12-14 06:03:19
18dqs2z,Api injestion,"Good evening everyone, I am new to DE amd I would like to know if there's anybook or material I can read to understand how to turn complicated jsons into a dataframe",2023-12-08 16:38:13
186agw6,Focusing on results in a CV,"I was trying to revise my cv, and while looking for some advice on reddit, the tip I come across a lot is to describe your professional experience in term of results instead of tasks.

And I started to see a lot of examples where CVs look too much alike, and they all include something like: ""improved performance of the system by 800% and saved the company $2M+""

While the premise behind this approach has some merit, in the sense that you should highlight what you achieved in this role and what matters at the end are the results, for a recruiter reviewing your CV it doesn't give him any idea what are really your skills, the tasks you have been working on daily, and the kind of situations you were exposed too. Those numbers can mean anything, and definitely not the best indicator at how good you are at your job.

I know that at the end of the day businesses care about how much money you can generate for them, but this approach seems a little forced.",2023-11-28 23:34:25
1851f1e,Web scraping/database ingestion,"Hello, I am doing my personal project right now where I scrape the data and in the end I would like to have clean database with data feasible for analysis. I was wondering how shall I approach this - shall I load raw data to postgres database1 and then prepare another python script, which would clean the data from database1 and load it to postgres database2? I guess I would also need some metadeta for db efficiency and operational processes (I don't want to load offers to db2 if there is the exact same data already ingested)",2023-11-27 12:03:09
180gm5f,Is there an NBA API for free that has teams match score?,"Hi, I want to extract the match scores of my favorite basketball teams with Python and load them to a created database. 

For example, all matches played by the relevant teams starting from 2023. 

There is an API Client called ""nba\_api"". It might work for me but not sure. I will check further.

Any suggestions or ideas would be great!",2023-11-21 13:09:20
1801h2u,How to approach a Data pipeline design question in Interview ?," Hey guys, I came across this interview question for Data pipeline design. Here is the question - Design a data pipeline for a user report dashboard that shows the need-to-know top Alexa user requests by different countries every hour. I want to understand  


1. What clarifying questions to ask before approaching to solve the question ?
2. how to approach this question ?

Any guidelines or reference article would be appreciated. ",2023-11-20 22:45:59
17xtb5i,Created Hive tables STORED BY ICEBER; SELECT * returns empty,"Hi reddit,I'm trying to implement a lakehous with hive 4 and iceberg and i think i'm almost there but...When I create a table like:

    CREATE TABLE ice (id INT, name STRING) STORED BY ICEBERG;

And then insert some data

    INSERT INTO ice VALUES (1, 'one');

And then select

    SELECT * FROM ice;

I don't see the inserted record.  


EDIT: I do see the parquet files on storage, and see the folder and file of the metadata, as if the table was created as location\_based\_table, But documentation says that if no specific icber.catalog is set, it will use the defaul hive catalog.

What am I missing?I suspect that is something related to the metastore, but can't find what...Thanks in advance, cheers.",2023-11-18 00:11:33
17wmkgs,What people are missing about timeGPT,N/A,2023-11-16 13:13:42
17vv8sl,Is it really hard to contribute to Apache Spark [codebase]?,"I've seen discussion and videos were they say it's really hard to contribute to the framework just because they accept only from existing commiters. I want to contribute in order to learn, advance my career and give back to the community and I don't want to dig in learning the internal codebase and later have no chance.",2023-11-15 14:49:57
17rzs4x,Mock interviews for data architectures and data pipeline design rounds (mid level),Hi I haven’t seen much on this in terms of structure or mock interviews so wanted to ask if anyone is up for few data pipeline/architecture mock design interviews over the next week?,2023-11-10 08:39:01
17qfis6,"Azure data engineer or GCP data engineer , which one to choose?","Hi all, i am working in an MNC and working as GCP data engineer,i am in dilemma to learn Gcp data engineering or azure data engineering...dont take me wrong, in service based companies even if u have some different skill they ll take u into that project and asks us to learn, that is the reason for my question...i am a bit interested in azure data engineering and lots of resources to learn(lots of jobs) and I have learned a bit about it.But here as I am working on various data services in GCP, definitely I need to learn GCP more...",2023-11-08 06:19:50
17mvy3s,"Between these two College courses, which one is better for Data Engineering?","Good morning, everybody. Looking to get into Data Engineering and graduating soon. I have one elective left next semester and want it to be useful (at all) towards Data Engineering.

I think option 2 is the obvious choice but I don't currently work as a Data Engineer, so insight from people in the field would be great! Thanks in advance.

Option 1: **Data Mining**

**Course Description**:   This course will provide an overview of topics such as introduction to data mining and knowledge discovery; data mining with structured and unstructured data; foundations of pattern clustering; clustering paradigms; clustering for data mining; data mining using neural networks and genetic algorithms; fast discovery of association rules; applications of data mining to pattern classification; and feature selection. The goal of this course is to introduce students to current machine learning and related data mining methods. It is intended to provide enough background to allow students to apply machine learning and data mining techniques to learning problems in a variety of application areas. 

&#x200B;

Option 2: **Scalable Databases**

**Course Description**:  After reviewing relational databases and SQL, students will learn the fundamentals of alternative data storage schemas to deal with large amounts of data (structures and unstructured). The course covers big data and the development of the Hadoop file system, the MapReduce programming paradigm, and database management systems such as Cassandra, HBase and Neo4j. Students will learn about NoSQL, distributed databases, and graph databases. The course emphasizes the differences between traditional database management systems and alternatives with respect to accessibility, cost, transaction speed, and structure. Part of the course is dedicated to access, handle and process data from different sources and of different types using Python. The course provides hands-on practice. ",2023-11-03 14:09:12
176js4y,Data transformation startup Prophecy.io lands $35M investment,N/A,2023-10-12 22:42:19
174q5m8,Stream Processing: Is SQL Good Enough?,N/A,2023-10-10 16:58:55
16yv1yl,Advice on Azure Data Engineering Career,"I'm planning to switch my career to Azure data engineering. In process I did a course on ADF and did hands-on in dataflows, pipelines etc. As a next step should I learn Azure Synapse analytics, Pyspark or Azure data bricks and Pyspark? I'm bit clueless as to choose which one? I have 13years experience in Oracle data integrator, etl, data warehousing and sql. Please guide.",2023-10-03 15:33:42
16xwqjc,"What is the go to tool/SaS product for allowing non technical internal users to explore our DBT models, select fields from various models to export data to Excel",We would like to have a self serve approach to data consumption,2023-10-02 13:31:41
16jaacv,"I still find 8th Kyu on codewars difficult, but very easy on edabit is ok..."," Hi everyone,

So I am practicing Python but for a month or two and I still find solving 8th (the easiest) on codewars quite challenging to solve.

I was doing fairly ok with ""very easy"" on edabit but I have hit a paywall there.

Are there any alternatives for very easy challenges like the ones on edabit but that are free.

Do you have any tips to get to solving 8th kyu on codewars? It feels like I am never progressing :(. Feeling hopeless tbh.",2023-09-15 11:08:54
16idhds,dbt + looker best practices,"Hello!

My team is thinking about using looker for our BI part. We also want to define metrics in looker.

What are some good tips or ressources that in your experience will save us time in the future?

For example, I'm thinking if we should create another repo or integrate looker into our dbt repo. 

The 2 are closely linked since I might need to create a field upstream in dbt to use it in looker but I'm afraid if we split them, some fields will end up getting created in looker since opening 2 PR can be a PITA.",2023-09-14 09:08:50
16h1vuy,What to Expect in the SQL Screen,"When gearing up for interview preparations, there are some recurring SQL patterns that you should pay special attention to. The “sum of a case statement” is a classic example, often used to categorize data into segments and then aggregate results accordingly. The “join with between” is another pivotal pattern, necessitating the merging of tables based on value ranges rather than exact matches.

&#x200B;

[https://medium.com/p/6c0eec62ade6](https://medium.com/p/6c0eec62ade6)",2023-09-12 20:19:41
16bmnos,"A fast, shareable, and AI-powered docs catalog for dbt Core","Hey data friends 👋

I'm super excited to to share a preview of some new stuff I'm working on: ✨ Turntable Discover ✨

Data teams struggle to keep their documentation up to date and actionable. Today they have to stitch together Notion docs, lineage tools, yaml / markdown files and maintain jobs to generate it.

We’ve built a seamless way to ingest, discover, and share your warehouse's documentation all in one place. Discover is integrated with Github and dbt core, so you can get setup with a magical docs experience in only a few minutes including:

⚡️ Super fast search - search across table names, column, descriptions and canonical sql to find the data you're looking for

✨️ AI-powered semantic search - can't find a substring query that matches? Use semantic search to find data models that are tricky to find (ex: ""reps"" -> ""sales people"")

🔬Column-level lineage view - trace back the origins of a particular column and navigate across models with an inline column level lineage view

🔗 One-click sharing - share documentation in one click with other teammates on the same OAuth domain

We’re giving Discover to select teams in a private beta before rolling it out broadly. If you’d like to try it out, you can DM, comment below, or sign up on our waitlist ([turntable.so](http://turntable.so/)) Looking forward to hearing your feedback 🙌

Check out a quick demo of how it works here:

[https://www.youtube.com/watch?v=sY0NefWRpKQ](https://www.youtube.com/watch?v=sY0NefWRpKQ)",2023-09-06 15:05:48
16av7fj,Data Solution Architect Insight Roundtable,"&#x200B;

https://preview.redd.it/rmmb01sf7hmb1.jpg?width=1080&format=pjpg&auto=webp&s=73490ed2298b99d340b6e228767238e064fb3fea

Exclusive Invitation: Data Solution Architect Insight Roundtable

We're excited to invite you to EPAM Poland's upcoming roundtable session dedicated to the world of Data Solution Architects and their impact on large-scale data-driven solutions.

At this event, you'll have the unique opportunity to learn and engage with our esteemed panel of top data professionals:

🔹 Uladzimir Kazakevich, technologist with over 20 years of professional experience, Head of Data & Analytics Practice CEE

🔹 Peter Kortvelyesi, IT consultant, data architect & delivery lead to Fortune 500 clients

🔹 Michal Bienkowski, solution architect in the Data/Cloud space, expert with 16+ years in the field

When: 07.09. (Thursday)  I  Time: 17:00 CEST  I  Location: [http://datazen.top/0gaps](http://datazen.top/0gaps)

Your questions and perspectives will drive the second part of the conversation. At the end, you'll also have a chance to participate in the quiz and win e-gift cards.

Save your spot now by registering here: [http://datazen.top/0gaps](http://datazen.top/0gaps)",2023-09-05 17:59:00
168xzmf,Qlik Replicate + Informatica // Qlik Replicate + IBM Datastage: What is it used for?,"Hello everyone,

As part of a study into the use of integration solutions, I've noticed that some users seem to be combining different tools, in particular Qlik Replicate + Qlik compose + Informatica (powercenter/ICS) or even Qlik Replicate + Qlik compose + IBM DataStage. 

Is this the case for you or have you already seen this type of usage? If so, what does it involve?

Thanks for your help!

WinterCod!

&#x200B;",2023-09-03 14:08:42
168c06m,Top universities for Masters in Data Engineering,"Hey there, I'm looking to solidify my expertise in data engineering. I have a bachelor's degree with a fairly okayish GPA, got a pretty good offer to work as a data engineer, worked for a couple of years and now I'm looking to do my masters abroad. If y'all could help me on what options I can look at, it would really help!",2023-09-02 19:53:15
165aflp,How can I kickstart my career in Data Engineering with only internship experience?,"I was introduced to data engineering during my final year internship. However, I ended up getting a full-time job in full-stack development. Do you have any suggestions on how I can transition to a career in data engineering?",2023-08-30 10:00:43
15ynly9,Creating and understanding the customer journey over time,"I am very new to data science so i was hoping explain how a business with separate databases for CRM, Social media platforms, SEO, VOIP call data, , 1 containing crm data, 1 containing SEO data, one from voip call data, 

as an example scenario, in an effort to answer the question: what percentage of our client base came from which lead source. then, of the social media aquired clients, how many came from facebook? what was the average length of time that client was a customer for? and what are the main reason for cancelation by percentage.

Is this a monumental task? I'm not even sure of the order of magnitude of this project? is this a 50K project, 500K, 5 Million? ",2023-08-23 00:18:56
15v03op,Any Meta Engineers?,"Do you or someone you know have war stories of data engineering at Meta? 💪🏻 I’m really interested in their data engineering jobs compared to other big tech, some of which don’t even have data engineering job titles.

Any advice for an aspiring engineer? I’ve been in the role for several years now, a data wonk for 15 (was analyst / data scientist / statistician previously). Kind of stuck in a Microsoft stack, managing Hadoop cluster, data, and Spark jobs, and pushing the team toward cloud adoption. But we’re never going to move at the speed I’m able to work at, or use all the tech I’m able to pick up. So any advice is welcome 🙏",2023-08-18 23:37:50
15u2o2t,API + Python + Snowflake - Noob question,"Hey everyone! I have been in the data engineer parts for only a few months now but I think I have progressed quite far. I revamped the company data ecosystem into mainly a combo of Fivetran + Snowflake + SQL + Power BI. And things are just fine with this, however, I now need to find a way to extract data from a data source that is not avaiable in Fivetran. So I heard Python could do a simple API request to fetch the data and send it to Snowflake.

&#x200B;

But as a noob I have no idea where to start with Python. I have never used and I am not sure if it would be better to use a cloud tool? or the Python terminal itself? I do want a automatic solution and not rely on my computer to always update the data but I am not sure the best way to go. 

&#x200B;

I have also seen that Snowflake has a Python worksheet but I am also not sure if it would be the best and how to begin using it.

&#x200B;

Unfortunately chat gpt has been not really useful at least the times that I have tried so here I am.",2023-08-17 22:57:34
15sxrmr,Best database versioning/migration tool for clickhouse?,"I am looking for a suitable version and migration tool with CLI to use within GitHub action that is compatible with ClickHouse but haven't been successful so far in finding an easy to implement one. 

There is [this](https://clickhouse.com/docs/knowledgebase/schema_migration_tools) list provided by ClickHouse but the options are not that much. What might be the best tool based on my requirements?",2023-08-16 18:25:07
15s4747,Interested in Data Migration?,"Hey fellow Data Engineers,

Are you passionate about diving into the intricate world of data migration? Whether you're a seasoned pro or just starting out in the field, if you're fascinated by the complexities and challenges of moving data from one system to another, I invite you to join us at r/DataMigrations!

In the DataMigrations subreddit, we're all about exploring the nuances of data migration processes. Share your experiences, ask questions, and discuss best practices with a community of like-minded individuals who truly understand the ins and outs of this critical aspect of data migration.

Whether you're dealing with ETL pipelines, schema transformations, or cross-platform migration strategies, this is the place to connect with others who share your passion. Let's come together to learn, collaborate, and tackle the challenges that come with ensuring seamless and efficient data transitions.

Join us at r/DataMigrations and let's embark on this data migration journey together. See you there!",2023-08-15 20:33:21
15p8hx7,Ballpark Cost Estimates for Data Engineering of an ML System,"Hey r/dataengineering,

I am finishing up an internship at a company where I built a very basic data warehouse with data pipelines to 3 wearable biosensors.  It's a small prototype for a larger initiative the company is envisioning.  

What they are envisioning is an ML system that gets biometric data from users via wearable devices and predicts whether a certain health event is going to occur for a disease they are targeting within 7 days on a daily basis.  There are academic papers that have accomplished this and some companies are doing similar things, so they are confident it can be done.  They are also envisioning allowing users to visualize their data to assist with decision making related to their health.

They liked what I produced, and now are trying to come up with ballpark estimates for how much this project might cost years from now when it is scaled up to tens of thousands of users in production.  I tried to make some estimates based on what I built, but threw up my hands as I think it is impossible to make anything close to accurate predictions based on my prototype.  I have decided that a better method might be to ask this subreddit for thoughts on costs for that type of project, whether it could run between the tens of thousands, hundreds of thousands or millions per year (or more).  Basically get peoples' sense on what best I can deliver in terms of thoughts on costs for this management that is considering this initiative in the long run.

I am the only data person at the company, so it is a bit of a lonely place to make a projection.  I am sure they would build out a team if they decided this is worth pursuing, but they still want ballpark estimates in the meantime.

Appreciate anyone's thoughts in the comments, and if there are any experts who might be willing to connect 1 on 1 for a more pinpoint answer that could be hugely appreciated as well.  Thanks!",2023-08-12 16:10:37
15fs76t,Ambarish Donga DatabricKS Solution Architedct interview Tips,N/A,2023-08-01 23:21:45
15fbsog,Hiring Managers: Which Education Path Would You Prefer?,"All else being equal, which education path is better to you in a candidate (or does it not matter?)?

Business degree -> MBA -> Bachelor’s Comp Sci

Or

Business degree -> MBA -> Master’s Comp Sci

Thanks!",2023-08-01 12:54:11
15bzh0d,How to get free azure subscription(I do t have student id and by 1 month free trial credits are over),"Hello folks, 

Im preparing for azure dp -203 certification and I really want to practice services in azure. Due to some work issue I was not able to make use of $200 credits free trial, now it is over and asking for pay as u go subscription and here in this subscription it is charging amount for some of the resources, I hardly practiced anything. I need to practice different resources required for data engineering and do some projects which would help to get a job. I don't even have student id. May I please please know is there any other way we can access azure resources for free. 

I have already wasted most of my time is searching how can I acess free subscription to practice.
It would be really helpful if I get some tips here. 

Thank you.",2023-07-28 14:45:38
15ahrnj,"My dear data engineering friends, which data lake you are currently using or do you intend to use?","Why do you need a data lake, instead of just S3?

Which data lake do you use? Delta Lake, Apache Iceberg, or Apache Hudi?

Any comment is welcome!!!",2023-07-26 21:05:10
158ao7m,Is Data Engineering a Complex Career? Challenges Faced by Data Engineers,N/A,2023-07-24 13:26:44
1560kjy,2023 Women's World Cup Predictive Analysis,"In honor of the Women's World Cup that started yesterday, I decided to push myself out of my comfort zone and create my first analysis using a machine learning model where I use CatBoost to predict the outcome of the Women's World Cup. Major thank you to Gustavo Santos who's [initial work](https://github.com/gurezende/World_Cup_2022/blob/main/Results/FIFA_WorldCup2022.png) for the men's world cup last year allowed me to streamline my own analysis. To make things fun, I decided to play against the model in an installment I like to call [Monica vs the Machine](https://dev.to/monimiller/monica-vs-the-machine-womens-2023-world-cup-analysis-kf8).  View my [Github repo here](https://github.com/monimiller/womens_wc_23). Let me know what you all think!  
",2023-07-21 21:19:53
155rtng,Do you use Databricks SQL with DBT?,"I know DBT + Snowflake is a very popular combination.  I am curious if people use DBT with Databricks SQL?

[View Poll](https://www.reddit.com/poll/155rtng)",2023-07-21 15:50:49
14pho9o,Legacy system assessment model: Retire or not to retire?,"My team and I have been re-building our company's data architecture. In the process of doing so, I got together six key principles to transforming data architectures and thought I would share them, as a strong data architecture is crucial for businesses looking to stay competitive in the digital landscape, as it improves decision-making, time to market, and data security. When executed with efficiency, a resilient data architecture unleashes unparalleled degrees of agility. 

Balancing contemporary software with outmoded systems that bear a long list of constraints and issues presents a complex predicament for CTOs and CIOs. In this composition, I delve into these questions:

* Establishing if the system should be labeled as legacy;
* Merits and demerits of retaining legacy software;
* Evaluating disparate types of systems’ traits;
* Proposing a blueprint for pinpointing the correct answers to legacy systems.

### Recognizing a Legacy System

To ascertain if stakeholders regard the system as legacy or otherwise, you can question yourself using the prompts below. A positive or negative response to the system will suffice. Bear in mind, some of these are subjective and you might prefer to answer Maybe or Unsure:

* Does the system play a vital role in business operations?
* Is the system dated? Was the latest update significantly in the past?
* Has the system been modified to fit organizational objectives?
* Is the system deteriorating as modifications are implemented?
* Do maintenance costs escalate as modifications are implemented?
* Is the system based on antiquated languages?
* Is the system’s documentation scant or nonexistent?
* Is the system’s data management subpar?
* Is the system’s support capacity restricted?
* Does the system lack the architectural design to adapt to upcoming requirements?

If more than half of your responses are positive, you’re dealing with a legacy system. So, what comes next?

### Assessing the Pros and Cons of Legacy Systems

Before you choose to modernize, transition, or entirely discard a certain legacy software, you should comprehend its pros and cons.

The advantages include:

* High costs of modernization compared to maintaining the legacy system;
* Reliability and familiarity associated with legacy technology;
* Traditional applications are generally fully merged with corporate operations and continue to function efficiently, proving their utility over time;
* Modernizing software could halt business operations, while legacy systems guarantee smooth, uninterrupted procedures.

The disadvantages are:

* Talent challenges – scouting for new professionals to sustain and support obsolete technology is increasingly like chasing a mirage;
* Modern business processes often require automation, which legacy systems don’t accommodate;
* Incompatible and outdated procedures;
* Legacy systems trigger compatibility issues;
* Legacy systems may not receive security updates, leaving them exposed to external breaches that hinder the system’s operation.

Depending on their priorities, CTOs from different companies might weigh the cons more than the pros and opt to retain the old tech or vice versa.

### Legacy System Evaluation Model

Several evaluation models exist for software, each spotlighting different features. The Open University's research group suggests a model derived from pre-existing ones that merges business and technical aspects with modern architectural features and organizational considerations. This creates a broader, unified method that acknowledges the real-world complexities of legacy systems.

The model's outcome can then be depicted on a decisional matrix that indicates a proposed solution. (See Image1 I attached).

The attributes you need to evaluate, and their corresponding values to locate your system on the matrix, are as follows:

For every attribute, decide if the response is positive or negative in the legacy system's assessment. Then, assign a value from very low to very high (from 1 to 5; don’t know is 0).

Business Attributes:

* Financial worth: Market worth, Profitability ratio, IRR;
* Data worth: Proportion of mission-critical archives, Proportion of application-dependent archives;
* Usefulness: Range of business function coverage, Current usage frequency, Metrics for customer/user satisfaction;
* Specialization: Proportion of highly specialized functions, Proportion of generic functions.

Technical Attributes:

* Maintainability: Amount of code lines, Function points, Control flow, Knots, Cyclomatic complexity, Rate of dead code;
* Decomposability/architecture: Architectural modularity, Proportion of modules with distinct concerns, Expandability, Interoperability, Architectural style, Consumption;
* Degradation: Backlog growth, Defect rate increase, Response-time increase, Maintenance time per request increase;
* Obsolescence: System age, Operating system version, Hardware version, Technical support availability, Security, Legality, System evolution required for business goals.

Organizational attributes:

* Internal development & maintenance;
* Outsourced development & maintenance, 
* Technical maturity, 
* Commitment to training, 
* Skill level of system support, Response to change

To convert the business attributes and technical attributes into final values, the authors use the following equations: (See Image2).

The combined values can then be plotted on the decision matrix from above that indicates a recommended solution. 

For more articles like this, please visit my blog: [https://ainsys.com/blog/2023/02/08/legacy-assessment/?utm\_source=linkedin&utm\_medium=social&utm\_campaign=data\_engineering&utm\_content=legacy\_systems&utm\_term=ITarchitecture](https://ainsys.com/blog/2023/02/08/legacy-assessment/?utm_source=linkedin&utm_medium=social&utm_campaign=data_engineering&utm_content=legacy_systems&utm_term=ITarchitecture) ",2023-07-03 12:29:36
14nezz0,(P.S )these are for a bachelors degree,"Guys  which do you advice me to do . (Computer science major + artificial intelligence specialization) or (Data engineering major + artificial intelligence specialization) , plus who makes more a software engineer or data engineer?",2023-06-30 23:04:12
14mudip,2.5k parque files.. merge them or create data warehouse for AWS project?,"Hi, i have 2,500 parquet files of historical data of the stock market. Around 50 GB. I’m working on my thesis project which is to create a deep learning model for forecasting the stock prices by taking in account multiple variables. Also, I am planning to create a site where you can filter the data, set date ranges, etc.
MY QUESTION IS: should i merge all the files into a big single file? Or should I create a data warehouse where i can access this data? .. any other suggestion? I’m also planning to implement AWS during this project, so i would like to know the best approach. Thanks!",2023-06-30 08:16:28
14mg6hm,Experts wanted: exploring the gap between AI hype and real-world decision making,"Hello everyone, I'm writing my master's thesis and I would like to conduct 1:1 interviews with data scientists, BI practitioners, and executives to understand their perspective on the topic.

&#x200B;

In the business context, we often hear statements like ""AI enhances decision-making,"" ""AI can tell executives what to do"" and ""data-driven decision-making is easy"". However, I saw a considerable amount of misinformation, snake oil solutions, references to Gartner, and the same hypothetical use cases over and over.

&#x200B;

Essentially, I am conducting a reality check on the capabilities and limitations of AI technologies (ML, expert systems, fuzzy logic, etc.) in assisting analytical decision-making within a business context. I aim to determine when it is reasonable to adopt these solutions. If AI proves to be unattainable for most companies, I want to provide managers with practical advices on what they should focus instead (e.g. data culture, centralization and availability of data, real problem identification).

Lastly, I am also interested in the role of human intution in decision making, because it plays a role in ""managing the things you can't measure"" or those situations that are not practical to model.

&#x200B;

If you are a data scientist, BI practitioner, or executive, I would greatly appreciate your insights and expertise in this field. 

Please send me a DM to schedule a 30-minute interview at your convenience. Your participation will have a significant impact on my research, and your insights will help shape practical recommendations for businesses navigating the AI landscape. Together, we can uncover the truth behind AI's potential and empower decision-makers with accurate information.

&#x200B;

Thank you in advance for your time and contributions.

&#x200B;

Note: All information provided will be anonymized",2023-06-29 20:52:51
14hsq4x,Career opportunities,"Hey , does anyone know of any roadmap to become an SME in Databricks.",2023-06-24 13:10:50
13sqyxw,Querying Data on Apache Hudi with StarRocks - Apache Hudi [MAY 2023] Community Call,N/A,2023-05-26 22:51:13
13k56q9,Benthos in airflow,Can someone please explain how to run benthos in Airflow? I can't find any guides online,2023-05-17 15:35:24
13j3x50,Big data engineers can leverage ChatGPT to their advantage without fearing for their job security.,N/A,2023-05-16 12:32:58
13j3k1p,Databricks for Data Product Project," Hello guys,

I'm currently working on an project that involves creating a data product—a web app that will provide analysis capabilities to our customers. I have a few questions and would greatly appreciate your insights.

1. Processing and Storage: The plan is to use Databricks for processing the data, with the data being stored on a data lakehouse using the Medallion Architecture. However, considering the project's requirements and the fact that the data volume is not substantial at this stage, I wonder if using Airflow to orchestrate tasks and process the data might be a better fit. In this case, the data would be stored in a SQL Server data warehouse. What are your thoughts on this? Should I stick with Databricks or explore Airflow as an alternative with a SQL Server data warehouse?
2. Data Load: If we decide to proceed with Databricks, I'm specifically curious about the best way to perform a load from data available on a data lakehouse to an Azure SQL Database. Our backend will rely on this database for serving the app's functionality. Are there any recommended approaches or best practices for achieving this integration efficiently using Databricks notebooks?

Thanks in advance for your help!

TL;DR: Should I use Databricks or Airflow for a data product project involving data processing and storage in a data lakehouse? If Databricks is the way to go, what are the best practices for loading data from a data lakehouse to an Azure SQL Database using Databricks notebooks? Alternatively, would Airflow with a SQL Server data warehouse be a better option?

Looking forward to your insights!",2023-05-16 12:17:05
12sp2yq,How to model the dataset for a IR image stream,"I have a IR image stream, each frame has a 640x480 resolution and uses 16bit to store the actual raw data in each pixel. I have never worked with images nor IR images. What I want to do is to think about how to save this stream of images into a file to save it on the Cloud.

Are there best practices?

Should I think about storing the images onto a structured dataframe file like parquet? E.g. the first columns the flattened encoded raw data, and the other columns the metadata of the image?",2023-04-20 06:46:58
12njryb,Front-end for OLTP database options?,"I'm currently looking for a on-prem application solutions for end user's to input data into a OLTP database. Something like what you could do with MS Access where you could create a front end. The database could be Azure SQL DB or just SQL server. Would I need to learn how to be a software developer or are there other ""low code"" solutions out there?",2023-04-15 21:42:39
12imwm1,Is Airflow the best solution for scheduling pyspark jobs?,I have seen a lot of mixed views on airflow and was wondering if there is a better alternative. If you do not use airflow let me know what you use for scheduling and metadata storage.,2023-04-11 15:12:09
12i8l01,How to query a Excel book?,"Hi, I'm thinking if there Is a way to do a  query on a Excel book without use tools like mssql server or acces?

Now I'm new in a company and all Tha information that we have Is in Excel sheets. I want to do some querys to extract valuable information meanwhile i ask if i can install mssql server",2023-04-11 04:40:50
12ckhom,"A list of tips, footguns, design patterns for running CDC in production",N/A,2023-04-05 13:31:43
12ciisx,3 Things Our Software Engineers Love About Data Contracts,N/A,2023-04-05 12:20:35
12bhr14,Beteer skills update route :,"Existing Analytics experience (Cognos , tableau, SQL etc ) +

[View Poll](https://www.reddit.com/poll/12bhr14)",2023-04-04 12:33:51
127qpbw,Databricks documentation,"So far been fairly underwhelmed by the quality of documentation on data bricks - which I started using a month back. They have very good SEO - so anything you search for you get data bricks links only. 

And I find the stuff too basic. Like today I was searching for “machine learning models on large datasets using databricks “ and I’m led to an article (on the databricks site) showing a model built on 50000 data points. 

So how do you learn databricks, especially if you’re not a data n00b?",2023-03-31 16:23:36
125u8ur,Using Airflow and Prefect,Is anyone using both Airflow AND Prefect? What are your Prefect use cases? I am thinking of using Airflow for batch jobs and Prefect for the real-time ones. Would that make sense?,2023-03-29 17:14:49
12523tl,Why does DBT model have such a weird format for configs in models?,This config block cannot be parsed by IDEs I use (Datagrip/PyCharm). Why doesn't dbt simply uses a json of same name to do the job? Why invent this format? This really bugs me as I have to write a translation script to generate some dbt model files and json files are obviously a lot easier to dump on disk.,2023-03-28 20:58:40
123w6io,How to place myself in DE career ladder?,"Hi guys… first time here.
I don’t know if we had this kind of discussion before, but I would like some guidance here

I have a long experience with Analytics and Data transformation… but never had a DE job.
My last job was as Analytics Engineer and my scope was the same as DE. Now I am a digital transformation coordinator and totally frustrated. 
They sold me an opportunity of create a data lake from zero, create and architecture and train data analysts… I ended creating power points and using excel to create data flows. 

But since I have zero experience as data engineer, it is quite complicated to land a job. I know how make API requests, create an ingestion flow… but I am no software engineer. 
I don’t have knowledge in Kubernetes, Helm, NOSQL,and DevOps. I only know Python, Spark, SQL, AWS S3, RDS, EMR… and a little bit of Flink and Kafka. 
My degree was in Materials Engineering but I shifted my area to BI/DA 6 years ago. Dataviz, Modeling, Analytics.. I can handle easily.

I am no junior… but I believe I am no senior. My job is frustrating and these layoffs worsen my mental health, in terms of find a new opportunity.

What should I do to go back to what I really love, that is Data Engineering? Should I focus on Junior roles or try Mid level? Senior is out of question.

Thanks in advance, I feel lighter talking about it here.",2023-03-27 18:42:51
11nahid,Just Got Access to RedShift Data in Tableau and using DBeaver - Next Steps,"Hey everybody just got access to our new redshift data warehouse using Tableau and other tools. I am the only one with access and want to know what my next steps would be. 

For more context the ware house is synced to our financial niche CRM daily and grabs data from there. We are having a hard time building a proper flow. We think that the structure of our niche CRM was copy pasted due to the way the connections are set up when I look at a diagram of the tables. Need help knowing best way to reorganize their mess visually so I can start to understand their keys and connections. 

Any advice is great.",2023-03-10 01:25:41
11kiog0,"While this may be a better post for dbt slack, I have trouble getting traction there at times.","I want to grow my skills in ci/cd & right now our dbt cli / dbt core operation is relatively basic. We have a docker image to support local dev thru production, use circleci but have little testing. 

Anyone have any really optimized method for cicd with dbt / docker (full stack includes gcp/snowflake / terraform / GitHub)

Maybe some low hanging fruit so I could get a quick win. I was thinking doing https://www.vantage-ai.com/blog/how-to-use-slim-ci-with-dbt-core but need some more hand holding on setting up. 

Any good articles or packages you’d recommend?

Also any ideas on data infrastructure would be taken. We use terraform for role granting in snowflake and infra  but open to ideas on how to improve so I can be less of just a DE and more of an infra/ops skillet also",2023-03-06 23:58:12
11jz116,Airflow bigquery operators vs traditional bigquery client methods,How much faster is to query and insert data with bigquery operator in airflow vs Python code which uses google cloud bigquery client methods to create and query the table in airflow?,2023-03-06 13:31:11
11hc2iu,Repository management,"How do you structure your ELT jobs repositories? I am thinking of having one repository for extraction/ingestion python jobs, one for transformation, one for aggregations, one for validations/quality but my concern is each repository could be pretty large. On the other hand it still should be better to manage than keeing all the jobs in its own repository. What are your recommendations?",2023-03-03 19:17:40
11g6zaf,Architecture Review: Dagster vs. Airflow,N/A,2023-03-02 16:27:42
11emsf6,Feedback on our migration to a data lake?,"We're moving away from Snowflake. We'd like to use a data lake paradigm and will be using Azure. The Snowflake instance was not in Azure before.

I have several questions and I'd appreciate any help on any of them.

1. Any data loaded into the lake ought to conform to a directory structure from the very beginning. MS recommends something like, `{Region}/{SubjectMatter(s)}/{yyyy}/{mm}/{dd}/{hh}/` for IoT device data, which is similar to our use case. Right?

2. Any data loaded should be in a columnar format like parquet or avro for its meta-data properties and it's columnar/compression properties. Right?

3. What options do we have for getting historical data currently in SF into this format and moving it over efficiently?

4. Our source data is in a MongoDB instance. A Python workflow reads from it, does some basic parsing/formatting, writes a csv to S3, which then triggers Snowpipe for load. A rewrite of this should allow the same process but would write parquet to the data lake instead. Is there a better option? 

5. Recommendations for the query engine on top of the data lake? Seems like Synapse is the default. But We've never used it. 

6. Should we use Synapse, do you ""load"" all data lake data into it and then model to your marts? Or do you need to use some other tool to create bronze, silver, gold in the data lake and load those? Im not sure if this question even makes sense since idk how Synapse reads from data lake but my point is that this step is murky. 

Again, thanks for any help.",2023-02-28 22:57:55
11doz9v,Chaos Data Engineering Manifesto,"Is chaos engineering applicable to data pipelines? Yeah, nay, meh? 

[https://towardsdatascience.com/the-chaos-data-engineering-manifesto-5dc09a182e85?source=friends\_link&sk=05d2a17c0e1a736853cffd5f4a4d9482](https://towardsdatascience.com/the-chaos-data-engineering-manifesto-5dc09a182e85?source=friends_link&sk=05d2a17c0e1a736853cffd5f4a4d9482)",2023-02-27 22:31:06
11ajfjj,Data Engineering Level,"Can someone help in breaking down who/what the following represent: Data Engineer I, Data Engineer II, Data Engineer III. Also where does a Senior Data Engineers fits in in these L1, L2 and L3 description.",2023-02-24 05:23:31
119zwfp,What is a Data Lake Table Format? (Podcast),N/A,2023-02-23 15:07:31
119zts2,What's your work life balance as a DE?,"Since I guess we all like collecting data, let's share how you rate your WLB. Please also share if your additional work is mainly due to being on call (sometimes) or not.

[View Poll](https://www.reddit.com/poll/119zts2)",2023-02-23 15:04:22
116iqu0,Datalake - partitions vs folder structure,"I am not sure I understand datalake folder structuring correctly:

If I get everyday new file for lets say sales for some region I would save this file like this:

sales/region/yyyy/mm/dd/file

or

sales/file ?


Because if I understand it correctly - e.g. in Trino I pass the folder with the data (parquet files) and Trino itself loads the files. But if I would have each file in separate folder it would be a problem? I would have to create table for each file manually? Also if I go with “sales/region” I can create table with partitions by date and Trino automatically would create these “yyyy/mm/dd” folders? Sorry not sure I understand it correctly.",2023-02-19 17:33:06
10qx91l,Appreciate any advice on my resume :),"Appreciate any help or advice on my resume. I've been working at a consulting firm as data engineer from over 1.5 years and currently preparing for transitioning into a product company.

Thanks in advance ;) 

Have a few questions:

* Is the theme clean enough for recruiters?
* Did I include lots of information on my Experience? 
* Is it okay to mention project-wise experience under my current job?
* Should I include personal projects section as well as an experienced person? (note: My website & Github have a detailed portfolio)",2023-02-01 15:19:43
10qsp3w,[QUESTION]: Spark SQL to write data to storage,"Hi.

I am trying to arrange all my data pipeline with Spark SQL, so Scala/Python knowledge is required.

Now, suppose I have my final dataset, I want to write it to a new place (some new folder, no files yet), how to do that using SQL?",2023-02-01 12:05:15
10edrdl,10 evaluation criteria for Data Orchestrators,N/A,2023-01-17 14:35:47
109x86s,Is it really a Data Quality Issue?,"I am new to Data Quality. Recently I encountered a problem where we are getting a data feed from a source system which is ambiguous. The feed received gives status as Draft. However,

The front end sometimes shows status as open and sometimes it is draft,meaning when i go and open the web portal the status reflects as open or sometimes as draft.

I am very much unclear as to what the actual status should be? We are talking about the issue in the system which is the golden source.

Can someone explain what this issue is? And is this a DQ issue or something else??",2023-01-12 11:15:24
1062vj7,Looking for some database solution selection advice...,"Apologies in advance for the level of detail provided, but this is the best I can do in a public forum.

**Background**:  We are a small, lean startup that is in the process of selecting a database solution that will accommodate high speed/performance read queries for an analytics database.   The data will likely all be structured.   We currently use Postgres as our main database in the product.

We are basically looking to implement the ability to perform a query across the entire dataset which is primarily financial transaction / holdings data with some additional structured data tied to each client in the dataset.   Currently, these queries across the entire dataset run batch and take hours in Postgres on a one VM implementation (Like I said, cost is an overriding concern). 

We want to give users the ability to run a similar query **in near real time** as part of a new analytics tool.   It still needs to look across the entire dataset but the batch process needs to run it for 30+ different scenarios across the entire dataset.   The user would just be running it for 1 scenario at a time.

To give a very rough idea of potential data size as a starting point (not considering future growth that is expected), we are talking about 200,000 clients with 100,000,000 historical transactions, and other related data.    I know this isn't very specific, but just trying to give some very rough idea of where we are.   The total current Postgres DB in the existing product is between 150-200GB but will continue to grow.   We don't currently use replicas or scaling as we are a small operation and the overnight batch process works for the existing project.

We are envisioning creating a separate analytics database that is easily scalable and focused on reads not writes to provide users with a query tool with the performance we need.

I've looked at Snowflake but am concerned about getting locked into a proprietary platform where prices might get away from us.   We don't have a lot of financial resources to burn right now, and we need to prove out that we can get clients for the new application to bring in enough revenue.

Taking Snowflake off the table, I am trying to find a scalable, open source, cloud agnostic alternative (we may be installing this analytics database both in client's VPCs and our own cloud infrastructure sitting on one of the big 3.

We have been looking at Cassandra since it is open source and scales horizontally, but I'm not sure if it is the best solution for our needs.   I was hoping to get some input from people with Cassandra experience on whether or not we are barking up the right tree.   I'm somewhat familiar with NoSQL solutions as we used to run MongoDB for our less structured data.   A lot of the queries will be based on date ranges of data, combined with looking for other criteria such as transactions above a certain amount, or in a range.

I'm also open to any other suggestions that ideally are open source, where we have full control over deployment, are cloud agnostic, and importantly where we can easily control cost and tweak resources.   We want something that we can grow with, but need to start conservatively in terms of financial outlays.

If Hydra was more mature, it seems like it could be an interesting alternative that would let us build on top of Postgres which we're already using, but it looks too young to me for us to take a risk on.

Thanks for any input!",2023-01-07 23:18:40
ztbrbd,Data Engineering Tricks: How To Get Dirty Data Cleaned,N/A,2022-12-23 10:06:37
zoumpw,"desparate for a career switch, appreciate any guidance","Hello redditors, I have exprience 3 yrs of exprience in SAP basis administration. 

I'm quite good with SQL, I love the language. I've done some courses in Hadoop, Spark.

I'm bad at self-study. Could you guide me How can I get a junior position / switch to data engineering?

I'm desperately searching for an curriculum that could prepare me for interviews.",2022-12-18 09:20:57
zohw0f,Small Tech Company Tech Stack,"I'm building out a simple tech stack to be able to analyze Quickbooks and Hubspot data (and eventually connect with product data).

My planned tech stack is Quickbooks + Hubspot -> Prefect on Kubernetes -> AWS S3 -> AWS DynamoDB -> Tableau

Is this overkill? Or am I overestimating how difficult this will be?

3 YOE of experience as data scientist in a bigger tech company (a lot of this was done for me before, but I've got a strong technical foundation).",2022-12-17 22:16:17
zlrthq,Routing OSQuery Events via Apache Pulsar,"[Learn how to route osquery logs.](https://www.decodable.co/blog/routing-osquery-events-via-apache-pulsar)

OSQuery is an open source tool that lets you query operating system events using SQL.The events can be fed into a #streaming platform, in this case Pulsar, for subsequent transformation and routing on the stream using Decodable.

&#x200B;

[Route OSQuery Logs](https://preview.redd.it/3r1wlfxggv5a1.jpg?width=377&format=pjpg&auto=webp&s=e120b9e5b965459cfa1d9cecdc8242354980533e)

\#apache-flink #flink #security #logs #cybersecurity #osquery #sql #decodable #streaming #apache-pulsar #pulsar",2022-12-14 14:05:52
zjwgh4,How stressful is data engineering in NYC?,"Hi! I'm looking into moving to NYC and was wondering what's the average stress level and working hours for a DE there.

I understand this is super relative, but that's mostly what I'm looking for, personal opinions with some context on your situation.

If you prefer to follow some guideline, I think this would be the most helpful:
- type of company (FAANG, startup, finance, etc)
- experience 
- average stress level
- average working hours 
- how long in NYC

Big thanks in advance for any help",2022-12-12 11:33:50
z9p0gx,Can anyone give Snowflake Roadmap ?,Snowflake for beginners like from where to start,2022-12-01 14:18:35
z9400w,Are tools or projects more important for learning DE?,"Fork in the road. Two choices:

1. Fast paced, remote, late startup, average wlb (40h), exciting greenfield work, good rapport with teams, lots of opportunities in the futurestack: snowflake, prefect, dbt
2. Slow paced, remote, giant bank, chill wlb (<30h), limited scope of work, 30% more pay the first year, red tape.stack: airflow, pyspark, aws

Instincts say option #1 will give way more practical real experience. But is the tool stack here a deciding factor? Airflow and PySpark are staples, right?

Edit: After about a day, the votes are overwhelmingly for option #2. But nobody is leaving any comments to lay out their reasoning

[View Poll](https://www.reddit.com/poll/z9400w)",2022-11-30 21:34:21
z7vtzl,HELP: Jenkins ReInstalling Python Deps EVERY Run,"So my predecessor was running Jenkins/Docker stack for all DE jobs. My company wants to move away from docker. I'm running into an issue where every job, every run, installs all python dependencies before it runs. Not sure how to cache previous dependency installations so that it just runs the job. Each job currently is taking 2 hours to run. Help! :/",2022-11-29 14:38:30
z47d6k,How to create and use data flow in Azure Data Factory,N/A,2022-11-25 07:40:02
yx1r12,Ethical Concerns with Offer,"Hi all, 

Currently I have 2+ yoe, a master’s degree in engineering ( non-CS) and have been working in a F500 modern data stack company for 2 years. I started as a data analyst ( Covid lack of options), and have been a data engineer ( promoted from junior to mid level recently).  I currently make 93k. 

I was searching for new jobs and I have been interviewing pretty well with leetcode for sql/ python. I received one offer for 121k which was immediately revoked because the team decided “to go in a different direction” this week. I received a second offer for a tax preparation small firm for 132k, but I found our they are being  acquired by a Private Equity firm. The offer is great and I would learn a lot, but I feel conflicted as the PE firm was fined multiple times for medication theft/fraud by the NHS. I think I will reject the offer and wait for a less ethical fraught 
offer? Thoughts 

Edit: I turned down the offer. The recruiter was upset that I did not accept the offer, and that I had wasted time for an offer that could have been given to laid-off candidates. Personally, that upsets me because I was given three days, 72 hours, to accept which is perfectly reasonable. I no longer care about recruiters trying to guilt trip me for evaluating offers and making the best choice for myself .",2022-11-16 18:29:04
yw4n9k,Computer Science or Data Science Masters??,"Hello everyone, hoping I could get some advice from the group. I am currently working on my masters degree in data science. This degree is one part CS and one part statistics. However, I can feel my career path pulling me toward data engineering. I greatly prefer to build the ETL pipelines and data structures then combing through the data and performing analytics.

I am faced with the possibly of switching my masters to Applied CS where they have a track specializing in data engineering and courses that I feel would be much more relevant compared then the stats courses that a data scientist would need. 

I know in the grand scheme of things nobody really cares about what your degree is but I would rather spend my time, energy, and money learning skills that would be more beneficial to me then in statistics where I don’t think I will frequently utilize those skills once I leave the university. 

Any thoughts/advice/suggestions?",2022-11-15 17:55:49
ym1qng,Data Engineering @ Zscaler,I’ve got an interview lined up with Zscaler. Any thoughts on company culture and interview process much appreciated.,2022-11-04 15:19:12
yhkv6r,[Kusto/Azure Data Explorer] Kusto Detective Agency - Case 4 is live and now KDA has its own subreddit @ r/kustodetectiveagency - win prizes and Microsoft Credly badges whilst sharpening your big data skills,"Kusto Detective Agency's 4th (and so far hardest) case is live today. 

Reminder, you can play at [detective.kusto.io](https://detective.kusto.io/), this is an official Azure minigame to help users learn about Kusto (ADX), you can win prizes and official Microsoft Credly badges (many to be earned!), and you can get a totally free cluster by signing up with the instructions in the onboarding challenge.

Here's a thread on [r/kustodetectiveagency](https://www.reddit.com/r/kustodetectiveagency/) if you're after a hint, maybe some kind souls can help you out! (Or if you just want to show off your badges)

[https://www.reddit.com/r/kustodetectiveagency/comments/yhk0on/case\_4\_thread\_hints\_tips\_theories\_badges/](https://www.reddit.com/r/kustodetectiveagency/comments/yhk0on/case_4_thread_hints_tips_theories_badges/)",2022-10-30 17:48:09
yhfupx,job queue workflows/orchestration,"I've got 25 worker nodes (on prem data center) for a new Geospatial Data Conversion and Analysis SaaS App
I'm struggling to figure out best methodology to handle running jobs
We want normally one job on one machine because most of our tools and software is multithreaded and if not it's too much of impact on I/O or other resources 
Many big Geospatial data processing jobs are heavy CPU Dependenant and so most worker nodes are 32-64 threads 

We also have one spark cluster for running pyspark and geotrellis and Geomesa and mrgeo 
I was thinking of using Kestra or Luigi but these are new to us.
Anyone have experience in this and have some recommendations?
maps@techmaven.net
https://portfolio.techmaven.net",2022-10-30 14:37:18
xoeuup,Beyond Hard Drives: The New Ways We're Storing MASSIVE Amounts Of Data...,N/A,2022-09-26 09:55:35
wvh9dc,Will this Year-Up program help me become a data engineer? Or take the technology consulting route?,N/A,2022-08-23 06:27:52
wuqrdu,How To Maintain ML-Analytics Cycle With Headless BI?,N/A,2022-08-22 11:12:05
wmzizq,How to make pipeline code neat and concise?,"I'm using Python and Pyspark.

I inherited a pipeline that does the following:

    import business_logic1, business_logic2
    
    class main():
        _init_():
            self.table1 = ""table1""
            self.table2 = ""table2""
            if Boolean:
                self.table3 = ""table_xxx""
            else:
                self.table3 = ""table_yyy""
            ...
            # many many more tables
        
        def run():
            dict_business_logic1 = {
                ""table1"" = self.table1,
                ""table5"" = self.table5,
                ""table7"" = self.table7,
                ...
            }
            business_logic1 = business_logic1(dict_method1)
            business_logic1.run()    
        
            dict_business_logic2 = {
                ""table2"" = self.table2,
                ""table4"" = self.table4,
                ""table10"" = self.table10,
                ...
            }
            business_logic2 = business_logic2(dict_method1)
            business_logic2.run()
            
            # a few more business logics        
            ...
    
    main = main()
    main.run()

The issue with this is there are too many tables that I need to pass as self variables. I also feel this is really unpythonic but don't know how to make it more concise.

There are a lot of overlaps, such as a few tables are involved in every process so it's always in the dictionary (that gets passed as argument).

In general, how to deal with too many variables that need to be included in the pipeline? How can I make this more concise and easier to read?",2022-08-12 23:41:34
wmke50,newb needs help with schema and relationships,"Hey guys. 

Serious newb. 

I have absolutely no understanding of databases and I'm really struggling getting my head around the concept of keys and relationships. 

Basically what I'm looking to do is have a data set from Instagram. 

Attributes

User ID 
Username 
Followers 
Following
Hashtag

Etc. 

How do I link these together to query different trends and find interesting links in the data. 

I have no idea to set this out. Mainly because one user is a follower and a following and a hashtag. 

Do you understand what I mean. 

Thanks in advance",2022-08-12 12:45:26
wkqu92,Starting as a data engineer,"Hey guys how are you? I recently got a job at Rockborne as a data engineer consultant. I learn python, SQL, snowflake, excel and other things. My question to anyone is to what other careers can you go to from being a data engineer? I have seen people go I bioinformatics but idk what else. Thank you!",2022-08-10 07:11:24
wjecct,"Hi, could you share which accounts with good DE content are worth to subscribe on Medium.com?",I am looking for good recommendations.,2022-08-08 17:12:53
w0h03z,Any way to use Gcp cloud storage as AWS DMS target. I know there is no direct way. But is there any intermediate way ?,Same as title,2022-07-16 13:43:24
vwredz,"July 21, Free open source community event - PrestoCon Day 2022. This is a great event to learn more about Presto, the open source SQL query engines. Meta, Uber, Bytedance, Apache Hudi and many more will be sharing how they're using Presto for next-gen data architecture. Fully virtual and free",N/A,2022-07-11 19:54:26
vwqghk,Identify data quality issues on data ingestion pipelines with dbt and re_data,N/A,2022-07-11 19:15:18
vwjrz9,"Read more about AI Authority, Spark Shuffle Service & success metrics for product analytics, and more in the 92nd edition of the Data Engineering Weekly",N/A,2022-07-11 14:29:50
vsqx5v,Interview question,"Hey , I need help with one question asked in interview, 

    Tell me a Challenging time you had a Pipeline broken in middle and how did you identify and solved the issue ?

I have build ETL pipelines but didn't faced much huge issue which i could answer in interview so i was rejected , can anyone tell me a scenario where they faced this issue I will use this in my further interviews. I am failing in Managerial interviews please help me",2022-07-06 14:09:58
vs2vlr,DataEngineering Interview at EasyGenerator,"Hello, 

I have an online assessment with EasyGenerator for DE position. Has anyone taken it? If so, what can I expect?   
If anyone has attended the interview process as EasyGeneator? Can you please share your experience?",2022-07-05 17:11:14
v7mk6e,June 2022 Snowplow Product Office Hours,"Join our Open Source Product Manager as he takes us through the Snowplow Open Source roadmap for 2022 and beyond.

[Register now](https://snowplowanalytics.com/events/june-2022-product-office-hours/?utm_source=reddit&utm_medium=post&utm_campaign=product-office-hours&utm_content=june-22).

&#x200B;

https://preview.redd.it/7dj6hifyqd491.jpg?width=2467&format=pjpg&auto=webp&s=d7e7bafd9cad5036230858518de29f7717d9e6ae",2022-06-08 10:41:04
v5n6tw,Good Sources for learning Data Transformation / Data Cleaning with Python?,"Hi,  although it's more of a Data Science question but since during the ETL process data has also to be transformed and cleaned I would like to ask if anybody can recommend some good sources for learning Data Transformation / Data Cleaning with Python, especially during the ETL process and for Analytics or ML purposes. It can be for example with Pandas or Spark. Any course recommendations from sources like Udemy, Coursera etc. would be great.",2022-06-05 21:35:19
v1xg7n,Automation of SQL Server restore from S3?,"Hello - 

&#x200B;

I'm working on migrating a data pipeline for one of the apps my company uses. The app does not have an API and we can't access the database directly. The vendor suggested this round about way of retrieving the data and I wanted to see if this subreddit had any ideas about how to actualize this on AWS.

&#x200B;

The application's data is stored in SQL Server but we can't access it directly. Instead, we have to download a .bak file via SFTP, restore the database, and then move the data from there. My thoughts on how to achieve this are:  


\- Execute Powershell Lambda to download the .bak from the SFTP and deposit into S3 -> easy

\- Create SQL Server RDS instance -> Using Terraform? Cloudformation? Boto in Lambda?

\- Execute Python Lambda to restore the .bak file into the RDS instance -> easy, RDS comes with a stored procedure to do this

\- extract and load the data from the RDS into warehouse -> Looking to use Fivetran for this

\- Destroy the SQL Server RDS instance to avoid charges -> Using terraform? Cloudformation? Boto in Lambda?

&#x200B;

Looking for any advice for how to accomplish and orchestrate these steps. Trying to avoid Airflow if I can.",2022-05-31 18:58:35
uwkaec,Our Path to safer AI: Sharing data without compromising privacy,N/A,2022-05-24 06:18:43
ultpii,Anyone here bounce or in the process ofboucning from DE to VC?,"Might be making this switch in 6 months or less. 

Not sure how different/better it would be...unless VC's offer finder's fees to staff who ID the next FAANG, Airbnb, etc.",2022-05-09 15:24:43
ugtqx7,I work as a Database developer and administrator for 8 years but not working on DE specific tools or tasks. How to prepare CV and be confident for DE interviews without real life experience? Which online course or certification is better? Please share your views.,NA,2022-05-02 16:38:24
uedng8,Up-skilling DE Teams,"How are you up-skilling your DE teams when moving away from legacy tech, new a more
modern data tech stack?",2022-04-29 04:14:54
uca4ix,Good sources about advanced scraping?,"Hello

I build many scraping based on node + puppeteer + PostgreSQL as data storage and queue.

Everything works great, but I want to develop myself, my next goal to scraping data from + 100 000 000 pages (I have my proxy server with squid). When I can find sources to learn advanced scraping data, optimize performance, etc?

what would you recommend to me?",2022-04-26 11:14:54
uammsy,Aws glue data catalog vs external tables,"Hello all,

I have data lake in s3 and currently reading it through snowflake (external tables)
Has anyone tried aws glue-catalog, how would it compare to snowflake external tables?

Does it make life easy for ML Or DS team, if data is in aws data catalog?
Just wanted to know benefits of  aws data catalog vs external tables in snowflake

Thanks,
Robert",2022-04-24 04:22:01
u9gzxo,[interview prep] DE at Amazon Ads team,"I have the first tech round at Amazon next week and the HR email says it would be on SQL, scripting language of my choice and ownership principles.

Did anyone here interview for this role? I use snowflake at work but my practice is rusty.

Any tips, resources and interview prep courses for structured preparation?",2022-04-22 15:14:52
u44561,The next product,"We are trying to find out potential features that we can build in our product which can assist data scientist / Data analyst / Product Managers in deriving value from data such as observability, cataloging etc.  We are trying to find out a vertical to focus on for the phase 1 of our SaaS offering. Can you guys please suggest what all features are in most demand.",2022-04-15 09:22:51
u3bp36,Need ideas for my first data engineering project,"Personal Skill: Python (Baisc), SQL (Only Querying)

The current setup that my organization is working with is as follows: a python script that pulls data from an api, wrangles it for relevant variables, and outputs a csv file. This data set is then timestamped and then appended to a master data set which contains historic data of the same variables i.e. instances generated when the script was run on previous days. This master data set is then used to generate BI insights on a tableau dashboard. The insights are based on how the variables change over time. But to generate this data, the script is (and needs to be) manually run everyday so as to get that day's metrics.

My organization wants me to automate this. Rather than have someone run the script daily, they want to have it run automatically, without fail, everyday. Also, this all takes place locally i.e. from the office computer and the data and scripts are stored on the same. Organization would like me to have this stored on a database of sorts from which the dashboard queries data. Same thing with the python script. So I wont be able to use windows task scheduler.

I'm working through the [data talks club data engineering zoomcamp](https://github.com/DataTalksClub/data-engineering-zoomcamp) and have a loose idea on what my potential workflow should be: somehow have the python script run on google cloud platform (or the like) and push the generated csv to the table stored on a database, also stored on GCP. That means I need something to schedule running the python script (Apache Airflow apparently) and push it to a db (Postgres) hosted on GCP. It sounds like a solid plan, but since this is my first time working on a project like this, I'm a little nervous. Could you please verify my though process, and also suggest possible (simpler) alternatives? I must mention that i need to handover this project to a client who then will be in charge of the data backend and the dashboard, so it needs to be relative simple and easy during the transition.

Sorry if I've been vague or incorrect in my terminology. I used to work as a data analyst and this backend stuff is new to me. Thanks in advance.",2022-04-14 07:27:41
tst0m1,What questions to expect for the Cloud FinOps Interview with CTO and COO?,"Hey there, for the background I am coming from Financial Analytics and am on the softer side of Data Analytics/Science. Obviously, I have experience with AWS, but more on end-stage analytics and data science (mainly Python transformations, algorithmic modeling, dashboard reporting, etc.).   


So, since there are a limited number of specialized FinOps professionals, I guess I can have a chance for the position. With that, what should I rewind before the interview? Their main focus seems AWS cost allocations and budgeting.   


Also is it a good idea to specialize in Cloud FinOps? In my opinion cloud budgeting is going to have a higher demand in near future.   


Thank you",2022-03-31 05:49:20
tfiscz,SSIS Lookup multiple rows...,"So in my Data Warehouse, I have a bridge table to handle the multi-valued dimensions. I've made unique groups in that table like ""USA"" has ID=1, ""USA, UK"" has ID=2, ""USA, UK, Canada"" has ID=3, unpivoted though, so they are all individual rows. 

The problem is, in the fact table, for an observation that has ""USA, UK"" as DimCountry, I can't get the key 2 from the bridge table, I don't know how to. Can anyone help me with that? Thanks in advance.",2022-03-16 14:22:00
tf995j,How to learn and excel in data engineering stream?,"I'm an ETL developer from last 8+ years and so far I haven't worked on new age tools or bigdata stuff. Currently I work in informatica PowerCenter. I want to upgrade myself and stand in the current crowd. Could someone help me with the roadmap to work on the skillset required of data engineer, because whenever I search I somehow get lost in the vast number of tools and was unable to target one tools. For me it feels like every tool is important and couldn't select one. And my friends used to suggest me to change the stream, like leave data and move to DevOps stream. After all I worked for 8years in data I don't feel like changing my stream. And learning DevOps is also not a easy task, lot of learning and unlearning required there too. So whatever it may be, for sure this is not a easy task.

It would be very much helpful for me if I get the community help. Thank you all, I hope I get the guidance I'm looking for. Cheers!",2022-03-16 04:27:52
teci9y,Acceldata - the leader in Data Observability - raises $35M in series B round,"Acceldata, enterprise data observability cloud, on Wednesday announced to have raised $35 million in Series B round led by global venture capital and private equity firm Insight Partners with participation from new investor March Capital and existing investors Lightspeed, Sorenson Ventures, and Emergent Ventures.

[https://www.entrepreneur.com/article/388316](https://www.entrepreneur.com/article/388316)",2022-03-15 00:50:20
t4eum1,Amazon BI Engineer interview help,"Hi all,

I have an interview for a BI Eng role @ Amazon.

Anyone have any tips?

I am most worried about the SQL technical portion. I rate myself as average, as I have not written many complex queries and concepts, from stored procedures to CTE.

Should I do some questions on Leetcode? I feel like I cant make up enough time to do the hard SQL. I just haven't put the hours in and my current job doesnt require heavy sql use. Feeling in over my head lol.

Any input would be appreciated.",2022-03-01 18:36:08
t200xb,How to Install Docker and Run Container on VM in Azure,N/A,2022-02-26 15:48:10
soswlk,Handling slowly changing dimensions in an Activity Schema,[https://www.narrator.ai/blog/how-to-handle-slowly-changing-dimensional-data/](https://www.narrator.ai/blog/how-to-handle-slowly-changing-dimensional-data/),2022-02-10 00:48:28
snsxg1,Python interview,"Hi all, I have a Data Engineering interview next week and it will consist basically of two SQL and Python screening tests. I come from a data analysis background and my most recent job has been as a data modeler/architect, so I'm fairly confident that I'll be able to complete the SQL section without trouble. 

Python on the other hand, although not completely foreign to me, is not my strength. I've used it several times in the past, but mainly for ad hoc projects, where I've basically managed to Google my way out until I come up with a code that works, but I'm sure isn't by all means the most efficient or even the standard way to write Python. 

I have a few days until the interview and I wanted to ask if you could recommend some specific areas for me to revise/study. 

Thanks for the help!",2022-02-08 19:43:52
se9uww,"I'm publishing a free tutorial series about working with data in Python and Pandas. I just published video 4 in the series, which is all about cleaning up dirty data, which is an essential tool to have when you're automating data workflows. Hope you get something out of it!",N/A,2022-01-27 21:57:39
s2sdrb,Help for Project's Data Source,"Hi! I am trying to build a project for my uni's FYP and decided that I will go all-in so that it can double up as my personal portfolio as well. My idea is to build a modern batch end-to-end data pipeline with containers orchestrated with Airflow, from ingestion to visualization.

Originally I was hoping to collaborate with a company for the data source (to develop a POC for the pipeline), but as time goes on I began to feel that it is incredibly difficult for companies to share their data to a student. Therefore, I began to look into the possibility of using open source data or Live API.

The problem is, I am facing difficulties of choosing the most appropriate data sources. I hoped my project could accomplish and demonstrate these characteristics below:

\- Integration from multiple diverse data sources

\- Batch import every X time (means the data should be timestamped and constantly updating)

\- Data sources rich enough for a few visualizations (possibly ML)

Can anybody advise me what data source that I can potentially use to accomplish this? Thankss!",2022-01-13 06:14:50
rkwqin,A Schemaless Data Store within YOUR SQL Database,N/A,2021-12-20 20:46:16
ri8jzg,"ETL (Extract, Transform, Load). Best Practices ETL Process And Lifehacks",N/A,2021-12-17 04:04:33
rckdx2,Here's how I built ClickHouse data visualization with a metrics API layer,N/A,2021-12-09 15:47:26
r9fvaf,Moving TBs of json files from GCS to AWS S3,"Hi, 

does anyone have experience moving a large amount of data from GCS to AWS S3? any insight here would be helpful. Thanks!",2021-12-05 13:36:09
qw4zun,Data (Science/Architect/Engineer/?),"I've reviewed a couple variations on this topic but my question is less about what these roles do (as I think I have a reasonable understanding of job responsibilities) and more about which would provide the best compensation/career path for a mid-career transition from DBA.

Basically I've been a multi-platform DBA (but mostly Oracle) for 15 years, and I'm being given the opportunity to transition to a new (but adjacent) career path, so I'm wondering as we go into 2022 the advantages/disadvantages of these (or other data paths I havent listed)...

In addition to my DBA role I have some familiarity with python, data visualization (Tableau/SSRS), ETLs (SSIS/Boomi), and Linux admin in case these are relevant.",2021-11-17 17:59:50
q7hf4g,recruiting interview participants for PhD research on web search at work in data engineering,"Do you:

* work in data engineering?
* and search the web at work?

My name is Daniel Griffin and I am a PhD candidate from the School of Information at UC Berkeley. I am doing a research project interviewing people in data engineering-related roles to understand how they think about and use web search in their work.

If you decide to participate in this study, we would schedule a 1-hour one-on-one interview over phone or video chat (though you could end the call as soon as you would like).

With your permission, I will record the call and take notes during the interview. To protect your privacy, I’ll use pseudonyms for your name and company in any research publications based on this work.

I am unable to provide monetary compensation, but I would be happy to share my research findings with you (that offer stands even if it does not work for us to do an interview at this time).

This is completely voluntary. You can choose to be in the study or not. If you would like to participate or have any questions about the study, please email at [daniel.griffin@berkeley.edu](mailto:daniel.griffin@berkeley.edu).",2021-10-13 18:29:07
q70ffr,Best channels to recruit Sr Data Engs?,"What are the best job boards for Australia, Singapore, and Indonesia?

Our centralized team in Manila, Philippines is going semi-distributed. We’ve had general success with Kalibrr as a recruiting channel for Philippines tech talent. 

As we expand our recruiting pool to Singapore, Australia, and Indonesia, what job platforms should we explore?

We are hiring Senior Data Engineers. Our technologies include AWS Spark, AWS Athena, AWS Glue, MySQL and ELK.

Thanks!",2021-10-13 01:23:25
ptvpe6,What's wrong with this CI/CD setup?,N/A,2021-09-23 13:50:54
podqsz,Resume critique and advice,"Looking for feedback on my resume. Even though my job title is Senior Data Analyst, there are only 3 of us on the IT team so it's over 50% user support, hardware installs, Firewall and phone system admining etc. I've been doing courses outside of work constantly for the past 5+ years and I am looking to make the jump to full time Data Engineer. 

https://preview.redd.it/09htp6mbtjn71.png?width=628&format=png&auto=webp&s=ad4483c4276232d8c3b0732d0580234fc35b51f4

I need to have a surgery so won't be looking until January. Is there anything that I should REALLY focus on to give me an edge when the time comes to start applying. Also, my total comp package right now is about 130k (so my employer tells me) would it be unreasonable to look for 110k starting salary in a large city? Does anyone know of any good paid Data Engineer mentorship sites/companies?",2021-09-14 22:59:32
pggbqv,Would a data engineer role map be useful?,"A while back, I saw a data engineer ask the DevOps roadmap guy to make a roadmap for DE. 

So I decided to start on a role map that might also help. 

Here's a very early version:

[It's early days and I'm learning more about DE by the day.](https://preview.redd.it/vnu7tffe32l71.png?width=2191&format=png&auto=webp&s=f062b3ce0cbaa15164fdba1231fed9e4b4840400)

Already made one for SREs (below)

[SREs seem to be finding this role map useful](https://preview.redd.it/zu0g8c1mz1l71.png?width=2511&format=png&auto=webp&s=5a5a968da717fa6b595340738a2790c5e32581b0)

What do you think? Should I continue with the DE role map? 

Also, I will need help with this, as DE is a new space for me. So any contributors will be welcomed with open arms. I'm not going to set up a Git repo yet. Going to keep it no-code (Trello) for now.",2021-09-02 11:53:17
peq4be,KAFKA STREAM,"Anyone experience with KAFKA STREAM with Scala language? I'm a beginner in Kafka and currently, I'm taking help from Documentation but I completely lost (Due to heavy terms used in it.) 🥺

Can anyone share helping material or any hands-on experience tutorial or guide? 🤗

Thank you. 🤩",2021-08-30 20:12:29
pcbc1n,Microsoft Azure Data Engineer Certification DP 203,N/A,2021-08-26 23:35:12
panj05,Python read a .z02 file,How do you load these with Python?,2021-08-24 13:39:19
p7hkxs,The Modern Data Stack: Open-source Edition,N/A,2021-08-19 15:55:03
p6xiyq,The Anatomy an Active Metadata Platform by Atlan,N/A,2021-08-18 18:49:45
p5dy5o,Graduate Data Engineer interview advice & help needed!,"I’ve been given this task for the final stage of an interview: ""Drawing on your knowledge and experience, describe how Sony PlayStation can make best use of its data?""

 

The presentation should be 15-20 minutes in length with 10 minutes for questions at the end. There are no specific guidelines for the presentation, you can choose to approach it as you wish.

What would be good things to say? Also, what kind of questions can I expect in the competency interview of a data engineering role? Any advice appreciated!",2021-08-16 11:12:10
p569hk,Thought on recent Data Mesh discussions over Twitter,N/A,2021-08-16 01:32:57
p1z7c3,"Introduction of Snowflake data warehouse for cloud | Many Data Workloads, One Platform",N/A,2021-08-10 21:52:03
ox73g6,How to create AKS Cluster?,N/A,2021-08-03 16:29:12
owfi8c,Looking for suggestions,"I am trying to document scenarios faced on a daily basis and the possible solutions or work around which will make things easier for others who might embark on a similar journey.

I am thus looking for suggestions and pointers to improve upon my writing skills.

Please do help me in reviewing my latest article on Medium :

[Medium Article](https://christo-lagali.medium.com/snowflake-key-pair-authentication-setup-9a7dc73cdc5e)

Any Feedback would be appreciated!",2021-08-02 14:20:50
owbcl0,"[Free course] Real-time analytics for ecommerce, on billions of rows",N/A,2021-08-02 09:53:22
op0ylz,"Dagster 0.12.0 - Solid Level Retries, Pipeline Failure Sensors, and Easier Unit Tests",N/A,2021-07-21 22:45:38
oninro,Azure DP-203 -Data Engineer Associate Certification,N/A,2021-07-19 17:21:11
okwwk1,Query optimisation resources,"Hi guys, can you tell me or share with me some good resources for SQL query optimisation (redshift)",2021-07-15 16:54:55
ojpntz,Starting a SaaS company without data skills,"Hi guys,

Long story short, after 10 years doing supply chain management in a Fortune 500 company I want to start my own business.

It drives me nuts that a major corporation is still relying on an outdated version of Excel for its day to day activity. I think I can design a software to help business take better decisions towards and for their supply chain.

Design it is one thing, coding it (well) is another.
I know a bit of Python, SQLite and Power Bi, just enough to produce a MVP (minimum Viable Product) but far from getting anything into production.

To get it right, I would need the following functions / skills:
- web scrapping
- API
- form management
- data storage (fields + files)
- database management (MySQL, PostGre, MongoDB...)
- Extract Transform Load
- cloud services (Azure or AWS)
- website and App creation

Which material would you guys recommend as a next step of my training and why ?
I have a couple of months in front of me plus some real life examples to train on and stay motivated.


PS, apologizes if I am posting in the wrong sub",2021-07-13 21:39:21
oefu7n,Text Preprocessing: How to handle Emoji ‘😄’ & Emoticon ‘ :-) ’?,N/A,2021-07-05 20:39:23
oc35n6,"Data analyst bootcamp, should I stop and transitions to data engineer?","Hello I'm currently taking a bootcamp for data analysis. Looking over it, it's most likely dealing with how to explain data in business term.

I know it deals with sql, python and excel with visualization.

However I enjoy the most dealing with sql, and python. I find it the most fun out of everything. I'm asking this because I want to see if data engineer deals more with those program and if I should change my curriculum.

Can you guys tell me your experience transitioning to data engineer from analyst and why ?",2021-07-02 04:56:35
o7vl0x,Senior Database Engineering interview - AWS Redshift,I have a Senior Database Engineering interview coming with AWS Redshift. Any tips for preparation you all can provide will be helpful.,2021-06-25 20:14:49
nwvsp1,Transition from Data management-Data Analyst-Inter to Data Engineer,"After graduating, starting looking for jr data scientist roles but end-up getting into Data Analyst role.In which i have to support a ETL pipeline which is built using various AWS services(EMR, EC2, SNS,Lambda). Mostly my work is on data profiling, metadata mapping, data validation, writing some custom scripts using pyspark and responsible for onboarding the files from different source systems to delta lake using our ETL pipeline & troubleshooting the feed failures as well. My contract is about to be getting done in few months. Started looking for new job, but mostly getting rejects for any data analyst job since they are looking for people who have experience on analyzing the data much deeper , finding insights, Gap analysis, A/B testing, visualization which i have managed to explain by reading multiple articles based on real time scenarios but it didn’t work out well etc. With the data engineering roles it’s completely different if i explained completely my pipeline i am able to answer any questions based on AWS but mostly they are asking questions on hive and different data warehousing questions still i come how managed to answer it but when it comes to pyspark i am able to manage but different real time scenarios it’s difficult to answer then I started looking for Azure data engineering roles since azure data factory is similar to my pipeline but they used  ask about control-M scheduler which I don’t know at-all.

I have good knowledge on aws, azure, databrick, pyspark and Sql but they use to ask more questions on hive, snowflake data warehousing, snowflake I don’t know how much i have to study for data engineering role
But luckily i got a data engineering role finally but I don’t how to manage it.

If any of you any good articles on loading the data into hive for staging purposes, triggering a lambda or emr using cli or any real time articles with help me a lot.But my question is should of look for data engineering roles or look for business roles since have good understanding in most of the big data tools i am completely confused??",2021-06-10 19:11:57
nrpu81,AZ 303 - Azure Architect Certification Prep Series - Azure SQL [NEW!],N/A,2021-06-03 22:29:15
nr49p1,Microservices vs Data Engineering field - which is more appropriate,"Hi everyone, I have 11 yoe and am intending on changing to either Microservices or Data engineering. I dont have practical experience in either of them but feel that both have good future. MS are more like backend and so with the programming background that I have, I can understand. But DE is very new topic to me but I do understand the gist.

&#x200B;

Do you have any suggestions or something like that here or at least the kinds of things that I have to watch for in DE, its future etc.",2021-06-03 03:55:08
nq74nh,How do you move the data between different systems?,"How do you move your Data from source to destination? Which format (Xml, Json, Csv etc?)? In Batch or as  streaming? Which tools/technologies are you using?",2021-06-01 23:00:44
njyd2c,Is this true? I haven’t noticed this yet but was curious if others have seen issues with this?,N/A,2021-05-24 13:46:42
narmck,DE or DIE digest #2. May the Force be with you,N/A,2021-05-12 15:37:02
n9ccci,Sync Data Warehouse -> Slack,"This is pretty cool, syncing changes to your data from a data warehouse into Slack using SQL: [https://hightouch.io/blog/using-hightouch-and-slack-to-power-sales-ops-workflows](https://hightouch.io/blog/using-hightouch-and-slack-to-power-sales-ops-workflows)",2021-05-10 18:29:55
n6xkhl,Apache Flink SQL client on Docker,"Building streaming SQL data pipelines with a Dockerized version of Apache Flink SQL client

[https://aiven.io/blog/apache-flink-sql-client-on-docker](https://aiven.io/blog/apache-flink-sql-client-on-docker)",2021-05-07 12:41:07
n4yqkm,"Airflow 2.0, relation ""connection"" does not exist","hello! we're migrating to airflow 2.0 and I'm seeing  a connections error. my entrypoint.sh file has these default connections:

    delete_default_connections() {
      declare -a DEFAULT_CONNECTIONS=(
        ""airflow_db""
        ""aws_default""
        ""azure_container_instances_default""
        ""azure_cosmos_default""
        ""azure_data_lake_default""
        ""beeline_default""
        ""bigquery_default""
        ""cassandra_default""
        ""databricks_default""
        ""dingding_default""
        ""druid_broker_default""
        ""druid_ingest_default""
        ""emr_default""
        ""fs_default""
        ""facebook_default""
        ""google_cloud_default""
        ""hive_cli_default""
        ""hiveserver2_default""
        ""http_default""
        ""local_mysql""
        ""metastore_default""
        ""mongo_default""
        ""mssql_default""
        ""mysql_default""
        ""opsgenie_default""
        ""pig_cli_default""
        ""postgres_default""
        ""presto_default""
        ""qubole_default""
        ""redis_default""
        ""segment_default""
        ""sftp_default""
        ""spark_default""
        ""sqlite_default""
        ""sqoop_default""
        ""ssh_default""
        ""vertica_default""
        ""wasb_default""
        ""webhdfs_default""
      )
    
      for CONN in ""${DEFAULT_CONNECTIONS[@]}""
      do
        su -c ""airflow connections delete $CONN"" airflow
      done
    }

but i'm seeing this error after running

    docker-compose build --no-cache

then

    docker-compose run --rm webserver initdb

this is the error, this error populates for each value in the default\_connections  :

>sqlalchemy.exc.ProgrammingError: (psycopg2.errors.UndefinedTable) relation ""connection"" does not existLINE 2: FROM connection        \^\[SQL: SELECT connection.password AS connection\_password, connection.extra AS connection\_extra, connection.id AS connection\_id, connection.conn\_id AS connection\_conn\_id, connection.conn\_type AS connection\_conn\_type, connection.description AS connection\_description, connection.host AS connection\_host, connection.schema AS connection\_schema, connection.login AS connection\_login, connection.port AS connection\_port, connection.is\_encrypted AS connection\_is\_encrypted, connection.is\_extra\_encrypted AS connection\_is\_extra\_encrypted   
>  
>FROM connection WHERE connection.conn\_id = %(conn\_id\_1)s\]\[parameters: {'conn\_id\_1': 'webhdfs\_default'}\]

I'm thinking that perhaps the issue is with this line:

>su -c ""airflow connections delete $CONN"" airflow

I've tried --delete instead and --delete --conn but no luck.

&#x200B;",2021-05-04 21:25:09
n2hipe,Data Science Tools for Remote Collaboration in the Cloud - Data Unbound,N/A,2021-05-01 12:37:06
n229lp,Airflow Email Operator at Enterprise Level,"What is the proper way to setup airflow email operator to send email alerts after a task to my team distribution email? I definitely know hard coding the distribution lists credentials in the config file is not how I should do it. Let alone, I don’t have the login credentials to the email (why would I lol?). Basically I’m trying to use the email operator to send us alerts after another task instead of having the email alert logic written in the previous task’s python script.",2021-04-30 20:08:59
n21pho,Data Engineer Career Goal,"My goal is to be a data engineer; however, I have no experience. I would be a complete newbie to this field. I've been working on my IT skills since January, but I know that I have lots to learn. I have basic Python skills and intermediate SQL. If I take the data camps data engineer course, I'm sure I will do well but is it the right decision since I can't get a job with no experience. What kind of position should I target as I try to get started. I want to align my studies so that I'm not all over the place.",2021-04-30 19:42:05
n08g7t,Detecting deviations in data,N/A,2021-04-28 06:15:50
mvirm6,Public Release of Delight - A Spark UI complement with CPU & Memory metrics that will Delight you! Works for free on top of ANY Spark platform. Install our open-source agent and try it out!,N/A,2021-04-21 15:57:55
mlxps8,Check out the recording of last week's Swiss Streaming Meetup Session!,"[https://www.youtube.com/watch?v=do-Jl9CHLHA](https://www.youtube.com/watch?v=do-Jl9CHLHA)

Learn from Ant Kutschera, an independent & freelance software architect and developer, representing the Swiss Mobiliar about **Apache Kafka Streams** and from Nicolas Frankel, Developer Advocat from Hazelcast about **CDC implementation** 🚀 🚀 🚀",2021-04-07 08:40:23
mlxlnn,Tutorial on how to draw a boxplot with mean values using Base R & the ggplot2 package,"Hey, I've created a tutorial on how to draw a boxplot with mean values using Base R & the ggplot2 package: [https://statisticsglobe.com/draw-boxplot-with-means-in-r](https://statisticsglobe.com/draw-boxplot-with-means-in-r)",2021-04-07 08:31:05
mc4njf,Adding and subtracting intervals on ClickHouse,N/A,2021-03-24 12:38:24
lwx5hk,How Oracle manages Airflow in research and production and highlights on Airflow 2.0 - live webinar event,N/A,2021-03-03 15:56:48
lrfpu7,🔴LIVE SOON [11 AM EST]: 𝐀𝐩𝐚𝐜𝐡𝐞 𝐊𝐚𝐟𝐤𝐚 𝐯𝐬 𝐀𝐩𝐚𝐜𝐡𝐞 𝐏𝐮𝐥𝐬𝐚𝐫 - which one to choose?,N/A,2021-02-24 15:39:14
kexngs,[Help] A few minutes to fill an Academic Research Questionnaire,N/A,2020-12-17 13:42:08
kedmwc,Architecture to store and fetch millions of images?,"AWS is not an option due to company limitations. Need to be able to call api to be able to store and save an image on the cloud and also fetch. Need to be able to fetch images with relatively low latency from the cloud from our api application (Java) with millions of images stored. Any ideas appreciated. 

Edit: will only be fetching a single image at a time per request",2020-12-16 17:33:02
k84awa,11 Tools for Productive Data Analytics Professionals - The full stack data scientist toolkit,N/A,2020-12-06 23:07:54
k4i9kh,A Data Prediction For 2021,"The data suggests that data engineering is here to stay, and having observed how data science has evolved as a profession, and especially how first-movers gained an almost unfair advantage compared to whoever tries to get into the game today... it is safe to say that the right time to start data engineering is yesterday.

[https://www.dataengineering.academy/pipeline-data-engineering-academy-blog/data-trend-report-prediction-for-2021](https://www.dataengineering.academy/pipeline-data-engineering-academy-blog/data-trend-report-prediction-for-2021)",2020-12-01 10:14:00
k3y4j5,"Updating existing SlackClient package in docker container, then getting 'no module named 'slackclient'","Hello! New jr data engineer here!

We currently have slackclient==1.3.2 in our requirements.txt file but I need to change to version 2

 (I'm trying to get slack notifications working with the SlackApiPostOperator and seeing if this might help)

When testing locally I changed slackclient==2.9.3 in requirements.txt file, did the following docker commands:

 **docker-compose down**

**docker-compose build --no-cache**

**docker-compose run --rm webserver initdb**

**docker-compose up -d**

and now all my dags are failing saying `""No module named slackclient""` 

Am I missing a step?

https://preview.redd.it/y7nth8x9de261.png?width=1680&format=png&auto=webp&s=ca8cd84a9c092ee9b12081d6e9c86ce96a25c871",2020-11-30 15:39:49
k2rkjp,Data Warehousing for Business Intelligence Specialization on Coursera?,"Has anyone done this course?

[https://www.coursera.org/specializations/data-warehousing](https://www.coursera.org/specializations/data-warehousing)

I'm coming to the end of the first course in the specialisation Database Management Essentials [https://www.coursera.org/learn/database-management?specialization=data-warehousing](https://www.coursera.org/learn/database-management?specialization=data-warehousing)

I've learned quite a bit, but tbh the course is a bit of a drag and I'm not sure if I'm going to continue with the remaining 4 coursers for a few reasons:

* It's  badly structured. There are multiple modules in each week...e.g. they refer to module 6 and 7 in week 4. And assignment instructions are along the lines of ""to complete Module 7, check out module 5.""  As a result it's hard to navigate.
* Multiple assignments per week...e.g. an assignment after each module in week 3 (module 4 and 5 in week 3). Just have one at the end.
* Each assignment requires you to download multiple txt files to complete it. e.g. the questions are in one sheet, while a diagram you need to refer to is in another sheet, that's located in another section of the Coursera site.
* Peer grading: you often have to save your work as a screenshot and a text file and submit both. You also have to have multiple windows open why you correct your peers' work

Looking at course 2 Data Warehouse Concepts, Design, and Data Integration, some students are complaining about having to install some software, and the assignment is not in line with the current version of it.

I would absolutely hate to be coming to this course as a complete n00b - I know a lot of SQL - so I easily skimmed through that part of course 1.

Overall, the course (i.e the first course in the 5 course specialisation) was handy for learning about database modelling and the remaining weeks (5 and 6) look a bit more concise. Otherwise the course is very verbose and long-winded, and could be structured much better.

Anyone know of another course that deals with database design, ETL etc.?",2020-11-28 17:22:57
k1ikny,The PostgresOperator: All you need to know,N/A,2020-11-26 16:47:39
jzkzfk,Curious about best data integration tools,Can the reddit community recommend me the best data integration software?,2020-11-23 16:37:18
jwfj2i,How to Build a Career in Artificial Intelligence as a Fresher,N/A,2020-11-18 13:12:56
jukemb,Consume kubernetes secret from KubernetesPodOperator (Airflow)," I'm setting up an Airflow environment on Google Cloud Composer for testing. I've added some secrets to my namespace, and they show up fine.

    secret_token = secret.Secret(
        deploy_type='env',
        deploy_target='SQL_CONN',
        secret='m-secrets',
        key='token')
    
    YESTERDAY = datetime.datetime.now() - datetime.timedelta(days=1)
    
    with models.DAG(
            dag_id='composer_set_controlm_secret_kubernetes_pod',
            schedule_interval=datetime.timedelta(days=1),
            start_date=YESTERDAY) as dag:
    
      kubernetes_secret_vars_ex = kubernetes_pod_operator.KubernetesPodOperator(
            task_id='ex-kube-secrets',
            name='ex-kube-secrets',
            image='eu.gcr.io/$PROJECT/$DOCKER_IMG:latest',
            namespace='default',
            cmds=['python'],
            arguments=['call_api.py'],
            secrets=[secret_token]
      )

 As you can se above, I am running a docker image which call my **call\_api.py** program, I would like to print the secret passed via the KubernetesPodOperator as below :

*call\_api.py*

    if __name__ == '__main__':
    print($secret_token) ====> how can I do this ?

Can't find any valuable resource on the documentation...",2020-11-15 11:18:28
jg2y4h,On-Premise alternative for Fivetran,"Many businesses these days would like their data to stay in their network or cloud. ETL Tools like Fivetran provide just provides a SaaS option. With [Sprinkle](http://sprinkledata.com/), when it comes to an On-Premise setup, the data is run on the customer's virtual machine within the customer's cloud to ensure no data leaves the network boundary.  
[Sprinkle Data](https://www.sprinkledata.com/), a data integration and analytics tool enables On-Premise setup. Not just that, unlike most tools the payment model is not based on the number of rows, the rows are unlimited with Sprinkle. So, the users wouldn't have to worry about the costs as their business and data scales.

But, if you want a SaaS deployment, we offer the same as well. [Sign up](http://sprinkledata.com/) for a Free Trial today and evaluate for yourself.",2020-10-22 16:28:13
iyc49y,🎁 A collection of Kafka-related talks 💝,N/A,2020-09-23 15:19:38
iry4s1,Airflow Executor Purpose.,"I'm not quite able grash what an executor does in the airflow architecture.

As far as I can understand it is responsible for allocation of resources and also the type of executor decides how tasks will be executed.


The usual definition says it is the mechanism that gets the tasks to run.


And I am not sure about the relation between the scheduler, executor and queue either.

If anyone can elaborate that will be great.",2020-09-13 13:19:27
hxm6ir,ETL with AWS Lambda + SQS pipelines?,"Wondering whether this is a common / smart approach for ETL / data processing workflows.

[View Poll](https://www.reddit.com/poll/hxm6ir)",2020-07-25 12:48:47
hhh2jy,"Should I do go for MNC with NO idea about job role Or go for ""Data Engineer"" role at 30 people startup in Bangalore, India ?","Hello guys, I am 2020 CS Bachelor just passed out, I got Campus placement offer from MNC (biggest IT service company in India , hired 30k grads in corona, giving x amount ctc. No idea about the Job Role just basic entry-level position )  so I always wanted to give a shot for better offer, and DS/ML  related role, so i am about to get offer of almost 2x ctc and ""Data Engineer""  role in thier data science team with total 30 people startup in Bangalore, so what do you thing what should do? which offer should I go for (while interviewing on of my manager said they have enough funding for next 2 years and they are looking for a data engineer who could transition into data science) so basically I'm in confusion what should i do. I always wanted to do data engineering, Please provide your input considering covid effect on job-laid off, future growth",2020-06-28 16:12:27
gw3t7t,Seeking advice Bootcamp and move to DE career path,"The Linux Foundation has what looks like a pretty good deal ($600) on their [Linux Foundation Cloud Engineering Bootcamp](https://training.linuxfoundation.org/training/cloud-engineer-bootcamp/) . I'm on break from school, where I'm studying Analytics. After a year working as a Data Analyst, I want to slightly pivot from the Data Science path I'm pursuing to a Data Engineering path. 

This looks like a good technical foundation, since I don't have a CS background or proper programming experience. I'm getting better at python and trying to implement good practices. I have logged a good bit of time in Fedora but I'm mostly copy pasting Unix, Bash commands if I need to get something done. 

The course covers system administration, devops, containers, and Kubernetes with system admin and Kubernetes certifications. 

Seems like I'd be a good candidate. Would this bootcamp be a good move to kickstart a move toward Data Engineering?

If not, any advice on alternative steps would be much appreciated.

Thanks for your time.",2020-06-03 22:03:56
gvhrky,Software Engineering course recommendation?,"After a thorough review of the Data Engineering field and my ultimate goal of becoming a Machine Learning Engineer, it seems that these roles are basically Software Engineering.

I have enrolled in Jose Portilla's Python course and also contemplating his SQL course. Do I need to learn software (development) engineering and is there any recommended udemy courses?",2020-06-02 22:47:34
gnru9v,CPU bound tasks vs I/O bound tasks,"Hello,

I have a questions on a CPU vs I/O bound tasks.

CPU bound tasks:

\- Execute faster if you optimize the algorithm

\- Execute faster if your processor has a higher clock speed (can execute more operations)

I/O bound tasks:

\- Program is reading from an input (like a CSV file)

\- Program is writing to an output (like a text file)

\- Program is waiting for another program to execute something (like a SQL query)

\- Program is waiting for another server to execute something (like an API request)

&#x200B;

Where are SQL queries executed? Probably on a server, but what if the SQL database isn't running on a server?",2020-05-21 06:45:15
gb7akb,"(Irvine, CA) Data Innovator- $130k- 180k + Benefits","-bitotech factory of the biopharma space
-they have their own resources to create their own drugs
-ton of drugs get researched but hardly any get approved because of a lack of data 
-they identify drugs that are working and they sponsor them and make sure they’re more spread out and used
-they end up developing the drugs that weren’t finished with clinical trials that they look to capitalize on
-they closed a deal for $3 BB 

Skillset: 
Senior Data Scientist, Architect, Engineer - SQL, Python, Spark, strong ed/degree, enterprise company, data, excellent comm. Must have healthcare experience",2020-04-30 23:49:43
g32v94,Data Engineering MSc Dundee,Hi.  Can anyone who has been on this course please provide feedback of their experience and also how useful it was for entry to Data Engineering roles? Thanks.,2020-04-17 14:19:07
g2e1za,How weird and ugly is writing queries like this?,"    SELECT product3       AS product
        , seller18        AS seller
        , date(timestamp) AS date

I'm referring both to the commas and the spaces to align the names. I have landed into a company that uses this. I really dislike it, but I don't have any objective argument to fight against it.

Do you have any, beyond that this is ugly? Is there something like PEP8 for python for SQL where it is clear that this is not the way? Am I crazy and this is not that bad?

Thanks",2020-04-16 12:33:56
evlga1,Want to switch career from Security Analyst to data engineer,"Hi Guys,

&#x200B;

I just need advice from experienced folks here, im currently a Soc analyst and handling vulnerability assessment project (gathering raw vulnerabilities report on qualys and transferring the useful data to MS Excel -> and making a dashboard report on Power BI.) I realized that im enjoying it rather than the security side stuff. However i do have a little background with SQL and python. Do you think i can transition to Data engineer with just self study? 

&#x200B;

Thanks in advanced",2020-01-29 11:02:59
bjq9y8,Build system for data engineers,[https://github.com/prodmodel/prodmodel](https://github.com/prodmodel/prodmodel),2019-05-02 03:21:43
aarnyf,"Having Trouble Installing Airflow: ""python setup.py egg_info"" failed","# UPDATE: Resolved

Setting the environment variable `export SLUGIFY_USES_TEXT_UNIDECODE=yes` fixed the issue. Thanks everyone!

# Issue

I want to try and experiment with Airflow on my local machine, but I haven't been able to get past the first step. I've been following instructions on the [official documentation](https://airflow.incubator.apache.org/installation.html). I receive the following error when using pip3:

    > pip3 install apache-airflow
    
    # Main error
    Command ""python setup.py egg_info"" failed with error code 1 in /private/var/folders/yp/gw_kk_h15tx5b9dptzk5rfmr0000gn/T/pip-install-4xu47h_9/apache-airflow/
    
    #Output
    Collecting apache-airflow
      Using cached https://files.pythonhosted.org/packages/02/4c/44af9c88308fc07322c42a19a8b8a692a2ffae9f9100ab46339981621b1e/apache-airflow-1.10.1.tar.gz
        Complete output from command python setup.py egg_info:
        Traceback (most recent call last):
          File ""<string>"", line 1, in <module>
          File ""/private/var/folders/tp/3ws321d514d0j0g8gbg5vnkc0000gn/T/pip-install-om4vdeym/apache-airflow/setup.py"", line 394, in <module>
            do_setup()
          File ""/private/var/folders/tp/3ws321d514d0j0g8gbg5vnkc0000gn/T/pip-install-om4vdeym/apache-airflow/setup.py"", line 259, in do_setup
            verify_gpl_dependency()
          File ""/private/var/folders/tp/3ws321d514d0j0g8gbg5vnkc0000gn/T/pip-install-om4vdeym/apache-airflow/setup.py"", line 49, in verify_gpl_dependency
            raise RuntimeError(""By default one of Airflow's dependencies installs a GPL ""
        RuntimeError: By default one of Airflow's dependencies installs a GPL dependency (unidecode). To avoid this dependency set SLUGIFY_USES_TEXT_UNIDECODE=yes in your environment when you install or upgrade Airflow. To force installing the GPL version set AIRFLOW_GPL_UNIDECODE

OS: Mac OS X

Python Versions Tried: 3.7.2, 3.6.8, 3.6.5

# Troubleshooting Steps

* Installing various versions of Python (from python.org). Initially I started with Python 3.7, however, I read there was an issue with [reserved keywords](https://stackoverflow.com/questions/53176846/command-python-setup-py-egg-info-failed). I then tried 3.6.5 but still received the same error.
* Upgrading pip3 and setup tools as per this [Github thread](https://github.com/facebook/prophet/issues/418)
* Installing python via brew

Any help would be greatly appreciated!

&#x200B;

&#x200B;",2018-12-30 02:10:30
17ae5be,Best boot camp for novice/intermediate data engineers?,"I’m a data analyst (4 years exp) turned data scientist (2 years exp). The last 10 months I’ve been working as a de facto analytics engineer but I’m wondering what’s the best way to break into a full-time data engineer role and build a good foundation. Should I do a boot camp? If so, what are some good ones you’d recommend?",2023-10-18 01:07:04
174j9ov,"[Help] Tried highlighting what Databricks does ""in-house"" for a project. Is this accurate?",N/A,2023-10-10 11:53:06
1ajcdye,Using data to avoid the spread of disinformation.,"If anyone is interested in learning how to control data to avoid spreading propaganda and disinformation, there is a live webinar hosted by Nesin on the 7th of February for the Extract Data Discord Community. 

The webinar will discuss special techniques for gathering data, using OSINT (Open-Source Intelligence), extracting proprietary knowledge, and detecting disinformation. 

You can join using this link - [https://discord.gg/4d8F7nv7Ns?event=1196748704548405269](https://discord.gg/4d8F7nv7Ns?event=1196748704548405269) 

Note\* This is a discord event and the link will take you to the discord community, where you will find the calendar invite. ",2024-02-05 09:13:38
1aegsqh,Solutions Architect,"Fellow architects, what’s your favorite architect you’ve built and why?",2024-01-30 05:21:26
18z7acj,Data Engineer Learning roadmap for 2024,N/A,2024-01-05 13:48:29
18vi3gn,Azure Data Engineer Interview Help,"Hi all, I am a data analyst and have been prepping for this role for a few weeks now. It's time I start applying for interviews. A bit nervous as I am going to have to lie of 2.5 years experience as ADE instead of DA for salary sake. 

Firstly, if anyone is applying for same role pls do get in touch with me so we can share our interview questions/experience. 

Secondly for the community, as someone with 4.5 YOE and 2.5 YOE in ADE, what qsns can I expect apart from the ones in SQL and python as that I can manage.

Also, if someone could tell me how their project architecture is, and how they handle transformations, data cleaning, etc in pyspark, it would be very helpful. 

Thanks a lot. Looking forward to listening from you industry folks.",2023-12-31 21:54:23
18ky2wk,Design a real time notification system,"Hi folks, suggest a data pipeline to read sales data from a website and send a notification if a product crosses x number of orders in last 1 hour. Use cloud computing.

My thought is to do something like below, please help with suggestions:

Webserver > aws managed kafka > spark structured streaming with 5 min sliding window over 1 hour > store 5 min aggregate in a time series db like influx > lambda to aggregate over 1 hour and send sns notifications, schedule lambda to run every 5 mins > delete data older than 1 hour in database.",2023-12-18 02:34:23
180mbx3,Are you using chatgpt in your sql workflows?,"I have seen text to sql tools based on chatgpt out there that can help in sql development etc.

Just wanted to see if you use, how are you using and how beneficial it has been for your teams.",2023-11-21 17:26:43
17rdim4,New Video! Get a tour of the Airflow UI and the most helpful views 🤩,N/A,2023-11-09 13:58:14
17qncj3,"What are the things/tools that should be learnt to become a skilled GCP data engineer in 6months,include any frameworks or other things which will be helpful in writing pipelines etc...","My manager has pushed me into a GCP data engineer role, where myself and other person are from my company, we both need to work for the other company and their team members work with us. They were well versed with all the gcp services and apachebeam , terraform etc...and we had a background of devops, now changing to Data Engineering becuz of the project.
Yeah pls provide a list of the tools or things to become a skilled GCP data engineer ",2023-11-08 14:54:15
17g44zd,Seeking Advice: Is a Master's Degree Necessary for a Data Engineer?,"Hello everyone,

I come from a humanities background and have been working as a Data Engineer (DE) for roughly two years. Prior to landing this role, I familiarized myself with Java fundamentals and various big data tools. However, in my current position, my responsibilities revolve mainly around SQL with a bit of SHELL scripting. The internal platforms we use are pre-defined, and most of the data modeling tasks are handled by my supervisor. Lately, I've felt stagnant in my growth.

While I understand that obtaining a master's degree may not be essential for everyone in this field, I'm keen on expanding my horizons, experiencing a different educational environment abroad, and possibly exploring other career opportunities.

I'm currently contemplating master's programs in Computer Science, Data Analysis, or Business Analysis. However, I'm open to other fields as well, especially if they lead to well-paying roles with a good work-life balance, even if they are not directly related to data engineering.

I'd appreciate any insights or suggestions on the following:

1. Would a master's in Computer Science significantly benefit someone in a DE role?
2. Are there other fields or programs you'd recommend considering, based on my aspirations?

Thank you in advance for your guidance!

\-----------

Now I'm leaning towards pursuing a degree in Computer Science. However, I do have concerns about the job market in 2-3 years and whether my limited experience might be a hindrance when job hunting. Any insights on this would be greatly appreciated! ",2023-10-25 12:56:45
17b0jhs,What would you spend $180k on?,"I have some money I can use for a short outsourced project (~$180k)

Company profile: pharma manufacturing; ~800 people across a few sites, most people rely on data of some sort, mainly SAP. ~200 people have office roles and as such drill into data lot more. Our team is 3 people; me (~ 10 years data science + little bit of engineering background), 2 very junior (but good) others. No scope for more headcount now

I started the team in the company 2 years ago. Since then, we have done a lot of quick win analyses/apps (source data -> target outcome), without tackling any of the data integration, data governance, reporting challenges. I now feel it's time we begin building out our data pipelines, and in particular, pulling data from our most important platforms (CRM; ERP - SAP, time series sensor data), and the various set of ""important spreadsheets"" scattered around the place. Once we have the core data stack in place, I hope we'll be able to manage better and deliver more as a very small team

SAP (onprem S/4HANA) is the most important data source. We have a bit of SAP experience (relevant data objects and tables), and quite a bit of business domain understanding (what reports and analysis people want). Given some of my experience with SAP, I am very averse to wanting to go down the path of building more in SAP: BW4/HANA, SAC, ...

What I'm thinking is that we build a short 3-5 page request for proposal (RFP) to ""build our data stack"" without actually specifying the tech stack we want, with some high level requirements of

* Enable iterative building of data integrations from System X, Y, Z into a cloud platform staging area (e.g. data lake). These data integrations will only very rarely be required to be faster than daily batch jobs. We have a preference for Azure
* Enable data transformations from staging area to build views/marts suitable for business reports. We have a preference for SQL, Python, R for transformation and PowerBI for reporting 
* Enable data analysis suitable for detailed investigations. We have a preference for R, but Python is desirable too
* We have a preference for popular and open tools with minimal maintenance

Another direction to go down would be data governance, but I feel that we can only really get started on that once we have a data stack

The main challenges we have are (a) compliance - we need to prove data landing in reports is the same (conditional on transformation) as in the source system, (b) low experience/skill IT team - we ask for X and they don't know what it is

We've just signed up for Microsoft Fabric - which looks like we might be able to get started quickly - but I feel it's not a full enough platform yet, so it's a risk for us to dive into that

Thoughts? Anything I can add that would be helpful?

Thanks very much",2023-10-18 20:30:31
177apz9,How did you get your job?,"Hey all, I’m currently an automation analyst and aspiring data engineer. I’m looking for jobs under “Data Engineer” on several popular sites like Indeed and LinkedIn, but there doesn’t seem to be a lot of activity at the moment. With that, how did you find your current job, and what is your official title?",2023-10-13 22:34:28
16vqhqi,Explain It Like I’m Five - ECS vs Kubernetes/EKS,"Can someone explain the benefits, if any of Kubernetes/EKS over AWS ECS over single machine EC2?

Context: I’m exploring more robust and scalable deployment options for Dagster to primarily run DBT models in Snowflake, but also some native python ingestion processes and in the future also help facilitate ML training for a small/medium sized team. Also, we’re currently deployed on a single EC2 machine code base, and app backend/webserver. I’ve read through the Dagster docs and tutorials which don’t really help explain which might be better for x or y reasons and I’m still fairly new to the field and learning as I go…

Thanks everyone in advance!",2023-09-29 23:07:45
166fjlo,Does Data Lakehouse has a strong business proposition?,Is it really better pragmatically? What justifies for someone to migrate from siloed data lake - warehouse setup to a lakehouse one?,2023-08-31 16:18:12
11ft68u,Don't Touch That Spreadsheet,N/A,2023-03-02 04:42:28
103clkv,I don't know what to do - need advice,"I am Big Data Developer and I have offer an offer as Foundry Support Engineer(much better money - 2times more per hour) in big USA company [Palantir]. I wonder if it good choice or not

Reponsibilities o Foundry Support Engineer in Palantir:

- Developing a deep understanding of Foundry applications so they can leverage problem-solving

- Building technical expertise on specific parts of the platform to provide technical support to Data Engineers working in Foundry;

- Collaborating with product engineers to identify, root cause, and ultimately resolve bugs surfaced by users;

Tech stack : Python, PySpark",2023-01-04 19:31:54
zkspb3,Is DE more fun than AE?,"fun for me = quick feedback of things that I'm doing are working or not

Hey, guys!

I was working as a front-end developer and was attracted to the possibilities that data could bring to us and humanity. Curiosity and life-meaning searching.

I took an opportunity in my company and I ended up working as an AE. The problem is, 80% of my time is trying to figure out my company structure and business rules to, in the end, barely reach reliable data. And, of course, almost any coding, which I miss a lot.

Today, in working with Dataform and bigquery. We'll probably migrate to Databricks.

How much coding I would have as a DE?
If you were an SWE before, where do you extract fulfillment or at least, entertainment, now?

...

I know work is work, but I can't stop thinking about which area I could have more impact on or if all could be impactful, which one have more fun.",2022-12-13 10:43:17
zhm02s,Looking for an experienced DE to walk through/critique take-home python assignment Saturday morning/afternoon,"Hoping to find someone to hop on a call and look over a take home assignment for a mid-level DE job with me. 

To be clear, I'm not looking for anyone to do any work for me-- just critique the answers I have already written. At my current job nobody writes python, so I don't have any experience writing python in a shared codebase and the conventions that might come along with that. As a result, I'm concerned about my code coming off as amateurish. 

If any generous soul would be willing to help me out for a little bit, it would be much appreciated. Am willing to compensate at your hourly. Feel free to dm or comment. Can be discord/teams/Skype etc.",2022-12-10 07:25:36
xp2tk7,Handle millions of HTML files,"Hello!

So, recently I built a massive webscrapper in the Google Cloud. I basically stored around 2 million records in the raw format: HTML.

But now I simply don't know how to handle 2 million of this files. When I zip 200k in a folder, my computer seems like it's bursting into flames (note: I actually have them all, but doing from 100k records each - don't recommend hehe).

If they were a JSON, I'd probably rely on Snowflake (as they are zipped in a single location in a GCP bucket). Any ideas or tools that may help me to keep it duable?",2022-09-27 01:44:44
xdwfmv,Hey! Do you guys have any tips on how to break into the US Data Engineer jobs market as an entry level Data Engineer from the UK/Europe?,I'm desirous of breaking into the US jobs market as I know the salaries are much more respectable over there. Do any of you have tips for this?,2022-09-14 08:30:49
wzbfgm,Breaking into data engineering as a recent Bay Area math grad/CS minor with more experience in software dev?,"I just graduated and have been on the hunt for software dev positions to no avail, I’m extremely capable and adaptable I’m just having trouble landing a position. I’ve been told my resume is more tuned to data science since a lot of my programming projects incorporate external data and I have a math degree. How feasible would it be to teach myself the relevant/ tools (I already know python/other languages) and break into an entry level position in the coming months? I just want work experience.",2022-08-27 19:34:20
wo6ruj,Which language is more useful for understanding NoSQL Databases - R or Python?,"If you had to choose between learning R or Python to grasp NoSQL databases and how they work, which one do you think you'd use?

If you could please say why, that would be really useful as well. Thanks!",2022-08-14 13:46:06
vt72ej,"Exploring Delta Lake's ZORDER, and Performance. On Databricks.",N/A,2022-07-07 02:04:48
rnojts,Anyone here ever use the Huawei Cloud Stack?,"Starting working as an ML/DE eng at a startup. They have some super weird setup with dvc that just isn't working. No shade on dvc I think they just set it up incorrectly. 

Anyway I'm redoing their architecture mostly from scratch and had a good s3 storage system with a Django api servicing access requests thing going when all of a sudden our CEO got a ton of Huawei credits for Christmas (don't know, don't wana know) and now all of a sudden I'm on the Huawei stack. 

TBH it basically looks like an AWS rebrand for the Chinese market (again, no shade, the tech world was built on that kind of thing), so I'm hoping it won't be too bad to switch up the API calls, but the SDK looks... not good. 

Anyone have experience working with:

- Huawei ECS 
- Object Block Storage (OBS) (Definitely not S3)
- Huawei Postgres RDS?

Again I'm hopin its straightforward but there's always gotchas and tricks and my mandarin isn't so good.

ty and merry xmas eve!",2021-12-24 15:30:38
pz0mti,Anyone hear with a few years experience do masters and have it really propel there career. If so what was your journey ?,"Or is the vibe more like try and get 5 years exp and score a job as close to 200k TC as you can and hope you like your team and that’s the realistic ceiling? and ideally do this without a masters ? 

(Not that that’s bad ) 

I kinda wanna get a masters in eu or USA (maybe ML/AI ?) for fun (few years in analytics / de ) but maybe just a waste of money ?",2021-10-01 04:35:42
mrz4kd,Suggest any book related to data Engineer,"Hi. I am trying to switch into a data engineer role. Have experience in python & SQL.   


Can anyone suggest me a good book/share any resources that might help me learn?",2021-04-16 08:49:30
lognjm,Undergrad data engineering intern expectations?,"Hello, this may be a question that is too general because it can vary from company to company, but I was wondering what the general criteria or expectations are from an undergraduate students  applying for data engineering internships. I know this can vary from place to place but if there are any hiring managers out there what do you typically look for a in a undergraduate candidate ? Basic software skills? ETL experience? Data modeling knowledge? SQL + Python knowledge? A lot of the concepts aren’t taught in schools so I’m curious as to what the baseline evaluation is for undergrad students applying for summer internships for data engineering.I’d assume a lot of learning on the job with various tools too so it’s not like we would need to know how to airflow coming in right?",2021-02-20 21:02:50
18mc050,UK Data Engineers: How and where did you find a job?,"Hi everyone,

I'm a data engineer with 10 years of experience in Europe, seeking a job in the UK. Despite holding settled status and not requiring visa sponsorship, I'm facing challenges in my job hunt.

How did you find a job? I'd appreciate any insights or advice on effective strategies or platforms for finding data engineering roles in the UK or just general advice for someone in my situation.

Many thanks!",2023-12-19 20:44:18
1aqizct,From Data scientist/analyst to Data engineer,"Hello everyone,

I recently graduated (M.S Data science) and completed my internship as a data scientist/analyst in France. As a Junior i want to jump into data engineering because I know this role would suit me well and I'm higly interested in that role. The problem is taht I'm struggling really hard to get interviews. My competences are mainly Python, PySpark, , SQL, NoSQL, PowerBI and AWS (I know how to deploy my models). I'm curently teaching myself new technologies as Hadoop and ETL (Talend). The problem is that I'm struggling super hard to get any interview due to my non experience as a data engineer. 

I'm a bit lost and don't know what to do anymore. I then come to you to ask you something: What do you expect from a junior data engineer ? What technologies are you looking for the candidate ? Why a candidate and not another ?

Sorry for my bad english and thanks to everyone replying !",2024-02-14 09:30:52
1aqft3w,How large is the market of text data entry classification/annotation? And how do data entry outsourcing companies work?,"In my understanding, structuring unstructured data is a necessary activity in our life. For example, if you want to have a financial report of what you buy this month, you first need to write down your transaction, like `fish 50k`. This is raw, unstructured data. Then at the end of the month you need to label/annotate/classify the data like this:

* Object: `fish`
* Type of Object: `food`
* Place of transaction: `market`
* Type of place of transaction: `offline`
* Consumer: `myself`
* Type of consumer: `myself`
* Price: `50000 VND`

And that's just one piece of input data. Imagine how large the data that specialized companies or projects (medical, law, finance, etc.) need to handle. In my understanding, their options are:

* Have the staffs to do that manually, or
* Have a dedicated data entry clerk role, or
* Outsource that to a data entry company, or
* Outsource that to a data entry freelancer, or
* Outsource the automation task to a freelance programmer, or
* Buy similar solutions from big data or information system companies

Now, I wrote an app to automate this process. Technically ChatGPT can also do this, but its approach is statistical-based, while this app's approach is rule-based. If the raw data is just keywords, then this app is much faster, cheaper and more accurate than ChatGPT.

Anyhow, with this app I guess I can work on multiple data entry jobs at once. So my options are either as a data entry freelancer or an employee of an outsourcing data entry company.

It will be easy as long as the clients only care about the final result. However, I have no insight on outsourcing data entry companies. From what I got, it's likely that they have a dedicated system to manage all data entry tasks. I guess I can only get benefit if:

* They haven't implemented automatic classification system, 
* Raw data from the system can be copy-pasted to outside 
* Resulted data from outside of the system can be copy-pasted into it
* The task they give me is only about classify/annotate text raw data
* I can work online. (Or if I must work offline, then at least there is no overseer observes me, which I guess it's not possible.) 

I've tried to apply to them to gain more insight but haven't got any success. I'm still in my way to look for such company.

In general, do you know how large the market of text data entry keyword detection and classification/annotation currently is? And how do data entry outsourcing companies work?",2024-02-14 05:55:50
1apy1bd,Free Badges.,"Hello, yesterday I earned my third free badge from Snowflake Essentials (warehouse, market & pricing, developer). I also have the AWS Cloud Quest badges (Cloud Practitioner, Machine Learning, Solution Architect). What other free badges do you recommend? ",2024-02-13 16:46:25
1ap2asm,snowflake - tracking changes,"hi team,

DE here ;)

i am wondering.

we were asked to report on some historical data for salesforce tables, however we didnt have any snanspots created properly. we only had 1 that included limited set of columns.

I am wondering: since we have no clue what 'our executives' might wanna know in th future i wonder if it is at all possible to snapshot entire objects (all columns current and future) for tracking history changes?

I was thinking of creating SCD Type 7 all critical salesforce tables.

so we will have 

1. raw schema - where we extract data to
2. snapshot schema - scd type 7 of every table

&#x200B;

**questions**

\-is it at all easy to maintain ?

\-anything to keep in mind?

\-is there any better simpler way ?

&#x200B;

**I am doing it for the very first time.** 

&#x200B;

Thx

&#x200B;",2024-02-12 15:16:32
17ipv7s,language models for data cleaning,Has anyone used language models for cleaning data? I'm thinking about using a small language model to clean and standardize property records. What models did you try/are using? What kind of prompting do you have?,2023-10-28 23:20:37
12l18c0,Good places to find contracts?,"Basically have a group of a few experienced data engineers and we were looking for contracts to setup as side work (with a few of us, can handle work with just about 10 hours commitment each in addition to day jobs).  


Anyone know a good place to look for small data engineering contracts?  Thanks in advance!",2023-04-13 19:46:09
16cgbp7,My Experience with Joe Reis is that he's only in it for book sales or conference seats,"Have reached out to Joe Reis on a number of occasions to discuss all things data engineering, wanted to share that experience hasn't been wholly positive. Most discussions just ended up in buy my book it's the bible, or attend my event. Appreciate time is money in a capitalistic society but he's far from the authority on Data Engineering. Keen to hear other people's thoughts on this.",2023-09-07 13:47:24
19etrv3,Resume Recommendations for Job Seekers,"I am a data engineering manager and I am currently hiring for a data engineer. 

For those out there submitting their resumes for jobs: please understand that you don't have to list out every single thing you've ever done or technology you have used. 

Lately, I've been opening resumes and it's just like a wall of text of technologies used and every little thing you have claimed to do in the past 5 years. It's like you're throwing pasta at a wall hoping something will stick.

Doing this is the fastest way to have me gloss over and click ""no, do not proceed"".

When I open a resume I want to see that you have actually read the job post and have tailored your resume to address those needs. 

I can't have someone submit a resume saying they made vba macros for automating excell workbooks or that you were a security architecture consultant that oversaw the implementation of archer when I'm looking for someone who has worked with python, gitlab, jira, confluence, gcp {composer and with luck datflow and apache beam} (yes this is all mentioned in the job post). I don't need to know you know how to use MATLAB or Tableau to create dashboards. I need to know you know how to write SQL statements beyond ""select * from"" . Know how to write SQL? Awesome! what performance problems have you encountered and how did you optimize that query? I need to know (for my non senior position) that you know how to at least write a quick Python program to consume a csv, add a derived column to it and with luck you know how to load that payload to postgresql or even big query.

My bar is not very high for a non senior position but you have to at least look like you showed a little effort in customizing your resume to fit my needs laid out in the job post. 

Hell, I'll decide to set up an interview with someone even if they just know the basics (python and pandas/polars) as long as I can get a feel, from your resume, that you hit on the top 3 things I hire for; trustworthiness, aptitude and willingness to learn on top of the basics from a tech perspective. 

If you have all that but you don't know how to ""git push orign main"" or how to set up a DAG or shard some postgresql tables or understand how to build out a proper data model or how to implement jinja templating into your data pipeline or build and deploy terraform or even how to debug broken beam code... That's fine. We can teach you, you will learn and eventually you will become a senior. But, you have to give me something to bite my teeth into or I just can't move you forward. Sorry.",2024-01-24 22:48:01
18r5efi,Career Dilemma,"
I am a seasoned Senior BI Developer with a decade of experience primarily in the Microsoft BI Stack, including SSIS, SSRS, Power BI, SSAS, and T-SQL programming. I also have exposure to Azure cloud tools like Azure Data Factory and Azure Data Warehouse. Recently, I made a career move to a new organization, accepting a mid-level BI specialist position for financial and job security reasons, with the intention of transitioning into data engineering.

However, my current role at the new organization is turning out to be quite different from typical BI or data engineering positions. It involves manual processes and lacks exposure to cutting-edge technologies. While the job is stable with low pressure, I am now questioning whether I made the right decision.

I have two options in mind and would appreciate advice on the following:

1. Return to the previous organization. Despite being volatile in terms of job stability, it has expressed interest in retaining me by matching my current salary. However, this would mean returning to the old technology stack.

2. Stick with the current organization for the next 6 months to a year, utilizing the stability and lower job pressure to self-teach and transition into data engineering, eventually moving to a role that aligns better with my career goals.

I am seeking guidance on which path would be more beneficial for my career growth",2023-12-26 10:35:33
177uos5,Have I been doing CI/CD in data engineering?,"I've been managing a team package using a Git equivalent and an internal tool to execute cron jobs for scheduled script execution. I've also developed a script to parse logs to get a report on job failures to troubleshoot if it's a script side issue or  some configuration issue.

My main confusion is that the approach I've been doing is quite simplistic compared to the articles I see online and tools used definitely are not industry standard, or tools I just cobbled together on my own.",2023-10-14 17:35:07
1940jpl,Why the Modern Data Stack sucks for data consultancies looking to productize,N/A,2024-01-11 12:26:59
18xnan6,Data engineer job offer,"I have job offer for data engineer at fintech Smbc NYC location base 135k and one more offer from Walmart Dallas location 110k, which one is better regarding the company and location",2024-01-03 16:24:04
18k4oox,Mid Senior Data Engineer looking for guidance in interview preparation plan,"A little bit of my career background. I am a MS in CS, batch 2015. For 5 years I was a software engineer II. Lost job due to Covid then restarted my career as a Data Engineer but at level 1. It hurts me a lot that I am a DE I at my current job even when my job responsibilities/work is closely matching a senior data engineer job role. Well, I also cannot switch jobs currently due to work visa restrictions but looks like in about 2025-26 time range, I can make a job switch safely without hurting my visa formalities.

In my current job I am working heavily with Pyspark, Python, Airflow, Datadog, K8s, mentoring juniors (my company is hiring recent grads on level II where as I am at level I and I mentor them but still call them juniors fml), AWS, Databases (RDBMS or NoSQL), Snowflake, MSK and Confluent Kafka, Gitlab, CICD, SQL, Bash, Grafana, Opensearch, Loki etc. I have created highly scalable and robust data software applications to fulfill data requests from other teams in my company. I also do data solution architecture-ing to discuss how to make data reach from point A to point B, I join meetings with other teams and negotiate the contracts with them to fulfill their data requirements and what our infra can offer.

Of course, I am not happy being titled as a Data Engineer I. But I have to deal with it till at least 2025-2026. So I have about 1.5 years to prepare for interviews, read books, be a better candidate for my next job. I need guidance on how to do a proper planning to be successful in cracking interviews.

I have following books on my radar to prep for the interviews:

1. Designing Data Intensive Applications
2. The Data Warehouse Toolkit
3. Elements of Programming Interviews
4. Beginning Database Design Solutions

Other than that, I am going to go heavy on Leetcode for Python, SQL, system design questions.

I need guidance to know what am I missing in my preparations and what technologies am I missing to have hands-on in my current job. The more I know current trends of the market, the more I would be able to prep for my next interviews in upcoming years.

Your guidance and suggestions are welcome. Its my time to hustle and solve the problem when its hurting me the most. Thank you!",2023-12-17 00:22:36
17h530o,Orchestrate Airbyte and DBT with Dagster,"# Orchestrate Modern Data Stack

Vlog on how to orchestrate Modern Data Stack with Dagster.  We orchestrate the entire project; Extract and Load part carried out with Airbyte and Transformations via dbt. We use Dagster as the orchestrator. With new updates, Dagster makes it very easy to import a dbt project and expose each dbt model via the DAG.  

[https://www.youtube.com/watch?v=8340\_gU\_Zy0&t](https://www.youtube.com/watch?v=8340_gU_Zy0&t=8s)

&#x200B;

Topics covered:

* Data Orchestartion
* Setup Dagster Project
* Configure Airbyte and DBT with Dagster
* Run Modern Data Stack with Dagster

Tech Stack: **Airbyte, Dagster, DBT, Python**",2023-10-26 19:56:22
16wagnh,Staff Data Engineer,"Hello, I am a senior data engineer with 7 YOE. I got an offer for a staff data engineer. My question is what skills I need to upgrade to be staff data engineer?",2023-09-30 15:57:24
145kwll,Mention azure adf and azure darabricks challenges u faced from your experience,What are the challenges u faced on azure adf and databricks from your experience,2023-06-09 23:56:01
12uvhve,What do you use?," Data Engineering peeps, if you move data around (whether it be Modern Data Stack or something else), can you let me know.

Do you use ETL or ELT logic?

What tools do you for each of the E, L & T steps?

\#datafam #bigdata #dataengineering",2023-04-22 05:10:23
19de5wg,Entry level + Fully Remote... How unrealistic?,"My goal is to get a fully-remote, entry-level job in data engineering. How unrealistic is that?

Be brutally honest... better to crush my dreams now rather than study for months only to find out later.

I have no experience in data engineering, but I do have a phd in a stem field and 4 years working as a data scientist",2024-01-23 02:52:12
17gdnxs,Databricks 60x faster than snowflake for transformations..?,"Reading this [blog](https://medium.com/dbsql-sme-engineering/why-snowflake-is-slow-for-transformations-and-my-4-insights-from-dbt-coalesce-2023-c05af48e4298) on medium and found it interesting, can anyone confirm/disect the argument? :-)",2023-10-25 20:02:58
13emfhj,ELT 101 - The Why And What Of ELT (Or The Why NOT Of ETL),N/A,2023-05-11 12:47:43
128wol5,Professional Data Engineer,I've a 6 + 9 month experience in internship & FTE in a popular heath care company which mostly oriented around analytics development through reports for clients. Is it worthy to get a GCP certification to advance on the career or would the experience alone can make a difference in interviews?,2023-04-01 19:49:18
116qth0,Considering switching to ML/AI after 8 years in DE,"Correct me if I’m wrong but in overall it seems like ML folks are better off these days, considering the recent shift to AI. FAANG is currently only hiring for AI positions. Comp has always been slightly higher on average imho.

Was wondering if anyone has gone through similar transition and hoping to get some advice on whether it was worth it or not.",2023-02-19 23:03:50
11r5rdw,what do you guys do with leading zeros,"i prefer to keep numbers as numbers instead of varchar.  but everyone is obsessed with keeping leading zeros for their id's.

what's the logic here?",2023-03-14 13:13:10
19d9p1u,CEO of Data Engineer Academy Here - Q & A Next 24 Hours. Ask me anything!,"Ask me anything related to data, data engineering, your app process, a blueprint, etc.

My goal is to provide as much value as I possibly can as a thank you to this community.

&#x200B;

We’ve helped hundreds privately and thousands in total to get a role in the data space. We work with staffing agencies, companies, and deal with thousands of apps getting sent out every month. Today, I want to share that inside view in hopes that someone can benefit from it.

&#x200B;

I’ll do my best to get to every question!",2024-01-22 23:22:46
14lcl25,LakehouseIQ: Your new AI overlord,N/A,2023-06-28 15:30:22
196nl41,Using LLMs to draw simple insight from tabular data: a discussion,"Hey hey all, data/analytics engineer here,

I got a lenghty topic, hopefully worthy of a lenghty discussion. I'm looking into LLM-s and how they could be used for assisting in understanding reports.

I would like your opinion on the following idea I am exploring.

**Business Problem**

Our company got loads of reports generated, more than humanely possible to read (that's another discussion to have, but many of these are legally required, etc.). It would be really cool that for a given report, there would be a few sentences highlighting things.

A Report's (Reader's) Digest if you will. Things like

* ""It seems that compared to 2023 Q1, in 2023 Q2 our sales in department x has increased by 6.4% percent which is unusually high.""
* ""Our social network reach has been constantly falling on Facebook since 2023 February.""
* ""The average customer rating has been dropping for 5 consecutive days.""

**Understanding limits**

I get that LLM-s work with string data, so i'm guessing this isn't a straightforward thing to do. I'm also thinking that currently LLM-s won't figure out insights on their own without guidance, thus questions should be agreed upon with business. (Altough self-propelled insight finding would be a holy grail.) So with this in mind:

* probably business questions should be agreed upon
* probably business shouldn't ask their questions directly from an LLM agent
* LLMs are not great at understanding tabular data so this is not an obvious path ahead

**Brainstorming ideas**

Architecture-wise, I would be using Databricks (something we actively work with), loading a report in as a delta table, then use a [LLM serving endpoint](https://learn.microsoft.com/en-us/azure/databricks/machine-learning/model-serving/llm-optimized-model-serving). This is fairly new stuff coming from them, it's still in public preview, but it can leverage LLama-2, keeps the data on databricks side so it seems secure. (GDPR says hello.)

I found of course Pandas AI, that could be useful in combination with databricks somehow? I also found this [fairly recent paper](https://arxiv.org/pdf/2401.04398v1.pdf), which talks about ""Evolving chain for table reasoning"" on L[LamaIndex linkedin page](https://www.linkedin.com/posts/llamaindex_chain-of-table-use-llms-to-understand-activity-7151983658596229120-fOo3?utm_source=share&utm_medium=member_desktop) that proposes a chain-of-table querying that looks interesting. It seems something closer to what LangChain is doing. Some [medium.com](https://medium.com) articles i found: [1](https://medium.com/@murtuza753/using-llama-2-0-faiss-and-langchain-for-question-answering-on-your-own-data-682241488476) (this is about documents), [2](https://ameer-hakme.medium.com/unlocking-context-aware-insights-in-tabular-data-with-llms-and-langchain-fac1d33b5c6d) (this is about tabular data)

**A maybe functioning idea**

Okay so let's see if the following makes sense, it is very highlever and very brainstormy:

1. Agree on with business what are the core questions they want to know from a report.
2. A LLama-2 model is setup in Databricks with serving endpoint
   1. EDIT: so probably not Llama-2, but something along this line: [https://ameer-hakme.medium.com/unlocking-context-aware-insights-in-tabular-data-with-llms-and-langchain-fac1d33b5c6d](https://ameer-hakme.medium.com/unlocking-context-aware-insights-in-tabular-data-with-llms-and-langchain-fac1d33b5c6d) or PandasAI working with a databricks-stored llama2 (afaik that is supported)
3. A job runs daily:
   1. A report is ingested some magical data engineering-y way (my fellow data engineers know the pain)
   2. Prompt engineering: do some setup with the LLM agent (no clue yet what could be needed, but it's a good guess that this is required - to prevent hallucations, etc).
   3. Prompts from business are loaded from some config file
   4. Results are stored in a separate delta table with following schema:
      1. Some primary key
      2. Some report identifier
      3. Business unit or stakeholder identifier
      4. Prompt
      5. Answer
      6. Timestamp
   5. A SQL Warehouse endpoint is available and can be queried (by PowerBI for example that gets the latest answers.)

**Problems and limitations**

How do we check if the answers are correct? Monitoring results is very blackbox-y at best. Ideally endusers needs to be trained on the purpose of the Report's Digest is to highlight stuff, but they have to verify it themselves. Getting feedback from endusers can be also complicated

**Conclusion in an ideal world**

Business managers like to look at PowerBI reports, and with this new feature they get an immediate highlight of the day, the top 3 questions they always wanna know about comes with immedaite answers. ""We have an increased count of customer service tickets compared to previous week"". The manager looks at the graph and it seems to verify this statement.

Engineer team walks into the sunset with cool music in background

&#x200B;",2024-01-14 19:14:50
18ravdf,Definitive Guide to Debugging Dbt,"Found this gem on Medium. Probably written by someone on this sub.  I run into a lot of cases where people don't understand where their dbt errors come from. Must read for anyone using dbt.

Thanks to u/badketchup for a better link.

https://www.arecadata.com/the-definitive-guide-for-debugging-dbt/
",2023-12-26 15:44:35
18gqg6q,What do you want to know about orchestrators?,"Hi folks, what are you interested to know about orchestrators? 

I am looking to do a deep dive on the more popular ones and write about it. 

What I am interested to understand is what use cases are each of them best at, and the kinds of criteria **you** use to make that choice.  


So my questions to you are:

  
**- what do you want to know about orchestrators?**

**- what would you compare them on?**

\- **what killer features or orchestrators do you think I should check out and investigate?**  


Currently on my list are mainstream orchestrators such as Airflow, but also smaller ones, such as git actions. So tell me what you want to know, and we'll dig in :)  


Thanks in advance!

&#x200B;",2023-12-12 16:54:15
18b8t2x,Are there any Python libraries for Data Cleansing ?,"Hi everyone,

I hope you’re fine. 

Right now, I’m working on a Data Quality Proof Of Concept for my company. 
I managed to find some good libraries like « ydata-profiling ». 
Now the hierarchy wants me to add another layer to the solution which consists of cleaning the « bad » data that doesn’t comply to our requirements. 

I’ve implemented one using Pandas, as I receive only csv files.

I was wondering if someone know or has already implemented something like this, cleaning Data and subtracting rows that don’t comply to some requirements from the original dataset, using an open-source library in Python ?

Guidance appreciated,
Thanks in advance folks.",2023-12-05 10:06:44
18f38wx,Please Stop Using Google Analytics,N/A,2023-12-10 13:07:12
14jpoj6,"IS IT NECESSARY TO LEARN SSIS, SSRS AND SSAS","So I am on data engineering learning track, and i have learnt SQL, and Python, but I wanted to ask if its important or needed to learn SSIS, or should i just go straight to pyspark or cloud data warehouses. I already did a course on udemy that took me through the fundamentals of Datawarehousing though.  


N.B I must say, i have learnt a lot from the discourse on this channel. Thank you all so much",2023-06-26 18:50:12
11ctwrk,Is data engineering over-hyped too like data science?,"
I head towards data science before 1 year ago which ended up with realizing after six month that the field is enourmously saturated. I was 2 years experienced with mobile development (Java), so working with software was a lot more engaging and efficient for me so I decided to go on maybe Data Engineering.

As far as I see, data engineering is not much hyped as being a crazy data scientist or AI/ML engineer. I think people generally stay away from data engineering because of the need of software development background, and they step into data science because they do not aware of the holy math behind the scene.

Those are my guess. I wonder if it is true.

Today junior data analyst positions are full for most regions. However i encounter with jr data engineer positions.

I ask if there is life in data engineering :) If it is over-saturated too, I will focus on cloud engineering before I finish the university (kinda sophomore student).",2023-02-26 21:53:30
198ax1v,Where to start learning the technicalities of what this sub discusses as a PM hoping to get into the tech world.,Transitioning from the Marine Corps soon and getting my MBA. I want to be able to understand what you guys talk about and be able to converse about it for companies like google/amazon. Does any one have any recommendations? Was finance in undergrad and specializing in Data Analytics for my MBA.,2024-01-16 19:04:58
1910hct,I just want to be a good DE,"Hello everyone. Maybe this sounds silly, but I've been working at a data startup for 2 years. I have worked with Python, PySpark, SQL, multiple AWS services, DBT, and several other things. However, the company lacks good practices, and thanks to my good performance, I am increasingly taking a greater role in the projects.

Is it foolish to have a feeling that the project is ""a time bomb"", believing that at any moment it can explode? Is it something normal?

Finally, if you can recommend ways to learn best practices, or at least what the basics are for a project to be well put together from the beginning, that would be great. (courses, books, YouTube channels, etc.)",2024-01-07 19:42:40
18y7jzw,Need Advice: Data Analyst to Data Engineer,"Hey there, I'm currently working in a B2B SaaS startup as a data analyst with a role that's a mix of data analysis and data engineering. I've been involved in various data engineering tasks, such as creating pipelines from ELB to BQ using Python, leading a project on Product Metrics involving ETL (shell scripting), dashboarding, and handling pipeline failures. My team is considering exploring Airflow as well.

I'm contemplating whether to switch jobs because my current title doesn't fully reflect the work I'm doing, or should I stay and continue learning in my current organization. I know it might seem like I'm fixated on the title, but I'd appreciate any advice or insights. Thanks!",2024-01-04 07:42:20
18wggcl,Best way to design a webscraping pipeline,"I am designing a webscraping pipeline that I want to integrate in an automated pipeline that runs on schedule. Each hour (let's say) i have a scraper that scrape some betting websites and consolidate the data. I am doing the scraping in python. The biggest challenge that I foresee is managing events. For instance a game between A and B that happens on dd/mm/yyyy is a unique event. What is the best way to manage that. I have never used kafka but is that considered one of its use case? 


My initial idea is to have a python script that scrape the website. It's scheduled by airflow. 
I was considering integrating the scraping functions in api calls, but I don't really know how that would look like. 

Can you please give me tips and hints on how to approach this problem with the best practices.

Do I need to use docker for example?

Thanks a lot",2024-01-02 04:25:06
18szyuz,check out my new sub stack,"[https://substack.com/@datasketch](https://substack.com/@datasketch)

I write about my learnings in data world in quest to help others.",2023-12-28 17:52:56
187bit7,I need some Career Counselling,"Hi I was posting this hoping to get a good suggestions or career tips or some career coaching. I am currently working as a Data Engineer in a FinTech Industry (almost 2 years now). Ive been on other industry for 4 years so a total of 6 years being a DE. One of my supposed mentor in the company already left a year ago and my 2 year DE colleague also resigns from his post recently
(current) At first it was really exciting because this is my first exposure on building pipelines in a cloud environment, first python development projects (came from java-spark core platforms with various rdbms exposure) and I know its a good experience and surely is, but given the 2 years of stay,  now in a verge of quitting - cannot deny that I am demotivated and Im afraid of being stuck for the next 2 years doing the same thing. I want to stay but that means I have to stay motivated to be productive. I am also considering moving to a different path like Data Architecture or Data Ops. But I dont know where to start.",2023-11-30 06:05:06
1823qih,"44 Best Resources to learn Data Engineering (YouTube, Books, Courses, & Tutorials)",N/A,2023-11-23 15:34:15
1817mpu,What can you expect from Apache Doris as a data warehouse?,N/A,2023-11-22 12:06:15
17bx78h,What questions would you like to ask a Senior Data engineer in the Health Management Industry (medicare plans)?,"I have an good friend  (bachelors in politics and and another in CS) who is a senior data engineer, and I thought it would be valuable to share his knowledge with others who are interested in getting into the same field. This is kind of a ‘if you don’t know, you don’t know’ what to ask situation for me. So, I would like to know what questions would be of value to you. I can post the questions and his answers here if you want as well.",2023-10-19 23:42:49
1795ofw,"Introducing the legendary rock band, ""Flink Poyd"" – where the hard-hitting rhythms of #Kafka, the slick note changes of #Debezium, the fast rifts of #Flink, and the powerful user-facing vocals of #Pinot come together to create a symphony of real-time analytics.",N/A,2023-10-16 13:13:20
174u7a3,Any open source solution for enterprise class data storage,"I've been a web application developer for years, just changed to a new data-science related work .  I was asked to give a software solution for data storage. It should be open sourced and good for enterprise-class usage. The data are all kind of forms with metadata and should be distributed stored with duplicate copy. In the future they could introduce ML for further analysis. I have studied ElasticSearch and Hadoop, ElasticSearch can ingest un-structured data as well as indexing and searching (even vector searching) , Hadoop HDFS can do distributed storage. in addition AWS Cloud offers everything we need (S3, RDS, Redshift). But none of them are up-to-date or a open source solution. Any suggestions ?",2023-10-10 19:45:17
16zx2xk,What comes first: Solid data foundation/infrastructure or products?,"Greetings Data engineering community!

I've been following this excellent forum for a while and I've found it a rather joyous experience. So here goes first question (which I think isconceptually  a common one, but didnt find a specific previous one that suited)

Some context. Previous backround is primarily analytics engineering role, with emphasis on working from an extremely solid data foundation to creating machine learning models, data analysis, business intelligence. The whole analysis pipeline end  to end in a nutshell. From this experience, I've been instinctively impressed upon the enormous degree of freedom a solid data foundation provides, the speed/agility one can move on new problems and products, and just general confidence in data quality and integrity. Overall enormous respect to people who create these foundations and allow people to only have creativity be the limit to what can be made. 

Now, I've recently started a place where this thinking is turned entirely on its head. I dont want to go into specifics, but its essentially the opposite with products first and infrastructure later (or never)

So I've realised, like any good scientist,  I need to triuangulate my thinking. Maybe I am wrong in my thinking, and products need to come first before there really is any foundation, or will a foundation never come when one never attempts to build anything to scale and structure.  What do you guys think? I am hoping for conflicting explanations of how people have approached and solved this, so as to inform what approach i should pursue. To be clear, I do believe the foundation should be made, but I am rather worried my perspectives are very limited by selection and experience bias. 

Also, this forum is awesome. Ive learnt so much 

Appreciate the responses! 

&#x200B;

&#x200B;

&#x200B;",2023-10-04 20:03:43
16z055o,"I’m a software engineer, I want to be a DS or DE. What should I do?","Hello everyone. I’m a recent grad, i’ve been working as a SE for about 4 months now(first career job). The company is good but i dont see myself here long term because of the location, I had to move away. I graduated with a BS in information systems and have knowledge of SQL, tableau, power Bi and most of all python(I use it at work everyday). With SQL, Power Bi and tableau, i had a bunch of data analysis courses. In about 5 months if I wanted to switch jobs to a more entry level Data focused role, could I just apply or would i need something to make me look more attractive to companies? Like maybe get my masters in DS(i want to at some point) or some certificates, is that necessary?

Basically should i just go ahead and apply or am i wasting my time going up that route as i am now?",2023-10-03 18:53:18
16p5t70,Is A European Role Change To DE hard?,"Hello, 

I am looking for advise /reassurance on how to change jobs into a Data Engineering role within Europe. 

I have worked in marketing for the last 4 years, but have been hating it. In my roles, I have been gravitating towards data and tech more than the marketing elements. Recently, i was made redundant due to the company collapsing. So, I am taking this opportunity to switch. I looked into bootcamps, but I can't really afford the price of €7,000. I can probably pay about €1,000 for learning etc (gotta keep bread on the table). 


I have taken some courses in the past in python. I am not versed in object oriented programming, but have a very good understanding of the fundamentals and have created a few NLP projects. And I also have a very low level understanding of SQL and mongoDB.

I am wondering if anyone else has also switched careers in the European zone and what your experience was in changing? If there are books, courses or certs that would help the switch?

Also, Is it in 2023 a bad time to change careers? (I keep being told by my friends that it's the wrong time ) 😭

Also a bit off topic, but would a scrum cert benefit the role?


Any help would be appreciated 👍",2023-09-22 09:33:57
16osu1j,I need assistance in developing an automated analysis system. Can you help me with that?," 

My company's data analysis workflow relies solely on Google Cloud's Looker. Currently, we manually export data from the company's main system into an Excel file and then upload it to Looker to demonstrate the charts and visualizations. However, this process is slow due to the large amounts of data. Therefore, I have been tasked with creating an automated process from scratch to analyze the data and display visualizations similar to Looker's interface, but with more advanced data analysis tools like Python, Pandas, Jupyter, or SQL to make the system faster.

I already have experience using Python, Pandas, Jupyter, and SQL in previous projects, but I need suggestions on how to utilize these tools for my current project. I am also willing to learn new skills if it will help me complete the project successfully.",2023-09-21 22:02:37
1678rl7,Looking for some topics for my blog that DEs would be interested in,"Please suggest topics that you are looking forward to or really curios to learn.

If interested in existing content you can find here https://www.junaideffendi.com/.


Thanks",2023-09-01 14:31:41
1666j4y,Super advanced SQL,"I am also partly new to DE world, but this question has been busying my mind for a long time already. Does any one of you who has got experience in the field think the following super duper god level advanced topics are necessary to learn or have used them and said “nah, not necessarily needed”:

PL/pgSQL;
PL/Python,Java,whatever language it is;
Functions, procedures, routines;
Triggers;
Rule system;
RLS, CLS, access security things;
Extensions;
IPC;
Partitioning, Sharding
and etc.

Or do you think only some of these are a must to learn and others just for show off?",2023-08-31 09:42:50
15zxjqz,Empowering Business Success: The Role of Data Engineering and Management in Customer Analytics,"In the age of information, data has emerged as a pivotal asset for businesses across all sectors. The ability to harness this data effectively through data engineering and data management is paramount for accurate insights and informed decision-making. Customer analytics, fueled by data engineering and management services, has revolutionized the way companies understand and engage with their clientele. In this article, we delve into the significance of data engineering and management services in customer analytics, and how these processes are evolving through data modernization services.

&#x200B;

**Data Engineering: The Foundation of Insights**

Data engineering lays the groundwork for extracting, transforming, and loading data into systems that enable meaningful analysis. It involves the technical processes of collecting, ingesting, and structuring raw data from various sources into formats suitable for analysis. Data engineering services are crucial to ensure data quality, reliability, and accessibility.

**Data Collection and Ingestion:** Data engineering begins with the collection and ingestion of data from diverse sources, such as customer interactions, online behaviors, transactions, and social media. This raw data may be stored in data lakes or warehouses for further processing.

**Data Transformation:** Raw data is often messy and unstructured. Data engineering involves transforming this data into a structured and usable format. This process includes data cleansing, validation, and enrichment to ensure accuracy and consistency.

**Data Integration:** Businesses gather data from a multitude of sources, including web analytics, CRM systems, and third-party vendors. Data engineering services integrate these disparate datasets, creating a unified view of customer interactions and behaviors.

**Data Storage:** Efficient data storage solutions, like data lakes and warehouses, are designed to handle the vast amounts of data generated daily. Data engineering ensures that data is stored in a way that allows for easy retrieval and analysis.

&#x200B;

**Data Management: Nurturing Business Insights**

Data management is the practice of organizing, storing, and safeguarding data to ensure its accuracy, accessibility, and usability. Effective data management is vital for making strategic business decisions and improving overall operational efficiency.

**Data Governance:** Data management services establish policies and procedures for data usage, privacy, and security. This ensures compliance with data regulations and builds customer trust.

**Data Quality:** Maintaining high data quality is essential for meaningful analytics. Data management involves processes to identify and rectify errors, inconsistencies, and duplications in the data.

**Data Security:** Businesses handle sensitive customer information. Data management ensures that data is protected against unauthorized access, breaches, and cyber threats.

**Accessibility:** Data management services provide controlled access to data for authorized personnel. This accessibility accelerates decision-making by enabling quick access to the right data.

&#x200B;

**Customer Analytics: Understanding and Engaging Customers**

Customer analytics is the process of analyzing customer data to gain insights into their preferences, behaviors, and needs. The goal is to develop strategies that enhance customer engagement, satisfaction, and loyalty.

**Personalization:** Customer analytics services utilize data to create personalized experiences for customers. By understanding their preferences and behaviors, businesses can deliver targeted content and offerings.

**Segmentation:** Segmentation divides customers into groups based on shared characteristics. This enables businesses to tailor marketing efforts and strategies to each segment's unique needs.

**Predictive Analytics:** Customer analytics employs predictive models to forecast future customer behaviors and trends. This information assists businesses in proactive decision-making.

**Churn Prediction:** Businesses can identify customers who are likely to churn (stop using their services) using customer analytics. This allows companies to take preventive measures to retain valuable clientele.

&#x200B;

**The Role of Data Modernization: Ensuring Relevance**

In the era of rapidly evolving technology, data modernization services have become indispensable. They involve upgrading existing data infrastructure and processes to meet the demands of modern analytics and insights generation.

**Cloud Adoption:** Data modernization often involves migrating data and operations to cloud platforms. Cloud-based solutions offer scalability, agility, and cost-efficiency.

**Real-time Analytics:** Data modernization services enable real-time data processing and analytics, allowing businesses to make decisions based on up-to-the-minute information.

**Automation:** Modernizing data processes often involves implementing automation tools that streamline data collection, transformation, and analysis, reducing manual intervention and improving efficiency.

**Hybrid Data Ecosystems:** Data modernization supports the integration of on-premises and cloud-based data systems, allowing businesses to leverage existing investments while harnessing the benefits of the cloud.

&#x200B;

**Conclusion: The Path to Informed Business Success**

In today's data-driven landscape, the ability to extract actionable insights from data is a critical factor for business success. Data engineering and data management services provide the foundation for accurate, reliable, and accessible data, which is essential for customer analytics. By understanding customer behaviors, preferences, and trends, businesses can tailor their strategies and offerings to enhance customer engagement and satisfaction.

Furthermore, the concept of data modernization introduces the element of agility and relevance. As technology continues to advance, businesses must adapt their data processes to remain competitive. Embracing cloud solutions, automation, and real-time analytics through data modernization services ensures that companies stay at the forefront of data-driven innovation.

In conclusion, data engineering, data management, and data modernization are the pillars that support customer analytics, driving business growth and success. By investing in these services, businesses lay the groundwork for informed decision-making, personalized customer experiences, and sustainable competitive advantage in the ever-evolving digital landscape.",2023-08-24 10:01:38
15n5fl2,What positions do you think id be a good for/salary range I should be looking for,"Hello everyone!  

I will be graduating Wgu's Data Analytics program in about a month. My list of certs include CompTIA project+, CompTIA data+ AWS Cloud practitioner, Microsoft's Data Engineering on Azure certification. I have a firm grasp on dimensional modeling, data architecture, ETL pipelines, predictive analytics with machine learning, python ,R, sql, power bi, tableau, etc. I also have some experience writing api's for full stack applications that I gained from boot camps before I attended university.  This was using the MERN stack with some C# thrown in.  Ideally Id like to get into data engineering, a cloud-heavy role, or machine learning as I have had some coursework in those subject areas and would love to expand my skills in those areas. I also attached my degree requirements that list my coursework so you can see what I've been working on. My big question is what roles do you think would be the best fit for in the data space and what salary would be reasonable to expect in this current job market? My brother can get me a position at the consulting firm he works at but its for 55-65k starting as a BI analyst essentially. I live in the PNW in the Boise area and this seems rather low for cost of living nowadays. What do you guys think?

&#x200B;

&#x200B;

&#x200B;

https://preview.redd.it/ihinqclul8hb1.png?width=1920&format=png&auto=webp&s=58da94cbffeed1ab75bfd877025ec1f323e7bc09",2023-08-10 07:31:32
15ln3gv,Imposter Syndrome,"Hello house, 

Let me start like this I currently work as a senior data analyst for my company and I have experience in data science with different project and research paper.

I have always wanted to pivot to data engineering for a while now, taken multiple certifications on Azure and other resources.

I got a job as a data engineer in an oil and gas company. I cleared all the exams(3) and interviews (2) with flying colors.

I had a deep knowledge in all what data engineering is all about, the only issue is I have never worked as a data engineer 👨🏽‍💻 much is expected from me with the high level I performed but I feel like this was because I have verse experience in data analysis and consulting which helped me.

I primary use Azure DataBricks and ADF cloud, delta lake table(unity catalog) for warehouse and ADLG2 staging storage.

SSIS, SSRS, SSAS and SSMS for private and local.

Power BI & Datamarts for visualization.

Is this enough or is there something more to data engineering, I might sound silly but I feel there expectation of me is really high.

Let me add I also work as a data engineer in a publishing company in the US as an author/writer.",2023-08-08 16:32:13
14x8uz2,Getting hard to become a DE,"Hi, I have experience doing web scraping in Python + MongoDB, I'm trying to watch videos, and so, but I can't. I know, the ammount of info available should be enough, but its hard for me. I have a good programming background I think  


The point is, anyone need help in a project? Maybe I could do some DE-like work with the thinks that I already now while I learn new things, I think that maybe could be a good way to learn about Data Engineering. If not, maybe some of you could give me some tips and if you were in my situation before, how you end becoming a DE? It seems imposible and the only I see is the days going and going and I feel that I'm learning Nothing.  


Thank you very much guys ",2023-07-12 00:40:28
13vyyk6,Welcoming bit.io to Databricks: Investing in the Developer Experience,N/A,2023-05-30 19:04:17
135esx0,The intersection of DE and Physics,"Hi everyone,

I have a question in my mind for a while. You might consider it a little bit strange. My question is ""Are there any intersecting areas of data engineering and physics?"". Let me clarify what does my question mean.

I am currently pursuing a bachelor's in physics but am primarily interested in software engineering topics (particularly data engineering). Considering that, I want to go on as a data engineer after my graduation. The problem here is that I do not want to lose all information I have learned in my physics courses. I am wondering if my physics knowledge make me some advantages in the field. What would be the best for me to get the most benefits of my physics and math knowledge?",2023-05-02 08:06:48
12uklg5,Noah Gift - Speed Up Python dramatically With CUDA GPU,N/A,2023-04-21 22:19:39
12plo5v,Synchronizing data using a new message broker: A case study.,[https://medium.com/gastromatic/synchronizing-data-using-memphis-dev-a-case-study-2e6e9a7b5512](https://medium.com/gastromatic/synchronizing-data-using-memphis-dev-a-case-study-2e6e9a7b5512),2023-04-17 15:34:44
12pk6gn,We need feedback,"Hi, community!

We are working on a new CDP product. Our product is focused on data engineers. For example, Python support! :)

Our concept is we give the platform you provide the code. 

Does somebody have 15 minutes this week for a quick call? We need some feedback from the community!

Thanks!",2023-04-17 15:02:57
12ocwja,thinking of studying set theory,do you think that would help in a DE job?,2023-04-16 15:19:02
12jjuem,Have you heard about Data Moshpit?,"I don't know, but this idea seems to me like one of the best data events I've heard of. 

[http://www.data-moshpit.com/](http://www.data-moshpit.com/)",2023-04-12 11:59:19
12gzzs7,How are the quality and quantity of DE applicants nowadays compared to years past? Current demand?,"Is the demand of data engineers still growing in this current market environment? I heard there is still a severe shortage of qualified applicants for data engineering roles. I'm not sure how the current layoffs are affecting DE and the supply.

I am currently enrolled in a DE program and have the option to switch to full time for real client projects, but that will require me to leave my job, but I can finish the program faster with real experience on my resume. Overall I am very confident in my current abilities as I had past experience with coding so the courses and topics were all intuitive. I basically want to know what the competition and demand looks like, if people are still looking I want to switch to full time in the next 2 months, otherwise I will keep my job and take it slow.",2023-04-09 23:40:19
1201ggk,Interview at Square for Data Engineering,"Hi guys,

I have an interview scheduled with Square for a data engineer role. Can anyone suggest how to prepare for the 45 mins SQL round, manager round, 4 back to back rounds? Any resources to prepare for the SQL round would be helpful. Please ping me if anyone has gone through the interview round at square. Thanks in advance",2023-03-23 23:27:26
11zx87o,Electrical Engineer Transitioning to Data Engineering,"I am in the process of transitioning from an electrical engineering career to that of a data engineer and I wanted to get some feedback on my current roadmap, possible pitfalls, and areas I should focus on more/less.

A little background on myself, I am a mid-30s electrical engineer in the power generation simulation industry. I am the principal simulation engineer at my current company (very small). I've been considering a career change for a while now as the current field I am in is very niche and unfortunately the software I use is extremely industry specific. I wear many hats at my current job and one of them is dealing with all the company data. It's on a relatively small scale and is a very minor part of my job, but organizing data, creating dashboards, and producing action items based on the data is one of the few parts of my job I really enjoy. This has led me to the decision to pursue a career change to data engineering (that along with it being more transferable across industries and having higher potential compensation).

Beside dealing with databases for I/O and DCS architecture, there aren't very many transferrable technical skills (other than generic ones like problem solving, logic outlining, etc.). So, in my free and downtime I've been grind skills towards data engineering. I've been pursuing courses in Udemy and trying to get through as many daily challenges as I can. So far I'm through with Harvard CS50, a Python course (100 days of Python), and a SQL course. I feel confident in all three areas. 

My next step was going to be a short course in Linux and then a generalized data engineering course for fundamentals before I start focusing on distributed systems, cloud, orchestration, containers, compute (snowflake/databricks), etc.

Where I expect to run into issues is as someone in their mid-30s with 12 years of experience in a different field applying for just above entry level jobs. I plan on building a portfolio of projects before starting applications later this year but I don't think that will be enough. So, my plan is to start and own a data engineering division at my company and find ways to incorporate it into our work. This will help me build out my portfolio, let me add some experience to my resume so I'm not applying for jobs with zero experience in the field, and also shows some initiative. 

I was hoping to get some feedback from you all on my current plan, where I'm going wrong (I'm sure there is a lot), and what you think I can do to improve and expedite this process. Or if it's even a smart process to be undertaking, haha. Thanks for the help in advance.

TLDR: Older electrical engineer attempting to transition to data engineering and badly needing help on what steps to take. Cheers.",2023-03-23 21:00:25
11tz3ik,Knowing only powershell is enough?,I don't know Shell/Bash can i replace it only with powershell?,2023-03-17 18:17:43
11sz5li,Why you might not even need a data platform,N/A,2023-03-16 16:38:42
11jskte,"Potentially switching to Data Engineer, please advice :)","Hi,

I'm currently employed as (semi) Solution Architect where I work mostly with a specific product suite. The work mostly consist of analyzing, troubleshooting, maintaining, implementing, upgrading and migrating customer environments with some customer support as well for simpler tasks.

&nbsp;

In my current team, things are moving slow and we are not adapting to new technologies (such as proper infrastructure orchestration, docker, workflow pipelines or monitoring and so on). I've tried multiple times but some of the seniors like to have it ""as it has always been"" and I can't seem to get things to change since they have strong voices in the company. It saddens me since I believe that working within IT you kinda need to be aware of ""new"" technologies and it's good to be at least curious about it. What I do like about my current job is doing the backend stuff (fixing broken things, for eg application config files or apache/nginx problems, sorting out DB problems, optimization and so on).

&nbsp;

I recently got 'head hunted' to an interview as a Data Engineer. But I'm unsure if it will be a good fit. Reading up on the career it looks to be quite different pending on where you work. Some are deeming the role as a slave by GUI (spark?) to help customers get out data, while others talk more about pipelines, fixing broken pipelines, setting up models and making sure customers get what they need by supporting the backend. 

&nbsp;

I find that I have become more interested in infrastructure as code and automation with Docker & Kubernetes. Going the data engineering route, will that take me further away from that? How much are you able to use code in your daily work (ie Python or something equivalent).

&nbsp;

What possible routes is it after going down the Data Engineering route if I find in a few years that it isn't for me? 

&nbsp;

**TL:DR**
&nbsp;

I want to do backend stuff and solve problems, interested in infrastructure as code, automation and Python in general. Is Data Engineering a good career choice and what paths comes down the road from this role?",2023-03-06 08:05:46
11j0fcx,Current Scenario for Data Engineering Positions,Hello Everyone I would like to know how is the current market situation for Data Engineering Positions as a Junior Engineer with one year of experience I find it difficult to spot opportunities. If anyone has recently successfully gotten opportunities would love to hear what are things required to get more interview opportunities.,2023-03-05 15:57:56
1ap0ygf,dlt (Data Load Tool) adds Databricks and Azure Synapse destinations,"Hey folks!

dlt (data load tool) library added **Databricks** and **Azure** **Synapse** destinations. Now you too can benefit from schema inference, evolution, management, column level lineage and data contracts.

**How to try?**

Simply run **pip install dlt** and you're halfway there. For the detailed setup, check out the docs:

* [Databricks](https://dlthub.com/docs/dlt-ecosystem/destinations/databricks). Do you already have the dlt “delta live tables” Installed? see [Here](https://www.notion.so/Databricks-notebook-instructions-980832a90fab4a98b6c8aa010d47646e?pvs=21)
* [Azure synapse](https://dlthub.com/docs/dlt-ecosystem/destinations/synapse)
* Want to try on duckdb? here's a colab [notebook](https://colab.research.google.com/drive/1H6HKFi-U1V4p0afVucw_Jzv1oiFbH2bu#scrollTo=e4y4sQ78P_OM)

**I'm eager to hear your thoughts:**

* Have you worked with dlt (data load tool) and Databricks before?
* Did you try running data load tool on databricks but got delta live tables already installed? this [guide](https://www.notion.so/Databricks-notebook-instructions-980832a90fab4a98b6c8aa010d47646e?pvs=21) might help
* How do you currently take data from apis into Databricks or Synapse? Do you use python or something else?
* Any pain points you think this integration could solve?

**Community-powered**

Finally, shoutout to Evan and his colleagues from [swishbi.com](http://swishbi.com/) for their hard work on the Databricks integration. Collaborations like these are what push the envelope forward in our field.

Do you have some asks from dlt, or interesting stories or use cases you want to tell the world about? Tell us in the  [\#sharing-and-contributing](https://dlthub.com/community) slack channel

Looking forward to your insights and discussions!",2024-02-12 14:15:01
17w64wu,would these skills benefit me as a data engineer ?,"
Power bi 

Making APIs

Web scraping

Bot development",2023-11-15 22:40:11
17lt848,Noob data engineer up against it,"Hey all,

Have a tight deadline and struggling to figure this out as a relatively new engineer (business background).

I need to compute change data for marketing customer data… the data is in the form of a materialized DBT table in bigquery. After it updates, I have a python based process that is orchestrated in dagster that creates a dataframe from that table, and compares it to a csv copy of the table from the last run. I need to format user update requests from this changed data, and send all new rows and any changed values from an existing row.

I’m having a bit of trouble successfully implementing this change data computation using pandas… is there any better way or best strategy to approach this?",2023-11-02 02:22:12
169r2hc,Are the supply & demand of data roles is imbalanced??,"Ken jee is Data scientist. He has been doing data work from 7 years. In my opinion he is good at explaining how the data industry works and seems legitimate.

Although I have a question to ask, Is this true the supply is very less in data engineer roles? I even heard from Mike west (who is data engineer, and working for several years with big companies, also creator), that data engineer is probably the most sexiest job than data scientist or any other tech jobs.

I just want to start a discussion here regarding this. The reason is that many people complain a lot that the job market is dry and there aren't many jobs available. I genuinely believe it's because of their competence as data engineers.

Source: https://youtube.com/shorts/L6lXKdP4Qbg?si=fjUKofmNk7DE75yf",2023-09-04 12:46:07
120ev9g,Why You Should Become A Data Product Manager In 2023,N/A,2023-03-24 09:18:39
1alyjfc,What will be your preparation strategy for FAANG ?,"Hello talented people of this group . I wanted to understand how you guys prepared for interviews for FAANG / or any top tier company as a data engineer. 

Any tips and tricks",2024-02-08 15:41:03
16hqwpl,Airbyte made huge progress on Postgres replication performance,N/A,2023-09-13 16:02:06
18z3xr8,Is the demand of data engineers dramatically reduced?,"I have a great skill set and expertise as a data engineer/scientist, but my country(Nepal) has little to none openings.

Is reddit cool to find a potential employer? Can we connect on LinkedIn?

I bet, just one interview and you'll see my potential value as a Data Engineer/Scientist.",2024-01-05 10:36:49
11op8hf,how to chatGPT proof coding interviews,I'm a senior engineer and am interviewing several candidates over the next couple of weeks. What are some things you guys would do to make the coding interview chatGPT proof/ make it hard to use chatGPT?,2023-03-11 17:06:12
16yf8wu,Data Engineers - Backbone of MDS,N/A,2023-10-03 01:49:17
13gevpu,Whats the view on Apache Iceberg?,Views will be critical to understand what the community thinks about Apache Iceberg and whether it can dethrone Parquet.,2023-05-13 11:20:17
12qx3y5,Semantic layer meets OpenAI—and now your business users can query in plain English on Slack,N/A,2023-04-18 17:20:07
114ewfs,Europe data salary benchmark 2023,N/A,2023-02-17 08:58:32
19dutmk,Hello,"Hello, is it too late for a 23 yo to go for data engineering, I think need at least 5 years of studying, I don’t do anything right now. Is investing the next 5 years in data engineering or data science a good deal that i will benefit from in the future?",2024-01-23 18:14:11
18aw6bb,What parts of Data Engineering do you think will be automated by AI?,"As title...

I've been building [phidata](https://github.com/phidatahq/phidata)'s Junior DE for automating day-to-day DE tasks using GPT4. Its not perfect but helps me bootstrap python code for data analysis, which saves me an hour or so here and there. 

I'm reasonable pleased with it. Will give it a 6/10 in current form.

What parts do you think will be automated?

https://reddit.com/link/18aw6bb/video/and4vbsssc4c1/player",2023-12-04 22:25:39
17zmuko,"Which one has higher pay is it azure data engineer or GCP data engineer in India and in USA? Experienced people kindly also mention the pay , we ll able to know how much we ll be getting?","I am just confused between which one to pick , that's why a bit of dilemma becuz I felt azure is easy with all the UI stuff, but my company is using GCP which involves a lot of writing code for pipelines when compared to azure.",2023-11-20 11:53:03
1alffov,Is switching to Python from Scala/Java in big data worth it?,"I recently found out that my manager wants to assign me to a project in my company that will be in PySpark. The thing is that I have never really worked in Python, I've always been a Java/Scala dev. Should I agree to participate in that project, or should I stick with Scala/Java? Maybe if they don't find a Scala project for me, I'll have to find another job, idk.

I'd switch to Python, but it feels to me like some toy language that is a wrapper around something written in other languages, I've seen code bases in Python, they get messy, there's just a bunch of functions in each file, instead of having a class per each file with some name that tells you what it does. Also the absence of type safety is kind of annoying. Today I also found a bug in Apache Iceberg for which there's a workaround in Scala, but no workaround in Python, which made me think that Scala is better, once again.

On the other hand, I've been thinking that I should probably delve deeper into Airflow, I've always just kind of coded in Spark, maybe I'd write some simple stuff in Airflow, but I never actually coded anything complicated in it. Airflow is in Python and there's pretty much no alternatives, because everyone uses it. Can one consider themselves a data engineer without actually knowing much of Airflow?

I know some people will probably say ""Why not learn both?"" or whatever. In my experience it's not a good idea, because sometimes, in job interviews they ask in-depth questions about a particular language, for example how the GC works in the JVM, you may even be given some code and asked if it compiles... etc. You may have to write some stuff in Java or Scala using functional programming, and if you have been focusing on Python for a few months, you literally forget certain details of the syntax and tell the interviewer ""Sorry, I forgot how to do this one little thing in Scala, I need to google, because I've been doing some python recently"" and they may think that you are not the right fit for this job, you know?

What's your opinion on all this? As you can see I lean towards sticking with Scala, but I know almost nothing about Python, so maybe you guys can convince me that I should switch.",2024-02-07 22:19:45
19371i1,Overwhelmed + Impostor Syndrome in DE position,"Hi pals 

So, last month I started a position as a data engineer at this big company and it's something really new to me. 

For context, I have about 1 year of experience and at my last job I mostly worked in azure data factory, mantaining an ETL fmk with that + sql server + databricks (a lot of sql, a bit of pyspark). I also did some monitoring reports in powerbi. 

The company is transitioning a lot of erp data into the cloud, and there's a big project involving different countries but at the same time it still relies on outsourcing for developing features for this other platform so there's some silos there and I feel so overwhelmed cause I was given the task to develop a feature but I feel like i have't even grasped the whole architecture and really wasn't aware of the dimension of it all... I'm also working on a language I haven't worked in a while so that's also affecting my impostor syndrome.

It's been a month and I feel like I have made very small progress, any advice on how to deal with this? Any of you been in a similar situation before?",2024-01-10 12:20:58
191s99i,Is the data team a data producer or a consumer?,"I had this discussion recently with somebody on LinkedIn, but I believe we can have a much better discussion here.

I believe we are more consumers than producers of data.

\- You depend on the data the developers write in PostgreSQL.  
\- You depend on the data salespeople input in Salesforce.  
\- You depend on the data accountants add to Xero.

If these people produce low-quality data, there’s almost no chance for you to build high-quality data products.

Trying to build trustworthy dashboards, models, and insights is like relying on a crappy third-party API to build an outstanding app.

You need to communicate that idea clearly with stakeholders. They need to understand how their actions affect the results you produce.

I'm not saying data engineers are not producers for analysts, but I think we are primarily consumers.

What's your opinion?",2024-01-08 18:35:51
123d9st,Apache Airflow vs Dagster - side by side comparison,N/A,2023-03-27 06:01:08
1ahggrt,"As a new person into DE, should I still chase this despite the market?","I'm a young adult entering a computer science college in october, I'm scared that it's gonna hard to get a job despite efforts and having a internship. I'm mostly concentrated on data engineering, other than data science/analysis, so I don't know surely if its a saturated subdomain yet like web development. I heard people applying to 300 applications and getting only one job accepted. Should I still concentrate on this, hope for the better or just learn trades?  


If this post isn't unrelated to the subreddit, feel free to ignore it or diss me, I'll take it down",2024-02-02 22:40:13
17mexj6,Lord of Data,"Was thinking of deploying a new IDE to our engineering team called CodeFortress. 

thought of using pygame to write a top-down RPG that looks like Dwarf Fortress where I have a character that I can navigate with to enter different rooms or spaces and mine ore, harvest herbs, skin leftover animal hides and defeat mobs. 

Each time I complete one of those actions, there is a chance for random loot. 

One idea is to have some of that random loot be productive things I need to get done at work. 

For example, after a few pings of that pickaxe, not only did I receive 3x Ore but also a rare SQL optimization that I can apply to my codebase, or new metrics that have been added, or entire models created — that I can approve, modify, or publish to a remote branch — and then continue mining or questing or whatever else I was doing. 

Questing becomes the new pair-programming with 5 and 10 man raids that unlock rare and epic loot. 

Looting an item of greater-than-uncommon value will initiate a git workflow that results in a successful merge only after merge conflicts squashed. 

For an intern's commit to result in a successful merge, a /roll of 20 or greater must be achieved, otherwise bits must be spent to re-roll until the commit ultimately is successful. 

Code commits become completions used to train the model using the relevant prompts along the way as the user tried for a successful loot. 

Where it gets really strange is that the higher level the version number of the repository, the harder it is to down bosses in raids and defeat mobs guarding resources. 

Client deadline estimation will be driven by the level of each of our characters. 

At max level, the end game is made up of several 20-40 man raids in increasing difficulty that require weeks worth of farming and research into internal game mechanics to master. 

Sorry boss, our frontend team keeps wiping on Lord Automagus, they can't access the loot we need to patch the latest version of our platform and our customers are blowing up the support channel asking why Dishurt is still the main tank. They demand that Benjimus main tank this fight instead.",2023-11-02 21:55:03
1alrme0,Is masters necessary?,"I am a 1 YOE Java/Scala developer working for a corporate tech company located in Turkey. During my time here im trying to improve myself in used technologies in the Data Engineering field to land a more Data focused job. I heard that due to Data being a trend in general nowadays , many companies also would like candidates to have a masters degree , how true is this? Also if its true, is it worth to get a distance education degree from major european countries(I cant attend to classes if its not distance education due to my work hours). I would also love some advices about getting into Data Engineering as a beginner :)

Thank you all for your help",2024-02-08 09:09:05
194x2i4,Do you need university for data engineering,Do you need a university degree to be employable? If so is a bsc in statistics(with courses in programming and economics as well not just the theoretical maths)good?(This is what I'm pursuing).Also does it matter what grade you finish with or is having the degree enough? If you are from europe(as that is where I'm studying) your answers are especially relevant. Thank you!,2024-01-12 15:00:39
18z3cl6,Is this SQL joins cheat sheet correct?,"Hey, came across this joins cheat sheet, and it seems to me like 'album' table is not referenced in this query so how could album title be in the result. Am I missing something?

&#x200B;

https://preview.redd.it/wxtn8alvglac1.png?width=645&format=png&auto=webp&s=0b374aa7c2a7428657e255bdd85726dce3c760f7

https://preview.redd.it/oizzzwvuglac1.png?width=642&format=png&auto=webp&s=9480fffa0842d02439d7cf7fd939bf02ad40c9ce",2024-01-05 09:57:29
18udgs8,Experience with Windmill for data pipelines?,"I've spent the holidays playing around with Windmill (https://windmill.dev) and so far I'm very impressed with it. I'm wondering if anyone here have tried running it in production for some time and have some experiences to share? 

It's not a tool for data pipelines specifically, but it still seems quite fit for purpose. I actually see its generality as a big advantage for us. We're a small team and we have lots of little utilities and integrations that currently exist in the form of somewhat disorganized scripts and repos. With Windmill it seems we can bring all of that together in a single platform.",2023-12-30 11:14:48
18ie2zo,Introducing LaunchFlow: The Developer Platform Built for Python,N/A,2023-12-14 17:41:52
16xe83b,Feature Engineering,"""Seeking guidance on learning feature engineering. Should I prioritize Python libraries for feature engineering, like Scikit-learn and MLlib, while maintaining a basic understanding of machine learning, given my background in executing PySpark projects using Databricks? Any recommendations on efficient ways to dive into feature engineering?""

Kindly advise video courses and or books 

If someone has experience please share",2023-10-01 21:57:15
15jr3va,Orchestrate SQL Data Pipelines with Airflow," 

 

# Orchestrate SQL Data Pipelines with Airflow | Schedule SQL scripts with Airlfow | ETL with SQL

📷[**Blog**](https://www.reddit.com/r/dataengineering/search?q=flair_name%3A%22Blog%22&restrict_sr=1)

Vlog on how to run and schedule SQL scripts with Airflow? | SQL Data Pipelines | Airflow | 

[https://www.youtube.com/watch?v=glzj7p7Yrrs&t](https://www.youtube.com/watch?v=glzj7p7Yrrs&t=15s)

Topics covered:

* SQL Data Pipelines
* Orchestrate SQL Data Pipelines with Airflow
* Build ETL Pipelines with SQL and Airlfow

Tech Stack: **Airflow, SQL, Postgres**",2023-08-06 14:33:45
15hn2bt,is it necessary to be a data analyst or data scientist before becoming a data engineer?,"Hello programmers

I am a cs student I have a question for people who are working is it necessary to start in a previous role before being a data engineer or it is not necessary?

By the way, do you recommend the datacamp data engineer path?

sorry if my english is weird i'm not from an english speaking country :p",2023-08-04 02:03:33
15evmot,"CS student here, wanna know if Data Engineering would be a good path.","I am starting my 2nd semester at a CS course and i have been pondering about my future paths and career and i liked the ideia of Data Engineering.

On my first semester i used Python for some NLP/LLM projects and really liked it, usually more applied stuff using established frameworks, i also got some nice stuff with professors lined up about this topic (ML/AI/NLP)

I also heard some quite interesting Data Engineering projects (pipelines, cloud hosting and Pyspark to creating continous data flows) and liked them.

So then i suppose the closest things to my interests would be a Machine Learning Engineer, but then i heard MLE jobs are not only fewer than DE but also usually tougher on academic requeriments (big emphasis on having a masters), then i learned of DE which has more job openings, usually less requiring of academic titles so i guess that more fitting.

Also many DE jobs ask for ML experience so all my interests and projects could go towards building a CV of a DE with knowledge of ML 

I mainly want to ask you guys hows the job market for Data engineering? one of the reasons i dont want to get into Data Science is not only preferring a more engineer and software role but also the fact that it seens to be a quite saturated market, is DE less saturated than DS? what do you guys think?

&#x200B;",2023-07-31 23:28:52
1909k7c,"In your opinion, are there different types/classes of data engineers?","It’s understood that data engineering is a subset of software engineering, but as time goes on, it seems there are more distinctions forming around different types?

For example, Starbucks was hiring for a senior data engineer who would focus more on semantics, ontologies, metadata, cataloging, and master data management. It didn’t mention building pipelines, but was expected to have previous experience doing so.

Then you have data engineers who are effectively data-flavored forms of devops, and focus more on platform engineering. 

Likewise, business focused front-end data engineers, who were previously labeled business intelligence engineers, are now commonly referred to as analytics engineers.

Lastly, what about data engineers who aren’t necessarily creating new pipelines, but optimizing existing ones? Or data engineers who work in shops who have opted to buy (Fivetran, ADF, paid Airbyte) instead of build?

Just curious is all.",2024-01-06 20:40:04
1akisp6,How AI can automate web scraping,N/A,2024-02-06 19:52:46
18h8vaw,What it's like watching performance tests,N/A,2023-12-13 04:46:30
17u0fn6,Should I Finish my Masters in DS or Just Get Some Certifications/Upskill?,"I have 3 classes remaining after this semester to finish my MSDS. I have 3 YOE working in low quality DS/DE roles. 

Currently working a really trash DE job for the past 3 months that is just a 50 hour a week on-site BI role for $110K. Previously left last company after 6 months which was also a trash job where my DE skills stagnated. 

Should I quit my MSDS, that has been of marginal quality so far, in order to allocate those ~10 hours per week to upskilling via certifications (AWS Data Engineer, Databricks Spark Certification, Astronomer Airflow Certification) and leetcode? 

My plan is to interview Q1 next year to try and save my career with an actual DE role using in demand technologies.",2023-11-13 02:17:15
1554o9y,"i have encountered the weirdest process of de interviews ever, and is there anybody want to team up and mock?","so i applied to a data engineer job and by far i have passed 3 rounds (phone screen, 1st vo, 2nd vo), now they invite me to attend on site next week, to go through another 4 rounds of interviews.....

i feel like they want to kill me

so by far i havent been tested a single question of python or sql, and its a very surprising thing to me

they tailing me about pipeline design, system design and api design, yes, you are seeing this right, api design, as if its interview for sde not de

so now im trying to prepare for the next, and also the final round. i thought if anybody is interested or going through similar preparation process, maybe we can do a mock for each other. i mainly want to look for buddies who wants to do prep on design and not the sql/python codings.

add me on discord if you wanna team: Elaina#5305 ",2023-07-20 21:59:57
13yelms,A question for all data engineers:,What work did you do as interns/junior-level DEs and how did it change as you progressed?,2023-06-02 14:40:28
164ddm5,Should alarm bells be ringing if you're invested in the MDS ?,"Question as same as the title, should we be worried ? This is mostly in reference to the Modern Data Stack set of tools in particular DBT , Fivetran , Meltano, Airbyte -  (Not so much snowflake ) .  If you read the blogs and follow Data Twitter/Reddit it would appear these companies are massively over valued and are in desperate need of revenue to keep going. With the dbt fiasco over the last two weeks as a prime example. Is there any chance  they could go under and take their open source offerings with them or is just a panic about nothing from several quarters ?",2023-08-29 09:14:12
16iyjo9,Layed off 2 months ago and have been grinding leetcode to get another staff/senior role,"But only 1/3rd of the interviews are traditional leedcode (data structures) and aglos. The other are coding but more architecture or erroneous python string manipulation. 

I feel like with the market now you not only have to finish the leetcode but also faster than the market, any ideas on how to target non-leetcode companies? Or I am just gonna stay unemployed until I become the best leetcoder out there ? Very worried about how to pay the bills , and it just feels like it’s me vs leetcode and driving me crazy. Sorry, I know this is more of a vent just want to hear if anyone has any advice bouncing back in this market as a sr de.

I just feel like there is such a wide amount of information I am quizzed on it’s hard to prioritize my time on what is the best roi. I get it - if your a leet code hero it helps but I’m not certain I’ll ever be good as some at leet coders and I think I am a great data engineer and I really enjoy the work so just feeling a bit lost on how to tactically Re-enter the career",2023-09-15 00:20:43
1alx2ph,Want to upskill and get into data engineering,"Hello all,

I have been working at Deloitte as my 1st job for 3 years now. I work in a software dev team, but i mostly have worked on SAP HANA, PL/SQL, and have dabbled a bit in Snowflake, Redshift, Glue, etc

Things are saturating, and I want to shift for better learning ,and obviously a better pay.

But I don't want to continue with current work, as the technology i work on is a bit dated. I don't like React and Node based development. (I have tried, gone through courses, i couldn't create any interest in it sadly). I know I love databases and like to solve the issues in them, especially SQL.

So I have come to the conclusion that Data engineering is probably a good job to aim for. But I have to upskill for it while working. How should I go about it? From my research I think I have to work on NoSQL, master my SQL concepts, distributed systems, Linux, and also tech like Kafka, Spark, etc 

Also, do these jobs pay well? Or am I going in the wrong direction financially speaking?",2024-02-08 14:34:41
1ajopph,"What do you all think of smaller more boutique firms and businesses outsourcing their open roles to Bengaluru/Bangalore, India?","If you were entry to mid-level and you find that these firms passed you over during your initial job search, are you more likely to look for roles at these companies further down the line?

If the US job market is flooded with capable people with FAANG experience and/or 5+ years experience, from layoffs who aren't able to find jobs either; are these people more likely to pool together and form businesses? What are the implications of outsourcing to the degree that I'm seeing for the US labor market and data engineering overall?",2024-02-05 19:11:15
1aieg60,Data Engineer Academy Looking To Partner With Employees For Referral!,"We're always looking to bring value to our clients and now we are looking to give senior employees currently working at tech companies the opportunity to earn via internal referral programs!  


If you're interested in earning side income, DM me with your name, YOE, title and current company.  


Cheers!",2024-02-04 03:38:16
1ac74ox,Is the job market up for DE in south east Asia now?,"How is the job market for data engineering in south east Asia now? Or is it still bad as last year? I have been on market for six months now and not receiving any enquiries(not even HR calls).


PS: I am a foreigner to these countries and so my job chances are limited to the opportunities which can sponsor visas in those countries.",2024-01-27 09:28:20
19e5hpp,Are you employed ?,"

[View Poll](https://www.reddit.com/poll/19e5hpp)",2024-01-24 01:52:38
19c3sst,How valuable are Certifications?,"I have the Databricks ML / DE Professional Certifications.

I'm currently a DE wanting to move to a better (higher paying) DE position or an MLE position. How are the certifications regarded by the companies, especially at FAANG? If they are not that well regarded, what else should I be focusing on to transition to an MLE position?",2024-01-21 13:49:25
19892yb,Choicing beetween DE vs DS,"Why starting as a DE might be better than DS, considering maybe going to MLE in the future? I like the DE area and I think maybe strong skills like a DE combined with enough DS could be the key and could it be that DE will be more valued in the future as DS is today?",2024-01-16 17:53:10
197sqkq,[Advice] - Computer Science Degree,"I'm currently in honey moon with de (maybe some of you been here idk) and also in the final year of my Bachelor's degree in Civil Engineering. Started working as a Data Engineer in a startup, and this experience has sparked a serious interest in computer science for me. I'm thinking about pursuing a degree in Computer Science after I graduate. From what I've looked into, it would take me about two more years to complete, which means I'd have dual degrees in Civil Engineering and Computer Science by 2026.

I seek for advice, will I be wasting my time?",2024-01-16 03:27:54
1974l0z,Managed Dagster Hosting,"I am curious about the ways of deploying Dagster which requires the least amount of knowledge and hassle to manage (load monitoring, version updating, etc.) the underlying infrastructure, i.e. focus on writing pipelines in Dagster. Setting up a managed K8s cluster and deploying Dagster there seems error-prone for a small unexperienced team.

Logically, Dagster Cloud can be used to get a managed version of Dagster. However, for us, its pricing model is too expensive.",2024-01-15 09:15:05
190trrs,Dbt developer certification,"I am planning to take dbt certification exam , can you recommend any resources?",2024-01-07 14:51:41
18clw9c,Which dialect of SQL should I learn?,"Hello.

I work for a government that host its data in a datalake in HPE, and we connect to it using Apache Drill and our IDE is DBeaver Enterprise Edition.

But no one in our team can tell me what exact dialect of SQL I should be learning, some say it is MySQL, some say it is HQL.

So I am confused as to which SQL/HQL I should be learning? I have a background in Oracle SQL.

&#x200B;

Thanks. ",2023-12-07 03:06:30
18anene,Continue with DA or switch to DE?,"Hi Redditors, I’m looking to ideally contract (in UK) or freelance remotely from anywhere in the world to the extent I can make a living or build a small agency within the next 2-3 years. 

I’m currently a data analyst with 1 year’s worth of experience with mostly Excel, SQL and PowerBI.

I’m also doing a data engineering boot camp and learning Python with the Crash Course Python book. 

I also publish my learnings on medium to stand out a little in the job market, I was previously in healthcare for a few years before my role as a DA. 

I’m looking for my next opportunity now, is it better to go for DA roles and get better at that (1,270 jobs on LinkedIn in UK) or DE roles? (2,069 jobs) I’m also slightly worried about the competition in DA entry - mid level jobs. 

I’m also a bit worried I’m not technically adept enough for a DE role right now even though that’s what I’d prefer. However within another 6 months I should be alright. 

Also is freelancing remotely as a DA much easier than a DE, or now that there is an increased uptake of cloud technology, the location of DE’s don’t matter as such anymore so freelancing and contracting will be just as fine? 

I’m also getting on in age just a little bit and would like to work for another 10-15 years max before “retiring early”. Therefore I’m not really looking to climb the corporate ladder, I’d just like to use these skills to earn significantly more in the next few years. Any advice on navigating this crossroad would be greatly appreciated!",2023-12-04 16:06:52
184oosh,Transitioning into Data Engineering,"Hi All!

I currently have 3.5 years experience in primarily Teradata Architecture/ Development and looking to transition into Data Engineering. My skillset is: Teradata SQL, Query Performance Tuning, writing Teradata BTEQ scripts, Unix, AutoSys, basic Python). I briefly dabbled little bit with Hive. 

I mainly work in a Teradata shop and want to expand into more diverse data engineering tech stack. I understand that most interviews are based on sql+python+data modeling plus questions around my experience. 

Can you point out which areas in Python to focus for interviews? I don’t have much industry experience in Python in my current role. What courses would you suggest to get a solid recap of Python?

Also which data structures and algorithms you suggest to learn/practice? Any resources on here ?

Thanks!",2023-11-26 23:45:09
17pxwqm,What do you think are key things every data engineer should know about the modern data stack?,I will be doing a presentation on 'mds' so I would appreciate any insight in case I forgot something important,2023-11-07 16:18:34
17318vx,1TB enough space for Data Engineering/Machine Learning?,"Hey guys,

I wanna upgrade to an Western Digital SN850X 1TB and I wonder if 1TB is enough or should I go for 2TB?  
I wonder if big SSD's are needed these days since everything is cloud now?  
",2023-10-08 15:00:17
170dfah,Think I can learn spark+scala at the same time?,"I'm a DE with 4+ YOE, I work mostly with SQL, python, typescript and sometimes pyspark, but the latter only at a superficial level.

For career purposes I wanna deepen my spark knowledge and was recommended the spark bundle course from rockthejvm. However, the course is in scala and not pyspark.

Do you think it's possible to follow the course properly without any experience in scala, and just figure it out on the go?

Let me know what you think!",2023-10-05 09:22:25
16wghdl,AWS Glue vs Azure Data Factory,What are the strengths and weaknesses of AWS Glue vs ADF?,2023-09-30 20:08:30
16jeb6m,Traps and Pitfalls of Using SQL with Jinja,N/A,2023-09-15 14:04:24
16235rv,"Hi Everyone, I am learning Data Engineering and made my first project using help from google and chatGPT",[https://github.com/ksanjeev284/Data-Ingestion-Pipeline](https://github.com/ksanjeev284/Data-Ingestion-Pipeline) \- Please check and advise how I can improve and learn more as I try to get into Data Engineering.,2023-08-26 18:46:29
15zso1l,What to prep for a Director of DE position,"I'm applying for a Director of Data Engineering for a large e-commerce company.  I'm currently a Director of Software Engineering (not DE) at a reputable medium sized company with interesting data and non-data related projects under my belt.  I'm interested in this role for some career and some personal reasons.

I'm trying to prep for the interviews, and my weakest aspect is actually core technical data engineering.  My SQL skill is rusty, my Spark + Snowflake experience is mid-level.  However my people management (both up and down), project management, presentation skills, solution architecture, mentorship, career development, infrastructure+ devops knowledge, fullstack knowledge are great.

I would like to think that at Director level low level DE knowledge (such as optimizing SQL statements) is less important and strategic management skills are more important.  I would like to hear what other Directors or Managers have to say about this.  Thanks.",2023-08-24 05:35:31
15tiwez,Has ChatGPT dropped the Bomb on DE's?,"How reliant are you on ChatGPT or other LLMs to get work done as a DE?

[View Poll](https://www.reddit.com/poll/15tiwez)",2023-08-17 09:57:32
15ix0ey,How To utilize SQL in current job to get ready for next step? (pls dont remove this post),"I posted to this reddit a week ago and it got removed (not sure why as it was a genuine ""where to start"" kind of post. Hopefully this one does not get removed.)

&#x200B;

I have been self teaching myself SQL (via SQLBOLT, brain is currently mush trying to understand LEFT JOIN clause) for the last week with zero coding skills prior (Economics major and have worked in Finance Analyst for last 7 years) and wanted to know how I can use SQL in my current position to get me ready for becoming a Data Analyst in 2 years (end goal is to become a Data Engineer but I understand I would need some experience in data in order to make that big of a jump, hence DA first then DE in my roadmap)

I work in excel a sh\*t ton so maybe I can start there if possible to use SQL in excel? Just wanted some input from those on the ""inside"" on what I can do in order to become proficient in SQL in my current function before attempting to jump over to a DA role. Studying SQL on the side is obviously going to help, but I want to also try and use what I am studying in my current role to further solidify what I am learning. 

&#x200B;

SELECT How To

FROM Help",2023-08-05 14:23:28
15ieps5,Delta Lake Introduction,N/A,2023-08-04 22:56:11
14vq5gx,Good Windows Laptop for Big data practice,"Hello everyone,

I'm seeking your input on purchasing a suitable laptop for coding purposes, specifically for practicing Hadoop and other big data tools. I would appreciate your advice on what key features to consider to ensure I make the right choice.

Given the abundance of options available, ranging from i3 to i9 processors across multiple generations, I find myself in a state of confusion. While I understand that the latest models tend to be more expensive, I am primarily looking for a budget-friendly option. 

Budget: ₹45k-50k INR 

PS: this is for my friend.

[View Poll](https://www.reddit.com/poll/14vq5gx)",2023-07-10 10:27:00
14rguyt,Professional Data Engineering Community,"Hey Data Engineers,

We recently [announced](https://www.reddit.com/r/dataengineering/comments/14dgupv/new_de_community_looking_for_mods/) that we would be opening up a new community for data engineering professionals to network and join in-person events. We had over 300+ signups on the waitlist with data engineers from over 33 countries. We've sent out emails to everyone on the waitlist - if you didn't receive an email, please check your spam folder or reach out to info@dataengineering.wiki.

We are now generally accepting members and you can [join here](https://community.dataengineering.wiki/). 🥳

&#x200B;

[This could be you](https://i.redd.it/7t1ig7dlf6ab1.gif)

We've had a lot of offers to help out and we've put together a few ways you can get involved in the professional community. If you're interested in any of these, please message the mod team:

1. **Volunteer to speak:** we are putting together some local and virtual meetups and are looking for speakers to share something they are passionate about.
2. **Help organize a meetup:** if you're interested in starting a local meetup in your area let us know and we can connect you with speakers and resources to help you get it started.
3. **Join as an existing meetup:** we can create a space just for your local area which you can customize and use for free to operate your existing meetup.
4. **Contribute to our newsletter:** we run a free monthly newsletter and are always looking for interesting ideas to share.
5. **Share your story:** share your written story about how you got to where you are today and give advice and inspire others who might want to take the same path.
6. **Share your feedback:** most importantly, we need your feedback to keep improving this community. If you would be willing to leave us a review that we can use externally, please let us know!

As always, we are listening to your feedback and using it to shape the community and we cannot do it without you. Thank you to everyone who has offered their time, help, and expertise to make our community great.",2023-07-05 17:10:29
1437zj8,What API's Data formats do you use?,"I have an idea on how to completely change how you write data science analytics and  ETL. It is based on an assumption around data formats and common ways data is delivered. 

So it would be really nice to have a collection of data schemas and API's to throw at what I am trying to do and see where it doesn't work.

I've never been involved in acquiring data and my googling so far suggests the more I personally know about a domain the more likely I can find a service selling data for it. I am not an expert in all domains and my theory should work in all (not much point to it, if it doesn't).

So I thought I would ask the community, what services/schemas are typical for you?",2023-06-07 09:02:59
138734e,What Data tool/product would you build?,"If you had the choice and 6 months of free time, which data tool would you build/fix/rewrite and why?

Serious responses only, please.",2023-05-05 01:53:28
1310uyl,What is Z-Order?,"What is Z-Order?

Z-Order is a method of sorting data to cluster data based on multiple fields equally. So instead of sorting by X and then Y, you sort by X and Y equally. This can be great when data is often searched based on X and Y.

How does this work?

Essentially imagine all the data is sorted into four quadrants, something like this array:

[
  [], [],
  [], []
]

We sort the data as following:

- If X is 1-50 and Y is 1-50 it will be sorted into the upper left quadrant (first sub-array)

- If X is 51-100 and Y is 1-50 it will be sorted into the upper right quadrant (second sub-array)

- If X is 1-50 and Y is 51-100 it will be sorted into the lower left quadrant (third sub-array)

- If X is 51-100 and Y is 51-100 it will be sorted into the upper left quadrant (fourth sub-array)

By clustering the data in this manner if I search for data where X is 32 and Y is 58, I can search only the relevant subdivision of clustered data eliminating the other 3 subdivisions from my search saving me lots of time.",2023-04-27 18:14:52
12jlrhj,Is ‘data engineer’ job category even purposeful?,"I’ve seen salaries for senior data engineer jobs ( in HCOL) range from 90k to 500k. The latter being hardcode software engineers to the former being drag and drop GUI users. 

What are your thoughts on this disparity of work done by data engineers? Should they even be called the same thing?

I feel there should be sub category for the two segments. The same could go for “data analyst” as well, but I think the word “analyst” is a bit more vague than “engineer”.",2023-04-12 13:09:16
11weshk,DBT Documentation Generator w/ChatGPT,"Hey, 

I wrote an article showing how to automatically generate DBT documentation with ChatGPT, check it out

[https://www.nintoracaudio.dev/data-eng/2023/03/18/dbt-generator.html](https://www.nintoracaudio.dev/data-eng/2023/03/18/dbt-generator.html)",2023-03-20 10:16:26
11ostqt,Anyone here introduced chatpt into their teams workflow - if so how.,"Not asking if you just tried to type into gpt to build a pipeline but you found a pragmatic way to use the tool to assist your team operations. Ie write a jira ticket or pipeline documentation ect. And in turn your team uses it routinely , discretely and effectively",2023-03-11 19:32:14
11a4mhv,Snowflake,My new favorite website -https://status.snowflake.com :(,2023-02-23 18:18:44
15k6vai,"Dropped into Azure as a newbie 60 days in, using Dynamics / Synapse / PowerBI/ Pyspark (sql). What do I need to learn? WTF is fabric?","I am doing modeling in python to maintain a next best action python model in that tracks relationships in dynamics. 1. the system is expensive to run 2. the data is ugly / messy. I'm supposed to be supporting our sales org with data insights as a ""data analyst"". I like the notebook approach of synapse. I feel like azure can go infinitely deep. Is anyone else using dynamics and doing analytics with it?",2023-08-07 01:38:17
1ar5loq,SQL SERVER: How do I stop unneeded table in a view from running?,"Hope I am explaining this right.  I have a query that's running slow.  It is pulling a couple columns from a view that has multiple tables and many columns.  One of those tables is LEFT JOINed and is not necessary for the query, but SQL Server is still including it on execution.  This is the join path of the VIEW:

FROM NeededTable1  
LEFT JOIN UnneededTable2  
on 1.primarykey = 2.NOTprimarykey

Is there a way to tell SQL Server not to include that table in the execution plan, since it is irrelevant to results?  It isn't joining via its primary key, unfortunately, and unfortunately it won't be practical to change the key.  Wondering if there's another way.",2024-02-15 03:03:36
1aqq4x1,"Favorite BI Tool, and why?","What is your favorite BI tool, where do you usually serve the data from (data warehouse, database, data lake).

What do you like best about it and what techniques do you use for accelerating BI (cubes, extracts, reflections).",2024-02-14 15:53:06
1aolkgc,Experience with Abbott?,"Hi. Anyone has any experience working in abbott as senior DE. Want to know the work culture or pros/cons working at abbott IL location. 
I have gone through the glass door and indeed but i don’t trust them as negative reviews can be deleted from them.",2024-02-11 23:17:09
1ao82u9,My Data Pipeline Orchestrators Journey,"
Sharing my journey of different data pipeline orchestrators used for large scale batch pipelines, discussing migrations, reasoning and much more.


Read full story: 👉 https://www.junaideffendi.com/p/my-data-pipeline-orchestrators-journey


Let me know what orchestrators have you used and what are some key points.",2024-02-11 13:34:45
1anymuj,How to design a BI landscape from scratch,"Hey! I am currently thinking about latest trends in BI and drafting an infra concept for a fictive small/mid size company with limited budget and ressources. The goal is to set up a completely new analytics landscape from scratch. It is required to be easy to set up with a small team and cost-effective from the beginning, but scalable in the future.

I would see a strategy like:
1. Set up a basic data storage (e.g. data warehouse / data lake / lakehouse structure), connect the first data sources, peocess it to a reporting layer
2. Grow in data, data sources and dashboards + users
3. Move into first steps in advanced analytics and data science

I am personally strongest with the Microsoft tech stack, so I would decide for Azure + Power BI as a foundation.

What I have seen in other companies, is a Azure Data Lake as a foundation to collect data. Then use case specific applications on top: Synapse/Cosmos DB/SQL DB as a data warehouse. Databricks/ADF for ETL and processing.

However, I am not sure if that would be an overkill for a lightweight start. Maybe starting with a simple DWH would be a better choice?

Other considerations might be:
- Cloud vs onprem
- MS Fabric
- Support frameworks like snowflake, fivetran...
- How to ensure flexibility and scalability

**TL;DR: How would you design a simple and cost-sensitive but highly scalable BI landscape as of 2024?**",2024-02-11 03:41:54
1anrsfe,Considering DE or DS as a field. Would start with Data Entry be a good start to get my feet wet?,"Tldr Would Data entry be a good intro job before committing to Data Engineering/Science?  


So I am considering changing from web dev to Date Engineering or Data Science. While I 'decide' or start to get into data stuff. I BELIEVE a good path for me to take would be to get into some role like Data Entry where I get to handle data and enter it for x reasons. Just to see if I get overly bored working with data and looking at it all day. I've done it before, briefly and enjoyed it. Customers would call and I would put their data into a spread sheet for various reasons.   


So right now I am thinking about applying for Data Entry, then Data Analyst roles, and then either data engineering or data science. I'm under the impression it wouldn't be much of a skill set change anyway.   


Also, while I'm here, how competitive is DE hiring market?",2024-02-10 22:08:06
1anpffk,Turing College as DE professional - worth it to step up my career?,"Hi everyone,

I am looking for tips and advices regarding the possible enrolling in the Data Engineering program of the Turing College.

My background: I have a master in computer engineering and I am working as consultant in business intelligence and data engineering for almost 5 years.

My goal: to work on projects with the ""hottest"" technologies, as per my experiences I have always worked on more outdated or niche architectural stacks. 

I have always tried to keep my knowledge updated, but yet couldn't work on real or near real projects. And the lack of concrete experience, despite a basic knowledge and plenty of experience with other technologies and tools, is forbidding me to find a new job opportunity as a mid/senior.

So my final question is: is it worth the expense and time, when my first goal is to gain real project experience and access the network with the support from other professionals? 
Definitely the classes would be also useful, but maybe in some points redundant; and if based on self-study of externally available resources (O'Reilly books, Coursera and other s as far as I read) it might be not that relevant.",2024-02-10 20:22:24
1algef8,Is it better databricks cluster or compute engine?,"Let's say that you have 10 compute engine that last for 20 minutes with minimum specs and you have one databricks cluster with 10 notebooks.

They do the same process, same code, same resources (ELT), which one is better in terms of cost?",2024-02-07 22:59:47
1ak6qm0,Engineer vs PM,"
Hello everyone,

I've recently embarked on a new journey as a Project Manager, transitioning from my previous role as a Data Engineer. I find both domains equally fascinating and am confident I could dedicate years to either path without any issues.

However, when considering the aspects of financial benefits and career opportunities, I'm at a crossroads. I'm contemplating whether to advance further into technical expertise and aim for senior roles within engineering, or to fully embrace the Project Management path.

I would greatly appreciate your insights and advice on which direction might offer the best prospects in terms of both remuneration and professional growth.

Thank you in advance for your guidance.

I live in Europe just to clarify (if it makes any difference)",2024-02-06 10:29:57
1ajbglk,Copilot in Microsoft Fabric Notebooks,"I have been trying out Copilot in Microsoft Fabric Notebooks and so far it seems to be working quite well. Time to time I have encountered some minor bugs with it. Have you already tried Copilot in Notebooks and what are your thoughts and experiences with it?

  
I created a short demo about Copilot's Magic Commands in Fabric Notebooks.  
Feel free to watch it if you are interested in Copilot Magic in Microsoft Fabric.

  
Link to the video:  
[https://youtu.be/4hh1e6oy2A4](https://youtu.be/4hh1e6oy2A4)",2024-02-05 08:05:55
1agyc09,Product Data Teams 101,N/A,2024-02-02 07:41:15
1afpxfa,Help Talend,"Hi hope u r all doing good, i am looking for a video or a free course to understand Talend and to know how to work with it. If you can help me that would be awesome i really need to master this tool because of my internship. thank you.",2024-01-31 19:14:16
19fncyj,"ECS and Databricks to design, develop and maintain pipelines?","
Just got an interview invite to help out a team that uses Amazon ECS for container orchestration and Databricks. 

My guess is the ECS is used to help distinguish various dev environments but doesn’t Databricks do that already? 

Where does Amazon ECS come into play here? Anyone know?",2024-01-25 23:41:51
19e6ofb,SW engineer learning about ETL pipelines,"Software engineer here just recently joined a team heavily focused on data science and data engineering. The expectation is that i take their scripts and refine and improve their automation. While still ramping up on the data science and data engineering problem space, coming from a software engineer perspective would it make sense to use a tool that offer ci/cd pipeline capabilities to create an ETL pipeline? Ive just normally been exposed to ci/cd pipelines with tests, static code analysis tools, etc that promote/deploy an application to an environment. Whereas, from my understanding, the new team im on wants to gather data, normalize said data, and upload it to a data warehouse. So from a tech stack perspective would it make sense to also use tools that offer ci/cd pipelines to achieve that? Like basically break up their script into stages so i can also create stages to test/ensure quality of the data prior to moving on to the next step, as opposed to traditionally having devop-specific stages .Or are there some obvious reasons that im just not aware of as to why people use tools like domino, aws glue, snowflake or airflow instead? Like maybe its an issue of scalability when dealing with large volumes of data? Any feedback would be appreciated.",2024-01-24 02:49:39
19dsj3n,Pytest on Databricks,"I'm pulling my hear out with this one: 

We've upgraded our cluster from 10.3 LTS, to 13.3 LTS and all the unit tests are not running . For 11.3 and everything in between it's the same.

We even have a simple one that just checks if a given table has data. 

As in: 
assert df.count() > 0 

And for each and every one of them we get: 

OSError: [Errno 95] Operation not supported: 'Workspace/Repos/..../tests/__pycache__'

Test called using
If __name__ == '__main__':
    pytest.main([test_file.py])

Made sure these files are 'files' not 'notebooks', tried to delete them, put them back. Nothing seem to work. 

Any advice?",2024-01-23 16:36:37
19aw5tk,Custom GPTs for data science and data engineering,"Originally I built these for my own use; but I’m quite happy to see my custom data science and data engineering GPTs on the OpenAI GPT Marketplace are doing well with 500+ and 100+ users.

They are pretty straight forward, referencing the leading data science and data engineering vendors’ online documentation and some of my favorite resources.

If you have a paid version of ChatGPT, please try them out and let me know if you have any suggestions or feature requests.

https://chat.openai.com/g/g-u9rFlUhxK-data-science-consultant

https://chat.openai.com/g/g-gA1cKi1uR-data-engineer-consultant",2024-01-19 22:47:03
19aff6p,Super-fast deduplication of large datasets using Splink and DuckDB,N/A,2024-01-19 09:40:33
198ifht,"beginner here, can i get some advice on which tools to use for each major step of the pipeline? (bonus if little to no cost) dataset is flight data",N/A,2024-01-17 00:11:14
192mrnf,Database Engineering,"Q&A.

&#x200B;

I am attempting to rearrange the data collection and presentation processes of a business I just joined as I know they are dinosaur in their ways. I have no experience with data engineering. The goal is to create a database to store all of our data, some numerical some text. This database will need to connect to a website and or a mobile app to create a dashboard style presentation of the data. In a long run, hypothetical scenario also be able to integrate with GPT technology to create our own GPT.

&#x200B;

If anyone could point me in right direction, that would be a big help.",2024-01-09 19:12:41
191hlw3,Needs for Distributed Processing,"Hello everyone!

I’d like to know what you think are the needs for distributed processing (which focuses more precisely on MapReduce treatments, Spark/Hadoop frameworks, etc.) in the industry atm.

I’m currently working for key accounts customers and I don’t see any need really. Still, that’s the career I’d like to pursue.

Is it outdated already? Too niche?

Do you have any advices on:
- Kinds of structures I could aim for,
- Most relevant companies and/or
- City hubs to work in/for?",2024-01-08 09:47:08
191dcxw,What kind of ML do DE jobs usually require knowledge of?,"I know DE is different than MLE but it's true that in quite a few small/medium size companies both positions aren't well defined and can share work and attributions


So I ask: what subset of ML knowledge and exp do you think is asked the most by the market in these cases ? From your personal experiences

1) Statistical ML: More statistical models like Regression and such 

2) Deep Learning and Neural networks in general: Trendy stuff with Tools like TF and Pytorch

2.1) Computer Vision: Subset of the above

2.2) NLP: also a subset


I am an aspiring DE learning the basics of the field but I do want to know a bit about ML for career purposes, but I know the field is very deep and complex, so if I could focus on a subset that's more likely to improve me DE career that would be great

So I am excited to hear your thoughts guys!",2024-01-08 05:16:06
190i0sh,Review for Serious SQL course (Data With Danny )?,"Looking for reviews on this course. I am trying to join a course which has case study approach to teaching SQL, I want to solve most real-world problems as possible to gain more confidence when working with SQL or when getting interviewed on it. Thanks for your input.",2024-01-07 03:06:07
18yf2ya,"Starting Data Engineer Career, potential first data job","Hello all!

&#x200B;

I am an aspiring data engineer introducing myself for the first time.  I am hoping to receive advice and insight from time to time for this community.

A little about me;

* I am currently an IT Technician (going on 3 years) for a large company (2500+ employees).
* My company is currently in the process of building a Data Warehouse Department.  I have been assisting the Data Warehouse Manager sporadically for the past two years.  So far, I have created a few Power BI reports, given some presentations, and helped build an ETL pipeline using Azure Synapse.
* My current skillset is somewhere between beginner/intermediate SQL, Python, and using Synapse.

This year, the Data Warehouse Manager has finally been given the green light to hire two data engineers, a junior and a senior.  I have been told by the manager to apply for the positions and other managers recommend me for the position as well.  

The first green-lit project for the department is to load data to an on-prem data warehouse using SSIS.  I haven't used SSIS before, so the Data Warehouse Manager has given me the heads up to go ahead and learn it. 

What recommendations do you guys have for learning SSIS, what resources to use and how to go about it?  What advice do you all have for trying to make it into data engineering and being part of a team that will be building a data warehouse from the ground up?

&#x200B;

Thanks everybody,

&#x200B;",2024-01-04 14:58:03
18xrpfl,Seasoned Professionals - What should I learn after Python. C++ or Java,"Like the title says, I'm looking to get another language under my belt besides Python. Those working in Data Engineering, do you think C++ or Java would be more ""bang for my buck""?

I've been using Python for almost a decade and would like to pick up another programming language. Took two Scala courses a while back, but other than working in Spark (Which I don't do for work) I found few other uses for it. I took Java MOOC, but didn't keep up using it cause I got confused a bit with the framework that we were using. I have done some Rust, but I think they hype around it is dying down and I honestly think it will go on to be a good system programming language, but not really a good one for every day use. Haven't used C++ since high school, but willing to re-learn/play around with it.",2024-01-03 19:30:01
18xf26l,"Migrate from ""MySQL+ClickHouse"" combination to Apache Doris",N/A,2024-01-03 09:04:32
18w6mjf,Which book do I need?,"Hello, I’m a data engineer with experience of 1 year for now doing practical ingestion and ETL using Azure and Building simple DWs on the cloud for customers.
I need a book to improve my conceptual understanding of data engineering not only the practical part, note that I’m someone who is losing interest on reading continuously and don’t read books easily.
Which one should I read first?

[View Poll](https://www.reddit.com/poll/18w6mjf)",2024-01-01 21:01:50
18u58g9,Need Suggestions and Opinions,"I’m a Azure Data Engineer with 3 years of experience I have been working with Azure Services to Migrate and Ingest Data into SQL Data warehouse using ADF, ADLS, ADB, Unix and some other Azure Services which are required in between like VMs and all

I have been stuck and not able to switch jobs as Azure Data Engineer as many need AWS 

Give me some suggestions about how up improve my current skills or should I move to AWS stack or some Project of PySpark and Data Engineering ones which can help me grow


I’m mostly confused with what to do next",2023-12-30 03:02:44
18r4573,Moving data transformation solution from AWS to GCP,"Howdy,

I've been tasked to migrate one of the existing solutions from AWS to GCP. The solution is rather simple, it's a combination of Lambda, Batch Job (fargate running ecr image) and S3. Basically lambda triggers batch job which does data processing and saves the outcome to s3. On top of that we have Athena running queries on that outcome s3 bucket.

Would you be so kind and recommend me a blog post, video tutorial or some other material, that would help person that is fairly proficient with AWS services to build something similar on GCP considering mentioned architecture?

Any recommendation is helpful.
Thanks!",2023-12-26 09:08:28
18q189h,Looking for Feedback/Contributors: Learning DE Roadmap,"We just created a [new issue](https://github.com/data-engineering-community/data-engineering-wiki/issues/49) on GitHub for anyone who is interested in collaborating on a DE roadmap for the wiki. I know about one popular roadmap (link in issue) that has been shared a lot but is no longer maintained. Today I reached out to that creator to see if they would be open to collaborating and letting us adopt/update it.

It's the holiday season so it may take longer than usual to incorporate and respond to feedback but please share here or directly on the issue with any ideas/comments/suggestions!",2023-12-24 18:49:21
18oj17p,New Video 🥳 Creating Pipelines without Airflow Knowledge 😳,N/A,2023-12-22 16:37:56
18naxfs,Azure Data Lake Storage Gen2,"This article is about , ""Understanding Azure Data Lake Storage Gen2"" This article will cover: 💡

1- Why Azure Data Lake Storage Gen2

2- How to enable Azure Data Lake Storage Gen2

3- Azure Data Lake Gen2 vs Azure Blob Storage Gen2

If you are interested to understand Azure Data Lake Storage Gen2 you can access the full article here: [https://devblogit.com/understand-azure-data-lake-storage-gen2/](https://devblogit.com/understand-azure-data-lake-storage-gen2/)

Don't miss out on this opportunity to transform your data practices and stay ahead of the competition. Read the article today and unlock the power of Azure Data Lake Storage Gen2! 💪[\#Azure](https://www.linkedin.com/feed/hashtag/?keywords=azure&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7143410880200732672) [\#DataManagement](https://www.linkedin.com/feed/hashtag/?keywords=datamanagement&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7143410880200732672) [\#Analytics](https://www.linkedin.com/feed/hashtag/?keywords=analytics&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7143410880200732672) [\#DataLake](https://www.linkedin.com/feed/hashtag/?keywords=datalake&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7143410880200732672)",2023-12-21 01:29:24
18l45pg,Line-of-Sight Analysis in Digital Elevation Models using Python,"&#x200B;

[Line-of-Sight Analysis in Digital Elevation Models using Python](https://preview.redd.it/8brtr07yn07c1.png?width=1107&format=png&auto=webp&s=31cbc7e5ad55619fc6743a666f897a9f6da4ce14)

[Line-of-Sight Analysis in Digital Elevation Models using Python](https://spatial-dev.guru/2023/12/10/line-of-sight-analysis-in-digital-elevation-models-using-python/)",2023-12-18 08:47:12
18kqgcq,data-pipeline-compose - Data Engineering environment setup using Docker Compose,"Hi everyone! I've put together a Docker Compose setup that includes tools like Hadoop, Hive, Spark, PySpark, Jupyter, and Airflow. It's designed to be easy for anyone to set up and start using.

Just clone the [repository](https://github.com/carteakey/data-pipeline-compose) and spin up all services using \`docker compose up -d\`.

The purpose is to just streamline the initial configuration process without the usual setup hassles, which can often be a roadblock for someone trying to get their hands into DE.

Let me know if you have any suggestions / feedback.",2023-12-17 20:30:18
18gocy7,High Performance Airbyte Alternative (With marketplace & rev-share option).,N/A,2023-12-12 15:24:42
18ccu7z,DB replication into Databricks,"I work at a start up and have been running into issues replicating our MySQL OLTP database into databricks. 

What are other people using to ingest database records into Databricks? 

We need to be able to support:
1. Column data type changes 
2. Adding new columns to existing tables 
3. Adding new tables to existing schemas 
4. Creating SCD1 replicas of the MySQL tables themselves
5. Streaming the change data feed off the SCD1 tables  
6. All stored in unity catalog 


To reach a reasonable latency we chose to go with CDC replication and have tried a few patterns and all of them have been pretty wonky. 

1. Debezium reading MySQL bin logs and pushing the data to S3, running in confluent. Then used Databricks Autoloader to push the data into our bronze schema to create SCD2 tables. Finally, had a second stream to reconstitute the SCD2 tables into SCD1 replicas of MySQL. 

- this solution failed due to files arriving late to S3 and caused the non-latest version of a record to push up to our silver table


2.    Debezium reading MySQL bin logs, running in confluent, and using reading the Kafka stream with spark structured streaming to create our silver and bronze tables.

- this solution failed due to the Kafka schema registry not being able to encounter data type changes like INT->BIGINT well. 

3. Airbyte reading off the MySQL PKs of the DB and incrementally loading them into Databricks. 

- this solution failed because it can take a long time to return incremental batches off our indexes for specific tables. Also airbyte can’t handle schema drift into databricks. 


4. Arcion reading CDC binlogs and creating SCD1+SCD2 tables in databricks 

- This solution was awesome and supported everything we needed including column renames. But then databricks bought Arcion before we could sign a contract with them and now Arcion cannot license their product with us. 

-",2023-12-06 20:06:37
186guvp,Career Progression,"Here is my career progression.

0-2 years - SQL DBA/BI Engineer

2-4 years - BI engineer

4-5 1/2 years - Big Data Analyst

Now trying for DE roles and going through interviews.Got offer for a DE role, but have few more interviews to be completed.
Before working as a Big Data analyst I tried for DE roles but I was not able to move forward in the interviews as I didn’t have any professional experience with Cloud Technologies or big data tools/technologies.

Is this a general career progression or y’all got into DE roles much earlier in the career.?",2023-11-29 04:28:54
183mr6u,Snowpro advanced architect - tips for preparation?,"Hi,

Can you recommend any materials that helped you prepare for the certifications beyond Core? Especially the Architect version.

Thanks",2023-11-25 15:57:37
180d7cm,Using Databricks as IDE for dbt,"Hi all,

I want to use dbt with databricks.If you use dbt cloud, it provides you with online IDE and GUI. The integration with databricks works fine.

What if I dont want to use dbt cloud, but dbt core? I know that you can connect dbt core to databricks and start developing in local IDE (ex: VS code). ( github + dbt core + databricks)

* github: for version control
* dbt core: for dbt project management
* databricks: for deployment environment + data storage (unity catalog)

In a nutshell, my question is, can I use databricks as an IDE to develop dbt projects? (instead of developing in local)",2023-11-21 09:31:37
17xjs0q,OneTable is now live | Table format interoperability is not a dream anymore,"Full disclosure: A contributor to the project here.

I have seen folks discussing about Data Lakehouses and open table formats on this subreddit and even OneTable a couple times.

I'm excited to share that the project is now live and I wanted to thank the project's early contributors. You can learn more by visiting the repo: [https://github.com/onetable-io/onetable](https://github.com/onetable-io/onetable)

I'll keep an eye out on this subreddit and the post to answer any questions you may have.",2023-11-17 17:08:40
17xhqj2,dbt incremental models,"I'm trying to incorporate dbt and was reading on incremental models. In my case, the past data of the source frequently gets changed without notification. How to ensure that my downstream models have the correct data? Is manual backfilling the only option?",2023-11-17 15:36:25
17u4596,"What do your users accept as a ""fast dashboard""? Is the expectation of ""fast"" changing?","

[View Poll](https://www.reddit.com/poll/17u4596)",2023-11-13 05:52:00
17tbd16,Personal Finance Dashboard,"Hi all, trying to play around with some personal projects and I'd like to build connections between my banking accounts and a SQL server. First off, just want to make sure this is possible as a consumer (main account is with Bank of America) and secondly, what tools should I use?

I found a few youtube vids that were accessing banking transactions using Postman and the Open Banking API. once I manage to gain access to my bank's portal, is it just a manner of loading the data onto something like MySQL? Thanks!",2023-11-12 03:17:49
17s9r7u,Please help me understand how the parquet sink in ADF works,"In Azure Data Factory, I have two pipelines that are using the same copy data activity. The first pipeline takes data from an Oracle table/view and sinks it to a .parquet file. The result is a single file in the blob container I want. Pretty straight forward.

Pipeline #2 uses a SAP CDC connector and gets some data from a SAP ODP source, it then uses the same ds as pipeline #1 to sink the data in .parquet. The result here is the .parquet file as expected, but also a folder containing .txt files, that contain the data.

Schema drift, partitioning, load type and all other attributes are identical between the two pipelines.

What I don't understand is why the Oracle source is saved as a single file and the SAP source is splitted in a .parquet file and multiple .txt files.

Probably the question is too stupid and basic, but I cannot find an answer. 

Any help is highly appreciated.",2023-11-10 17:59:55
17rur6k,Applying Python Code to Big Data,"Hi Everyone,

I'm trying to apply python code on a table stored in AWS. My approach was to query the whole table through Athena in an EC2 instance and store it in a dataframe and apply the python code there. However, this request is taking a long time and was wondering if there was a way to cut down the time or a different approach. For reference, the table is around \~160 GB.",2023-11-10 03:11:44
17raw1l,Is it a good pracise to build foreign key reference in delta lake,"Hello!

I'm actually building a project that will use data form different sources at different frenquencies.

Dimension tables : load every 4 hours

Fact table : load every 30 minutes

Fact tables can reference dimension data that haven't been pushed to our silver/gold delta tables

In the exposition layer (powerbi) we will build a star schema that will link the fact and dimension tables

How do we deal with foreign keys in the fact tables ?

is it a good pratise to build them when loading data in the gold layer ?

how to deal with fact tables that have data not referenced in the dimension yet ?

tables are built in delta lake using azure databricks 

thanks !",2023-11-09 11:28:00
17pyioz,Pipeline Tools By Industry Use and Adoption,"There are a lot of posts about which ETL tools to use. These are great for people who just want to know what's out there, but for people who want to understand the longevity or the adoption level of a solution, it isn't very helpful.   


For that, you'd want to see two main metrics, growth and adoption. So to start, are there any existing resources to gauge these metrics?  


For discussion of current state, here's my surface level first pass for adoption levels. Ranked from highest to lowest for four categories: DB, Cloud Platforms, Data Warehouse, ETL Tools, and Data Modeling Tools   


# Databases

1. SQL Server 
2. Postgres
3. Oracle 
4. MySQL

# Cloud Platforms 

1. AWS
2. Azure
3. GCP  


# Data Warehouses 

1. Snowflake
2. Redshift
3. BigQuery
4. Tera Data

# Independent ETL Tools / ETL Orchestration 

1. Airflow
2. FiveTran
3. Dagster
4. Alteryx
5. Matillion  


# Data Modeling Tools

1. DBT
2. Wherescape  


&#x200B;",2023-11-07 16:46:06
17mzyxp,BI Analyst -> Data modelling & Data Engineering,"Hi! 

In the last 2 years I’ve been working as a Business Intelligence Analyst (understanding the business needs then query the reports/ build dashboard) 

Now, I’ll be responsible for designing data bases, handling the structure and data flow, and some data engineering responsibilities (managing the tables). 

I need some guidance to know what to learn next? what should I focus on? what course or YT videos/ series you recommend to watch? 

Every advice will be really helpful. 

*if you can provide a learning roadmap I’ll be grateful*

Thank you in advance:)",2023-11-03 17:12:40
17md0gg,Switch from senior instrument and electrical engineer to data engineering,"I am looking to switch from senior electrical engineering role in a chemical plant to data engineering. 

Not sure, if this is the right decision for me, I have accumulated a grand total of 18 years of experience in addition to Bachelors and Masters degree in the subject matter. 

I have been pushed off to a support role due to the nature of the company. I am considering switching over to data engineering through a bootcamp. There is a bootcamp called wecloud data in toronto. 

they want $10k for course and they seem to have tie up with industry.  I already hit my salary ceiling in my career. When i look around I get up to $140K at other places. 

Would data engineering pay me more? I am not sure if I am doing the right thing. I do have to quit my job to study for it. There is no way I can study that much along with keeping my stressful job.

&#x200B;",2023-11-02 20:33:10
17k6nqz,Real-time Change Data Capture for Postgres Partitioned Tables,"Blog - [https://blog.peerdb.io/real-time-change-data-capture-for-postgres-partitioned-tables](https://blog.peerdb.io/real-time-change-data-capture-for-postgres-partitioned-tables)  
This is one of my favorite features that we shipped recently. Building Change Data Capture (CDC) for Postgres Partitioned Tables involves handling various scenarios that need care and emphasis.

We at [PeerDB](https://peerdb.io) are building a specialized data-movement tool for Postgres. So supporting this feature was a given and a self expectation. This feature was one of the top asks from customers. We will keep adding more such native features as we evolve. 🐘 😊

Also, don't forget to see the demo - it was a very candid one. 😊 I enjoyed covering different scenarios incl. adding new partitions, adding new columns, dropping partitions etc and showing how PeerDB handles replication for each of these cases.",2023-10-30 23:14:17
17j6s46,Should I Use a .py or .ipynb File for Creating Data Pipelines? Pros and Cons,"I'm currently working on creating data pipelines and I'm torn between using a **.py file or a .ipynb file** for my code. I have some thoughts on the matter, but I would love to hear your opinions and insights.

Here are my current considerations:

Using a .py file:

* With a .py file, I will be able to test individual components of the pipeline more easily.
* It allows for a more traditional development workflow, where I can write and run tests using frameworks like pytest.
* .py files are easier to debug with IDEs like VS Code, making it easier to find errors.
* Execution of .py files starts fresh, unlike .ipynb files where some variables can get carried over from the last execution or deleted cells.

Using a .ipynb file:

* With a .ipynb file, I will be able to check the data while creating the pipeline, resulting in a better developer experience.
* It provides an interactive environment where I can explore and visualize data.
* Some tools like Databricks have a notebook interface for creating pipelines.

I understand that using a .py file may be more suitable for testing and maintaining the pipeline, while a .ipynb file may provide a more interactive and exploratory experience. However, I would really appreciate your thoughts and experiences on this matter. What do you think are the pros and cons of using each file format for data pipelines?

Looking forward to your insights and opinions!",2023-10-29 16:36:40
17j31i8,Job offer - help me make a decision,"I work in the USA now on H1B and have a Canadian PR. We have been thinking of moving to Canada for a while now(the GC backlog in the USA for folks from India is crazy).

I have secured a job offer in Canada.

1. TC  132K
2. commute will be around 9 hrs/week (we plan to live near the US/Canada border)
3. almost 5 weeks off + 9 statutory holidays
4. Work is on Databricks/Spark with Airflow to support Data Science/ML team.

My Dilemma: The pay is considerable lower in Canada, It has got me thinking if I should decline this job offer, stop job hunting in Canada altogether, and focus on getting a remote job in USA that would hire H1B. Then cross border commute to Buffalo(NY) and make 50-60K Canadian more with the USD-CAD conversion rate and higher salaries in the US.

Do you suggest:

a. I decline the job offer, and search a remote job in the USA?   
b. Accept the job offer, and keep looking for a better TC job from Canada either within Canada/USA?

**ps:-** My current job pays the same TC in USD and work is in SQL with Redshift(ELT).",2023-10-29 13:33:47
17hf8x2,Does anyone enjoy building dashboards???,"

[View Poll](https://www.reddit.com/poll/17hf8x2)",2023-10-27 04:14:33
17h4g9e,dbt + flink for streaming,Have anyone tried materializing models by integrating dbt with Apache Flink to deal with kafka. Anything to beware of in the initial design.,2023-10-26 19:28:56
17gvyck,Career Advice in the current market,"I'm needing a gauge of the current job market as well as some advice about my situation. I graduated an extended boot camp in late 2021. I was shortly thereafter hired for a data role at a local firm. I transitioned from a soft skills focus to strictly data engineering. I was let go in May with a total of about 18 months of DE experience. I have found the job market very difficult. The most common reply I get is ""not enough experience.""

So, first: are others finding this to be a bleak time for job hunting?  It's not just me, right?

Secondly, to those who've lived this before, what should I do to increase my chances? I've widened my net, applied for more general software roles, gotten a certification or two, and done my best to keep my skills sharp. Should I make a broader presence on social media about my thoughts, interests, etc. to heighten my profile. Is there something else I'm not thinking of that the hivemind would recommend?",2023-10-26 12:59:02
17fyyj4,Airflow DockerOperator: End-to-End Machine Learning Pipeline with Docker Operator,Check out my latest Airflow video about  DockerOperator: End-to-End Machine Learning Pipeline with Docker Operator!,2023-10-25 07:05:06
17dvm06,Portfolio impact,How important do you think a portfolio is to getting a job in DE?,2023-10-22 15:18:41
17afqeu,java vs javascript as an additional language to learn?,"I use python and sql mostly for my job. I want to learn another language to have more competency. I touched c++ a bit when working on my esp32, but realised that the use of c++ in data engineering space is very niche and isolated to iot. If I can choose between java and javascript, thinking kotlin vs typescript to learn, which language will be more useful in general for data engineering?",2023-10-18 02:22:09
17a3jcp,anyone at dbt coalesce this year?,"i can't attend, but was wondering if anyone had a tl;dr of some of the big announcements, what was interesting, etc from the event?",2023-10-17 17:21:25
179dfxa,Building Blockchain Apps on Postgres,N/A,2023-10-16 18:48:39
179cq18,Clickhouse Cloud,"Hi all,

has anybody given Clickhouse Cloud a try? I have read about their Postgres Materialization and was pretty impressed about the possibilities that would get opened up by this feature? Has anybody some experiences in using DBT with Clickhouse?

Thank you for input. I am happy for every hint.",2023-10-16 18:19:08
178sv6a,What is your thought on HCL Domino?,"I do have a lack of experience, but I am curious. 

I never heard about the HCL Domino database, but I want to try it. Is it used for ETL automation?

Please let me know what your thought is. ",2023-10-16 00:03:33
177okev,10 + Experienced Data Engineer looking to for guidance,"Have 10+ experience in Data Engineering but only in inhouse Hadoop Clusters mostly in operations however most openings need cloud experince of a couple of years for next level.

I am scaling up in AWS and going deep into it.  
Giving AWS Practisioner next week. 
My concern is how to scale up in cloud for experience level i.e. learning nuances associated with my experience level. 
If any one is available please can we connect.",2023-10-14 12:35:38
17605w3,Kafka best practices,"Hi !   
I am working on data ingestion and schema comparison tool. One of initial assumption is to use S3 as a lake for incoming files. I'd like to create Kafka stream to connect it with final destination (to simplify let's assume it is PostresSql DB).  
I'd like to perform validation of the streamed messages based on precreated schema.   
Could you please share your thoughts on potential solution for this problem ? I've read about Custom Single Message Transformations, but without wider experience with Kafka, it is hard for me to choose the best practice way.   
Thanks ",2023-10-12 06:25:49
1750it8,Is Postgres Partitioning Really That Hard? An Introduction To Hypertables,N/A,2023-10-11 00:15:59
16yzlo4,Is the title really industry agnostic?,"What are your opinions on how a data engineer position varies between industries? I bet it's hard to generalise, but I'm sure there's some that are more likely to be chill and slow, some that can invest in the shiny tech etc. Where would you never send your resume to? Where would you look if you wanted a career jump? Hope to hear your hypotheses",2023-10-03 18:31:41
16sy0my,seeking help with solving block for data science team in a locked down AWS account.,"The company I work for maintains a very locked down AWS environment. We are not permitted to access a cloud environment that has a connection to our data (s3 and Redshift) while also having access to the internet. We have an internal nexus repository for pypi and cran which allows indirect access to commonly used python and r packages.

Our data scientists primarily run workflows locally and commonly use packages that are not available on these package managers like [stan](https://mc-stan.org/), [lapack](https://www.netlib.org/lapack/), [openblas](https://www.openblas.net/). They are keen to utilize larger resources provided by AWS cloud.

I'm trying to figure out how to get these packages installed on an ec2 that does not have internet access. 

Our devsecops team was kind enough to provide me with the ""Getting Started"" guide from Docker. I'm generally familiar with Docker and not sure how this helps. Unless the suggestion is to build a Docker image that contains these necessary packages, push the image to an internal repository, pull it into the ec2 and run it interactively there. 

Has anyone else been through a similar challenge before who can provide advice or point me in the direction of helpful resources?",2023-09-26 19:52:46
16s4pxk,Data quality checks and dashboards with a lines of Python code,"I've recently been working on an easy way to spin up dashboards, scheduling, alerting, silencing, logs, etc. around some of my custom data quality Python checks. Decided this past weekend to spin that out as a standalone and simple solution that anyone can use for their checks. If it's interesting, DM me and we can talk further",2023-09-25 21:35:18
16l6vhz,Delta table modeling,"Hi everyone,

I am interested in experience of Delta table modeling. 

Do you have productive examples or learning resources that you can share?

Thanks in advance!",2023-09-17 17:36:37
16k97q1,How to clear AZ 104 certification as early as possíble ?,"It has become mandatory to get certífication in AZ 104 to stay in my current project and my manager is asking me to clear AZ 400 certification as early as possible. So please suggest any course or any website or any Youtube channel or any platform to gain the required knowledge to clear this AZ 104 certification as early as possible.

Those who cleared this AZ 104 certification or those who have knowledge in this, Please guide me where and how to learn and clear the certification as early as possible.",2023-09-16 14:41:55
16fuzjq,Tips to pass Data Engineering interviews,"Hi, I am a fresh graduate who is hoping to move into the industry side. I actually don't have any work experience. Could you please give me some tips to follow to pass the data engineering interview and get a chance to find a job. Any responses are highly appreciated.",2023-09-11 13:14:27
16cloh6,Analytics platforms cost comparisons,"Has anyone here found a viable way to do any type of cost comparisons between data platforms? I’m trying to provide the roughest of estimates for AZ synapse analytics and Snowflake so we can decide on one of them, but everything is abstracted to SCUs or Snowflake credits. I get what both are conceptually, but have no idea how to apply it to even the roughest of cost estimation models.",2023-09-07 17:30:13
16aulka,Why Headless Analytics is the Game-Changer We've Been Waiting For,N/A,2023-09-05 17:35:27
16ahozt,Data Engineering interview preparation,"Hi, I’m a Senior DE in my current company and stuck in my job. I am looking to switch to a different company but the Interview process makes me nervous. Can someone help me with tried and tested tips to achieve success? I am good with the job but the interview process sucks for me.",2023-09-05 07:43:12
168qhax,How to prepare Snowflake DW based company for ML OPS?,"Hi Everyone, 

&#x200B;

I work in a company that primarly uses Snowflake as a DW, they never used Predictive analysis, just a bunch of tools, but the company wants to enroll into Machine learning, predictive modeling, could you reccomend how to fit this tools for a ML Ecosystem?

&#x200B;

Company Uses:

&#x200B;

Informatica MDM For ETLs

Snowflake for DW - Data Lake

Power BI for Dashboards

Jira for Agile Management

Control M (Not used yet but license is ready for scheduling)

&#x200B;

Honestly, company uses almost 0 python, any reccomendations?",2023-09-03 07:24:11
167c86t,Real-time analytics with stream processing and OLAP,N/A,2023-09-01 16:43:31
166c3n5,Trust your data - how to do simple row and column level lineage with dlt,"Hey folks,   


dlt is making a push towards adding more governance features, starting with **data contracts.**   


We will add the following modes as alternative to data contracts:  
 **\* evolve**: The current standard behavior, adapt the schema of the destination to the incoming data.  
 **\* freeze-and-trim**: Freeze the schema, and trim additional fields and subtables of incoming rows. This will still store all the incoming rows but will discard fields and subtable unknown to the schema.  
 **\* freeze-and-raise**: If any of the incoming rows do not fit the current schema, fail the current load. This will not store any data in the destination for this load and also not change the schema.  
 **\* freeze-and-discard:** Freeze the schema and completely discard any row that does not fit the current schema. Rows that fit the schema will be stored in the destination.  


if you want to give feedback to the above and influence direction, please comment here or join out slack (link on page on top)

In the meantime, I wrote a short article about what row and column lineages are, why we use them, and how to get them at loading with dlt.  


I hope this is useful!

[https://dlthub.com/docs/blog/dlt-lineage-support](https://dlthub.com/docs/blog/dlt-lineage-support)",2023-08-31 14:05:30
1669iuk,Deploying Airflow as a container on AWS,"Does anyone have experiences to share about deploying Airflow as a container using AWS ECS, either using EC2 or Fargate?

We currently have a self-deployed instance on an EC2. I’m wondering if this can give us a serverless option with less maintenance. I also wonder if this will be cheaper than MWAA, which will cost us about $1000 a month. Not to mention more flexibility.

I’m interested in using Airflow to kick off AWS services in the future, like Lambdas and Glue jobs. Currently we use Airflow to orchestrate Python pipelines and dbt jobs on the EC2.

Thank you in advance!",2023-08-31 12:15:03
164x1qp,"Top Open Source Alternatives to OLAP databases Snowflake, RedShift, and BigQuery",N/A,2023-08-29 22:45:51
164vpfq,[video] How Instacart Optimized Snowflake Costs by 50%,N/A,2023-08-29 21:54:42
15yv5sb,Starting as a Data Analyst for Faster Job Entry?,Could someone give me advice on a quicker way to land a job in the industry? I've heard that starting as a Data Analyst is a good way to go. I'm unsure whether it's better to start as a Data Analyst to enter the industry more quickly or to focus all my studies on Data Engineering. What do you thing?,2023-08-23 06:12:57
15wr7y4,CDMP Certification,"Hi everyone,  


I am thinking to take the CDMP ""Associate"" certification exam. I believe this will help me for my carrer in the data governance field.  


1- As per my understand, to get the certification, I should register to the ""Data Management Fundamentals exam"" (Link below), is that correct? 

[https://cdmp.info/product/data-management-fundamentals-exam/](https://cdmp.info/product/data-management-fundamentals-exam/)  


2- Any advice on how should I prepare the certification ? I'll download the eBook but do you have other suggestions?  


Thank ",2023-08-20 23:58:10
15vi81o,UX Research to Data Engineering?,Anyone made transition similar to this? Would love to hear some stories. Thanks!,2023-08-19 14:48:45
15ry3qv,Looking for feedback from Data Architects/Senior developers - Amazon gift card,"Looking for feedback from Data Architects/Senior developers for a tool we've developed which is similar to Pyramid Analytics. Would offer you a $50 Amazon gift card for your time (apologies we can't offer more as we are in startup mode). Took a year and about 10 people to create this but we really love it and think it has a ton of potential but want to show it to people in the industry to really see where it can be helpful. Please send me a DM or chat with your email and title if you're interested....Thank you in advance..

Key features: One tool that integrates all of the key features to create low-code/no-code Data Analytics solutions:

* Explore data - Explore any databases, data lakes, or Rest API directly from one interface
* Create virtualized view from any data source. Join disparate data sources and use SQL for everything including S3 Files and Rest API
* Take any view and turn it into an API for other applications
* Create visualizations
* Machine Learning with Auto ML, Jupyter notebook integration
* Amazon Glue like ETL interface
* Automation/Workflow to create data pipelines
* Works for regular as well as big data",2023-08-15 16:55:08
15o4ajp,Why You Should Create Your Next Analytical Project in Code,N/A,2023-08-11 09:41:25
15ioiih,Conundrum,"I am a data engineer with 6+ years of experience. We have built pipelines to bring data into our warehouse. Now my manager says ""now that we have brought in a lot of data, lets analyze it to find if we can provide the company with some insights on the data."" Shouldn't this be the other way round where we know what we are looking for?
Should i start transitioning into a data analyst? If yes, what are the ways to get started?",2023-08-05 06:57:00
15g6s9a,Lakehouse platform available for cloud and on-premise,"Would like to share new updates on the IOMETE lakehouse platform!

Scala Notebook

We are thrilled to announce an exciting update for Jupyter Notebook users! With our latest integration, connecting to [IOMETE's Jupyter Gateway](https://iomete.com/docs/starting-with-notebook) enables seamless data exploration and analysis from your local environment. Harness the power of Jupyter Notebook as you directly access and analyze data stored in IOMETE's data lake, making your data exploration process more agile and efficient than ever before. Unlock the full potential of your data resources and elevate your data-driven decision-making with this groundbreaking collaboration. Upgrade your data exploration experience today with Jupyter Notebook and IOMETE's integrated solution.

## Added sample SQL worksheets

Introducing the latest update Sample SQL Worksheets that are designed to elevate your data exploration experience to unprecedented heights.

With these pre-built SQL worksheets, you can now jump-start your data analysis journey effortlessly. Whether you are a seasoned SQL pro or just starting with data exploration, our sample worksheets offer valuable templates that cater to a wide range of use cases.

## IOMETE Data Lakehouse is Now Available for Google Cloud Platform Users! 🌟

We are thrilled to announce a significant milestone for IOMETE, your go-to data solution! As part of our commitment to expanding accessibility and empowering users across diverse platforms, we are excited to introduce the availability of IOMETE Data Lakehouse for Google Cloud Platform (GCP) users. You can easily [set up your clusters](https://iomete.com/docs/guides/deployment/gcp/install?utm_source=reddit&utm_medium=social&utm_campaign=gcpinstall) in any of the available regions on GCP.

## IOMETE Data Lakehouse is Now Available for Microsoft Azure Users! 🌟

We are excited to expand the reach of IOMETE Data Lakehouse to Microsoft Azure users, empowering them to harness the full potential of their data and drive innovation. Whether you are a data scientist, analyst, or business professional, IOMETE's availability on Azure opens up a world of possibilities for your data management needs.

Take advantage of this powerful combination today! To start with IOMETE Data Lakehouse on Microsoft Azure, visit for [detailed setup instructions](https://iomete.com/docs/guides/deployment/azure/install?utm_source=reddit&utm_medium=social&utm_campaign=azureinstall) and explore a new era of data-driven possibilities.

## Spark upgrade 3.3.3

As part of the IOMETE platform, Apache Spark is used for large-scale data processing. Spark is fast and easy to use. It can handle ETL processes, analytics, machine learning, and more. You can work faster, more quickly, and more efficiently with Apache Spark 3.3.3, a cutting-edge advancement.

As always, our team is dedicated to delivering cutting-edge features and integrations to enhance your data journey. We welcome your [feedback and suggestions](https://github.com/iomete/roadmap/discussions) as we evolve IOMETE's capabilities.

Thank you for being part of the IOMETE family. Stay tuned for more exciting updates in the future as we work together to transform the way you interact with data!

Happy data exploring! 🚀",2023-08-02 11:47:50
15ehk9q,WRITING CLEAN JAVA CODE,[https://digma.ai/blog/clean-code-java/](https://digma.ai/blog/clean-code-java/),2023-07-31 14:21:36
15dtcle,Career advice for a CPA exploring pivoting into Data Analytics or Data Engineering or Data Science,"I'm currently a CPA, working in b4 audit, my day to day work is really boring and I drag myself to do it. Before moving to the US, I worked in an international bank, heading the regulatory reporting unit. My responsibilities included designing, automating and submitting a complex set of regulatory reports which I really enjoyed. I also lead a data warehouse project where we designed and automated every single report. I have a good understanding of SQL, data warehousing, python, SAS enterprise guide and SAP Business Intelligence.   


My question is what is your advice for me to pivot into data engineering or data science? Should I seek a masters degree in data science or data engineering? Are online courses enough to secure a good position? What would you do if you were in my place?",2023-07-30 18:59:50
15a18to,Most complex pipeline you have built?,Keen to hear the details,2023-07-26 09:53:02
158bzov,How to Prepare for a Data Analytic Engineer Interview in a Large Retail Industry," 

I recently got an interview opportunity for the position of a Data Analytic Engineer in a major retail industry. Below is the job description provided by the company:

Job Description: The position is a combination of a Data Analytics Engineer and a Technical Project Manager under the Data team of the IT department.

Responsibilities include working primarily with SQL, Python, BI tools, and Google Cloud Platform to maintain the daily operations of the Data Warehouse and its data applications. This involves tasks such as data problem identification, data retrieval, data analysis, and application development.

During data-related projects, the role involves providing technical assistance and overseeing project quality.

Preferred Tools: MS SQL, Python, AWS, Google Analytics

Required Skills: No specific requirements Other Qualifications:

* Advanced SQL query skills, experience with BigQuery is a plus.
* Python data analysis and experience with machine learning libraries or other third-party machine learning services.
* Practical experience with GCP or AWS/Azure cloud services.
* Problem-solving abilities, including the ability to identify issues, propose constructive solutions, and implement them effectively.
* Enthusiasm for exploring the potential impact of data, actively learning, and generating ideas.

The interview process consists of a 60-minute online test and a 90-minute oral interview. I would like to ask for any advice on how to prepare for this interview, such as:

1. Topics covered in the online test (specific SQL or Python syntax to be used).
2. Types of questions the interviewer may ask during the oral interview.
3. Questions I can ask during the interview to demonstrate my interest and knowledge.",2023-07-24 14:17:57
156qfc5,My first article focused on the field of data engineering,Machine learning in data engineer,2023-07-22 17:44:44
14w4sar,How to Leverage Big Data Analytics for Sustainable Competitive Advantage,"As a seasoned IT architect currently working with a startup, I wanted to share my journey with you all, as well as some of the valuable insights I've discovered along the way. Here's an article where I dive into how organizations can use BDA to establish a sustainable competitive advantage, and some of the strategies to effectively implement it. I hope you find this beneficial in your own journey as well.

# How to Leverage Big Data Analytics for Sustainable Competitive Advantage

Today's technological era generates a massive amount of data, and businesses everywhere are trying to turn this raw data into actionable insights. Big Data Analytics (BDA) has emerged as a key strategy, helping businesses gain unique insights to unlock new opportunities and differentiate from competitors. Ignoring BDA might leave you trailing behind in the competition, or missing out on potential advantages.

To achieve the strategic benefits of BDA, you need to understand the processes that allow it to add value and remain competitive. This article offers a guide on how businesses can use several frameworks to evaluate BDA’s strategic value while avoiding pitfalls that come with improper implementation.

The VRIO framework (Valuable, Rare, Imitable, Organizationally embedded) can help assess the potential of BDA to create strategic business value. It prompts businesses to question if their BDA strategies offer valuable insights, are unique, challenging for competitors to copy, and supported by their organizational strategies and culture.

# Value of Big Data Analytics

The key strength of Big Data Analytics lies in its ability to provide unique insights that can be used to seize new business opportunities or counter competitive threats. These insights can improve various business areas, including business processes, product innovation, customer experience, and overall organizational performance.

# Uniqueness in Big Data Analytics

BDA becomes unique or 'rare' when few competitors can acquire or possess similar capabilities. Rarity in BDA can be evaluated in two ways: proprietary data content and analytical capability developed through experience.

# Imitating Big Data Analytics

BDA can be difficult and costly for competitors to imitate due to factors like time investment, the uniqueness of proprietary algorithms, and the maturity and culture of a company's IT department.

# Embedding Big Data Analytics

The final consideration is whether BDA can be organizationally embedded. It can be achieved when BDA aligns with the company's long-term business strategy and is facilitated by processes, policies, organizational structure, and corporate culture.

# Creating Value from Big Data Analytics

Creating strategic value with BDA requires investments in data assets, technological assets, and human talent. A conceptual framework can be proposed to describe how BDA creates strategic business value. This process can be framed by two concepts: Dynamic Capabilities and IT-Value Models. These models help in the capability building and capability realization processes.

# Building Big Data Analytics Capabilities

The process of turning IT investments into valuable BDA capabilities is dynamic. It includes managing and analyzing data to generate new insights. For this, companies need to develop a BDA strategy and understand how it can create tangible and intangible value.

# Realizing Big Data Analytics Capabilities

The real value of big data lies not in its volume but in the ability to derive meaningful and actionable insights from it. When utilized effectively, BDA can help refine business processes, develop initiatives, identify flaws or roadblocks, streamline supply chains, understand customers better, predict market trends, and develop new products, services, and business models.

In the end, creating value from BDA isn't just about having the right tools and capabilities. It's about using those capabilities to generate results, and then turning those results into actions that impact decision-making, improve customer relationships, enhance processes, and more.

I share more articles like this in my blog. If you're interested, you can visit: https://ainsys.com/blog/2023/06/30/bda/?utm\_source=linkedin&utm\_medium=social&utm\_campaign=data\_engineering&utm\_content=BDA\_analytics&utm\_term=BigData",2023-07-10 20:09:24
14lx2u7,https://rockset.com/blog/real-time-clinical-trial-monitoring-at-clinical-ink/,"Clinical Ink does real-time clinical trial monitoring using DynamoDB with an external index for real-time search, aggregations and joins",2023-06-29 06:16:52
14lb1yh,What Have We Learned From Using Pandas?,N/A,2023-06-28 14:30:25
14kijde,Analyze PostgreSQL Data using DuckDB and MotherDuck,N/A,2023-06-27 16:37:15
14ilf2o,How to minimize query fee when testing data pipeline?,"What’s the best practice to test data pipeline or data quality without query large data in production tables.
Our team currently build test datasets, tables in the dataset are sampling from production table. However, I don’t think this is a good way to ensure data quality.
How you guys do data pipeline test?",2023-06-25 12:45:53
14hhcjn,Help on Implementing Medallion Architecture in Synapse," 

I'm currently working on an internship project to build a proof of concept (POC) for a data warehouse on Azure Synapse. I have a few basic doubts that I couldn't find answers to, so I'm reaching out to the community for help.

I'm planning to use Synapse Spark pools and Delta tables in Spark instead of Databricks. I've learned about the three-layered architecture and intend to implement it. However, I'm confused about the purpose of each layer. I have specific questions regarding each layer:

Bronze: Is this layer primarily used for dumping full load files and other updates? Or do we have external tables on top of the data lake where we update the tables with the files we bring into the data lake? Does this layer consist only of ADLS (Azure Data Lake Storage), or does it include ADLS along with some Delta or external tables that require updates?

Silver: If we use Bronze only for storing files, my understanding is that the Silver layer will be more or less an exact replica of the OLTP (Online Transaction Processing) system, with some cleaning steps. What confuses me is that after implementing two layers, are we essentially creating a replica of the source system? If Bronze is used for both storing files and creating a replica of the source system, what is the significant difference between Bronze and Silver?

Gold: I've heard that regardless of the technology or tools we use, dimensional modeling is key to making BI (Business Intelligence) work more effectively. Is this the layer where we have our Kimball-style dimensionally modeled data?  


Thank you!",2023-06-24 02:47:09
14b3yer,Scraping dynamic websites using cloud based Scraper API and Python,N/A,2023-06-16 18:17:54
14ahntm,Where have your companies stored their Data Dictionary? How have you set them up?,"Title. Don't have to read any of the below, just my random thinking: 
-----
We're talking about creating a Data Dictionary.  What will be stored in it? Undefined.  

I've been asked where to store it.  Which isn't really the big issue in my mind (more, that these things tend to fail; how can this one not fail), but it's the immediate ask. 

We currently store various documentation in 8 different systems (too many)

  * Yammer
  * Teams/Sharepoint
  * A homegrown intranet page
  * Learning Mgmt System
  * ServiceNow
  * A new document mgmt system that's upcoming
  * Onenote
  * Dataverse tables fed via power apps

There are also 3rd party products that are expensive and I assume have weird caveats. My initial reaction is we don't need an 9th system with a subscription. Then again, I don't know what these do. I assume some cool things along with bizarre limitations. 

I lean toward keep it simple, either:
  * Excel file in Sharepoint with (mostly) read-only access, or
  * A series of database tables.  
   * This appeals to me for the queries that could be done...but to do that, also means a more to maintain. And these things tend to fail due to lack of updates, so maybe that's not a good idea. 
  * Power app feeding a dataverse table.  Then users could enter...but would they 1 year later? 

So then I'm down to an Excel file stored somewhere, starting with the bare minimum, and *if* that's kept up, expand it. 

Your thoughts though?",2023-06-16 00:01:34
145b31e,Is it necessary to learn Hadoop and Spark for cloud-native trends ?,"I have recently entered the field of data and have been working as a DA intern at a small startup for a year. The company utilizes GCP services, and my usual responsibilities include scheduling with Airflow, data modeling with dbt, and providing reports to the business team. I have noticed that I am more interested in the governance and operational aspects of data pipelines compared to being a DA.

Is it necessary to learn Hadoop and Spark for cloud-native trends if I want to be a DE in the future ?",2023-06-09 17:17:09
143ffuv,A beginners guide to testing data pipelines with dbt-expectations,N/A,2023-06-07 14:55:53
13zfuzo,ClickHouse & Apache Doris in Keyword Searching by Response Time,N/A,2023-06-03 15:16:33
13kszmz,Csv serialisation and deserialization,"Hey guyss wanted some thoughts/opinion/sugeestion on this
I have a use case of saving and loading pandas csvs the dataframes can contain datatypes such as lists date time dicts anything that you can imagine 
I am looking to find a way where i can properly save it and upon loading nothing gets changed",2023-05-18 08:45:34
13j87ao,MSc Data Science vs MSc Big Data vs MSc Data Analytics,"Hi everyone, please I would like you to help me clarify the differences among these three courses in terms of content, ability to cope and career opportunities. I had my first degree in Biochemistry, but have always loved to work with figures, and so fell in love when I had the opportunity to partake in a 6months data analytic bootcamp. Now I want to go for my masters, but confused about the differences among these three as they keep popping in my face.",2023-05-16 15:21:30
137x76x,Enhance your Data Team's Productivity with dbt and Gitpod,N/A,2023-05-04 19:25:19
12yrgq0,Is there an existing framework designed to store general information about anything? Almost how a human brain works?,"I'm brainstorming for a project and I would be interested in storing information about nouns and things associated to that particular noun. I want a database or framework for storing information about everything in a structured way. Almost as though an ISO structure exists for everything. 

For example, I want to store myself as an object. I also want to store things about me and associate them to the ""me"" object: what is my schedule today, what were my diary entries yesterday, what are my emails, who are my friends, what are my preferences, where was I born, what is my medical history, what was my grocery list a week ago, what TV shows did I watch yesterday and for how long. And the flexibility to do this for any number of people and any amount of associated information. 

Similarly, for the TV show I watched yesterday, I want to store a record of that and associated information. Who's staring in it? When was it released? If it was an episode of Iron Chef, and the key ingredient was beef, then I also want:

An entry for beef. Facts about beef. A mapping between this object and the ""me"" object that bought beef a week ago at the grocery store. 

A key to this is that if I want to associate beef to a grocery list that I then associate to me, I want how these things are associated to be predefined so that it's associated in the same way every time someone adds beef to a grocery list. 

It seems wild how complex this could get. And it almost seems like we'd need an ISO structure for every noun and how anything can relate to that noun. 

Does something like this exist, either as theory/concept or in reality?",2023-04-25 18:20:52
12sqo7t,Can a good Data Analyst/Scientist make a good Data Engineer?,"I’m currently working as a Data Analyst in the UK, and have been in my role for just over a year. My background is in Physics, including PhD (which involved a lot of data analysis). I mainly work in R and SQL, but also did some work in Python during my PhD.

I’m curious about Data Engineering, but I wonder whether I’d actually be able to make a good one. I say that because I don’t consider myself to be a “pure” programmer - like I could never see myself in a software development role for example. 

I understand the basics of DE, in terms of ETL (extracting data from the raw source, transforming it into a useable format, and loading it onto a SQL database, or AWS for example, so it’s accessible to analysts and data scientists etc.), but I don’t know much more than that. In terms of my skill set, I’m very comfortable with doing things like joins in SQL/R, and writing/testing functions in R.

Just after some thoughts/advice on those who have perhaps made the change, did you enjoy or regret it, and so on.

Thanks",2023-04-20 08:01:03
12kijdw,🔉 Work with dbForge Studio for SQL Server on Linux & macOS via CrossOver,"💡  dbForge Studio for SQL Server is a top-class IDE for developing, managing, and administering SQL Server databases.  

&#x200B;

🚀Now you can run dbForge Studio for SQL Server on a Mac or Linux machine

without compromising your experience. For that purpose, you can use CrossOver. 

&#x200B;

📰 Read more about IDE: [https://www.devart.com/dbforge/sql/studio/](https://www.devart.com/dbforge/sql/studio/) 

&#x200B;

⚡ Learn how to install and run:

[https://www.devart.com/dbforge/sql/studio/cross-platform-installation-guide.pdf](https://www.devart.com/dbforge/sql/studio/cross-platform-installation-guide.pdf) 

&#x200B;

✅ Download a 30-day trial version of dbForge Studio: [https://www.devart.com/dbforge/sql/studio/download.html](https://www.devart.com/dbforge/sql/studio/download.html) 

&#x200B;

\#dbForgeStudio #Linux #macOS #CrossOver",2023-04-13 09:19:10
12j33ae,"Is data engineering fun,challenging or interesting?","Whatever fun means to You.

Could You please elaborate on your answer  and ,if You want, also your definition of fun?

I have already finished a bootcamp on data engineering but i'm still struggling to understand what they do in a work environment.

Do You spend a Lot of your time fixing tables or csv files?

What would be high-level and low-level data engineering?

 I'm curious because i'm pretty interested in this field(just finished a bootcamp) and i would like to hear from actual data engineers in the field.

Thank you in advance for your answers.",2023-04-12 00:11:00
12hhz4z,Blog post: Using Azure Data Factory to read and process REST API datasets,N/A,2023-04-10 12:54:50
12fswm7,Looking for a DS4A Data Engineer Cohort 1 Fellow,"Hi, is there anyone who had completed the Correlation One DS4A data engineering cohort 1 last year who is willing to chat with me about the program? I have a pending offer in IT business analyst but not sure if I should hold out to finish the data engineering fellowship(part of cohort 2) to then search for a job in data engineering. Not sure how effective the program is and how easy will it be to find an entry-level position. Thanks in advance!",2023-04-08 17:40:42
12ajow3,Data Modeling — The Unsung Hero of Data Engineering: An Introduction to Data Modeling,N/A,2023-04-03 12:59:06
128cqg6,Looking for Review: Help me Improve my Notebooks,"Hope you are doing well, I would be grateful if you could review my notebooks and give your constructive feedback.

[https://www.kaggle.com/abmsayem/code](https://www.kaggle.com/abmsayem/code)",2023-04-01 05:46:52
11zn2s0,Automate dbt development testing in Snowflake with data-diff,N/A,2023-03-23 15:02:49
11xj52v,"Question about my role, thanks for reading. Bi Engineer or Data Analyst","Hi guys, how are you? Last week I started a job where I was trained in a specific area, I am currently working with SQL, Python and PowerBi, + maybe some ETL, what position am I developing, as a bi engineer or as a data analyst",2023-03-21 14:51:33
11vlh9m,"Video - DataOps in action with Nessie, Iceberg and Great Expectations",N/A,2023-03-19 13:30:16
11vi2mu,Writing Well: A Data Engineer's Advantage,N/A,2023-03-19 10:45:14
11udak6,Cassandra to Snowflake Headache,"I work at a small size start up and this has been my by far biggest challenge that still remains unsolved:

We have a legacy Java application still running that uses Cassandra hosted on EC2. The tables are mostly hundreds of millions of rows. 

Almost none of the tables contain a modified_at or created_at column and they aren’t partitioned/clustered in a manner where they can be read out incrementally.

I need these tables synced to Snowflake daily preferably hourly. Currently having a Python Fargate task running really weird queries to read the rows out incrementally without having to use ALLOW FILTERING in the Cassandra queries, but many rows seem to not make it into Snowflake. I think this is because large query results returned from Cassandra are unstable.

Only one person is left at the company that knows Java so editing the legacy application is not an option. I see Matillion has a Cassandra Conector I’m going to try, but I’m guessing it will run into the same issues as my custom Python service I made to do this. 

Any suggestions?",2023-03-18 03:35:18
11tupt6,Data engineer to quant engineer,Can a data engineer become quant engineer?,2023-03-17 15:42:51
11r2wf0,The importance of Data ingestion,"Hey folks, I had a piece of content I was working on. I'm an engineer and not a content writer so I would appreciate your feedback :D 

 

In today’s dynamic environment, the main data processing steps include: data ingestion from a source, data storage, data transformation, data cleansing, and data validation. After these steps, data can be stored and used in further analyses and data analytics applications.

Data analysts are eager to find new ways of processing data since this kind of information always grows in both volume and variety, and data-processing tools are being constantly updated (multiple times every year).

‍

##### What Is Data Ingestion?

Data ingestion is the process of transporting data from one or more sources to a target site for further processing and analysis. The data can be taken from multiple sources, including data pools, SaaS applications, IoT devices, on-premises databases, etc., and would usually end up in different target environments, including cloud data marts and data warehouses.

‍

##### Why Is Data Ingestion Important?

Data ingestion reorganizes company data to the desired format and helps ease its usage, especially during the extract, transform, and load (ETL) operations. Tools for data ingestion can both process a variety of data formats while simultaneously reorganizing large volumes of unstructured (raw) data.

Once data is ingested, organizations can employ analytical tools to get useful BI insights from multiple data sources. Companies can improve their applications and offer different features and services derived from the insights that are produced by ingested data sets. With proper data inputs, businesses can provide data analytics to authorized individuals more efficiently. Additionally, data ingestion brings the data to programs that need the most up-to-date data. For example, real-time data, when applied to the public transport system, can improve its efficiency (fuel consumption and traffic patterns), minimize arrival times, avoid congestion etc.

‍

##### How To Best Conduct Data Ingestion?

Data ingestion can be done in 3 different ways. More specifically, this can be completed through either real-time, batches or a combination of both processes, known otherwise as lambda (or micro-batch approach). Companies can choose one of the three types depending on their business objectives, IT infrastructures, and financial feasibility.

1. **Real-time data ingestion** is the process of the collection and transfer of data from multiple sources in real-time using tools such as change data capture (CDC). CDC continually monitors the transaction logs and moves the changed data without interfering with the database workload. Real-time ingestion is crucial in time-limited use cases, such as power grid monitoring or stock market trading, especially when companies need to react rapidly to new information. Real-time data pipelines are also important in making quick operational decisions and defining actions based on new insights.
2. **Batch-based data ingestion**, on the other hand, is the process of the collection and transfer of data in batches but in pre-specified time intervals. The ingestion process will collect data based on certain conditions, event triggers, or some forms of logical order. Batch-based ingestion is applicable when companies need to collect specific data on a less rigorous daily basis and or simply don’t need a constant inflow of data for real-time decision-making. An example could be a printed newspaper that collects information over 24 hours and publishes it (part of it) at a certain time.
3. **Micro-batch ingestion** is a data ingestion process that consists of both real-time and batch methods. The process includes the batch, serving, and speed layers. The first two layers index data in batches, and the speed layer instantly indexes the data that should otherwise be picked up by the slower batch and serving layers. This ongoing data transfer between different layers ensures that data is available for querying with no delay.

 

##### The Benefits of Data Ingestion

These Data ingestion techniques provide various benefits, enabling firms to manage data while also improving their market positions effectively. Some of the advantages include the following:

* Companies can save time and money: Data ingestion automates some of the tasks that are previously done manually by developers. With an automated system in place, however, critical developers can instead dedicate their time to other, more important tasks.
* Dev-teams can improve their software applications: After implementation, dev-teams can utilize data ingestion techniques to ensure that their applications transfer data quickly and provide a smooth experience directly to the end-users.
* Data is promptly available: Companies can gather data stored across various servers and move them all together to a unified environment available for immediate access and further analysis.
* Data simplified: Data ingestion implementation, together with ETL tools, will convert different data types into pre-defined formats and then transfer them to a single data warehouse.
* Improved decision-making: Real-time data ingestion allows businesses to uncover problems and opportunities on the spot, thereby making the right decisions at the right time.

&#x200B;

 

##### The Must-Have Features For 2023 / Incoming Trends in 2023

Data ingestion tools can gather and transfer all structured, semi-structured, and unstructured data from multiple sources to target destinations. These tools automate manual ingestion processes and undertake processing steps that move data from one point to another. Other important features to pay attention to in the upcoming period are as follows:

* **Data integration tools:** Traditional data integration platforms incorporate features for every step of the data value chain, and namely, the aforementioned data cleaning, data consolidation, ETL processes, data virtualization, and transfer and storage. They enable a regulated (and secure) flow of simplified data operations through increasing productivity without any processing delays.
* **AI-powered search**: An AI-Powered Search can bring site visitors what they need right off the spot, and this will help business owners achieve better customer satisfaction, higher conversion rates, and increased revenues. An AI-based search engine will display results that are personalized to individual users based on their profiles, desires, and various other tendencies.
* **Video-based search**: Implementing automated captions helps people consume media content effectively. With Omnisearch, you can utilize our advanced search functionality to find the exact video you need or navigate the database using filters such as topics, dates, and many more. Additionally, when you search for specific files, Omnisearch automatically tells you the relevance of various files to your search terms; this makes it quick and easy to navigate through your massive database to find and locate exactly what you need.",2023-03-14 10:48:31
11qxni5,"Planning to adopt real-time analytics DB/warehouse, any view on Singlestore DB. Earlier it was termed as MemSQL?","Trying to compare with Click-house. Any suggestion.   


Context: We are on the AWS cloud and currently using PostgreSQL, but the performance is not that great.",2023-03-14 05:28:34
11psw9x,Tool to map text in field to a collection to create rollups,"I get data for millions of records monthly from stores, and looking to map the store name to a consistent output. Ex:

Walgreens Store #123  
East River Walgreens  
Wlgreens Store #234  
Acme (Next to Walgreens)  


I'd like to create a column next to it with standardized outputs

Walgreens Store #123 ->Walgreens  
East River Walgreens -> Walgreens  
Wlgreens Store #234 -> Walgreens  
Acme (Next to Walgreens) ->Acme  


In a prior company 10+ years ago we used a tool called Oracle Data Lens that would be pretty robust at identifying misspellings (you'd click on the word an associate it to the correct spelling, then would create logic to identify it (""If Text Walgreens then Walgreens"") (""If Text ""next to X"" then blank"").  Is there a more modern product to do this?

&#x200B;

Note this is a simplified example, but the data would be consistent.  Using just regular expressions in a massive sp is not an ideal solution",2023-03-12 23:05:25
11oci3y,I have gotten a interview with databrics field engineering team SA role,I have searched everywhere but I am really not sure if  it’s a presales role or Technical one. Can anyone help me with your interview process if you had a similar experience. Any help is appreciated,2023-03-11 06:10:12
11my8rg,Pay for what you use: Cube Cloud pricing gets back to its roots,N/A,2023-03-09 17:33:23
11jzx6r,[PySpark] Read S3 x Read Local,"Hey guys,

I'm working with a dataframe with 30 million lines, using the AWS Glue docker image, but I'm having a very slow time to perform operations, like select, count and group by, when I read the files directly from s3.

I noticed that if I download the parquets from s3 and read them locally, this problem of slowness is solved and the operations work very quickly.

I tried to use cache() and persist() after reading the files from s3 but the performance was still very slow.

Do you know any way to optimize the performance without downloading the parquet files and reading them locally?

&#x200B;

Regards",2023-03-06 14:09:07
11h31pe,Clickhouse on Kubernetes,Hello everyone! Want to run ClickHouse on Kubernetes? Are you doing it already? We're running a webinar on the Altinity Operator for ClickHouse on March 7th. For beginners we have an intro to how it works; for experts we'll share lessons on operating at scale. Hope to see you there! [https://altinity.com/events/cloud-native-clickhouse-at-scale-using-the-altinity-kubernetes-operator-for-clickhouse](https://altinity.com/events/cloud-native-clickhouse-at-scale-using-the-altinity-kubernetes-operator-for-clickhouse),2023-03-03 15:08:35
11ge9rh,Do you like working in terminal or clicking buttons?,"In my team the hypothesis is that DEs don't like to use terminal, that they likely would avoid using a tool that requires you to use terminal and install frameworks, use CLI for creating jobs and running them, rather use UI to click around and have all the functionality without the need to open a terminal. I'd like to know honest opinion, is it the case? I find clicking things restrictive and also somewhat boring, but I understand that it makes things easier. Maybe most DEs don't have the need to know how to use a terminal because of existing tools with UIs. Is it the case?",2023-03-02 21:15:20
11f44qq,Data Observability: The Next Frontier of Data Engineering,N/A,2023-03-01 13:44:32
11aznf5,Tweepy streaming data,"Dear All, I hope you’re doing well.

I’m planning to create a project on twitter data, for that I need to get the streaming tweets, 

If anyone know how to do it, please let me.

I tried some resources available, but still unable to get the data.

Thanks in advance.",2023-02-24 19:24:20
118wer7,Is this a data engineering position?,"I found the following job description and was wondering if this qualifies as a data engineering position

>\- You will analyze, improve, and develop data architectures in Microsoft SQL Server to further develop our existing tool.   
>  
>\- You will always consider the compatibility of the data and structure with AWS services against the backdrop of migration / replication capabilities.   
>  
>\- You will replicate data where necessary and migrate data to the AWS Cloud where possible. In doing so, the end-to-end functionality of the applications based on this data is your top priority.   
>  
>\- You will collaborate on the data structure concept, the semantics model and the basic data collection, processing and usage concept with a focus on orientation towards the cloud.   
>  
>\- You will advise and support on all questions and decisions regarding data structure.

Any opinions? :)",2023-02-22 12:02:06
11700w4,Vertica Certification worth it ?,"Hi all

The project I'm currently on is using Vertica and we have been asked to go over the vertica Certification course but don't have to actually get the certification.

As this is a short project is it worth actually attempting to pass the certification or am I better of doing more mainstream certifications?",2023-02-20 06:48:49
18n5sqj,Cyber Sec vs Data Engineer,"What is the outlook of both the fields in next 5 years?
What’s the difference in compensation between the two roles? Are Cyber Sec professionals paid way more than Data Engineers for the same experience?",2023-12-20 21:31:02
13wmxpo,Free Mentorship in Data Engineering,"I am a cloud/data architect with 7+ years of experience. I love helping, guiding, and mentoring students and working professionals on technical or personal topics.

During my career in cloud and data engineering, I've received invaluable support from fellow professionals, and I'm eager to pay it forward. That's why I'm offering free mentorship to anyone who may benefit from it in these fields.

Our 20-minute calls can cover a range of topics, such as

1. Transitioning into cloud and data engineering,
2. Strategies for career growth,
3. Tackling technical challenges, and
4. Receiving feedback on your work.

As your mentor, I'll draw on my experience to provide guidance and support tailored to your needs. I'm passionate about helping others succeed in these fields and believe that mentorship can make a significant difference.

If you're interested in learning more or scheduling a call, please feel free to direct message me. I'm excited to hear from you and help you achieve your cloud and data engineering goals!

**Note: There are only 10 slots open due to my availability, these slots will be based on a first-come basis. I will open slots once I have some time again.**",2023-05-31 13:53:01
14rcaj9,Iceberg won the table format war: But not in the way you thought it might,N/A,2023-07-05 14:29:57
15n76ia,entry level jobs in usa,"guys of dataengineering, my company laid off 70 percent of their workforce this june. i have been applying for relevant positions since then and i am not getting a single reply back honestly. I have a year of exp working as a Data Engineer. Where are all the jobs right now? I understand market is tough or whatever, but seriously, why are companies being such dicks? I NEED A BASIC ENTRY LEVEL JOB WHY TF IS THAT SO DIFFICULT?  


Update: I just got mail from my company saying that my H1B application got picked but they cannot file it for me (of course) as I am not going to be on their payroll till then. what a waste man",2023-08-10 09:11:11
17fakcd,"Announcing v0.15: Interactive Declarative Migrations, Functions, Procedures and Domains",N/A,2023-10-24 11:30:09
179v26l,Do Cloud Certification help in hiring?,"Hi , I have 2 years of experience in Data Engineering and preparing for AWS SOLUTIONS ARCHITECT- Associate exam .Does it help in hiring and how much ?",2023-10-17 10:17:51
14mxlvn,How can I find a mentor?,"Hey everyone!

I'm a 27/M data engineer based in Europe, and I could really use some guidance in my career journey. As an immigrant and the only one in my family in the industry, I often find myself unsure of where to turn for advice.

I'm on the lookout for an experienced professional who has successfully navigated the data engineering industry and can lend me some insights to make better decisions.

If any of you have been in a similar position, I'd love to hear your stories. How did you manage to find that mentor figure who provided you with the guidance you needed?",2023-06-30 11:19:36
14lw0op,Databricks Data + AI Summit,Anyone else here? What are your thoughts so far?,2023-06-29 05:19:34
12u8ske,How We Increase Database Query Concurrency by 20 Times,"Most current OLAP databases are built with a columnar storage engine to process huge data volumes. They take pride in their high throughput, but often underperform in high-concurrency scenarios. As a complement, many data engineers invite Key-Value stores like Apache HBase for point queries, and Redis as a cache layer to ease the burden. The downside is redundant storage and high maintenance costs.

Apache Doris has been striving to become a unified database for data queries of all sizes, including ad-hoc queries and point queries. Till now, we have already taken down the monster of high-throughput OLAP scenarios. In the upcoming Apache Doris 2.0, we have optimized it for high-concurrency point queries. Long story short, it can achieve over 30,000 QPS for a single node (a 20-time increase in concurrency), by methods of **partitioning and bucketing, indexing, materialized view, runtime filter, TOP-N optimization, row storage format, short-circuit, prepared statement, and row storage cache.**

# 1. Partioning and Bucketing

Apache Doris shards data into a two-tiered structure: Partition and Bucket. You can use time information as the Partition Key. As for bucketing, you distribute the data into various nodes after data hashing. A wise bucketing plan can largely increase concurrency and throughput in data reading, because the system only needs to scan one bucket in one partition before it can locate the needed data.

# 2. Index

Apache Doris uses various data indexes to speed up data reading and filtering, including smart indexes and secondary indexes. Smart indexes are auto-generated by Doris upon data ingestion, which requires no action from the user’s side.There are two types of smart indexes:

* **Sorted Index**: Apache Doris stores data in an orderly way. It creates a sorted index for every 1024 rows of data. The Key in the index is the value of the sorted column in the first row of the current 1024 rows. If the query involves the sorted column, the system will locate the first row of the relevant 1024 row group and start scanning there.
* **ZoneMap Index**: These are indexes on the Segment and Page level. The maximum and minimum values of each column within a Page will be recorded, so are those within a Segment. Hence, in equivalence queries and range queries, the system can narrow down the filter range with the help of the MinMax indexes.

Secondary indexes are created by users. These include Bloom Filter indexes, Bitmap indexes, [Inverted indexes](https://doris.apache.org/docs/dev/data-table/index/inverted-index/), and [NGram Bloom Filter indexes](https://doris.apache.org/docs/dev/data-table/index/ngram-bloomfilter-index/). 

Example: `select * from user_table where id > 10 and id < 1024`

Suppose that the user has designated `id` as the Key during table creation, the data will be sorted by `id` on Memtable and the disks. So any queries involving `id` as a filter condition will be executed much faster with the aid of sorted indexes. Specifically, the data in storage will be put into multiple ranges based on `id` , and the system will implement binary search to locate the exact range according to the sorted indexes. But that could still be a large range since the sorted indexes are sparse. You can further narrow it down based on ZoneMap indexes, Bloom Filter indexes, and Bitmap indexes.This is another way to reduce data scanning and improve overall concurrency of the system.

# 3. Materialized View

The idea of materialized view is to trade space for time: You execute pre-computation with pre-defined SQL statements, and perpetuate the results in a table that is visible to users but occupies some storage space. In this way, Apache Doris can respond much faster to queries for aggregated data and breakdown data and those involve the matching of sorted indexes once it hits a materialized view. This is a good way to lessen computation, improve query performance, and reduce resource consumption.

# 4. Runtime Filter

In multi-table Join queries, the left table is usually called ProbeTable while the right one is called BuildTable, with the former much bigger than the latter. In query execution, firstly, the system reads the right table and creates a HashTable (Build) in the memory. Then, it starts reading the left table row by row, during which it also compares data between the left table and the HashTable and returns the matched data (Probe).

During the creation of HashTable, Apache Doris generates a filter for the columns. It can be a Min/Max filter or an IN filter. Then it pushes down the filter to the left table, which can use the filter to screen out data and thus reduces the amount of data that the Probe node has to transfer and compare.This is how the Runtime Filter works. In most Join queries, the Runtime Filter can be automatically pushed down to the most underlying scan nodes or to the distributed Shuffle Join. In other words, Runtime Filter is able to reduce data reading and shorten response time for most Join queries.

# 5. TOP-N Optimization

TOP-N query is a frequent scenario in data analysis. For example, users want to fetch the most recent 100 orders, or the 5 highest/lowest priced products. For such queries, Apache Doris implements TOP-N optimization:

1. Apache Doris reads the sorted fields and query fields from the Scanner layer, reserves only the TOP-N pieces of data by means of Heapsort, updates the real-time TOP-N results as it continues reading, and dynamically pushes them down to the Scanner.
2. Combing the received TOP-N range and the indexes, the Scanner can skip a large proportion of irrelevant files and data chunks and only read a small number of rows.
3. Queries on flat tables usually mean the need to scan massive data, but TOP-N queries only retrieve a small amount of data. The strategy here is to divide the data reading process into two stages. In stage one, the system sorts the data based on a few columns (sorted column, or condition column) and locates the TOP-N rows. In stage two, it fetches the TOP-N rows of data after data sorting, and then it retrieves the target data according to the row numbers.

# 6. Row Storage Format

As we know, row storage is much more efficient when the user only queries for a single row of data. So we introduced row storage format in Apache Doris 2.0. We chose JSONB as the encoding format for row storage for three reasons:

* **Flexible schema change**: If a user has added or deleted a field, or modified the type of a field, these changes must be updated in row storage in real time. So we choose to adopt the JSONB format and encode columns into JSONB fields. This makes changes in fields very easy.
* **High performance**: Accessing rows in row-oriented storage is much faster than doing that in columnar storage, and it requires much less disk access in high-concurrency scenarios. Also, in some cases, you can map the column ID to the corresponding JSONB value so you can quickly access a certain column.
* **Less storage space**: JSONB is a compacted binary format. It consumes less space on the disk and is more cost-effective.

In the storage engine, row storage will be stored as a hidden column (DORIS\_ROW\_STORE\_COL). During Memtable Flush, the columns will be encoded into JSONB and cached into this hidden column. In data reading, the system uses the Column ID to locate the column, finds the target row based on the row number, and then deserializes the relevant columns.

# 7. Short-Circuit

Normally, an SQL statement is executed in three steps:

1. SQL Parser parses the statement to generate an abstract syntax tree (AST).
2. The Query Optimizer produces an executable plan.
3. Execute the plan and return the results.

For complex queries on massive data, it is better to follow the plan created by the Query Optimizer. However, for high-concurrency point queries requiring low latency, that plan is not only unnecessary but also brings extra overheads. That’s why we implement a short-circuit plan for point queries.

&#x200B;

https://preview.redd.it/40g6ww69d9va1.png?width=1606&format=png&auto=webp&s=0aa4c9bb1d7c0ad802dd2b75808ed92409185317

Once the FE receives a point query request, a short-circuit plan will be produced. It is a lightweight plan that involves no equivalent transformation, logic optimization or physical optimization. Instead, it conducts some basic analysis on the AST, creates a fixed plan accordingly, and finds ways to reduce overhead of the optimizer.

For a simple point query involving primary keys, such as `select * from tbl where pk1 = 123 and pk2 = 456,` since it only involves one single Tablet, it is better to use a lightweight RPC interface for interaction with the Storage Engine. This avoids the creation of a complicated Fragment Plan and eliminates the performance overhead brought by the scheduling under the MPP query framework.

Details of the RPC interface are as follows:

    message PTabletKeyLookupRequest {
      required int64 tablet_id = 1;
      repeated KeyTuple key_tuples = 2;
      optional Descriptor desc_tbl = 4;
      optional ExprList  output_expr = 5;
    }
    message PTabletKeyLookupResponse {
      required PStatus status = 1;
      optional bytes row_batch = 5;
    optional bool
    empty_batch = 6;
    }
    rpc tablet_fetch_data(PTabletKeyLookupRequest) returns (PTabletKeyLookupResponse);

`tablet_id` is calculated based on the primary key column, while `key_tuples` is the string format of the primary key. In this example, the `key_tuples` is similar to \['123', '456'\]. As BE receives the request, `key_tuples` will be encoded into primary key storage format. Then, it will locate the corresponding row number of the Key in the Segment File with the help of the primary key index, and check if that row exists in `delete bitmap`. If it does, the row number will be returned; if not, the system returns NotFound. The returned row number will be used for point query on `__DORIS_ROW_STORE_COL__`. That means we only need to locate one row in that column, fetch the original value of the JSONB format, and deserialize it.

# 8. Prepared Statement

The idea of prepared statements is to cache precomputed SQL and expressions in HashMap in memory, so they can be directly used in queries when applicable.

Prepared statements adopt MySQL binary protocol for transmission. The protocol is implemented in the mysql\_row\_buffer.\[h|cpp\] file, and uses MySQL binary encoding. Under this protocol, the client (for example, JDBC Client) sends a pre-compiled statement to FE via `PREPARE` MySQL Command. Next, FE will parse and analyze the statement and cache it in the HashMap as shown in the figure above. Next, the client, using `EXECUTE` MySQL Command, will replace the placeholder, encode it into binary format, and send it to FE. Then, FE will perform deserialization to obtain the value of the placeholder, and generate query conditions.

&#x200B;

https://preview.redd.it/nwmpb1d2e9va1.png?width=1134&format=png&auto=webp&s=3b35ed7bac7cc43c38f013e21b0baccdd25d9b17

Apart from caching prepared statements in FE, we also cache reusable structures in BE. These structures include pre-allocated computation blocks, query descriptors, and output expressions. Serializing and deserializing these structures often cause a CPU hotspot, so it makes more sense to cache them. The prepared statement for each query comes with a UUID named CacheID. So when BE executes the point query, it will find the corresponding class based on the CacheID, and then reuse the structure in computation.

# 9. Row Storage Cache

Apache Doris has a Page Cache feature, where each page caches the data of one column. 

https://preview.redd.it/v6fvikn8e9va1.png?width=568&format=png&auto=webp&s=df84b2463b5c32750dcfb032166359f34db376e8

As mentioned above, we have introduced row storage in Doris. The problem with this is, one row of data consists of multiple columns, so in the case of big queries, the cached data might be erased. Thus, we also introduced row cache to increase row cache hit rate.

Row cache reuses the LRU Cache mechanism in Apache Doris. When the caching starts, the system will initialize a threshold value. If that threshold is hit, the old cached rows will be phased out. For a primary key query statement, the performance gap between cache hit and cache miss can be huge (we are talking about dozens of times less disk I/O and memory access here). So the introduction of row cache can remarkably enhance point query performance.

Full post link: [https://medium.com/geekculture/how-we-increase-database-query-concurrency-by-20-times-440f8b772fe3](https://medium.com/geekculture/how-we-increase-database-query-concurrency-by-20-times-440f8b772fe3)",2023-04-21 15:54:05
12c6w5u,"How much do Data Architects (let's say at the highest skilled person) actually make? More specifically, how much do they see in their checking account every 2 weeks (assuming bi-weekly payment, after tax-deduction)?","The reason for asking this question is I have seen a lot of variation in websites, like Glassdoor, and never heard directly from a person who is actually in that kind of a position. I would like to know in reality how much cash is deposited in their checking account.",2023-04-05 03:27:52
11gpj9z,Words in Data Engineering,"Which are the most triggering for you to hear?

[View Poll](https://www.reddit.com/poll/11gpj9z)",2023-03-03 03:09:13
18n0515,Quick rant: why does the team have me do BI developer job?,"I joined the team as a Data Engineer, and recently someone (BI Developer) got fired and another BI Developer got on medical leave for 2 months. The team was short staffed, they asked me to help out and I understand. However, the BI Developer got back from leave and they are still asking me to do their BI Development work. My manager manages the BI and Data Engineering team so we are in the same team, but I really hate that they are dragging me into BI and I feel that I no longer do Data Engineering works. I am all day doing SQL, Tableau and helping them to look into their Report SQL issue and it really pisses me off.

Edit: I left a data analyst job at a very good company to join this lousy company as a data engineer, in hope of doing more engineering and less analytics.",2023-12-20 17:32:24
18f4upz,Anyone here that works as data engineer in Canada as an immigrant?,"Hello everyone

Recently I have been thinking a lot to move to another country, specially Canada. So I wonder how is the job market for data engineers in Canada? How hard would be for a foreigner to landing a job? Anyone here that lives and work in Canada as a immigrant that can share some tips to move there?",2023-12-10 14:33:14
16111y2,"i got accepted in a data officer job, and i don't know what the hell i'm doing","Guys, i really need some serious advice here, few weeks ago , i applied for a data officer job, it was an application of desperation (been 5 months of applying after my graduation, no results), 

i got the interview, i did some basic reading on data officer, i was just bulshitting during the whole interview, saying fancy words and things i read with my data science knowledge, but the crazy part is they actually bought it, the people who were intervening me were the CEO and the HR manager, and apparently , they don't know much about data .

now i was surprised yesterday with a job offer, saying they were impressed of how ""well i understood the role"", 

the pay is not that great, and the job is not specifically what i want, but at this point, i'm kind of desperate, months are passing since i graduated and i kind need some experience and stability, the scary part is i don't know shit about what they are asking, things like using Salesforce to import, export and analyze data!  reviewing and evaluating projects , and deliverables, keeping track and providing monthly management reports, 

these tasks sound like a business' kind of job rather than data, but somehow they accepted me, and expecting me to start September, while my academic knowledge only gave me solid SQL, python, ML/AI and some basic data engineering skills, nothing about data management or officer.

i have two options now, dive in this job that i don't even know what i'm supposed to do, and be in an awkward situation, and probably get fired first month, or keep my miserable unemployment life going.

what do you guys think i should do?",2023-08-25 14:36:32
18xr0wg,Does my bad query leaves a bad impact?,"Guys, I had an interview at a company which gave me an assessment. The assessment was pretty long and I had to share my solution to them. I haven't really worked with geospatial data and used a technique that left a query running for three days and it has impacted their database performance. 

Anyway the relevant person talked to me via email and he was very polite and professional. I owned my mistake and gave a couple of resolutions too.

Now I have a presentation coming up at the company. I would be presenting my solution. What do you guys think I should be prepared of? Is it a straight up rejection in their minds. Or what kind of questions should I prepare myself for. 

I would appreciate any help. I am really frustrated at this moment.",2024-01-03 19:03:27
18rvasi,My personal LLM is slowly learning,"Been working on this for a few days over Christmas. It’s knowledge is based on the content of about 30 textbooks centred around Data Engineering and Data Science.

Accessing via Blink on my iPhone. (Keyboard layout is Dvorak before anyone asks)",2023-12-27 07:49:02
15duqal,Looking to get either a Data Engineer Masters or a Data Scientist Master: Tell me why you chose to go with your path!,"I'm currently a data analyst with two years experience and have a bachelor's degree in Actuarial sciences. I have some competence in SQL, PowerBI, SSIS and have decades of experience in Excel and have a reasonable enough understanding of Linear Algebra, Calculus (1-4), Statistics, , SARIMAX Time Series analysis that I believe that I could pick up many of the ML basics pretty quick. That's my argument for a Data Science Master's Degree. On the other hand most of the problems I see with the data projects that I'm envlolved in stem from the business leaders not understanding the importance of having solid data quality and data pipelines. I believe as time goes on in the next decade or so that more companies will realize they actually need Data Engineers as a solid basis to build their application upon. This is my argument of getting a Data Engineer Master's. Can anyone help me with some direction, insite or constructive criticism of my arguments that I have to help me get off the fence and get an education. ",2023-07-30 19:56:57
14usm1r,"Ima 35 year pioneer in database technologies, just retired, and would love to help you in your careers, and maybe have a little fun at your expense. Ama.",N/A,2023-07-09 07:58:39
113rmqn,A data mesh for the rest of us,N/A,2023-02-16 14:16:52
12hlw1l,"Data Analyst to DE, can I skip the entry level jobs?","Hi All,

I’m currently working as a DA for a big marketing firm. My main responsibilities includes building dashboards on Tableau and trackers within excel. I told my boss that I’m interested in a career in DE and he told me that he’s willing to expose me to the DE since we work closely with the DE department. Some of the things he says he’s willing to teach me is automation, ETL principles, and Spark. 

I’ve only been on this role for two months but Im planning on staying for at least a year or two to gain more DE exposure. Im also working on the DE dataquest career path. Is this enough to skip the entry level jobs in DE once i do start applying for a new role? I have a degree in STEM field but it is unrelated to programming.",2023-04-10 15:19:04
17ffhfw,ETL developer to FAANG DE: map?,"Hello guys, I’m currently an ETL developer and looking to prep myself up so that I can get into FAANG as a DE. Tools I currently use are SQL and Azure services. Can someone help me visualize the necessary tools I should have under my belt to get into FAANG ?",2023-10-24 15:28:17
16ahy02,How much time to learn Pandas ?,"I can write python scripts to mainly automate some stuff. But when learning Pandas and Polars, I get really confused.   
I am learning from Daniel Chen Pycon. Also learning Polars from Matt Harrison from PyCon.  


But I find it really difficult to grasp.  
How do I learn ? And how much time it takes to be able to write code on Pandas independently ?",2023-09-05 07:58:47
13m4sm7,What's the point of asking which RDMS you have experience with?,"I'm currently interviewing for data engineer jobs. I'm confused why interviewers even really bother asking which database systems I have worked with. I'm interviewing for low-midrange experience positions, nothing senior/lead/principal.

So far in my five years of experience, I've worked with Oracle, DB2, SQL Server, and probably others that I'm forgetting. Throughout all of these, I've interacted with them in almost the exact same way: Good old ANSI SQL. I imagine this is the same for most other candidates. It's not as though I've had to really exploit the nuanced benefits of Oracle vs. DB2. Maybe I use some slightly different functions sometimes, but it's 98% all the same.

So why are interviewers happy to hear when I have experience with their exact RDBMS? Not just from non-technical recruiters, but actual data engineers on the job as well. If they were checking for experience with things like Hadoop, GraphQL, MongoDB, etc., I would understand. But I'm just talking about traditional, relational, structured database systems.

Edit:

Thanks for everyone's responses! Very interesting to get input from others in the industry",2023-05-19 19:10:12
17s5j6k,The Data Engineering Case for Developing in Prod,"Hey folks, I got a lot of great engagement from here on a prior article of mine ""Graduating from ETL Developer to Data Engineer."" So I thought I'd share my latest because I suspect it will be controversial for this audience. I'm really interested in your perspective, do you ""develop in prod?"" What really is ""prod"" anyway? Thanks for any feedback.

[https://medium.com/@ericmccarty/the-data-engineering-case-for-developing-in-prod-6f0fb3a2eeee](https://medium.com/@ericmccarty/the-data-engineering-case-for-developing-in-prod-6f0fb3a2eeee)",2023-11-10 14:47:25
14t92fo,What could go wrong with docker containers?,"I understand that Docker containers eliminate the issue of 'it works on my computer,' but I have encountered a problem while developing an application using Docker Compose, where memory disk and RAM are utilized on the host computer (which is outside the Docker containerization).

Consequently, if I run this Docker Compose on a computer with low RAM or limited memory disk, it will result in an error, which defeats the purpose of Docker containerization. So, I have a few questions:

1. What are the best practices for managing memory disk and RAM requirements for Docker containers?


2. Are there any other potential issues that I should be aware of or able to configure when running the same Docker container on a different computer? This particular issue caught me off guard, and while it makes sense to me, I want to be prepared for any other potential challenges that may arise in the future",2023-07-07 14:36:01
11xhuu4,A dev-friendly spreadsheet product - yay or nay?,"Hi there, I’m one of the founders at EqualTo. 

We’re currently building a ‘spreadsheets as a service’ to help developers integrate spreadsheets. I’d love to hear your thoughts on the concept, and whether you’ve ever experienced any issues with integrating or embedding spreadsheets into apps or websites in the past? 

We’ve just launched our [open beta](https://sheets.equalto.com/#/license/request) if you’d like to check it out!",2023-03-21 14:04:43
11b40wx,Amazon vs Barclays ( for Data Engineer role ),"I have two offers for full time data engineer role, one at Amazon and other at Barclays. What are your views on which one shall I join?",2023-02-24 22:26:24
161zq6v,Here’s the data engineer roadmap if I were to start over!,"When I started as a DE, I was all over the place and my folks told me to learn a lot of tools/frameworks. The company I worked for also used so many tools, ran POCs and benchmarks with it, and I was kinda lost.

Looking back, I wish someone would have told me to focus on the foundations and go all in. Other tools can be picked up on a need-basis. 

Because the field is evolving, and there’s gonna be so many tools to accomplish nearly same things. 

Here is the Snowflake Data Engineer roadmap I put together: https://medium.com/snowflake/step-by-step-roadmap-to-becoming-a-snowflake-data-engineer-in-2023-18c823ba8b9c

PS: I’m a developer advocate at Snowflake which is why I put together a roadmap for Snowflake. But in general, any tool that let’s you use Python & SQL is a great choice.",2023-08-26 16:29:23
1ajanxk,Help me design a data pipeline,"Hey guys, I don't have much experience in data engineering, so I'm here to ask for advice. I am working on a project which consists of building a dashboard for the IT department of a bank. the dashboard should present information from log data. Log data includes security vulnerabilities, issues reported by the help desk, and logs showing who is working on those issues. The data includes information such as description of issues, when they are reported, which device is affected.... Data is provided via an internal API (I don't believe it provides real-time data streaming). I want to create a data pipeline that extracts this data, transforms it, loads it into a database, and then creates a dashboard from it. Normally this pipeline should run once a day. so I think an ETL should work fine. I was thinking of using Python and Pandas to perform ETL since the data is not very large.
The challenge is that alongside this ETL (which should be scheduled to run once a day), I want to achieve this functionality: If a critical issue is reported (server is down, high risk security vulnerability , ...) The IT department must be notified immediately (via the dashboard). How to implement such a pipeline. The data pipeline and dashboard must be deployed internally (no cloud services). Can you help me choose the right tools and give me some tips for designing this pipeline. THANKS.",2024-02-05 07:11:23
15yq7r6,What Data Structures and Algorithms Should a Data Engineer Study?,"For the data engineer coding screen, you likely will face algorithmic challenges as opposed to real world challenges such as loading a csv file. You should be comfortable with python basic and intermediate challenges. The below are the minimum skills you will need to master in order to succeed at a data engineering python coding screen.

Be sure you are spending time learning the correct type of challenges:    


https://medium.com/@seancoyne/what-data-structures-and-algorithms-should-a-data-engineer-study-a6a0cad90c31",2023-08-23 02:10:21
15txz1z,Should I move to the US or should I stay in Germany?,"Hello,

My question is pretty much the same but I will add more context.

I am 26 yo, senior data engineer with 5 years of industry experience.

i am from a South Asian country (Not India), I did my bachelors in Software Engineering here, worked for around 3 years here and then found a job and moved to Germany.

Now, in Germany, work life balance is good, you get all the state benefits, vacations, sick leaves all of it. If I think in the long term, it's a very safe country, humans are valued, socially you are secure etc, which makes life quite peaceful.

On the other side, you have to learn the language to get permanent here, I earn like 80k euroes per year and being single around 40% of my salary goes to taxes, which of course is an investment for my and my family's future, and lots of pretty places nearby to travel to and spend time at.

BUT, having your family move here is a huge pain in the ass if you get married back in your home country after moving here, it's almost impossible to have your parents visit you or move with you, and your buying power is less and you have to live in relatively smaller apartments/houses.

Recently, I came across a VISA for permanently moving to US which i could be eligible for, I reached out to some immigration lawyers and most of them said I am likely to get it and I can apply for it,

Now, this whole process is going to cost me almost all of my savings in legal fee and so, but what I am confused about is it worth it? Will I be able to find a job there with my South Asian university degree in US?

On one hand I see people with even average incomes in the US living in big houses driving nice cars, but on the other hand I see that it's not safe, you can be homeless or on the street, but on the other hand I see so many people living prosperous lives there (I am not talking about outliers or celeberities). Your parents can visit you with multiple entry visas, the market is bigger (but obviously more competetive as well).

I have never been truly jobless in my life, I once got affected by mass layoffs but even then I found another job before my guardian period even expired.

I also feel like if in US I could earn more, I could, may be, save in my young years and invest in some assests for passive income in my later years whihc seems very difficult here in Germany as after taxes, rent, expenses and family support you are barely left with anything.

And when I talked to other people here I realised I am payed very good salary for my experience bracket.

So people who have worked in both countries or people who are working in CS careers in the US, what would you recommend in this situation?

I am 26 yo already, just yesterday I remember completing my bachers at 21, and life doesn't stop. I want to put my mind to peace if i should just accept this place to be my home and spend all my energy here or try to move to a place with better prospects.",2023-08-17 20:00:39
11q8qol,Building the Next-Generation Data Lakehouse: 10X Performance,"A data warehouse was defined by Bill Inmon as ""a subject-oriented, integrated, nonvolatile, and time-variant collection of data in support of management's decisions"" over 30 years ago. However, the initial data warehouses were unable to store massive heterogeneous data, hence the creation of data lakes. In modern times, data lakehouse emerges as a new paradigm. It is an open data management architecture featured by strong data analytics and governance capabilities, high flexibility, and open storage.

If I could only use one word to describe the next-gen data lakehouse, it would be **unification:**

* **Unified data storage** to avoid the trouble and risks brought by redundant storage and cross-system ETL.
* **Unified governance** of both data and metadata with support for ACID, Schema Evolution, and Snapshot.
* **Unified data application** that supports data access via a single interface for multiple engines and workloads.

Let's look into the architecture of a data lakehouse. We will find that it is not only supported by table formats such as Apache Iceberg, Apache Hudi, and Delta Lake, but more importantly, it is powered by a high-performance query engine to extract value from data.

Users are looking for a query engine that allows quick and smooth access to the most popular data sources. What they don't want is for their data to be locked in a certain database and rendered unavailable for other engines or to spend extra time and computing costs on data transfer and format conversion.

To turn these visions into reality, a data query engine needs to figure out the following questions:

* How to access more data sources and acquire metadata more easily?
* How to improve query performance on data coming from various sources?
* How to enable more flexible resource scheduling and workload management?

[Apache Doris](https://github.com/apache/doris) provides a possible answer to these questions. It is a real-time OLAP database that aspires to build itself into a unified data analysis gateway. This means it needs to be easily connected to various RDBMS, data warehouses, and data lake engines (such as Hive, Iceberg, Hudi, Delta Lake, and Flink Table Store) and allow for quick data writing from and queries on these heterogeneous data sources. The rest of this article is an in-depth explanation of Apache Doris' techniques in the above three aspects: metadata acquisition, query performance optimization, and resource scheduling.

# Metadata Acquisition and Data Access

Apache Doris 1.2.2 supports a wide variety of data lake formats and data access from various external data sources. Besides, via the Table Value Function, users can analyze files in object storage or HDFS directly.

https://preview.redd.it/ftgtvwgp3ina1.png?width=1598&format=png&auto=webp&s=8caf29b871d2cca0aee5010b1678c69cfa59a124

To support multiple data sources, Apache Doris puts efforts into metadata acquisition and data access.

**Metadata Acquisition**

Metadata consists of information about the databases, tables, partitions, indexes, and files from the data source. Thus, metadata of various data sources come in different formats and patterns, adding to the difficulty of metadata connection. An ideal metadata acquisition service should include the following:

1. A **metadata structure** that can accommodate heterogeneous metadata.
2. An **extensible metadata connection framework** that enables quick and low-cost data connection.
3. Reliable and **efficient metadata access** that supports real-time metadata capture.
4. **Custom authentication** services to interface with external privilege management systems and thus reduce migration costs. 

**Metadata Structure**

Older versions of Doris support a two-tiered metadata structure: database and table. As a result, users need to create mappings for external databases and tables one by one, which is heavy work. Thus, Apache Doris 1.2.0 introduced the Multi-Catalog functionality. With this, you can map to external data at the catalog level, which means:

1. You can map to the whole external data source and ingest all metadata from it.
2. You can manage the properties of the specified data source at the catalog level, such as connection, privileges, and data ingestion details, and easily handle multiple data sources.

Data in Doris falls into two types of catalogs:

1. Internal Catalog: Existing Doris databases and tables all belong to the Internal Catalog.
2. External Catalog: This is used to interface with external data sources. For example, HMS External Catalog can be connected to a cluster managed by Hive Metastore, and Iceberg External Catalog can be connected to an Iceberg cluster.

You can use the `SWITCH` statement to switch catalogs. You can also conduct federated queries using fully qualified names. For example:

    SELECT * FROM hive.db1.tbl1 a JOIN iceberg.db2.tbl2 b ON a.k1 = b.k1;

**Extensible Metadata Connection Framework**

The introduction of the catalog level also enables users to add new data sources simply by using the CREATE CATALOG  
 statement:

    CREATE CATALOG hive PROPERTIES (
         'type'='hms',
         'hive.metastore.uris' = 'thrift://172.21.0.1:7004',
     );

In data lake scenarios, Apache Doris currently supports the following metadata services:

* Hive Metastore-compatible metadata services
* Alibaba Cloud Data Lake Formation
* AWS Glue

This also paves the way for developers who want to connect to more data sources via External Catalog. All they need is to implement the access interface.

**Efficient Metadata Access**

Access to external data sources is often hindered by network conditions and data resources. This requires extra efforts of a data query engine to guarantee reliability, stability, and real-timeliness in metadata access.

https://preview.redd.it/5uk9n6vi4ina1.png?width=1280&format=png&auto=webp&s=7279ee99fe6c1345a9fc4852d5fe17cec175193e

Doris enables high efficiency in metadata access by Meta Cache, which includes Schema Cache, Partition Cache, and File Cache. This means that Doris can respond to metadata queries on thousands of tables in milliseconds. In addition, Doris supports manual refresh of metadata at the Catalog/Database/Table level. Meanwhile, it enables auto synchronization of metadata in Hive Metastore by monitoring Hive Metastore Event, so any changes can be updated within seconds.

**Custom Authorization**

External data sources usually come with their own privilege management services. Many companies use one single tool (such as Apache Ranger) to provide authorization for their multiple data systems. Doris supports a custom authorization plugin, which can be connected to the user's own privilege management system via the Doris Access Controller interface. As a user, you only need to specify the authorization plugin for a newly created catalog, and then you can readily perform authorization, audit, and data encryption on external data in Doris.

https://preview.redd.it/ksssq9an4ina1.png?width=1280&format=png&auto=webp&s=c8445a7dba4449b9f43e54ba876e22a76352441f

**Data Access**

Doris supports data access to external storage systems, including HDFS and S3-compatible object storage:

https://preview.redd.it/qdiik8wk4ina1.png?width=1490&format=png&auto=webp&s=553006606cdb448f5ad0f2c8848dd5b3308c9b50

# Query Performance Optimization

After clearing the way for external data access, the next step for a query engine would be to accelerate data queries. In the case of Apache Doris, efforts are made in data reading, execution engine, and optimizer.

**Data Reading**

Reading data on remote storage systems is often bottlenecked by access latency, concurrency, and I/O bandwidth, so reducing reading frequency will be a better choice.

**Native File Format Reader**

Improving data reading efficiency entails optimizing the reading of Parquet files and ORC files, which are the most commonly seen data files. Doris has refactored its File Reader, which is fine-tuned for each data format. Take the Native Parquet Reader as an example:

* Reduce format conversion: It can directly convert files to the Doris storage format or to a format of higher performance using dictionary encoding. 
* Smart indexing of finer granularity: It supports Page Index for Parquet files, so it can utilize Page-level smart indexing to filter Pages. 
* Predicate pushdown and late materialization: It first reads columns with filters first and then reads the other columns of the filtered rows. This remarkably reduces file read volume since it avoids reading irrelevant data.
* Lower read frequency: Building on the high throughput and low concurrency of remote storage, it combines multiple data reads into one in order to improve overall data reading efficiency.

**File Cache**

Doris caches files from remote storage in local high-performance disks as a way to reduce overhead and increase performance in data reading. In addition, it has developed two new features that make queries on remote files as quick as those on local files:

1. Block cache: Doris supports the block cache of remote files and can automatically adjust the block size from 4KB to 4MB based on the read request. The block cache method reduces read/write amplification and read latency in cold caches.
2. Consistent hashing for caching: Doris applies consistent hashing to manage cache locations and schedule data scanning. By doing so, it prevents cache failures brought about by the online and offlining of nodes. It can also increase cache hit rate and query service stability.

https://preview.redd.it/flhystx05ina1.png?width=1080&format=png&auto=webp&s=2be0f541653ae4a453d523aae0de262485446ed1

**Execution Engine**

Developers surely don't want to rebuild all the general features for every new data source. Instead, they hope to reuse the vectorized execution engine and all operators in Doris in the data lakehouse scenario. Thus, Doris has refactored the scan nodes:

* Layer the logic: All data queries in Doris, including those on internal tables, use the same operators, such as Join, Sort, and Agg. The only difference between queries on internal and external data lies in data access. In Doris, anything above the scan nodes follows the same query logic, while below the scan nodes, the implementation classes will take care of access to different data sources.
* Use a general framework for scan operators: Even for the scan nodes, different data sources have a lot in common, such as task splitting logic, scheduling of sub-tasks and I/O, predicate pushdown, and Runtime Filter. Therefore, Doris uses interfaces to handle them. Then, it implements a unified scheduling logic for all sub-tasks. The scheduler is in charge of all scanning tasks in the node. With global information of the node in hand, the schedular is able to do fine-grained management. Such a general framework makes it easy to connect a new data source to Doris, which will only take a week of work for one developer.

https://preview.redd.it/beii5ic45ina1.png?width=830&format=png&auto=webp&s=42b1c1ebd680859a48c102cfac19ab198484a196

**Query Optimizer**

Doris supports a range of statistical information from various data sources, including Hive Metastore, Iceberg Metafile, and Hudi MetaTable. It has also refined its cost model inference based on the characteristics of different data sources to enhance its query planning capability. 

**Performance**

We tested Doris and Presto/Trino on HDFS in flat table scenarios (ClickBench) and multi-table scenarios (TPC-H). Here are the results: 

&#x200B;

[Doris vs Trino : Clickbench](https://preview.redd.it/0msrv3575ina1.png?width=1925&format=png&auto=webp&s=7c2d9e5e5d09035c2f845d4123ea5b12c7cfd39a)

&#x200B;

[TPC-H](https://preview.redd.it/va3rntv85ina1.png?width=1688&format=png&auto=webp&s=b57eb30d3e4ce85d41e69060e071883164f9fbd0)

As is shown, with the same computing resources and on the same dataset, Apache Doris takes much less time to respond to SQL queries in both scenarios, delivering a 3\~10 times higher performance than Presto/Trino.

# Workload Management and Elastic Computing

Querying external data sources requires no internal storage of Doris. This makes elastic stateless computing nodes possible. Apache Doris 2.0 is going to implement Elastic Compute Node, which is dedicated to supporting query workloads of external data sources.

https://preview.redd.it/t8ttffmc5ina1.png?width=1960&format=png&auto=webp&s=4ecc736b135b667d0986ac416de6b2bf5f15bc13

Stateless computing nodes are open for quick scaling so users can easily cope with query workloads during peaks and valleys and strike a balance between performance and cost. In addition, Doris has optimized itself for Kubernetes cluster management and node scheduling. Now Master nodes can automatically manage the onlining and offlining of Elastic Compute Nodes, so users can govern their cluster workloads in cloud-native and hybrid cloud scenarios without difficulty.

# Use Case

Apache Doris has been adopted by a financial institution for risk management. The user's high demands for data timeliness makes their data mart built on Greenplum and CDH, which could only process data from one day ago, no longer a great fit. In 2022, they incorporated Apache Doris in their data production and application pipeline, which allowed them to perform federated queries across Elasticsearch, Greenplum, and Hive. A few highlights from the user's feedback include:

* Doris allows them to create one Hive Catalog that maps to tens of thousands of external Hive tables and conducts fast queries on them.
* Doris makes it possible to perform real-time federated queries using Elasticsearch Catalog and achieve a response time of mere milliseconds.
* Doris enables the decoupling of daily batch processing and statistical analysis, bringing less resource consumption and higher system stability.

&#x200B;

**Links:**

Website : [https://doris.apache.org](https://doris.apache.org)

Repo : [https://github.com/apache/doris](https://github.com/apache/doris)",2023-03-13 12:46:36
15k0sbu,python specifically for DE positions,"[edited to clarify I'm asking about what to concentrate on for a DE in context of learning python generally]  

If a practicing/aspiring DE asks  ""what python do I need for DE?"" below are my thoughts about what to concentrate on/supplement  from a ""general python"" class, would like to get feedback. The core of such classes is about general path of execution control, and using lists, dictionaries, sets and tuple, manipulating strings. All that stuff is crucial, if you have it down cold you'll be better off for both jobs and interviews. 

But when general classes move on to bigger programs I'm interested in thoughts about *what's likely to be most crucial for DEs, or useful and not touched on* by such a class.


**likely covered, but less important for a DE than for a general SWE**

   Design to take advantage of OO concepts (any python class will cover instantiating objects and accessing attributes, which you do need)  
   multi-threading and multi-processing  
   tkinter or other desktop GUI framework  
   subprocess control (popen)  
   Dataclass (given the name, its surprising, but I haven't talked to DEs who use these)  

**likely less/not covered, often important for DEs**

   *comes-with libs*:  
   datetime -- deltas, arithmetic, date<->string, timezone handling, pytz & post 3.10(?) built-in  
   zipfile  
   csv  
   pathlib  
   re, and familiarity with string functions likely alternative (startswith, in) (if you use re and don't need it that might look bad in interview)  
   Decimal v float (rounding with money)  

   *Additional libs*:
   pytz (as a predecssor to datetime.timzone (still widely used))  
   chardet  
   pandas  (just understanding its filter syntax, like df\[df\['poo'\] > 3\], puts you ahead of the bottom candidates already)  
   requests  (and non-python you should take a short course on postman and curl & learn a bit about headers and tokens)  
   fastapi and/or DRF (read about both) (it's not obviously DE-ish, but 3 ""ETL"" teams I know have put up FASTapi stuff) (anyway DEs should have basic idea of HTTP apps, apis, basic REST theory, and at least know you don't know about SOAP)  

*edit* - Likely not covered at all in a python class but likely to be relevant: jinja2 (used in dbt) and sqlalchemy (used widely but esp. with pandas)

In my experience SQL is crucial and I think if you can't do ""medium"" leetcode sql you're likely to have problems in any DE interview, whether it's really relevant to that specific job or not.  

General algorithm/data structure knowledge isn't important for most DE practitioners day-to-day, but necessary to clear interviews at some companies (I've never encountered this in about 15 interviews thru my career, all < $150K/yr in today's US$).  If you're looking to enter the DE field, you should be familiar enough with LC to know if you can get up to speed on medium or hard LC if you need to.",2023-08-06 21:11:23
18wg4lm,Where does terraform fit in with CI/CD?,"I’m a little confused when I think of Terraform and CI/CD. As I know it, CI is the ability to continuously integrate new features and CD is the ability to continuously deploy new features. Usually there’s a Version Control System like git helping manage the CI portion while something like GitHub Actions manages the CD portion. Respectively accepting and deploying iterations of your code base.

Terraform however deploys the code you write directly from the command line. So does that mean it does not require a CI/CD pipeline? Am I forgetting anything here?

I suppose any declarative IaC should fit the bill. As a follow-up, though maybe this deserves its own post, I am curious how managing the `.state` and `.vars` files may play a role in this. Could needing to securely manage `.state` files give need for a CD pipeline for some reason?

Thanks in advance!",2024-01-02 04:08:05
18o2a06,How to skip non existent S3 objects loading in Spark?,"My application runs in a AWS service called Glue which is based on spark.

I am reading in millions of AWS S3 objects using spark but by the time it lists all the files and starts to process them some have been deleted by another application.

Is there a way to ignore these? I have tried the flag ignoreMissingFiles to no avail?

Also, semi related, does spark need to list all files before starting processing? I would have thought there was a way to load each file, process and then save since each file is independent. Is seems like all files have to be loaded first which seems crazy to me. Can I not tell spark to load one at a time?",2023-12-22 00:49:13
18jb8hr,BEST ETL TRANSFORMING PRACTICE,"This may seem like a dumb question, I am working on pulling data from an  RDBMS,  with a Python script and placing it into a data warehouse for visualization purposes. My question is, in this instance is it best practice to make as many transformations in my  RDBMS via SQL, create the table extract and load it. Or should this be done only in my data ware house. The data in my in my table I would be creating would be me averaging data, also not sure how that would effect me when using PowerBI.

TIA ALL!",2023-12-15 21:48:52
18hhk3e,What do we do ?,"I am a Junior data engineer with 1.5 yrs experience . I do have one doubt like which is better for us being a Data engineer ? Like, 
Is to provide Data to other team such as analysts so they can build an insight from the data and build a Dashboard which is actually possible for us too .
Or 
To build an end to end product (such as a dashboard) where we bring the transformed data and build the dashboard and give to the end users such as analyst.",2023-12-13 14:07:14
18frtzg,Carrer switch to Data Engineering,"Hi Everyone, 

I am a employee in an MNC having 2.5 years of experience and currently working on a support based project with the tech stack as webmethods which is a integration tool based out of Java and it is being to connect different systems / businesses. I'm bored and frustrated at the same time because the work is same and almost repetitive with no signs of growth and learning. That's when I decided to switch domain but

I was very confused while choosing between Data Engineering and Java. But I choose Data Engineering and started preparing for it. I currently have experience with python easy to medium, SQL mid to advance level and achieved two cloud certifications AZ-900 & AZ-204 currently preparing for DP-203. I have basic knowledge and hands-on azure df, Synapse and storage. 

I tried looking for jobs matching my skills but whenever I started checking the job description I always lack in some or the other skills such as Big data and Spark. So if someone can guide me to tackel with this and land a job in data engineer domain it would be a great help. 

Thanks in Advance :)",2023-12-11 10:47:52
18bl5sg,dbt basics refresher,N/A,2023-12-05 20:15:46
17rlhuz,Powering the Shift Left movement: Git-based systems as a catalyst for democratized data engineering,N/A,2023-11-09 19:55:22
16sutvy,Lifer DE Industries,"I’m 33 with a kid and another one coming along in a few months. Like people my age, Ive been staying at my job for about two years then I hop to the next DE role for more money. However I’ll eventually wanna settle down and get have some job security down the road. Any suggestions on which industries would be great for that? Was thinking something like defense companies like Raytheon or Northrup Grumman.",2023-09-26 17:52:50
16py2nb,MacBookPro 14 M2 vs Lenovo ThinkPad X1,"Hi guys,   
I really need some help and advice about which laptop to buy. I work as a contract data engineer/data analyst/data scientist, so I need to invest in something good. Based on the previous recommendations I got, I narrowed the decision to two laptops:  
\- MacBookPro 14 M2   
\- Lenovo ThinkPad X1  (would use Linux or dual boot Windows/Linux)

I have been using Windows laptop until now. The stack I use is mostly on cloud (Azure, Snowflake, terraform), but locally I run dbt, a looooot of docker containers, python, snowpark etc 

My current laptop is prone to dying when I multitask (which I do a lot), so I really, really need a good processor and a lot of RAM; At least 32GB, but 64 would be great. The advantage of Lenovo is that I can just buy a 32GB laptop and add more RAM, while Mac does not allow that.   


Could you share some of your experiences with these two laptops?  I would love to see some experiences from people who have been using both, Lenovo and Mac.",2023-09-23 07:32:01
16l4mj0,CS grad looking for remote DE entry pos but almost no reply. Plz help.,"I will make a quick post, since time is valuable to all of us.

So I am looking to graduate after a 5 years long Bachelor pursuit, from a decent University in my home country (Vietnam). During the last 2 years of this time I started learning and tried taking jobs relating to Data, ML/DL, all engineering, starting from small lab-environment experiment conducting to number crunching DA and infra for a ML-oriented-system mocking (I got exposed to MLOps tech and principles in a while \~ 1yr, but never got serious since my old company was not deploying any users-facing model). Then got bored of all the ML-sys PoC making/testing and started working for a relatively big data center which made uses of both good big-data engineers and math-heavy journal-printing data scientists, I was one of the more heavy lifting engineers in the data science team who code random APIs and backend for our ML system. Around this time I started to feel the need to be come a DE (I kinda liked engineering stuffs more than all the ML model tuning craps, and I believe Data comes first before any ML systems, or even a model deployment) and slowly decided to stop working about AI models and infras and start looking into data flow, quality and whats not. I started to write a lot more Spark, HiveQL queries, designing new ETL flows and start picking up books about data-intensive systems.

Then all of a sudden, I quit and went back to school for 6 months. My grades was degrading and I still have a Ms to follow, so bit of a paranoid back then, I decided to focus on this final Bachelor sprint. During that half a year of running around to fix my grades I did nothing Data-related.

Now I am back, with a financial burden (very personal so I ain't telling) and all-in-all just a lot of free time and wanting to pick up any Data Engineering entry level pos to start working in the field again. Here is when I started to terrify again. 

I don't even know how to start, my friends suggested me some remote job boards but the listings are few and far between and mostly for Sr. positions, I am starting to think that this whole remote thing is for big-brain best-of-the-bunch star engineers. Applied for around 50+ entry to mid level and barely getting even a reply. Besides the fact that I don't exactly have any credential as a Data Engineer, my experiences are all over the places with random tech and looked like I had career-related second thoughts every year. This might be my biggest shame if I had to name one.

Finally, what I pose here is a question: How can I, as a CS graduate can start looking, and preparing for a DE position. Is there any courses, certificates, books serving as a hidden requirement that I am not meeting. And most importantly, how can I even start remote working (some of my friends are remote workers and some friends of them are remote DEs, and I have no clue how they started doing it). I would love to hear about your experiences starting working as a remote DE.

Thank you for your time and attention. And I am terribly sorry for my tones if it comes off weird or off-putting, as English is only my second language and sometimes it's hard for me to articulate what's on my mind.",2023-09-17 16:09:11
16jnbn2,"I want to upgrade my skills, but don't know where to start.","Hi, first time posting here, sorry if this question was already answered. If so, please point me to those answers.

I've been a Data Engineer for the last 4 years, but I have zero academic backgroud, all my knowledge is from my job, as you might imagine, my theoretical knowledge is limited and contained to the tools I've been using.

I think my SQL is pretty good, and I've been working on cloud environment (Azure) for the last 18 months, so my Python is still evolving.

I wanted to enroll into a good online course that would give a good overview of the main topics for a Data Engineer and I've seen the IBM Data Engineering Professional Certificate from edx that talks about a lot of different things that I would like to learn. However, I've read here in some posts that this course is not what it looks like and it's not worth it. I've also seen the Udacity one, but it's too expensive for me.

&#x200B;

So, my question is: What is the best and most in-depth course you guys know that makes sense for someone like me?

&#x200B;

Thanks a lot!",2023-09-15 20:04:24
16iw751,Dimensional Data Modeling with dbt (hands-on),N/A,2023-09-14 22:34:55
160kjji,B.S Engineering --> M.S Data Science smart in 2023?,"I know this question gets asked a TON, but please hear me out. I have a B.S in a traditional engineering field, currently working in aerospace. I want the option of becoming a data scientist / data engineer, but am not quite committed as I do have other interests (like computational science, which is writing software that does physics calculations). I currently just started online grad school for this (Computational engineering). The curriculum has a few optional data analytic and ML classes, but nothing on databases or really advanced algorithms (it'll teach pytorch, tensorflow, ML models applied towards science/engineering based problems). I was also accepted into UCLA's Data Science and Engineering program, and it's not too late to switch for this Fall quarter, which I am considering. 

My question is: can I break into data science/engineering even without core understanding of databases, algorithms/data structures? Will my program set me up to transition into DS/DE or possibly SWE? 

I understand Data Engineering is almost like a software engineer as they can build the data pipelines, which sounds cool to me. My current program won't set me up with those skillsets unfortunately. The market is flooded with wannabe data engineers like me, so I'm worried if I do a DS masters I'll be pigeon held into one specific field that is already saturated at the entry level.

I'm posting this in r/datascience as well for multiple perspectives. Thanks!",2023-08-25 01:10:29
15ttdpc,Career progression - what titles to look at?,"Hi everyone, 

At the company I'm at we are moving some things around. I started as  an Software Engineer, grew within the role. Then I moved over to Data  Engineering and now Data Architect.

Right now, I'm reporting to a Senior VP. We work with data, SEO and building other things.

What's happening, we are consolidating teams that are under different  verticals but still related (there's just no cohesive strategy,  insights or anything). The idea is for everyone to work more closely  together.

Inside the new structure, there will be 2 VP's side by side. One  focusing more on management and one on planning. I work with the one  with on planning. 

She wants to keep me as her right hand. While she plans and deals  more with the planning of integration, I'd be focused on actually  overseeing the details of it and guiding things.

I do not want a VP title or C\* title. Usually these denote management  of people, budgets and other things that aren't technical. I want to be  focused on the technical. Architect the solutions, integrations, flow  of things.

So I'm coming here to ask, what would be a good title? What have you all seen as titles for a position like this?

Happy to answer other questions in the comments",2023-08-17 17:07:20
14spz88,How to Create a SQL Query Using AI in Less Time?,[https://dataengineeracademy.com/blog/how-to-create-a-sql-query-using-ai-in-less-time/](https://dataengineeracademy.com/blog/how-to-create-a-sql-query-using-ai-in-less-time/),2023-07-06 23:43:34
14rhh9l,"Implement AI data pipelines with Langchain, Airbyte, and Dagster",N/A,2023-07-05 17:31:45
142gprs,PSA: DataFusion is an underrated DataFrame library that let's you run fast localhost queries,N/A,2023-06-06 13:49:11
12pno1n,MLOps is overfitting - here's why,"Does MLOps really exist? Or is DataOps the real terminology? -->

[https://lakefs.io/blog/mlops-is-overfitting/](https://lakefs.io/blog/mlops-is-overfitting/)",2023-04-17 16:19:28
12f47e2,Starting to plateau,"I've been a DE now for about 5 years working at startups, FAANG, and medium sized companies. I'm currently a Sr DE at medium sized company and honestly feel I've plateaued in terms of technical skills. I've worked with all the big data frameworks (Kafka, Spark, Airflow, etc) in a managed setting, meaning Databricks spark, Confluence Kafka, Astronomer Airflow. I honestly don't know if it's worth me investing time into Kubernetes to actually deploy all these things internally. My goal is to switch careers and transition into a ML Engineer focusing on infrastructure. I think the best course of action in the short-term would be to transition into a backend Software Engineer. Anyone have thoughts or has gone through a similar situation in their data engineering career?

[View Poll](https://www.reddit.com/poll/12f47e2)",2023-04-07 23:50:03
12bn51b,State of Databases 2023 Survey + Giveaway,"A company called Basedash is doing a survey about databases, ORMs, data warehouses, etc. and giving away a 27"" Apple display. 

I just did the survey and got a referral link, so check it out. Not affiliated with Basedash.

https://2023.stateofdb.com/?referral=W25L9R",2023-04-04 15:48:51
125q9k3,Best practices of designing tables," Hello, What might be some good article or books that cover best practices of table designs? For example, naming conventions of table and column, creating primary keys etc. Thanks",2023-03-29 14:43:42
11vfsf5,Spark DF api,"In all the companies i've been, I never saw DF api being used in production, it's all SQL due to it being easier to read. Anybody who uses DF here in production and what's the reason for doing so?",2023-03-19 08:36:55
114kkus,Python course,"Hello everyone, I have a degree in CS, currently working as a BI analyst (want to transition to DE) and have 0 experience in python. I was wondering which one of these two courses is better:

* Python for Everybody on Coursera
* Go from zero to hero in Python 3 by Jose Portilla on Udemy

Please suggest if you have know better options! Thanks in advance",2023-02-17 13:42:13
18s1mal,Understanding Sorting & Partitioning (Find more at youtube.com/@alexmerceddata),N/A,2023-12-27 14:13:55
16vjab1,DuckDB + Delta Lake (the new lake house?),N/A,2023-09-29 18:27:04
167uqpv,Why is data catalog that important ?,"I understand data catalog is the central metadata , but I don’t understand why is it that important? I see lots of posts on LinkedIn, saying data catalog is important to implement, but never understood why. Can some help me in understanding?",2023-09-02 06:11:23
12idd74,Is DE a good role in terms of work and work life balance?,"Hello, 
About me: I joined the corporate world as an Data engineer just after graduating in mid 2021 (but my current work revolves around Data analytics, SQL and Python). Total exp: 1.5yrs

I am currently in a dilemma: whether I should continue as a Data Engineer by expanding my skills (learning Spark etc) or switch to SDE roles (safe evergreen option).

I have few concerns about DE role which is kinda stopping me from fully deep diving into it. 

1. Are data engineers considered 2nd class employees in a company? I have read that DE is a role that supports business functions and Data scientist and are generally not profit generating employees (away from business) hence their efforts can sometimes go unnoticed.

2. How does day in a life looks like? Work life balance and work environment?

3. Growth prospects: Can a DE transition/grow into leadership positions?


Looking for some guidance. Thanks in advance!",2023-04-11 08:36:30
11hf0yv,Do we need data people now with AI and ChatGPT?,"There's a lot of talk if AI will replace jobs; especially seeing how good it is at coding or correcting bad code

My 2 cents....  


1. As a true AI program, wouldn't the code get very asymmetrical if it only has itself as the feedback loop? Wouldn't it start writing pretty generic code after a while, lacking creativity and variety, seeing its own code as the training loop?
2. Why do we keep looking at AI as human vs bear? Why can't we see it as the symbiotic relationship that was created between humans and wolfs (dogs)? Why can't it be the perfect pair programmer assisting us in writing better code?",2023-03-03 21:09:31
166rste,Data pipelines with Python and SQL - Part 1,N/A,2023-09-01 00:14:14
12dteg3,Docker - Magic or Hype?,"Hi all,

I have seen a lot of posts regarding the use cases of Docker but not really any on how it actually works on a functional level. Many mention that it is great for dependency issues (one version of Python required for one process vs another) but how does Docker actually solve for this?",2023-04-06 18:33:23
16upuej,How viable is Data Engineering out of college?,"Some background: Student at T5 CS college. Got interested in this field after my dad made the switch from sysadmin to Data Engineer after \~20 years in the IT field. He says it's the best job he's ever had. I have a pretty solid understanding of Python, intermediate at SQL but actively learning it right now.

As with my dad, most people in this sub started as an IT/SWE/Data role and transitioned after around 3-5+ YOE. Ofc, that's a viable path for me but tbh not too excited at the prospect of starting a role I don't feel passionate about just to switch down the line. 

At the same time, new grad/internship DE roles are \*\*scarce\*\*. How should I approach cracking into this field? Anyone here find success in DE out of college?",2023-09-28 19:58:13
13tkp7p,DE courses recommandations for a new grad,"Recent Computer engineering grad here. I’m applying to data science and data engineering roles, but realize that I lack a lot of the backend technical skills. I’m proficient in SQL, Java and Python, but it ends there. 

I’m looking to take a course(s) with little data engineering knowledge. Ideally something that’s hands on (implementing through homework/projects) and does a wholistic overview of all the major concepts I see on job listing:
- Kafka and lambda architectures
- Data lake, warehousing and fabric concepts 
- Spark/Hadoop, Scala, NoSQL, Airflow, Snowflake 
- AWS or GCP or Azure (namely synaptics, data lake and databricks)

Because I’m using this to bolster my resume, I figured taking a shotgun approach in learning all the concepts rather specializing in one particular tech stack. Especially since I dont know what that stack will be once I land a job. I plan on putting this on my resume as form of “secondary/continued education”

I’ve seen course offerings on udacity (How to Become a Data Engineer), on coursera (IBM and Google certifications), udemy, datacamp, etc.

Does anyone have recommendations? I plan on doing this full time so time commitment (and how in depth it gets) isn’t an issue. 

Thanks!",2023-05-27 22:56:28
186aeh4,Anyone Else Stuck in a Bad Job? How Do You Cope?,"I am 27 years old, 3 YOE, stuck in a role at a bad company working 8-5 on site for $110K (20K pay cut from last gig). Toxic company culture as well and shoved in basement with IT support. Living at home and don’t want to be in this town. No wife no kids no friends. 

Only been 3 months and can’t find another job especially since I was only at my last gig for 6 months. 
Not even doing DE work, stuck doing low code BI work with piece of shit software called Domo. 

Debating quitting and doing my MSDS full time and finishing next July. Have $40K saved up. 

Anyone else in a similar situation? How do you cope? Not sure how to get out of this one anytime soon, given my spotty job history and job market.",2023-11-28 23:31:42
136x9q8,Data Engineering Bootcamp,"Hello, I am computer engineer graduate and I want to start career in Data Engineering, but I can not start by youtube I tried many times but it is not effective in my case, I need bootcamp which is affordable and will help me to get the skill to land the first job.",2023-05-03 19:41:52
18vwegq,How should I replace NaN values?,"I have a column named 'normalized-losses' in a csv file about cars, this column has 40 missing values, I thought about replacing them with the mean of the whole column but as I humbly know I can't do that unless the graph looks like a Bell-shape and there are no outliers or skewness, which appears to not be the case here unless I am observing it wrong, the x-axis is the values of the column and the y-axis is the frequency of those values. I would be glad to hear what would you guys recommend me to do in this situation. Thanks in advance

&#x200B;

https://preview.redd.it/iq0ty9pyut9c1.png?width=710&format=png&auto=webp&s=f29e1af711003115fe90d00b85fec080c01eb271

https://preview.redd.it/ignyqkpyut9c1.png?width=1037&format=png&auto=webp&s=51a2858f63d99edbe72dbd030f582f18a87197c7",2024-01-01 13:06:44
12ybcqo,Data Engineering and DataOps: A Beginner's Guide to Building Data Solutions and Solving Real-World Challenges,N/A,2023-04-25 07:04:35
1968ntq,In dire need of guidance,"I am a graduate student in data science with a 4.0 GPA but I feel lost. Imposter syndrome has really gotten to me and I try my best to upskill especially as I have great interest in data engineering. 

Aside my data science course work, I have enrolled in DataCamp's data engineering track to learn skills there but I somehow feel inadequate. I have completed about 47% of the track spanning 
- SQL Joins
- Introduction to Relational Databases 
- Database Design
- Intermediate Python 

I have reached out to many potential mentors but no one has responded. I need someone to guide me on this path. I am willing to join or work for free on your projects, I need exposure and direction on what to actually learn and practice. Help please.",2024-01-14 05:44:38
188pl76,Should I Learn Java First or Dive Straight into Scala?,"I'm a junior data engineer, currently working mostly with Python and SQL and I'm considering learning Scala to expand my skill set. However, I've never written a line of Java code, and some have advised that I should learn Java first before diving into Scala.  
Is this the right approach? ",2023-12-01 23:25:12
17eud9r,On My Third Bad Job This Year…How do I Salvage my Career?,"Had a turbulent year career and mental health wise and need some advice. I’m 27YO and about 6 months from finishing MSDS. 

Job 1: govt contracting employer making $90K in HCOL, was there for 3 years. Salary was low and put onto a multi year project I didn’t want to do, so I left in March. Was high performer there

Job 2: another govt contractor for ~130K and was hired as a DE. Put on a project with no DE work, so my performance was low doing unrelated tasks. Stayed for 5ish months, then had to relocate back home across country due to severe depression. 

Job 3: ~110K and full on-site in automotive space (think gas stations, car washes, etc). Took this job to keep money coming in. Been here for about 6 weeks. Bad/nearly non existent tech stack and lots of tech debt. Lots of stress due to organizational dysfunction. In my hometown, but don’t really want to live here 

I have a possible lead to go to another govt contractor, remote and for more money. Although I do want to work in private industry long term. 

Do I need to stick it out at my current job for a year+ to repair my now trashed resume? WWYD?",2023-10-23 20:34:38
149h80m,A Continuous Slack=>ChatGPT=>Google Sheets Pipeline using Estuary Flow,N/A,2023-06-14 19:48:40
144en5h,Data Architecture Best Practices: How to Build a Robust Data Infrastructure," In today's data-driven world, businesses need to have a solid data architecture in place to make informed decisions and drive growth. A well-designed data architecture allows for seamless data integration, efficient data processing, and scalable solutions. In this article, we'll discuss data architecture best practices based on the advice of Dan Sutherland, a managing director focusing on technology consulting. We'll also explore the different roles involved in building a modern data architecture, such as data architects, data modelers, data integration developers, and data engineers.

## Seven Best Practices for Designing a Data Architecture

1. Cloud-native Design: Modern data architecture should be designed to support scaling, high availability, and end-to-end security for data. This design allows for easy scalability without affecting performance.
2. Scalable Data Pipelines: Data architectures should support real-time data streaming and micro-batch data bursts to handle spikes in data pipelines, such as seasonal fluctuations or quarter-end data flows.
3. Seamless Data Integration: Data architectures should integrate with legacy applications using standard API interfaces, optimizing data sharing across systems and departments within an organization.
4. Real-time Data Enablement: Enterprises need the ability to deploy automated and active data validations, classifications, management, and governance with complete and visible data lineage.
5. Decoupled and Extensible Design: Data services provided to different organizations should not depend on one another, and it should be easy to add new capabilities and functionalities, such as adding data flow from Salesforce into your systems.
6. Domain-driven Approach: Modern data architecture should be driven by common data domains, events, and microservices, centered around the common business information model.
7. Balanced Investment: Consider the return on investment for your business when building a data architecture. There's no need to overinvest in modern data architecture environments and features if they're not needed for your business size and growth.

## Roles Involved in Data Architecture

A team of skilled professionals is essential for successfully executing these practices. Each member of the team brings their unique expertise to the table, ensuring that the organization's data strategy aligns with its overall goals. The key roles within a data engineering team include:

* Data Architect: As a senior leader, the Data Architect is responsible for translating business requirements into technology requirements. They define the data architecture framework, standards, principles, and reference architecture, which serve as the foundation for the organization's data strategy. In this role, they collaborate and coordinate with multiple departments, stakeholders, partners, and external vendors to ensure seamless integration of data solutions.
* Data Modeler: The Data Modeler creates conceptual, logical, or physical models of data sets, which provide a clear and consistent representation of the organization's data. By reverse-engineering databases, they identify standard labels and notations for use across departments, fostering consistency and streamlining communication between teams.
* Data Integration Developer: These individuals are responsible for designing and implementing integrations between software platforms, programs, and applications. Working closely with the Data Architect, they ensure that the organization's data systems are interconnected and function seamlessly, enabling the extraction of maximum value from data assets.
* Data Engineer: In situations where a Data Architect may not be present, such as in smaller companies, Data Engineers take on the responsibility of creating the vision designed by the Data Architect. They implement the data architecture framework, building the pipelines and infrastructure necessary to store, process, and analyze data effectively.

By leveraging the unique skills and expertise of each team member, a data engineering team can effectively execute an organization's data strategy, ultimately driving value and supporting data-driven decision-making across the company.

Building a robust data architecture is crucial for businesses of all sizes. By following these best practices and understanding the different roles involved in data architecture, organizations can make better decisions, improve efficiency, and drive growth. When considering your current data architecture, think about which roles are present in your organization and whether they fulfill the responsibilities outlined in this article. As you plan and invest in your data architecture, remember to keep a balance between your business needs and the return on investment.

I share these and other tips on building robust IT architecture in my blog: [https://ainsys.com/blog/2023/04/20/data-architecture-practices/?utm\_source=reddit&utm\_medium=social&utm\_campaign=data\_engineering&utm\_content=data\_architecture&utm\_term=ITarchitecture](https://ainsys.com/blog/2023/04/20/data-architecture-practices/?utm_source=reddit&utm_medium=social&utm_campaign=data_engineering&utm_content=data_architecture&utm_term=ITarchitecture)",2023-06-08 16:58:12
150yzmo,There should be data engineering courses in colleges,"Current software engineering education in colleges still targets building up OLTP systems.As a graduate you may know ACID, OS details, remote RPC really well, but after working 10 years, there is still a chance that you don't know what ""tumbling window"" is.

Why not just learn data engineering in your own time, you may ask.  The problem is they are ""unknown unknowns"".  You don't know you need to think of solutions from data engineer point of view.

Most software engineers only know that DE is just used for data science, is about building data pipelines for them. They don't know that DE patterns can actually help in SE domain.

One example is a project I took part in. We were supposed to build a real time trading risk monitoring system. Nobody was aware of Kafka Streams so we just handled the incoming data using plain Java code, stored the time window in a nosql db and used a single processing application, no cluster. 

Ironically the DE team was part of the project. And we just treated them as a downstream and sent some audit information to generate reports.

That's typical in the industry. There is a big gap between SE and DE.  Why? Because lots of people are not trained in school for the DE solutions and not lucky enough to meet colleagues who are aware of this.",2023-07-16 06:44:27
13qmjex,Rude.,"&#x200B;

https://preview.redd.it/a5oqed6hcs1b1.png?width=565&format=png&auto=webp&s=7a01676830a8f9b2addced22801c6f86e892a69e",2023-05-24 13:59:20
13onc2y,Projects for Mid level DE,I've been through a lot A LOT of articles and videos in search of a good hobby project that I can also showcase in my CV (cuz I'm creatively dead rn) and also learn something out of it but everything seems the same. So can anyone tell me what should I do.,2023-05-22 10:43:56
1ai1vcj,"I shared a Python Data Science Bootcamp (7+ Hours, 6 Courses and 3 Projects) on YouTube","Hello, I just shared a Python Data Science Bootcamp on YouTube. Bootcamp is over 7 hours and there are 6 courses and 3 projects. Courses are Python, Pandas, Numpy, Matplotlib, Seaborn, Plotly and Scikit-learn. I am leaving the link below, have a great day!

[https://www.youtube.com/watch?v=6gDLcTcePhM](https://www.youtube.com/watch?v=6gDLcTcePhM)",2024-02-03 17:53:48
195txyv,Using Databricks for Data Science/ML and Snowflake for Data Warehousing,"I've noticed people say on some old Reddit posts that certain companies use Databricks for their Bronze and Silver data layers, and then transfer this data into Snowflake for the Gold layer.

In such scenarios, as Data Engineers, we often need to reconnect to Snowflake from Databricks to retrieve data—sometimes a significant amount, depending on the table sizes and number of tables. This step is crucial when we have substantial data from sources not integrated into the data warehouse, as it allows us to enrich this data with warehouse data to create specialized datasets for data scientists' ML models.

Considering the use of both platforms, wouldn't it be more logical to fully establish the data warehouse in Snowflake and only transfer data into Databricks when necessary for creating these specialized, enriched datasets for data science and ML models?

I’m not familiar with the cost implications of these options, but I assume the latter approach might be more practical and efficient, especially for companies whose data warehouse teams have limited proficiency in Python.",2024-01-13 18:02:48
18btyeq,Looking for the Best Data Engineering bootcamp?,"Hey fellow data enthusiasts! I'm on the hunt for the best online data engineering bootcamp. I've done some digging, here's what I've come across. Would like to hear your thoughts on these and whether you'd choose them or not, and why?

Springboard Data Engineering Career Track

I've heard they offer solid mentorship and hands-on projects, plus that job guarantee is a sweet deal, but pricey, maybe? I'm on a budget.

General Assembly's Data Engineering Immersive

I like their reputation, and I'm all about those real-world projects and networking chances. I heard its boring and it can be intensive, anyone tried their course before?

DataCamp's Data Engineering Track

I like the idea of a more flexible, go-at-my-own-pace approach, and it's lighter on the wallet, but not sure if it will be as in depth enough to land me a job.

I also been looking at Data Engineer Academy, I been seeing their ad on Instagram saying they will guarantee a job, but they seem new and not enough reviews online.

Overall I'm leaning towards Springboard, but I'm open to your wisdom and personal experiences. Let me know if you've got any tips or other bootcamps in mind.",2023-12-06 02:51:02
17nxuhc,"In your actual work, what is your development environment? for data engineers who use Spark in work","I'm wondering other peoples work development environment for using Spark. 

My team is using VSCode and just python script. (Not using Jupyter Notebook)

Anyone there who can share your development environment?",2023-11-04 22:51:33
13wvj7p,Meltano (the open source data ingestion tool) launches its cloud into the public beta,N/A,2023-05-31 19:28:39
11kim2m,Am I a data analyst or a data engineer?,"Hi everyone, I am new here and looking to get some input from people that have more experience in the industry.

I worked in a digital advertising industry before, but very early on realised moving to a data role would suit me. So over 3 years I spent a lot of time learning R and Python, mostly to interact with APIs and automate reporting.

A year ago I finally landed a role of a data analyst for a DTC ecomm company.

The only problem is I don't really spend a lot of time analysing data. 

So far I have implemented Airbyte for data extraction, suplemented with Airflow orchestrated python scripts for stuff that Airbyte does not support, I wrote all of our DBT models and on top I am creating tableau dashboards for end users.

Our stack currently is Airbyte, Airflow, DBT, BigQuery.

Also I am the only person in the company that does anything to do with data. So I had to figure out everything on my own.

I have a feeling I have learned a ton past year, but I am also starting to realise that some of the stuff I did is very subpar (no dimensional modeling, non incrimental models in dbt, busines logic scattered all over the place etc.).

My contract is expiring end of this year and I am looking to you guys for some advice.

First of all I want to know if this kind of work I have described is typicall for a data engineer? 

I work remotely and have not had a lot of opportunities to talk to people who actually are employed as data engineers.

Second, is it enough to ge me into a junior data engineer position, or should I aim to learn some new skills?

I am pretty good at python, sql, R, Airflow, i know how to work with VMs and have experience with GCP. I am learning about dimensional modeling now.


Thirdly, should I stay at current position?

I am currently making $100k a year. But I am not really seing my company hiring anyone more experienced then me, if anything they might hire people under me.

I would really like an opportunity to work with a team and learn from more experienced people. Kinda sick of having to figure out everything on my own, but otherwise I like the type of work I have been doing this past year.

Thank you for a long read and thanks in advance for sharing your thoughts.",2023-03-06 23:55:40
190pl8i,Issues in loading data from source to staging layer,"I m working on a data warehouse migration project from private database to cloud one. Basically I raw data will be uploaded on a S3 bucket and then we have to ingest it in snowflake cloud data warehouse. We are following S3>Stage>PSA>DWH>DMT layer.

While ingesting data from S3 to stage we are facing a typical issue when there is bad data. Like for example there is country_cd column. And in stage we are taking it's data type Varchar(2). But in certain file vendor are mistakenly putting whole country name. That's why my code is breaking down. To ingesting file. 

At present to we are using replace function to avoid that solution. But that is costly from time n cost perspective since raw data might contain billions of rows. 

You people have lot of experience in data engineering. Can you provide some idea or lead, how do you overcome this issue. I don't want to send corrupt file back to vendor. Doing so will cost 2-3 days to the business. After every breakdown.",2024-01-07 10:47:18
18rsa3y,Thoughts on Meltano,"Not a DBE by trade, but do a lot of solutions and data architecture so a far amount of piping data from A to B. 

I’ve been nibbling on Meltano in the rare spare moments the past year, and am getting ready to fire up my first few pipelines. 

What are yall thoughts on it? Worth doing? Easy to instrument and run on k8s?",2023-12-27 04:56:19
18mw7iq,Would you take this pay cut for DE and another weird question,"Data Analyst with 5 years experience. I currently make 88k USD. I was offered a Jr. Data Engineering position at another company making 70k USD. I didn't meet the expecations on the regular DE role, but they really liked me so thought I'd be a better fit for the Jr. Role.

&#x200B;

My questions

&#x200B;

1.) Would you take this pay cut for the DE experience? I am 30... so I am wondering if this would pay off in a few years. I seem to be paid at the upper range for a Data Analyst.

2.) My current job is remote, and this other job is remote too. I am bored to tears at my current role and have asked for different projections, went above and beyond with documentation, building out things, and doing everything I know how. My bosses just kinda shrug and say ""do what you can."" Do you think I would be able to pull off both jobs remotely, even for a few months? Or is that a super bad idea.

&#x200B;

Thanks!",2023-12-20 14:44:21
18mjey4,Advice on job hunt please,"I finished bootcamp last week and started applying for jobs but i noticed it is so hard to find entry job for data engineering and they ask for experience like a year sometimes. The question do you think the bootcamp i did was 3 month and the portfolio is enough to get data engineer job or i should try to go for something lower and if so i don't even know what is lower then make my way up to DE maybe like DE internship or data analyst 

My portfolio now have 
*data pipeline on aws and connect to grafana
*Have another cli python program that takes order, records couriers as well, recorder on file or mysql database either as you want.
*And other projects such as like blackjack game
Soduko solver
*And machine learning project did in engineering degree used knime so i didn't use code to build it just interactive thingy.",2023-12-20 02:22:55
17v5yak,looking for users to pilot tool to automate adhoc SQL requests,"hey fellow SQL monkeys, some of you may know that my team has been drowning in adhoc SQL requests, most of them are simple and should be automated. The time spent on context switching and dealing with the stakeholders could be better spent on modelling, cleaning up our dbt project, and improving our pipeline orchestration.

Some background on me: I've been building and scaling data teams at startups the last 5 years. The startup I'm at now has about 300 employees with a 3 person data team including me. We use ""self-serve"" tools like Looker and hold periodic training sessions, but there's always data requests that can't be self-served and it blows up our backlog. Our team is stretched thin and we decided to deploy a LLM to automate these requests. It's been working very well, we're seeing most requests answered by the LLM, with only a few that we have to review. 

We've been posting in a few data communities in Reddit over the past month and it seems like this isn't just an issue at my org, that's why we're hoping to get your feedback to make something awesome and give time back to your teams",2023-11-14 16:27:40
17ooyfj,DE transition into MLE?,"Is it possible for a DE to transition into MLE? I have been learning about NLP and it’s been so interesting, and I would really want to learn more about MLE works. Given that it’s possible for a DE to transition into MLE, what topics should I study to get a shot?",2023-11-05 23:29:40
17njn65,Any recommendations for no-code data cleaning tools well suited for a non-technical end user?,"Hey there folks, so im currently in the brainstorming phase of designing data infrastructure from scratch for a very small company whose owner happens to be, well my brother lol. 

We plan on going over the details over this upcoming holiday vacation period, but from what I've picked up from our brief conversations about his business in the past he occupies a very narrow product niche where he's unlikely to expand his business horizontally (so probably not going to hire discrete teams managing separate business units) but his contract volume has increased to the point where hes expressed the desire for structure and automation.

Of course, I plan on running him through the usual 10,000 questions about the shape of his business and his short-medium term growth plans to make informed design choices, but my gut is telling me its going to end up running on a low-medium cost on-prem machine (that I will probably build myself, 15 years of PC building experience) and will need to be able to operate long-term free of oversight from any SQL-Python trained staff. 

Essentially all the complexity for this system will be making sure the Ingestion and data-cleaning portions of the system don't break down and keep in line with the data warehouse(s) schema(s). Im aware he WILL reach a point where he needs a full-time hire to manage the system and/or migrate to the cloud but for now, I suspect after I set everything up hes only going to need to make periodic adjustments to changes from is inbound external data sources primarily consisting of orders and receipts from his suppliers and clients. 

He works in a creative/artistic industry so I'm expecting his clientele to have very informal methods of placing orders and submitting payment. I may deem it not worth the effort to try and automate portions of this data input if it proves too erratic but id like to give it a shot and provide my brother a few options based on what hes willing to learn and manage on his own. 

He is quite smart but programming was never his interest so outside of some basic troubleshooting syntax commands I plan on writing up in the final documentation I don't want to push him into having to learn any more than he has to, keeping it click and drag as much as possible.",2023-11-04 11:25:57
17idwkw,Point of Prefect/Airflow in Event-Driven ETL+ML Pipeline?,"I come from the MLOps-side, so I have made ETL pipelines as necessary elements, but it isn't my primary focus.  Simultaneously, the company's data engineers tend to be recent grads, so not ideal for mentoring.

**I'm trying to understand why the additional overhead of Prefect/Airflow (or any other workflow orchestrator) is worth it in an event-driven, serverless ETL + ML pipeline?**

It seems I could just as easily (at least because I'm familiar with it) have an S3 trigger that fires off a series of Lambda transforms that are then ultimately consumed by a model on Fargate, which the last Lambda invokes with an API call.  With DLQs and log-based alerts, I have an element of resiliency and the team is aware if there is an issue with the alerting.  If I want batch scheduling, then I use an EventBridge alert instead.  What is Prefect/Airflow improving that offsets the need to manage and maintain it?

**If it doesn't benefit these use cases, how are people using it then, as this seems like a fairly common enterprise pattern?**",2023-10-28 13:33:45
15z7oi5,Should I choose software engineer or data engineer certification?,"As the title states, my current job role is just IT support and I have been given two options - to do a certification in either the data or software engineering and I'm not well versed in either.

I'll have to start from scratch, I'm gravitating more towards data engineering side because it seems more interesting and plausible for me since I am originally from a non-IT background as well. Two years in support role has me just googling up stuff or asking colleagues and seniors about things I'm unaware of. 

Somehow this switch itself seems almost impossible, but I don't want to give up before putting up a fight.

In addition I have heard the exam post certification is also extremely difficult and I get only one try - so any sources for in depth study/courses would also be really helpful to me!

Tldr: been given two choices and I'm not sure which one to take or if it's even possible.. could it be that they're just looking to lay off...",2023-08-23 15:49:25
15urns3,Apache Airflow 2.7.0,"📣 We've just released Apache Airflow 2.7.0 🎉

&#x200B;

https://preview.redd.it/i7jgql03swib1.jpg?width=1828&format=pjpg&auto=webp&s=73d8a21c04b7bc1d72a8e75f61319f04310e7043

&#x200B;

https://preview.redd.it/btzg9no3swib1.png?width=1263&format=png&auto=webp&s=64e2a7ae8d560632bc5c5d644db877864fae254b

New features include:

✅ Setup and Teardown tasks

✅ Cluster Activity UI

✅ Built-in integration for OpenLineage

✅ Allow Enable deferrable mode by default for all deferable tasks

&#x200B;

📦 PyPI:  [https://pypi.org/project/apache-airflow/2.7.0/](https://pypi.org/project/apache-airflow/2.7.0/)

📚 Docs: [https://airflow.apache.org/docs/apache-airflow/2.7.0/](https://airflow.apache.org/docs/apache-airflow/2.7.0/)

🛠 Release Notes: [https://airflow.apache.org/docs/apache-airflow/2.7.0/release\_notes.html](https://airflow.apache.org/docs/apache-airflow/2.7.0/release_notes.html)

🐳 Docker Image: ""docker pull apache/airflow:2.7.0""

📃 Blog Post: [https://airflow.apache.org/blog/airflow-2.7.0/](https://airflow.apache.org/blog/airflow-2.7.0/)",2023-08-18 18:02:52
13b0rkn,need help understanding dimensional modelling,"TL;DR: need some help understanding dimensional modelling. how, why and when dimension and fact tables are created? please suggest to me any tutorial/video/ reading materials with practical examples if possible.

So recently I was going through a [youtube video](https://www.youtube.com/watch?v=WpQECq5Hx9g) to do some weekend projects. most of the time, I try to understand what the project is all about and then implement it later in my own way. 

 (I have not seen the whole video), from what I understood, this was about creating dimension and fact tables from a dataset and visualizing them and it was for learning purposes.

but I am having trouble understanding, how one can create a fact table from a table (dataframe) that is already a kind of fact table and how a dimensional table can have transactional type data.

**initial dataset columns ->**  VendorID,tpep\_pickup\_datetime, tpep\_dropoff\_datetime, passenger\_count, trip\_distance, pickup\_longitude, pickup\_latitude, RatecodeID, store\_and\_fwd\_flag, dropoff\_longitude, dropoff\_latitude, payment\_type, fare\_amount, extra, mta\_tax, tip\_amount, tolls\_amount, improvement\_surcharge, total\_amount

**derived fact table columns ->**  \['trip\_id', 'VendorID', 'datetime\_id', 'passenger\_count\_id',     
 'trip\_distance\_id', 'rate\_code\_id', 'store\_and\_fwd\_flag','pickup\_location\_id', 'dropoff\_location\_id', 'payment\_type\_id','fare\_amount', 'extra', 'mta\_tax', 'tip\_amount', 'tolls\_amount','improvement\_surcharge', 'total\_amount'\] 

in my organisation, I mostly see, the architect giving us the er diagram, usually, we have the dimension tables and have to create the fact table from them(not transactional for my case,just for reference). so I need some help to understand these concepts.",2023-05-07 19:51:01
12d5euk,For the first time in my life I won a march madness bracket and used a ton of Feature Engineering with basic XGboost.,"Won 1, finished 3rd in my friends challenge that had 59 people, and my wife finished 3rd with it in her work bracket.  Curious if anyone else has ever used methods like this to win as I've never done this well on my own.  Also, I was actually nowhere close to winning the Kaggle competition, so I'm curious if the people who won that had some crazy good predictions.",2023-04-06 01:51:08
128n3wp,Looking for honest career advice.,"Hello, I am a 22 Year old Male. I am a self taught programmer who has a bachelors in an unrelated degree (Finance). Over the last few years I have been on a mission to break into the data science/engineering space. 6 months back I got my first big break landing a data engineering internship at top 100 global company (Market cap). The experience has been great however the company specializes in something unrelated to technology. Meaning the technology we create is not the product but instead underpins their main operations. I have so far heard from recruiters that if I am to work at a big tech company I should look to join a product driven company as my next step. 

To achieve this goal I outlined a 3 month period where I will get my professional data engineer certification from Google (I already have the ACE),  and release two ""production"" grade projects I have been working on. I believe this would be enough to land me a great role. 

My exam is in a few weeks and the project is close to release. I have however been offered a role at a Series C company in London as a data engineer. Good pay and a solid tech stack which relates to my experience. It also has an emphasis on streaming processing (Which I suppose is where the future of DE lies). They are expecting a response in two days. I would like to have some advice on how to approach this. Would you accept the first offer that comes your way? Complete your goals first and see what else is available? Am I being greedy or over my head?  Do you focus on tech stack above salary? I understand that any opportunity in DE is a blessing but this would entail a big move for me and a significant decision. Alternatively I could also stay at my current company. 

&#x200B;

Any advice would be very appreciated. Let me know what thought process you had at my age and what questions you would be asking yourself. 

&#x200B;

Thank you very much.",2023-04-01 13:53:54
1279fhc,Is class a good starting point for data engineering?,This class has everything I was looking for but is a good starting point https://coursera.org/specializations/python-bash-sql-data-engineering-duke and is the azure data engineer cert worth the work?  Thanks in advance.,2023-03-31 03:57:30
11rt2ih,Is this a career that I could logicly study for while at uni with the hope of getting a job after I graduate?,"The opinions on DS/DE online are almost split between to camps, the online bootcamp/ tech blogs camp who wants to convince me that taking 2 courses would prepare me for a senior level position where I would get paid a million dollar for being gEnEiOuS ArRifiCal daTa eNginEEr, and the other camp says that most DE jobs either require a Masters degree or couple years of experience so I seriously don't know what to do.

I am a 4th year CS student right now, and I got interested in data engineering after researching about it but I am not sure if I should focus my free time on studying it, OR work mostly on something like backend engineering with some studying of DE (something like a 80/20 split) in the hope of starting as a Backend engineer then transferring to DE when I gain experience.


Most of you work in the field so I am interested in your opinions.

Thanks!",2023-03-15 11:01:13
11od6q9,Is Apache Arrow DataFusion and Ballista the future of big data engineering/science?,They are written in rust so are very performant but how real is that they will be future of big data engineering/science?,2023-03-11 06:50:59
11nmrai,The difference between data scientists & analysts and what it changes for data engineers...,"I'd argue the main difference is their goals and, more specifically, the targets they serve:

* data analysts' insights support humans to make decisions
* data scientists' iterations improve algorithms to take decisions

And supporting both of these roles as a data engineer is very similar:

* facilitate the exploration of data 
* ""productionize"" the models they've designed (feeding viz or other tools (via rETL) or ML)
* improve their workflows (collaborative tooling, CI/CD, ...)

Only the actual tools used change, mainly because they're still being seen as very separate verticals in the data tooling landscape.

&#x200B;

What do you think??",2023-03-10 11:37:26
11ctnxh,Weird Question: What Really Differs Data Engineers from Cloud Engineers?,"I know that data engineers work mostly oriented with data. Building pipelines, scheduling tasks, manipluating data, mining it etc. What I dont know is what is it that cloud engineers really do.

Now, while I was looking for intern positions on data engineering I've realized that the job ads mostly include those:

Cloud software Aws etc, containers (docker etc), scheduling (airflow etc), parallel computation frameworks (hdfs, spark etc), bashscript, linux server knowledge.

However it seems to me that 70% of requirements do match with cloud engineering requirements.

I dont know where to ask this question. It matters both fields. But is it easy to switch between cloud and data engineering? Does they do mostly the same job. If it is, what are the most differing parts?",2023-02-26 21:43:47
1ah4h85,What does your day to day work look like?,"Guys, share us your title, yoe and your day to day functionality.",2024-02-02 14:06:55
1agemag,Trying to find a Job,"I just finished an  IBM Coursera data engineering professional certificate and have GIS database/python experience. I am looking for a remote work, Flexible Schedule, and part time or self paced. Any advice?",2024-02-01 16:13:14
1agdgu5,Data engineer interview,I'm reaching out to inquire if anyone would be available to answer a few questions regarding their job as a data engineer. I am currently working on a senior project and am in search of insightful sources. Your expertise would be immensely valuable.,2024-02-01 15:23:03
1aedw84,Google Maps Web Scraping: 3 Practical Use Cases and How to Make the Most of Them,N/A,2024-01-30 02:48:24
1ae5l9n,What do data engineer interviews (mid/senior) look like at hedge funds?,"I am not sure what to expect, if any of you have gone through mid/senior interviews, what do you suggest I prepare for?  (e.g. DS&Algo Leetcode style then Medium/Hard questions? etc)",2024-01-29 20:40:35
1acda11,Epic report writer to data engineer,"

Hi,
I currently work as a Business intelligence developer with my cogito,  clarity, caboodle, and revenue data model certs. I know that EPIC will be transitioning to azure in a couple years. I know BID cap out somewhere around 120k in my area.

How can I best prepare myself to be a data engineer within Healthcare? Should I prepare for DP 203 Microsoft certified azure data engineer and learn python? 
I want to be able to reach at least starting 125k+ fully remote salary in the midwest.

Thanks",2024-01-27 15:29:15
19dpdzg,How to migrate Hive custom functions to BigQuery UDFs,"Excited to share my latest blog post on migrating Hive UDFs to BigQuery SQL UDFs! Whether you're a data engineer or a CTO, this guide is crafted to simplify your migration process. Dive into the step-by-step approach and discover how to leverage BigQuery's SQL for effective data processing. #BigQuery #DataMigration #HiveUDFs  
[https://www.aliz.ai/en/blog/step-by-step-guide-to-migrating-hive-custom-functions-to-bigquery-sql-udfs](https://www.aliz.ai/en/blog/step-by-step-guide-to-migrating-hive-custom-functions-to-bigquery-sql-udfs) ",2024-01-23 14:16:29
1991m63,Got some time to mentor?,"Hello experienced devs,
I am a noob DE working in ADF, SSMS, data modelling and data warehousing with the government as a contractor. 

I am posting this as I recently switched from Manufacturing to IT and want to maintain a network with people that have exp in DE so I can learn and grow to become a completely capable software engineer; occasional chats/call is the most I’m going to ask for. Since I’m a contractor, I cannot connect/ask for guidance in my workplace

Couple of years back I was an IE working on process improvements. I slowly learnt to write sql queries, get data from different internal systems, analyze the data and find opportunities for improvement. 

One day I found out that what I was doing was not very different from our in-house data analyst’s role. 

Then, I learned Azure services and got a job as a contractor. Now I’m struggling to move up and would need your mentorship. I am in NYC area, so I can meetup at your convineance.

tl;dr: a noob DE want a mentor to help navigate through the data carrier.",2024-01-17 17:05:40
19730nu,Freelancing advice,"Hi, I am working as a data engineer since past 4 years in the same company. My expertise is very limited currently; one etl tool and SQL.   
This year I want to leave this job but before that I want to delve into freelancing. Which skills can I learn in the realm of data that are somewhat in high demand? Should I focus on data cleaning or visualization? Or should I learn cloud?

&#x200B;",2024-01-15 07:29:42
194728m,What kind of jobs in FAANG for an ETL dev?,"Basically the title, what kind of roles should I look at in FAANG as an ETL dev? My skills are adf, python, sql, warehousing.

Please also tell me how you find the right job for you?",2024-01-11 17:24:07
194304v,Reviews on Data Engineer Academy?,"Hi all, I was looking for a useful and interactive program that could help equip me with the appropriate technical skills I need for my analyst role- I’m moving more towards the data engineering side of my project but there are skill gaps I have that I need to fill. I’m just looking for structure without bootcamp commitment. 

I came across Data Engineer Academy and looked through some of their content. Does anyone have any experience with DE Academy? How does it compare with other alternatives you can vouch for?",2024-01-11 14:29:44
19019li,Thoughs on date formats,"A random though crossed my mind while working on a new datasource.

Usually we accommodate the date format for each data source, some are dd/mm/yyyy others are mm-dd-yy and so on.

How about converting everything to Unix epoch and leave the format for anyone consuming the data from the warehouse.",2024-01-06 14:34:42
18yhfnc,Help a 3rd world country colleague about hist future,"Hello my fellow table guys.

Can you help me about my future?

I'm from South America, working as DE for the biggest bank in Brazil. With my fiance, we're planning to immigrate to Canada, for multiple reasons, by the end of the year or the begining of the next. Can you guys help me with some stuff?

1 - What are the most in demand skills to data engineering in Canada?

2 - What do you think is the best city for me to keep my career in data engineering?

3 - How do you think is the most effective way to create network and apply to jobs?

4 - What to avoid?

5 - How about remote jobs? Does it still exists?

6 - Any other suggestions?",2024-01-04 16:39:51
18ulpgs,DBT (SQL ) for querying Non relational data & use case in Data warehouse,"Sorry newbie here with modern data stack , Say I am doing ELT , now lets say data is there in the warehouse with relational data and also lot of non relational data eg. xml ,json , mainly json , logs   


Now I want to do transformation via DBT   
1) is it targetted towards BI use case . . ie DBT is for helping making the star schema from the raw data ?    


2) Can DBT (yes via sql ) be used to query and transform both relational and non relational data from the data ware house .  
If yes then . . can u pls give one e.g. how will sql alone do join or query with non relational data . .  


2a)

If no then   
Or we are saying that club PySpark +DBT to query relational +non relational together for transformation   
3)Can DBT (Batch )+ PySpark(streaming) -effectively replace any major traditional or upgraded ETL tool like informatica ?   


I googled and read through previous thread on same but i am not able to infer a confident answer ,Any senior can guide would be grateful .",2023-12-30 18:11:12
18ukb0f,"For those who have done a Snowflake certification (Snowpro), do you get results immediately?","Title is pretty self-explanatory. I'm curious as to how quickly you get results after doing a Snowpro certification. I'm planning to do the Snowpro Core cert soon, and might schedule it sooner so that I can slap it on my resume/apps.",2023-12-30 17:09:35
18srpzd,Productionizing Jupyter Notebooks with Versatile Data Kit (VDK),N/A,2023-12-28 11:23:44
18ojyhx,Help with Apache Airflow + Spark,"Hello fellow data engineers, I am at a dilemma.  
How do you organize your transformation code?

For airflow to run spark jobs, it needs access to the .py scripts  
I would like to keep using Gitlab for managing the code base.

My initial thought was to build a custom operator that will pull the latest version of the code then use the command spark-submit to run the transformation.

How did you toggle this problem?",2023-12-22 17:18:39
18npho9,Exploring 3D Terrain Visualization with Python: A DEM and PyVista Tutorial,"&#x200B;

[Exploring 3D Terrain Visualization with Python: A DEM and PyVista Tutorial](https://preview.redd.it/j5mpe1ek1o7c1.jpg?width=1024&format=pjpg&auto=webp&s=a4ddbb06ac319a3328da49a7ee41596b857b9b9a)

[Exploring 3D Terrain Visualization with Python: A DEM and PyVista Tutorial](https://spatial-dev.guru/2023/12/17/exploring-3d-terrain-visualization-with-python-a-dem-and-pyvista-tutorial/)",2023-12-21 15:24:33
18i5i5m,The State of Cloud Data Warehouses - 2023 Edition,"Hi, post this here as well, because I spent some time gathering all the new releases of Snowflake (and competitors) in 2023 and mapping features against comparing platforms (and creating the pictures). Hopefully this usable for you as well

https://preview.redd.it/h9uh1s2dj86c1.png?width=1219&format=png&auto=webp&s=00efc02808eb614032adfb04a9e72df769d08cc1

[https://www.recordlydata.com/blog/the-state-of-cloud-data-warehouses-2023-edition](https://www.recordlydata.com/blog/the-state-of-cloud-data-warehouses-2023-edition)",2023-12-14 10:12:29
18c9eo3,Is it too soon for me to be applying to new roles?,"For context, I have just over 1.5 years of experience as a data analyst with an IT consulting firm and have been working with DBT, Snowflake and Azure during that time. I've learned a lot using this tech stack and I'm also studying for the Azure DP-203 exam with the goal of taking it this upcoming spring.

With how the job market is currently and my experience level is it even worth my time to be applying to Analytics Engineer and Data Engineer roles? Obviously having a cloud certification would be more beneficial but should I just wait until I hit the 2 year mark and have an Azure cert under my belt before even considering to apply elsewhere?",2023-12-06 17:39:57
18be3i0,Near Real Time Ingestion to DB using Python,"I have a scenario where we are receiving information on a Kafka Topic and are decomposing the complex message structure and storing the records to a relational DB. We originally tried using jq to decompose the json messages but we are not seeing good speeds. 

In your opinion is it possible to have a performant pipeline in Python or will Java be be required for such a task?",2023-12-05 15:15:51
185tdkb,Knobhead recruiters,"Has anybody else had recruiters ask for every damn think in their mental toolbox, when all the client actually wants is somebody to hold their hand and tell them what data is? AWS, GCP, Python, Redshift,SQL, TSQL, PSQL, cloud infrastructure, terraform etc.... come on give us a chance. Give us a use case and we will go to work.",2023-11-28 10:54:09
185mfcp,GCP professional data engineer exam question repetition,"Hello guys… hope you doing great.
I’ve this certification exam soon and I am studying with some past questions materials. If you’ve had this exam before, how often are questions repeated on a scale of 1 - 10 (lowest to highest)

Looking forward to your responses 🙏",2023-11-28 03:28:24
184y7cx,[Research] Tell me about your experience with Databricks,"Hello,

I'm a copywriter/technical content writer. I have to write an article about Databricks and I'd love to get some insights from people who use this platform on a daily basis. Could you please help me? :)

1. What does Databricks help you accomplish?
2. What are its most remarkable features?
3. Where does it fall flat or not deliver as expected?
4. Could you let me know why you chose Databricks over some other competitor?

Or, feel free to write anything about your experience with Databricks.

Thank you!

P.S. I'm an independent freelancer. I am not affiliated with Databricks.",2023-11-27 08:25:29
184jfyj,Experience with databricks sql? Opinions? Limitations? Positives?,"Any thoughts welcome, expecially when linked to power bi or tableau",2023-11-26 20:07:21
17wqu3u,"""Your data model should take into consideration possible performance issues""","I'm doing an analytisc engineering challenge where I'm supposed to create an MRR data model. I don't actually have real data to work with--I'm just supposed to think about what the source data and analytical model might look like, mock up a sample table and SQL queries to address common KPIs.

Part of the instructions is that I should consider performance issues in the model given that they have thousands of users. But how am I supposed to consider performance issues theoretically if I'm not working with data? At work I usually do ""experiments""--write two SQL queries and compare how they perform.

It's just that no matter what kind of model I develop now, there's really no way of telling whether it will be performant or not if there's no real data to work with. Or am I missing something?",2023-11-16 16:19:00
17woxqc,Switch to Atlan worth it?,"Hello Everyone, I'm currently working at Amazon as an IC. I recently got an opportunity at Atlan for a managerial position. Wanted to check with the community if the switch is worth it. TIA",2023-11-16 15:00:51
17ro2oo,New to Airflow: How to create a backfill DAG with a different task sequence and start/end date parameters,"Hi everyone,

I am new to Airflow and I have a question. We have an existing DAG(a>b>c>d>e>f) consisting of Glue jobs(the Glue jobs have date as their input), that processes data for a given date and writes it to an S3 bucket with the partition dataset\_date=$date. The team has asked me to create a backfill DAG that is different from the original DAG in two ways:

1. The task sequence will be different:Original DAG: a>b>c>d>e>fBackfill DAG: a>b>c>d
2. The backfill DAG will have two date parameters, start\_dateand end\_date, instead of one. The DAG needs to run individually for all dates in the specified range.

Here are my specific questions:

1. Is there a better way to create the backfill DAG than copying and pasting the code from the original DAG and removing the last two steps?
2. How do I add the start\_dateand end\_dateparameters to the backfill DAG and make it run individually for all dates in the specified range?

Thanks in advance for your help!",2023-11-09 21:51:31
17rn2mn,data Catalog tools benchmark,"Three years ago - I made a benchmark of the data catalog landscape. The market has evolved so much that I decided to update it. 

Do you feel like any feature/tool is missing? I'm trying to make this resource as accurate as possible so if you have suggestions - don't hesitate to shoot them my way. 

Full benchmark here: [https://www.notion.so/Data-Catalog-Tools-Benchmark-4bcbee621de243b6a34deaebd28180d0](https://www.notion.so/Data-Catalog-Tools-Benchmark-4bcbee621de243b6a34deaebd28180d0)

&#x200B;

[Full benchmark](https://preview.redd.it/fkk9xfs30ezb1.png?width=2408&format=png&auto=webp&s=e9ecdb6c7041bdbd006ced49a28062c4a82cacd4)",2023-11-09 21:07:56
17i4cb6,AI in dataengineering,"I was just very curious, how AI can affect dataengineering. As in which all area in a standard DE project can be replaced by AI.
The reason I am asking this to focus on  developing skills for AI which are DE oriented.",2023-10-28 02:48:02
178uxpm,Blog post: Handling physical deletes from the source and continue populating your analytical data store,N/A,2023-10-16 01:50:45
1750hbf,Data scrape,Anyone interested in talking about data scraping?,2023-10-11 00:13:57
174qp1c,On-prem setup for a lakehouse,"I'm working in a medium-sized company and due to regulatory reasons must work on-prem for now. 

I am setting up a data lake + warehouse solution to support our BI. The data isn't enormous (say 200 million rows across a bunch of SQL sources). 

I am considering a lake house type approach using either delta tables or Apache Iceberg tables so that a lot of the work would be reusable and easy to migrate if/when we get to move to Databricks or something. Does this sound reasonable?

What I am a bit confused about is setting up the infrastructure for writing data into the deltalake tables. Do I really need to run Spark locally to do that, or am I getting something wrong? In terms of computation Spark seems a bit overkill for our usecase, but I would really like to use delta tables or iceberg for the metadata niceties.",2023-10-10 17:21:14
174kfth,The Truth about Databricks vs Snowflake,N/A,2023-10-10 12:53:42
16ztexg,"Any Data Engineer in the Persian gulf( Dubai, Abu Dhabi, Qatar, Bahrain) ? How is the situation there?","Hello I am planning to move there but I want to know if it's a good plan. 

I have not experience so far , but next year I will be working in this in my country and after that I am planning to move, but I want to know insights about this career there.",2023-10-04 17:35:14
16xx761,What are some resources for interview preparation?,"I know for sql there is data lemur, leetcode, hacker rank but for big data , visualisation etc where to prepare that? Should I use chat gpt ? Is it a good idea",2023-10-02 13:51:34
16vbobm,Overloaded Spark cluster,N/A,2023-09-29 13:28:01
16tpb3v,Fresher taking interview of senior DE,"Hello,
Currently working in a startup and I am the only one who is working in Data Science or Data Engineering task. Joined in May 2023 as an intern.
Now,My CTO has asked me to take interview of senior DE, these guys have around 3-7 yrs of work exp, I am very much confuses what to ask!
Can you guys tell me! What are the fundamentals need to be asked",2023-09-27 16:35:02
16pk5vs,Connecting to a linux machine with Python?,"My reddit consultants, I need your help for this one.

I have an unofficial job offer to basically revamp the entire ETL process for a hospital IT team. It's my first job out of college, I have degree in natural sciences. The extent of my programming skills reaches creating basic flask applications with sqlite databases, hosting it on the cloud (PythonAnywhere or Azure), and maybe some local psql and postgres with bash. 

The current ETL process is as follows: data is entered into a third party proprietary software, extracted via built-in functions for export as xml/xlsx, transformed with vba, then a powerBI dashboard sits on top of the excel files. 

My proposal is to extract the data with Python (not sure if this third party software has a read/write API, but I was told that the folders and files of this proprietary software are accessible through on premise linux machines), transform with pandas, then load it into some sql db dedicated for analytics hosted with on prem servers. From there, we can export clean spreadsheets or create a dashboard on top of it with PowerBI. Then somehow, write the clean data from this sql db back into the proprietary software.

This was the vision I proposed during the my interview. My managers are not technically savvy, they're open to any technology to revamp the current ETL process, and they seem to like my idea. They consulted with someone with a proper CS background, and asked if I knew how to connect to linux machines with Python. I exaggerated and said ""yes I know how to use linux and Python"", as in I can type in python3 into a bash terminal and play with python locally. But I think they mean if I could connect to remote linux machines, maybe with something like paramiko? I dont have a formal CS background, I have no idea how authentication, VMs, etc, work. I have no idea how I would remotely access the proprietary software files sitting on on-prem servers. I dont know if I'll be given a work computer that doesnt have inherent access to the proprietary system, or if I could even download third party python packages. I need direction please.",2023-09-22 20:14:10
16jqgc9,Break My LLM Batch Inference Tool," Hi All,

When you have a chance can you give my AI/ML dev tool a look? I'm trying to get feedback and figure out what other use cases it could address.

The name of the python package is called [Burla](https://www.burla.dev/docs), the goal is to make it simple to run any python function, on thousands of CPUs/GPUs, with zero setup and just one line of code. I just got it running at 4k CPU concurrency and 500 GPU concurrency per user (I think).

I built this for batch inference because I've been working on a data science team at a logistics company where I constantly need to pass millions of rail, truck, and ocean freight routes through ML pricing inference models. If you can think of any other solid use cases or if you think the product is shit please let me know. All feedback is wanted even if you think the project is a dud.

**Command line setup**

    pip install burla    
    burla login   

**Python Code Example**

    from burla import remote_parallel_map   
    from time import sleep     
    
    my_inputs = list(range(1000))  ​    
    
    def my_function(my_input):              
        sleep(60) # <- Pretend this is some complex code!               
        print(f""Processed Input #{my_input}"")               
        return my_input ​     
    
    results = remote_parallel_map(my_function, my_inputs)   

**FYI: For feedback purposes anyone who uses Burla has 10k CPU and 1k GPU hours free.**",2023-09-15 22:08:33
16eav23,Testing Data in Apache Spark,"Blog post is out, sharing how to solve challenges related to spark testing, including both unit and regression testing. Special Mention of spark open source testing libraries by MrPowers.

https://www.junaideffendi.com/blog/testing-data-in-apache-spark/

Let me know how you perform testing.",2023-09-09 17:04:07
16cgkfi,AWS Certification Fee,"I have been planning to give AWS exams for certifications, although the price of it is just too high for me as a student. If anyone can help me with any sort of discount or promo codes I would highly appreciate it.",2023-09-07 13:57:41
164kamc,How Safe is Cloud?,What do you guys think and what’s the experience like storing and processing data in the cloud?,2023-08-29 14:40:26
163ysvh,Organizing Pyspark Transformations with Hamilton,"Hey folks! Elijah here, co-author of the open-source library hamilton ([github.com/dagworks-inc/hamilton](https://github.com/dagworks-inc/hamilton)). We just released a new feature for building out pyspark transformation and we're looking for feedback/thoughts.

  
If you're not familiar with Hamilton, its a tool to build DAGs out of simple python functions. The parameter name of the functions create dependencies to other functions (or external inputs), and the name of the function specifies the name of the output artifact, reference-able by other functions. If you've seen pytest fixtures, its a very similar pattern. You can try hamilton out at [tryhamilton.dev](https://tryhamilton.dev).

Hamilton has always worked well with pandas due to the index-first nature of the library -- one could write single function per column then join the results at the end. As pyspark has no notion of indices (and columns don't really function independently), we built an integration that allows you to treat column-level transformations as dependency-declaring, limited,-scope UDFs of pandas functions, pyspark dataframes, and pandas primitives, and compiles them together into the desired pyspark transformation at the end. You can the transformations as a DAG of your functions, enabling easier mapping of error -> code (although this is something we're still improving).

Our hope is that this enables you to write modular, unit-testable, and easy to read/debug pipelines out of pyspark code.

&#x200B;

We'd love any feedback/thoughts about the feature -- docs/README linked below:

[https://github.com/DAGWorks-Inc/hamilton/tree/main/examples/spark/pyspark](https://github.com/DAGWorks-Inc/hamilton/tree/main/examples/spark/pyspark)

&#x200B;

&#x200B;",2023-08-28 21:33:28
15yfx2r,Inherited Legacy/Spaghetti Codebase,"So I started a new job as a DE a week and  a half ago and I was absolutely shocked when I laid my eyes on the codebase. My predecessor, wrote the pipeline in the most old fashion way. Doing everything in her power not to use python data libraries only sql. She only use python as an automation framework, loading json objects to the db in multiple nested loops (endless). there are 4 databases! which 3 of them are staging because she apparently, I'm guessing, did not have any the desire or knowledge to write readable code and only rely on sql, which is fine but there are so many stages. For each of her 100 lines of code i can make them 10. But the logic is so vastly nested, barley readable, module inside a module and functions inside function, it's like a black hole. The data volume is small yet there are hundreds of tables, hundreds of stored procedures and hundreds of views and insane sql quries. 

I really don't know how to approach this. I think it's a bad idea to refactor the existing codebase, I'm still very new and barley understand the business. There is a backend developer which handled the stuff until I arrived that shares my opinions which is helping to get on my feet.

I thought maybe I can just try to build (very slowly - over a period of several months) new pipelines from scratch which are ""sensible"" by trying to rely on reports and bi system as a compose and build a new db based on what's there.

Or I should just maintain the current codebase (which is not the easiest on it's own ) and new pipelines i'll do in my  own way.

What do you guys think?

Sorry about the rant.",2023-08-22 19:29:08
15vokk0,feedback request : snowflake for data engineering,"As a data engineer using snowflake,

what tools and features do you think are lacking now ?  
which features should be improved and how ?  
which features do you like ?

I already did a similar feedback request for [the VS Code extension](https://www.reddit.com/r/snowflake/comments/13ffe1b/vs_code_extension_feature_request_give_your/). Please post any feedback about the VS code extension there. Some requests have already been answered (multiple result tabs) and some are being developed.",2023-08-19 19:04:14
15sfg1l,How would airbnb manage their folders and images?,"Hi all,

Im new here! I hope you don't mind me asking for advice. I'm a new dev and new to the data scene.

But I would like to ask if you were to construct an airbnb app, how would you structure the images in the sql and folders?

I would imagine it would state-> province -> city

 I think using a tree structure would be best for the folders, but would you have to implement the structure in the SQL? Would you need to store all the file names in a sql table called images, or like would you just save everything in the folder and just retrieve everything inside a folder?

Any advice is truly appreciated!",2023-08-16 04:31:05
15mag21,How to handle SCD in BigQuery,"I've just started a new job where part of the job is some data engineering. I don't have any work experience in that, but I know what I want from an analysis/reporting perspective. My employer knows this, so I have time to figure things out.

Since I have background in data analysis and reporting I'm pretty good with SQL and have some limited Python knowledge, but that is about it.

The data landscape is not very complex here, with only a couple of source applications that don't spit out that much data. Currently most of the data is dumped directly into BQ tables using Fivetran. After that there are some transformations to create tables that are used for analysis. However its basically a SCD1 solution, so its not possible to do historical analysis. At my previous job the date warehouse was in Oracle with SCD2 implemented. Since historical data is a must this is one of the things I want to fix, but its not clear to me what the best practice is for BigQuery specifically. Do I work towards an SCD2 implementation (I saw it used as an example use case for Dataform in the Google  documentation), or do I do something else (I saw a blogpost saying to snapshot all dimensional daily and put it in a partitioned table)? Either works for me from a data analysis perspective,  but what are your thoughts on this?",2023-08-09 09:37:33
15hzh6r,Bypass website blocks to seamlessly extract from e-commerce sites using Python,N/A,2023-08-04 13:01:32
15cboyc,How to choose the right ETL tool,"Hello hello! I just wrote this article on [how to choose the right ETL tool for your business](https://www.polytomic.com/blog-posts/how-to-choose-the-right-etl-tool-for-your-business).

Am I missing anything here?

Full disclosure: I work for the company where I made this blog post.",2023-07-28 22:47:41
15c3mxk,Stored procedures as a data engineer,"Hi , I need some help and advice on how stores procedures are being used as data engineer in migration project, from mysql to azure cloud. It says incremental loading.  I am new to this field and role and this is the job , I am reading about stored procedures but simply sql commands, but I am missing how exactly it is used and why? Anyone with such experience can please guide me ? So I can explain this in interview well.",2023-07-28 17:26:15
155g7xw,Replace airflow to aws step functions,"Hi there 
My company is trying to move in a path where all solutions need to be 100% AWS , and we (data team) was asked to replace airflow to SF , how do you guys see it? Would SF replace all features from airflow?",2023-07-21 07:01:32
152qsh4,"Seeking Advice on Securing Data Engineering, Data Science, or Data Analysis Jobs as a Fresh Graduate Based in Asia, Preferably with Overseas Opportunities and Dollar-Based Compensation","Any additional advice, tips, or personal experiences you can share would be immensely helpful in my career journey. I am eager to grow professionally and enhance my skills in these exciting fields. ",2023-07-18 06:50:07
150dwlj,Chat-GPT plugins and Data Engineering,N/A,2023-07-15 14:47:57
14uswtq,SCD Type 2,"SCD type 2 is all about managing changing dimension,but it expires record when fact changes . What is the context of dimension change here? Date dimension??",2023-07-09 08:15:43
14ub3q7,Azure Data Engineering (Azure Data Factory),"Hey,

I am conducting a workshop on Azure Data Factory for roles in Azure Data Engineering. It happens from 18th - 25th July. If you are interested, you can be a part of it.  
Just send me a DM.  

I am a professional and trainer and work for a leading tech company.
Price is kept at $75 USD for each participant.



It Covers complete aspects of ADF
- Creating pipeline
- Using multiple attributes of ADF to create pipeline
- Building Triggers in ADF
- Capturing logs of ADF.

It will be an extensive 7 days (14 hrs) workshop via Zoom.
Live session, with recording made available post the sessions.

First session will be free for all to join.


Thanks & Regards
Amit",2023-07-08 18:09:08
14ua40n,Becoming a better Engineer,"This time I wrote a non tech blog.

Let me know which approach you follow and how it works. Also, add new ones.


Thanks",2023-07-08 17:29:07
14rfc42,"Join, Merge, and Combine Multiple Datasets Using pandas","Data processing becomes critical when training a robust machine learning model. We occasionally need to restructure and add new data to the datasets to increase the efficiency of the data.

We'll look at how to combine multiple datasets and merge multiple datasets with the same and different column names in this article. We'll use the `pandas` library's following functions to carry out these operations.

* `pandas.concat()`
* `pandas.merge()`
* `pandas.DataFrame.join()`

The `concat()` function in `pandas` is a go-to option for combining the DataFrames due to its simplicity. However, if we want more control over how the data is joined and on which column in the DataFrame, the `merge()` function is a good choice. If we want to join data based on the index, we should use the `join()` method.

**Here is the guide for performing the joining, merging, and combining multiple datasets using pandas👇👇👇**

[Join, Merge, and Combine Multiple Datasets Using pandas](https://geekpython.in/multiple-datasets-integration-using-pandas)",2023-07-05 16:18:05
14m8os9,What is a Data Product? An In-depth Introduction,How did I do with this in-depth intro? [https://www.dataops.live/what-are-data-products](https://www.dataops.live/what-are-data-products),2023-06-29 15:57:22
14jijwp,Our journey at F5 with Apache Arrow (part 2): Adaptive Schemas and Sorting to Optimize Arrow Usage,"Here is the second part of my 2-article series on Apache Arrow and OpenTelemetry. In it, I describe the optimizations used to make the most of Apache Arrow, both in terms of compression rate and memory usage. Happy reading!",2023-06-26 14:16:46
14gz5us,How we built Gradient to optimize Databricks clusters at scale,N/A,2023-06-23 13:52:50
14ae03u,What does an ideal Data Engineering portfolio look like?,"I know the resources are here for learning tools and commonly found tech stacks, but if I were to try to formulate a portfolio, what would you all recommend would be the best thing to showcase? ETL pipeline example in Python? SQL queries?",2023-06-15 21:29:04
149djon,Is anyone now learning KDB+?,"I am interested in learning KDB+ because getting into fund trading industry is always what I desire.

I just realise so many investment bank trading teams and hedge funds are still using KDB+.

Is there any learning material or beginner project for me to learn KDB+?

Thank you!",2023-06-14 17:20:42
13q8f1d,Does my internship qualify as Data Engineering experience?,"Hi all, 

I am a computer engineering student currently working in a summer internship. I do not work with anyone considered a ""data engineer"" everyone works a sort of mixed role of programmers and planners and other random engineering things.

Anyways so most of my time I am working on a project where we are designing and building a system that takes some sensor data from multiple sources and different databases parses them in python, pushes the data to a postgres database then we use a Nginx to host a Grafana front end to visualize the data in the postgres server. It may seem silly that I'm asking if this is considered data engineering but because I haven't been around anyone with that specific job description and this has been my only internship I don't know if that is what I am doing.

Also my career goal is move to a architect/manager position out of university so I was wondering if anyone had advice on how to make that move as soon as possible out of school.

Thanks you!",2023-05-24 02:25:11
13l2cr0,Constructing JSON objects in Athena using SQL,N/A,2023-05-18 15:36:57
13boa43,Anyone currently hunting for their 2nd job? How’s the market?,"Have about 4 yoe mostly at one place. Current job is not working out so I’m looking to leave. 

Last go around I had a lot of interviews (last summer) but I hear it’s a little more scarce now. I haven’t paid much attention so wondering what y’all’s experience has been for non-entry level roles this season?",2023-05-08 12:55:44
137lajn,💡 What are Slowly Changing Dimensions (SCD) 💡 SCD Types 💡 How to implement SCD Type 2 in VDK,N/A,2023-05-04 13:50:50
136nzfb,Data Sharding in Apache Doris,N/A,2023-05-03 14:44:16
12qax7w,How to prepare for this position? See below,"https://jobs.cvshealth.com/job/17447961/lead-cloud-engineer-gcp-sql-python-devops-ai-ml-remote/


Heard CVS (us employer) is spark / sql heavy so easy / medium leet code? Or coderpad.? Do they also ask extensive hour long case scenarios to solve(panel)?

Anyone whose in this position or a similar one like sr DE? Tips?

Does the JD sound like they need devops guy too or assume devops knowledge should be there as it relates to ci cd cloud aws gcp etc?

Thanks 😊",2023-04-18 04:49:57
12paw9r,I got an interview for “Data Platform Engineer”,"I have been studying to land a data analyst role for a while, but an opportunity came up for this. The requirements included SQL and Tableau, which I’m comfortable with…so I decided to try my luck and apply. 

However, there’s also a requirement for Linux and Hadoop or something similar, and I’m a bit familiar with snowflake 

My issue is, my knowledge on Linux is basic. What do I need to know about Linux to be able to do this interview? 

Thanks in advance",2023-04-17 10:24:50
12ldrgk,My plan to become a data engineer,"I'm currently in the US Army and plan on becoming a data engineer after I get out. My plan is to take the Data Science: Analytics course on Codecademy, then start applying for part time jobs (I'm assuming I'd still be in the army).

 After the codecademy course, I plan on taking the new Google Data Analytics course to strengthen my data science skills. 

I'm assuming I'll get my foot in the door as an entry level data analyst/data scientist and then work my way up to data engineer.

I'm also going to be working on the data anlytics/data engineering cloud certs from the big three (AWS, Azure, and GCP)

What do ya'll think of my plan?",2023-04-14 00:53:54
12kr6rl,Materialized Views,N/A,2023-04-13 14:31:47
12ih4dp,The database inside out with event streams,N/A,2023-04-11 11:42:55
12b7k8b,Career in data engineering,"Hi everyone , i hope everybody doing well , so i facing a problem : i discovered programming since 2018 a year before my bacalureat, so i decided to  dive in it domain , the first year in college , i had no laptop, i remember when we writing c programs in paper , the entire year i learned the fundamentals of c , networking , os , vb.net , some algebra , the second year i switeched to another school ( high school of technology) , i chosee to study data science and business intelligent , i liked everything  starting from python pandas java uml , the summer come , i need to pass an intership , i did  a desktop app with access because , i don't want web dev so the next year  i m happy we have machine learning in the programme  every day i practice what i learned the second half of my second year ( third year after bac) we have an end project of the year , i chosed to implement a mobile app with flutter second summer another intership in ml i was happy , this year ( last year) we back to study software enginering i hated back to java ,i have to pass a final intership in web devlopement , so the  problem is i dont have the neccessary skills for web  dev , and i want to launch a career in data engineering so what i m supposed to do in intership forgetting data and focusing in back end to have good grade in my bachlor pr starting learning data eng ?",2023-04-04 03:15:34
126uq8r,Snowpark as ETL/ELT,"Hello DEs, I’m trying to mainly do the ELT process in Snowpark using Python since Snowflake is our newest DW platform. I was using Datastage as the main ETL tool before considering Snowpark. 

Does anyone have like a use case for this? 

I’ve explored with Snowpark using Python and the basic transformations can be done (join, split cols, merge, sort). I just can’t see how I can integrate all ETL/ELT components with Snowpark/Python, UDFs, Streams.",2023-03-30 18:09:40
124pw6d,Python/PySpark Interview Questions for Entry-Level Data Engineer,"Hey all,

Recently, I just received a date for a technical interview for an entry-level data engineer position at a large retailer Fortune X company. While I am extremely happy to hear that I get a chance to dive into the data engineering field, I am also extremely nervous and unprepared for the interview. The interview will be covering both Python and PySpark; I have some exposure to Python (nothing Leetcode-level at all) and absolutely zero experience with PySpark. The interview will be in about a week.

For context: I have a bachelors in industrial engineering and making a career jump from a process engineering role (tbh more of a “data analyst” then process engineer). In my role, I primarily work with Dataiku (very similar to Alteryx) by using built-in recipes, SQL scripts, and minor Python scripts to read, transform, and write data into BigQuery tables for other engineers on my team to use. 

My questions are: 

-	what types of Python and PySpark questions and topics should I expect for my interview?
-	what are some recommended technical programming resources that I should grind on before my interview?
-	if I try to start Leetcoding now, do I need to familiarize with all the data structures and algorithms? What should I primarily focus on? I tried Leetcode a long time ago and sucked at it
-	is it feasible to learn PySpark within a week or should I tell them that I have no experience with PySpark? This was never brought up in my prior interview and my resume does not mention PySpark experience at all

I really appreciate any feedback I can get!",2023-03-28 14:07:22
122a093,Coalesce for Snowflake,"No, not the sql function lol
Has anyone used this product? Pros, cons, thoughts? 
Thanks!",2023-03-26 03:43:55
120xqne,What steps and certifications should I get if I want to get into Cloud Engineering as a career?,"Hi! I have been looking into IT as a career recently, and settled on working towards working in Cloud Engineering. I was told to start working on getting a CompTIA A+ certification, but I am unsure if that is the first step, where to take the exam or training, or what the next steps would be. 

I want to get there through certifications if possible as I am not in a position to go to college.",2023-03-24 20:38:20
120jb4i,Help me find the right data ingestion tool,"I need to get the correct Modern Data Stack being:

Data ingestion (EL) tool --> Snowflake (maybe GBQ) --> dbt Cloud --> Hex

Data ingestion is the stumbling block at the moment.  I need to ingest ActiveCampaign and a handful of other bespoke APIs.  Tool must also be HIPAA compliant.  My thoughts were:

FiveTran - but it doesn't do bespoke APIs and not HIPAA compliant really ([https://fivetran.com/docs/security#hipaa](https://fivetran.com/docs/security#hipaa))

Stitch - feels a bit restrictive, but definitely not HIPAA compliant

Hevo - does ActiveCampaign, bespoke APIs and is HIPAA compliant

&#x200B;

But is there anything else there that's more cost-effective per month/year?",2023-03-24 12:51:26
11yupcg,How do you handle test data in production db?,"Good day,

For anyone using databricks, how do you handle ""test"" data in production systems? Do you filter the test records in silver layer or create new data models in gold layer with test data filtered out? Thanks",2023-03-22 19:59:55
11xvudc,Python Review for coding assessment?,"Hello,

I have a python coding assessment I must complete to move on to the next round of interviews
Is there a resource that is kind of a quick python refresher specifically for data engineering or data integration?

I do typically use python for my day to day tasks but I guess I would have peace of mind with a quick review before taking on this assessment.
It just wouldn't hurt to review also honestly.

What would you guys recommend?",2023-03-21 21:47:08
11unx46,Should I switch my role from snowflake developer to spark developer,"Hi Guys, need feedback on my situation.

I'm trying for a switch and I'm  not getting interview calls from big tech companies.

I'm currently working as a data engineer with 1.9 years of experience where my role mostly revolves around -

1.writing snowflake procedure for transformation 
2.Adding few lines in a spring boot application that's used for snowflake ingestion.
3. Python scripts to monitor some stats from snowflake.
4. Building automation using python.


 I like working on development projects rather than writing sql queries.

So, I get calls from serviced bassed companies and most of them don't setup interview as they think they can't match the expected salary. 

So should I start learning spark since I don't see data engineers with snowflake as primary skill getting hired in faang or big tech companies.

Disclaimer: I'm trying to switch because my current role looks like a support kind of work and I want to work development projects.",2023-03-18 13:01:58
11ujy5a,Open source data observability tools with UI?,"I'm looking for an open source data observability tools with UI, dashboards, alerting. 

I really like SodaCore but the UI is only available with SodaCloud. 

I also found  https://github.com/monosidev/monosi but it seems there are no activities in the repository from last year.

Tried to build something in Grafana, but I think it's more for monitoring server activity.",2023-03-18 10:00:16
11tdp0e,Protecting Your Pipeline From Malformed JSON/XML With Snowflake,N/A,2023-03-17 01:51:11
11o2wfr,How many candidates get tested on math and stats in a data engineering interview?,"Engineer here and its been 4 years since I last interviewed for a position and now find myself in those shoes. Looking at [https://www.reddit.com/r/dataengineering/comments/prdxfb/do\_you\_use\_math\_and\_stats\_as\_a\_data\_engineer/](https://www.reddit.com/r/dataengineering/comments/prdxfb/do_you_use_math_and_stats_as_a_data_engineer/), looks like data engineers predominantly in other companies and cultures don't apply general mathematics and statistics in daily work. But I would like to know if I need to brush up on my stats and math for the interview processes' or if that's a waste of time and I should just focus on SQL?",2023-03-10 22:27:49
11luuwy,DE openings for 16+YOE,"I'm currently working as a senior DE in service company. Currently working on on-prim netezza warehouse migration to cloud. I have good hold of SSIS,SSRS,REDSHIFT,PANDAS PIPELINE,AWS S3. I am trying for a job change as salary is very low as compared to daily work. I am not getting a single call for interview, is it due to my age (40+) or anything else? I am currently learning pyspark even though it is not getting used in current project but will it help to get call/ land in new job?",2023-03-08 12:15:42
11j5mrf,Instagram User Acquisition - Timeseries View,N/A,2023-03-05 18:05:18
11dlisz,Help getting on track,"Hello everyone i applied recently to a data engineering job but got rejected after passing technical interview and getting ≈ 80% on codility test,

My current job is python developer/data engineer basically where i am they had a project for about a year and a half and its done. 

I feel down and last time i practiced what i did as a data engineer was about 8 months ago, idk how to find a data engineering job anymore and more importantly how to practice what i know or need to know to actually land a job. 

I hope you help me find my way out of this",2023-02-27 20:12:19
115em3z,Debezium CDC Integration Blog/Tutorial,Could anyone please tell if you are aware of Debezium CDC Usage keeping Production method in mind. Reading from Some DB and propogating to DW (say Azure ADLS Gen2/S3),2023-02-18 12:49:36
17b730n,Is creating a web app the best way to expand data engineering knowledge while possibly building the foundation of a sellable product?,"I want to learn more about data engineering (architecture, pipelines, etc.), but I don't just wan to spin up Python/SQL and solve problems for the sake of solving them. I want to potentially build something that I can passively make income from in the long term. 

I know there is a skill gap between data engineering and web app development, but is there any better thing to work on to develop my skills and potentially a steady income stream than a web app?

I have an idea for a web app that would likely be very API heavy or might require a great amount of data scraping which would then need to be processed and organized.",2023-10-19 01:25:57
16ofd3b,Motherduck is now free for anyone to sign up for,N/A,2023-09-21 13:02:04
18o3fek,"ELI5: How do SQL Server, Databricks, and DBeaver work together?","I’m a newbie intern in business analytics, and my manager recently wrote in my performance review that he wants me to improve in SQL. I’ve written a few queries in SQL Server throughout the internship, but that’s the extent to which I’ve used it (I’ve mostly worked in Tableau and Excel otherwise).

For an upcoming project, he said I’ll need to connect Azure Databricks to either SQL Server or DBeaver. I’ve done some online research, but I’m still not understanding what Databricks and DBeaver are. What is the purpose of each, and how do they relate to SQL Server?",2023-12-22 01:46:36
18mo0u9,Does anyone actually use dbt for large datasets?,"Bear with me, I have not yet started using dbt.

dbt seems to be introduced to almost every stack out there. And I get it. It solves a lot of the issues with reproducibility and testing that we've had historically in data management.


HOWEVER, it just seems terribly inefficient!
I'm of the thought, that large scheduled loads are a thing of the past. Small incremental batches that run as data becomes available are much cheaper in hardware requirements, there is no issue if a load fails, because they can just rerun in seconds, and users get a much better experience with live data.


I mean, the above doesn't matter if a full load finish in minutes, but what what if your facts consist of billions or even 100 of millions of rows?

I can read that dbt support incremental loads, but it seems like a bit of an afterthought.


Am I wrong here? Is anyone succesfully running dbt in setups with large datasets? Do you provide ""live"" data?",2023-12-20 06:30:00
17vapzq,How long should it take to integrate a new data source into the data warehouse?,"A few hours? 

A few days? 

A sprint? More than 1 sprint? 

&#x200B;

What's a reasonable expectation? We often have an issue where business users expect a data warehouse to be a thing where you can just send data easily with a few clicks and then the users can query it. 

However we know it has to be ingested, tested, tables created, scheduled, upserted/merged, etc. But maybe our process is actually bad and slower than it needs to be. What is a reasonable expectation? ",2023-11-14 19:56:25
165zucv,Rookie question about Git,"Sorry to ask for a layman question here. What's data engineer's best practice in term of  git? such as once I'm done with my work, it's best practice to push to main, correct? My teams repo has 100+ branches and a lot of them never got pushed to main. Some are more than a year old. Some of those are deployed already. My manager who owns the repo never bothered to merge most of them. Context: branches are projects relatively independent and kind of one-off so it works okay so far. Team is small 3 people. Please give some feedback. Thank you!",2023-08-31 03:32:28
15rp0yr,"🚀 Introducing ""airflowctl"": a command-line tool to simplify your Apache Airflow onboarding experience without docker! 🛠️✨","&#x200B;

&#x200B;

https://preview.redd.it/e2y4gujs69ib1.jpg?width=914&format=pjpg&auto=webp&s=c825b88564b5a5120890475d95c05ead924f7e29

🚀 Introducing ""airflowctl"": a command-line tool to simplify your Apache Airflow onboarding experience without docker! 🛠️✨

📜 Repo: [https://github.com/kaxil/airflowctl](https://github.com/kaxil/airflowctl)

✈️ Install: `pip install airflowctl`

Key Features:

✅ Install and set up Airflow with a single command

✅ Initialize your Airflow local environment following best practices

✅ Seamlessly manage your Airflow projects.

✅ Support different Airflow/Python versions

✅ Manage different Airflow projects with ease

✅ Works out-of-the-box with the airflow CLI

✅ Works without containers

✅ Continuously display live logs of Airflow processes

The main goal for this CLI is for first-time Airflow users to install and setup Airflow using a single command and

for existing Airflow users to manage multiple Airflow projects with different Airflow versions on the same machine.

**#ApacheAirflow** **#CLI** **#airflow2** **#opensource**",2023-08-15 10:38:45
13cpkay,Airflow 2.6: A New Milestone in Data Engineering,N/A,2023-05-09 13:15:44
196pq5t,Should I accept a Senior Data Analyst offer even though I want to pursue data engineering?,"Hi guys,

I'm currently a data analyst with over 2 years of experience who's been trying to break into a data engineering role. However, I've been receiving many data analyst opportunities. 

So, one company I did an interview with have been vague about the role, so I proceeded and did two more interviews and then they told me the position is senior data analyst. Honestly, I really want to leave my current job but also don't want to work as a data analyst anymore.

I'm not sure if I can express that I want to work as a data engineer to them, so they might be open to it.

What do you think?",2024-01-14 20:45:19
17qk7w0,Is the 'vector-only' database era at risk with OpenAI's retrieval innovation?,"OpenAI's made a series of announcements and I am more focused on the one that talks about an OpenAI Retrieval tool that doesn’t require you to either create or search vectors. 

The new OpenAI Retrieval tool could disrupt the need for vector-only databases by allowing direct data integration with AI models, simplifying development, reducing costs, and potentially rendering separate vector databases less essential for certain applications, leading to concerns about their future relevance. 

What do you guys think of this? ",2023-11-08 12:07:25
15673lv,"Dont like Cloud, but DE","I really like coding, building ETL's, plain the overall architecture, building it but the Devops/Cloud part is smth thay i really dont enjoy that much.

Is there a way to be smth like ETL dev or software developer for data intentensive apps?

I wouldn't mind doing it if its only like 10%-20% of the job. In my current job is only like 1%

I dont want to be 40/50% of time configuring stuff at cloud.

Thx!!?",2023-07-22 02:01:30
13bnrwr,Is Meta's Velox execution engine going to disrupt big data science business market in coming years?,"According to Andy Pavlo it is going to be the future of databases but is it real?
https://twitter.com/andy_pavlo/status/1523666179247595520",2023-05-08 12:34:30
18id3ea,"why 1,048,576 rows?","I was looking into Azure documentation for clustered columnstore indexes and I noticed the following:  
"" Clustered columnstore indexes work on segments of 1,048,576 rows. As Azure Synapse Analytics has 60 nodes per distribution, the minimum recommended number of rows for a clustered columnstore index is 60,000,000. ""

why is it that the don´t close it to just 1million? also, why is this the exact same limit of rows on excel?

Is there someting I'm missing about this number?",2023-12-14 16:58:53
16xzcqx,Goodbye Spark. Hello Polars + Delta Lake.,N/A,2023-10-02 15:17:03
129f92i,Python Package to build ETL flows/dags,"Hi everyone,

I developed a python package to build ETL flows/dags. Each flow is defined as class. Its good for visualizing and running your flows and is notebook friendly.

    # example.py
    from flowrunner import BaseFlow, step, start, end
    
    class ExampleFlow(BaseFlow):
        @start
        @step(next=['method2', 'method3'])
        def method1(self):
            self.a = 1
    
        @step(next=['method4'])
        def method2(self):
            self.a += 1
    
        @step(next=['method4'])
        def method3(self):
            self.a += 2
    
        @end
        @step
        def method4(self):
            self.a += 3
            print(""output of flow is:"", self.a)

Running the following display command method gives this output

    ExampleFlow().display()

https://preview.redd.it/bgfx0yu1bgra1.png?width=418&format=png&auto=webp&s=72a782fec64878df504a4484140a4a50f3fd5343

&#x200B;

Repo link: [https://github.com/prithvijitguha/flowrunner](https://github.com/prithvijitguha/flowrunner)

PyPI link: [https://pypi.org/project/flowrunner/](https://pypi.org/project/flowrunner/)

Documentation link: [https://flowrunner.readthedocs.io/en/latest/](https://flowrunner.readthedocs.io/en/latest/)

Let me know what you think!

Feedback is welcome :)",2023-04-02 08:43:12
18um0uk,Informatica(latest cloud) with DataBricks vs Pyspark with DataBricks for modern Data stack,"What i am infering (not sure is ) the difference is with informatica there is a vendor lockin while with spark its not . . is it the main diffeernce .  
But I am not able to believe if its correct guess . . because i am seeing lot of big shot presentation on informatica +databricks [https://www.databricks.com/partners/informatica](https://www.databricks.com/partners/informatica)and lot of investment in informatica to support in modern cloud .So what is the use case of Informatica/talend +Databricks . . vs Pyspark +Databricks

***PLEASE NOTE***  : I am not referring to the old legacy ETL only tool functionality of informatica but comparing the latest clould one with DataBricks .  


Please note that i agree that Old ETL only informatica /talend had significant scalability issue , IMy qn is not wrt the old etl tool when data lake was never a concept .  


My qn is wrt latest  informatica /talend clould inegration with DataBricks which they  marketing themseves as highly scalable for big data . So is the quesion in that context   
 **Informatica(latest cloud) with DataBricks vs Pyspark with DataBricks**    
",2023-12-30 18:24:54
153llcj,Job application for a lead role “Please expand more on your experience with SQL”,Ummm weird question. At this point I dream in SQL sometimes and think I might think in it as much as I think in English. I think most lead data engineers know it super well.,2023-07-19 05:20:04
14pekpd,How much python do i need to know to be a DE? Currently on day 28 of 100 days of Code. Should I finish it ? or focus on DE specific skills ?,"Good day DE folks, I hope your are good today.

BACKGROUND:

A while back, I decided to learn python via the 100 days of code by Angela Yu. Currently , I am in day 28 of said course learning about GUI via tkinter.

QUESTION:  

How much python do i need to be DE? Should I finish the 100 day course or should I focus more in DE related skillset rather than more general python knowledge.

&#x200B;

Thank you.",2023-07-03 09:52:01
126qcwk,Twitch Steam - Learning + applying dbt for the first time,"My friend Matthew Brandt ([Matty\_TwoShoes](https://www.twitch.tv/matty_twoshoes) on twitch) is doing a Twitch steam next Thursday where he'll try dbt for the first time! (note: that's ""dbt"".....not ""dmt"".) Matthew brandt is not trying dmt over a live stream. That'd be incredibly inappropriate.

If you're interested in seeing what it's like to learn and implement dbt, check it out with me!

Matthew  will uncover the all the easiest + hardest aspects of learning dbt, and you'll get to enjoy watching all his successes and failures from the comfort of your home 🤣

If you'd like to add the event to your calendar, sign up below! Or you can simply join the event on 4/6 @ 12pm PT!  


https://www.operationalanalytics.club/events/unboxing-dbt-a-twitch-livestream-w-matthew-brandt-1",2023-03-30 15:30:00
15rjps5,My company just got bought. Is DE usually safe in transitioning phases?,So like the title says my company just got a new corporate daddy and I was wondering if any of you have gone through this and what I can expect as a DE?,2023-08-15 05:48:52
1adzfya,Why use Kafka when you can use a database with multiple tables?,"I was testing out a nosql solution using cosmosDB on azure. I know this is not conventional. But is it wrong to go with this approach, where the database can be considered as a broker and the tables(or collection) considered as topics? 

The producers add the data to the required  topics(table) and consumers can consume directly from the table. Assuming that there is auto scaling and that these can be upscaled based on the number of incoming messages, is this a viable alternative? And would this be cost effective in comparison to Kafka(Or event hubs)?

Assume that my usecase is adding messages and consuming the data for different purposes. (Processing, dumping to data lake, visualizing data)

Alternatively, is this how Kafka is designed in the backend or is there any other system?

Any answers would be greatly appreciated! Thank you!",2024-01-29 16:32:35
19dp67h,Am I romanticizing DE?,"Summary:

* B.S. in Microbiology & Chemistry 
* 10 years of work experience - clinical research, teaching, & sales
* No coding experience 
* Strong soft skills
* Currently working fully remote

Debating between nursing school vs a DE bootcamp/master program. I want to retain the flexibility of remote work while gaining hard skills to create stability in my career. My lack of experience in coding makes me nervous, however, I am willing to do what it takes to break into the field. Just need some reassurance...whether it's positive or negative.  

Am I turning this career path into a ""feel good"" dream or is it feasible? 

Much love. ",2024-01-23 14:05:53
19cq3qv,Are data engineers shyer than data scientists ?,"I have joined thesee two channel, r/dataengineering and r/datascience , and there is no doubt that the DS channel has much more activities much than the DE side.

And I also posted an easy-to-answer topic, 500+  views but no answers. [https://www.reddit.com/r/dataengineering/comments/19cpb97/the\_best\_way\_to\_reduce\_aws\_emr\_costs/?utm\_source=share&utm\_medium=web2x&context=3](https://www.reddit.com/r/dataengineering/comments/19cpb97/the_best_way_to_reduce_aws_emr_costs/?utm_source=share&utm_medium=web2x&context=3)

So,  are data engineers shyer than data scientists ? ",2024-01-22 07:22:11
15qw6ec,"For those of you who used to be data analysts, what difficulties where there when moving to data engineering",What difficulties did you have in any aspect whether it was trying to get hired or transitioning into the role? And how did you overcome it and what would you recommend to others trying to make that same move?,2023-08-14 14:24:37
1agtr1x,My First Day,"I work for a large enterprise and got hired to re engineer our HR data. 

We get our data from Kafka and from that the current development team made a database and tried to create a clean layer. I think they used mostly SAS. 

From that product they made went on to create some sub data bases for various analytic customers with some custom sql transformations.

Because the enterprise has evolved since that’s happened - they created another database as an “improvement” to the first one but didn’t migrate any of the customer data sets.

I feel like I want to convince the C suite to use Python to do a mass consolidation and create a semantic layer and connect everything with APIs.

Coming to this reddit I’m seeing a lot of “Python bad”. 

What am I missing?",2024-02-02 03:11:12
18ny8tf,Databricks: dbt or Delta Live Tables?,"has anyone worked on both dbt and Delta Live Tables with Databricks? What's the pros/cons?

Recently appointed data engineer, previously I was a data analyst for 5 years so I am quite new in building pipeline

We had consultants reach out and advises us that we should implement dbt in our Databricks environment so we reached out to Databricks for second opinion; Databricks demo'd us Delta Live Tables and it seems easier to implement compared to dbt but I am unsure whether the consultants just try to complicate the dbt framework so we always comes back to them in future (And get pinged with billable hours)",2023-12-21 21:46:27
18lyljw,OLTP vs OLAP databases,Why storage and compute can't be separated for an OLTP database?,2023-12-19 10:13:41
17zk7g7,Do you feel the need of spreadsheet like tools to view large datasets?,"Stumbled across two tools [https://rowzero.io/](https://rowzero.io/) and [https://www.gigasheet.com/](https://www.gigasheet.com/), that are spreadsheet like tools that support significantly more rows compared to excel \~1m rows. What do you think about these? ",2023-11-20 08:51:46
17piy88,Why are DA jobs becoming more and more popular?,"Nowadays, there are a lot of DA achievements in universities, which are very common, is there a bit of oversupply?",2023-11-07 01:29:06
17f9arc,Get 30x speedups when reading databases with ConnectorX + Arrow + dlt,"Hey folks

  
we at dlthub added a very cool feature for copying production databases. By using ConnectorX and arrow, the sql -> analytics copying can go up to 30x faster over the classic sqlite connector.

Read about the benchmark comparison and the underlying technology here: [https://dlthub.com/docs/blog/dlt-arrow-loading](https://dlthub.com/docs/blog/dlt-arrow-loading) 

One disclaimer is that since this method does not do row by row processing, we cannot microbatch the data through small buffers - so pay attention to the memory size on your extraction machine.  
Code example how to use: [https://dlthub.com/docs/examples/connector\_x\_arrow/](https://dlthub.com/docs/examples/connector_x_arrow/)

By adding this support, we also enable these sources:  
[https://dlthub.com/docs/dlt-ecosystem/verified-sources/arrow-pandas](https://dlthub.com/docs/dlt-ecosystem/verified-sources/arrow-pandas)  


If you need help, don't miss the gpt helper link at the bottom of our docs or the slack link at the top.

Feedback is very welcome!

&#x200B;",2023-10-24 10:10:14
177sfe2,Software Engineer to Data Engineer,"I have been a Software Engineer for more than 10 years with some experience with small scale Data Engineering projects. I got an opportunity to move to a full-time Data Engineer role for a good company which is investing heavily in a Data platform. I think it would be a great chance to learn and grow as an Engineer.  What are your thought?

[View Poll](https://www.reddit.com/poll/177sfe2)",2023-10-14 15:46:08
166nao6,dbt. What is it good for? A 42-second survey.,"I was wondering how other teams use dbt, so I made this very short survey to understand better. I'll share the results later in the comments.  


[https://forms.gle/zwwfDAVYGu37nz2W6](https://forms.gle/zwwfDAVYGu37nz2W6)",2023-08-31 21:14:35
15y6nnc,Excel will always be my favorite database,"Saw this article about Python in Excel, and it reminded me of my good ol’ days in Excel and Access.

Can’t wait to see what business users whip up next with this!!

https://techcommunity.microsoft.com/t5/excel-blog/announcing-python-in-excel-combining-the-power-of-python-and-the/ba-p/3893439",2023-08-22 13:58:10
15d8ie2,What am I doing? What is what I'm doing called?,"Hi,

I'm trying to figure out if what I'm doing is one job or several and if not, what the typical title is for the kind of work I'm doing.

I work at a company of 700 as a ""Business Intelligence Analyst"" alongside a Data Engineer. We both report to the Head of Engineering. I was part of an acquisition of a company of 200 where I designed the entire architecture, created warehouse, ETL'd apps to it, created reporting for end users etc.

What I typically DO is three-fold:

- I grab raw data from a warehouse (snowflake) (Having advised on its ETL requirements), manipulate it into a model in Power BI, replicate the tables required to run reports off'f that model in SQL, and then get the data engineer to create that table in our warehouse. I do this for 3 different product lines (acquisitions) being folded into one.

- In parallel I work with C-Suite and/or operations leads from research/sales/support/finance/Hr/half a dozen others. I gather their requirements for commissioned reports or upgrades to existing ones, create the reports in pbi, oversee the power bi workspaces and environments, advise on where to find data and what events are going to be needed.

- Advise on the transfer of and maintain access to the legacy warehouse.

Output's 3-fold:

- I consult on what the data Execs are looking at means

- I create reports/dashboards for departments to use

- I create semantic/processed data marts/sets and models for people to use

The above all feels like its a few different jobs strung together being done by one man. 

My question is this: 

What'd be the different jobs I'm doing were they all done by a team? Alternatively, What would the title for the one man be? Preliminary research seems to suggest it's ""Data Analytics Engineer"" but I'm looking to second opinion that opinion.

I ask because I'm considering a move to someplace less stressful/more profitable where the title's ""Architect"" and want to know what kind of roles I've held here for reference.

Thank you all in advance!",2023-07-30 01:19:19
14lcciw,Exploring Graphs in Rust. Yikes.,N/A,2023-06-28 15:21:06
14la4y2,👉 New Awesome Polars release! What's new in the world of Polars in June 2023 ? Let's find out! 🚀,N/A,2023-06-28 13:53:09
13y5nvm,Databricks Upsert on Delta Live Tables,"Is it possible to update records on Delta live tables with the incoming stream? 

Business case: We are getting data from our client every 15 minutes. Usually, some records needs to be updated while others should be simply appended. 

I have read some articles from the official Databricks documentation, but for me those are more tailored to delta tables, not delta live tables.",2023-06-02 07:09:33
13e3wcw,Semantic Understanding of SQL,N/A,2023-05-10 21:29:44
136wwbg,Data Modeling — The Unsung Hero of Data Engineering: Modeling Approaches and Techniques (Part 2),N/A,2023-05-03 19:27:43
12zh3pv,How to convince analysts/de to use DBT?,"I work at a company that has used sql server as the majority. We have made the move to cloud in the recent years and also have onboarded a data library with Starburst which is backed by Trino. I recently learned about DBT and thought it was an amazing tool to allow for documentation and more clarity on where our data is coming from with the built in DAG. However, every time i introduced it to an analyst or engineer they are all clutching onto their tsql and stored procedures. I know change is difficult but has anyone else made this transition at their company and how did you win them over?",2023-04-26 13:10:41
12krcy6,"Say Goodbye to ETL Headaches: Introducing DatAtlas, the No-Code Platform for Real-Time, Secure, and Scalable Data Collection","Hey [r/dataengineering](https://www.reddit.com/r/dataengineering/)! Meryem here from [DatAtlas](https://datatlas.co/),

As a former data scientist and data engineer, I've experienced firsthand the challenges of data collection and preparation. The time-consuming process of moving data from various sources, the lack of scalability and security, the limited real-time syncs, and the downtime issues have been a constant struggle. I've tried tools such as Fivetran, Airbyte, or Stitch in my past jobs, but unfortunately, they have not solved these problems yet.

These tools lack the ability to provide agents-based data delivery for secure, real-time syncs. They also have a lot of limited capabilities when it comes to transforming data and handling large data volumes. Additionally, the lack of on-premise offerings means that sensitive data needs to be shared with third-party providers, which can be a significant concern for many companies.

That's why I founded DatAtlas, the no-code platform that allows you to collect data from databases, APIs, files, and software in real-time, securely, and at scale. If you just want to see how it works, here is [a demo of the product we recorded.](https://share.vidyard.com/watch/naMAmEjmxgXPWFRkW54F8i?)

With our log-based change data capture (CDC) approach and on-premise agents, you can move your data efficiently, securely, and without putting any impact on your data sources. We provide unlimited scalability, and best of all, our platform can support up to 10GB/s of data volume delivery, scaling both horizontally and vertically.

So if you're ready to streamline your data collection process and start making your data projects faster, head over to our website and sign up for an [introduction call with me](https://www.datatlas.co/request-demo). We can't wait to hear what you think!",2023-04-13 14:37:29
1261zys,Next Quarterly Salary Discussion,"For the next salary discussion can we enforce a schema for all the comments ?  
I'm trying to create a script that extract all the tech stacks and count them to get some insights and I'm spending like most my time just parsing the non-technical words hahaha.  


I'll share the script when I have a beta version if someone wants to contribute !",2023-03-29 21:55:51
14n9sfi,It do be like that,N/A,2023-06-30 19:32:21
1499w55,Dagster DBT and OpenMetadata,Check out an E2E example of the beneof the new dagster DBT Integration in my blog post https://georgheiler.com/2023/06/13/unlocking-advanced-metadata-extraction-with-the-new-dbt-api-in-dagster/,2023-06-14 14:51:15
1akfodk,Data engineering as a beginner?,"Hello,

I don’t have any experience, knowledge or whatever when it comes to data, it was even recently that I discovered the whole field. From my limited things that I’ve read I think I’m more inclined for DE rather than DA as I would like to build stuff. 

( no software engineer or development experience either ) 

So my question is: how much harder is it to skip DA and go straight to DE?

Question 2: I’ve stopped at datacamp.com - is this a good platform to learn?

Thanks in advance for anyone who comments and reads. 

Just as a side note I’m going to be let go from work so I’ll have quite a bit of free time to devote (outside my resume senders and parental duties) so I’ll be able to spare 5-7/8 hours a day at a minimum. I know it’s a rather stupid question but is there like an average timeframe of time needed to be somewhat job ready? (I live in Eastern Europe so the competition I think is not as fierce as like in the US for instance)",2024-02-06 17:46:49
18za481,"Integrating Airbyte, Prefect & dbt 👇","Dive into a hands-on data engineering project this weekend! This [GitHub repo](https://github.com/airbytehq/quickstarts/tree/main/airbyte_dbt_prefect_bigquery) showcases the combined powers of Airbyte, Prefect, and dbt in a practical setting.

In under an hour, you can establish a complete data stack ready to manage e-commerce sample data. The project uses BigQuery, but you can adapt it to any other data platform with slight modifications. Just follow the instructions in the README.

This project is designed to be straightforward yet adaptable, perfect for professionals with limited time and those eager to learn.

I look forward to your thoughts and suggestions for improvement!

Disclaimer: I’m part of the Airbyte team, and it’s my personal interest to help fellow engineers experiment and learn

&#x200B;

[Prefect DAG](https://preview.redd.it/58whu832dnac1.png?width=2388&format=png&auto=webp&s=6c8a1dae20c2f3638c6818e9dbdad3e14735f4b6)",2024-01-05 15:56:29
12i3iqy,What are some tips to get a data engineering job with a gap of a few months.,"Hello,

I am a young graduate in data science and I am currently looking for a data engineering job. I had a health problem after graduation so I could not apply to jobs immediately. Thus I have a gap of a few months and I am currently having trouble finding a job. I suspect employers are reluctant to pick me because of this gap. I would like to have a more attractive profile as a data engineer. What are some tips to have better chances of landing a job in Data Engineering and which story I could tell about the gap to avoid mentioning health problems?

Thank you,

Best regards.",2023-04-11 01:19:49
19cgwl4,so Where should i go after reading 'fundamentals of data engineering ' book,yes any Book or course recommendation would help me being better data enginer or any advice 🙏🏼,2024-01-21 23:17:12
18hsdkk,How are pay raises looking this year?,"I started at my current company one year ago. Raises were in June, and no one on the data team received one. There's the possibility of merit increases at the end of this year, but no word on it yet and some of us are skeptical we'll get anything at all. Just wondering how the things are looking out there.",2023-12-13 21:58:16
1846tp3,LinkedIn vs Twitter vs Reddit for Data Engineering content,"I'm starting my journey to share how to learn and build data/cloud engineering solutions with practical guides and open source code. I was wondering which platform you guys think is better for broader reach? I've heard that the LinkedIn algorithm is no longer boosting posts, and many are shifting to Twitter. Any strong opinions on these two platforms? What other platforms should I consider?",2023-11-26 09:12:15
175zng5,Why you don't like about data engineering consultants in your company?,There seems to be lot of dislike in software engineering in general for consultants specially they talk about big things and complicated processes with very less actual work etc. What is your experience in general working with consultants in data engineering world?,2023-10-12 05:53:31
13zsuqi,AI will make data engineering very valuable," ""Hello everyone, I'm considering starting a career in the field of data and I'm trying to choose between Data Science (DS) or Data Engineering (DE). However, I believe that the future will be heavily focused on AI (as we can see with the influence of ChatGPT), and companies will need to ensure high-quality data to feed these AI systems.

This trend is already happening, as many companies have hired numerous DS professionals in recent years, only to realize that they don't have good quality data to work with. As a result, they have started to hire more DE experts to bridge that gap. Now, things are improving even further, because despite the results that some DS teams can offer to a company, they recognize the value of having skilled DE professionals on their teams. So, what do you think will happen when companies implement AI, which can provide even greater value compared to some DS teams? It is highly likely that there will be a substantial increase in demand for talented DE professionals.

These are my thoughts on the matter, although I am not currently working in the field. I have been following some excellent YouTube channels that discuss these topics, and I'm highly interested in pursuing a career in this field.   
In fact, this very thought is what is making me lean towards pursuing a career in Data Engineering rather than Data Science. Given the increasing importance of ensuring high-quality data for AI applications, it seems like a wise choice. With that in mind, I would greatly appreciate any advice or insights you might have on this matter.""

What are your thoughts on this?""",2023-06-03 23:01:43
1akz3j6,Should I leave my data engineering role for data science,"Currently on 38k in London straight out of uni. I’m coming onto have one year of experience and have found other jobs in data science for about 45-50k. The roles are Python heavy which is ok for me because I did data science as a masters. My current role is purely sql and azure.

Can anyone with experience let me know if the job prospects for data science are greater than data engineering?

Both would be in the same industry (insurnace)",2024-02-07 09:26:00
1aim16y,Open Source Data Engineering Landscape 2024,"Hi All

I've done a comprehensive research and published a blog post on **data engineering landscape** within Open Source space, presenting most prominent tools and services for each data platform layer.

&#x200B;

https://preview.redd.it/owywbbix00hc1.png?width=4381&format=png&auto=webp&s=fa7fa48b424d88b7c6c9fa2ad54131a24fb5b7aa

Check out the full blog post:[https://alirezasadeghi1.medium.com/open-source-data-engineering-landscape-2024-8a56d23b7fdb](https://alirezasadeghi1.medium.com/open-source-data-engineering-landscape-2024-8a56d23b7fdb)",2024-02-04 11:38:21
1bt7iv2,Monthly General Discussion - Apr 2024,"This thread is a place where you can share things that might not warrant their own thread. It is automatically posted each month and you can find previous threads in the collection.

Examples:

* What are you working on this month?
* What was something you accomplished?
* What was something you learned recently?
* What is something frustrating you currently?

As always, sub rules apply. Please be respectful and stay curious.

**Community Links:**

* [Monthly newsletter](https://dataengineeringcommunity.substack.com/)
* [Data Engineering Events](https://dataengineering.wiki/Community/Events)
* [Data Engineering Meetups](https://dataengineering.wiki/Community/Meetups)
* [Get involved in the community](https://dataengineering.wiki/Community/Get+Involved)",2024-04-01 16:00:34
1bt9j2j,Freelance data engineers - what's your story?,"What is it like going from full time employment to freelance?

How long have you been doing it? How many clients have you had? How much money have you made?

Any important things to keep in mind before making that move?

Do you regret it?",2024-04-01 17:15:55
1bta1pw,Imposter Syndrome,"Hey everyone, I’m coming here for a little advice. 

I got a BAA, I did a bunch of stuff wound up in banking and fell in love with data , then started programming.

I recently finished the Rutgers Data Science Boot Camp a few months ago, but I’ve been busy just started putting in applications. 

I got a initial call back for a data engineer position (mainly ETL). I’ve been very honest about what I can, and can’t yet do. 

It seems like they might actually hire me on and I’m feeling a little bit of imposter syndrome and afraid I may be talking myself into a job I’m not qualified for. 

 Thanks for taking the time to read. 

Edit: speech to text sucks. ",2024-04-01 17:35:31
1bt3ign,Are stored procedures just steaming piles of garbage for testing,"

For reference, I’ve been involved in two data migrations, one using dbt for transforming pipelines through snowflake and one using native stored procedures/tasks in Snowflake. 

I’m kinda just really bent out of sorts on the concept of using procedures that call other procedures, declare result sets and use a series of temp table outputs to update them.

I find it’s incredibly difficult to debug errors as the code that has been built has to be tested as a singular function of actions as opposed to individual segments in the form of CTE’s.

If I have unexpected behavior in joins or need to debug, with dbt I can just work from model to model testing and modifying combinations of CTE’s until I identify where the issue is. With stored procedures I pretty much have to rebuild the procedure from scratch to add temp tables or build individual operations to test.

It’s quite maddening and I would honestly like other perspectives to help me understand how people debug stored procedures and generally take advantage of them.",2024-04-01 13:18:46
1btdb5z,The first-ever children’s book on data quality,"""*Mastering Data Quality and Your ABC's* chronicles a brother and sister’s journey to fix a broken data pipeline by using data observability to monitor, alert, triage, and help resolve the issue before their parents find out. Once they’ve identified the root cause, fun ensues and the siblings unlock the potential of reliable data to power their new LLM.""

(This was an April Fool's joke that actually got me): [https://www.montecarlodata.com/blog-monte-carlo-releases-mastering-data-quality-and-your-abcs-worlds-first-ever-childrens-book-on-data-quality/](https://www.montecarlodata.com/blog-monte-carlo-releases-mastering-data-quality-and-your-abcs-worlds-first-ever-childrens-book-on-data-quality/)",2024-04-01 19:37:38
1bte8of,"Simple Beginner Tutorial for Creating Data Flow Diagram with Draw.io, and Making an Animated GIFs","[https://www.youtube.com/watch?v=fBYY08Zp9LQ](https://www.youtube.com/watch?v=fBYY08Zp9LQ)

https://i.redd.it/z83pi4n3fxrc1.gif",2024-04-01 20:11:52
1btcix5,What is a real-world project you recently worked on? What technologies/platforms did you use? What kind of data? How much data? etc.,"I'm primarily a SQL Developer, have been for about 12 years. I want to start branching out and getting more into data engineering, cloud technologies, etc. I'm currently learning how to set a lot of this stuff up, but I just don't see the big picture and how modern approaches are better than older ones, etc.

My past experience has been to basically build everything from scratch in C# or PowerShell. My previous company didn't use any modern tech. If we needed to grab a file from some data provider's SFTP site once a day, for the most part, we built it from scratch in C# or maybe PowerShell and it would always go into SQL Server and then we'd have SQL procs that transform and normalize that data.

I can sit and blow through online tutorials, Pluralsight courses, YouTube courses, etc all day long, but I never see any of them giving real-world use cases. I'm tired of seeing the same damn pie shops data pipeline lol (that's the example Pluralsight courses always use).

I also don't know what this type of work looks like at larger companies. I've always worked at small companies, so our idea of a data pipeline was very small...grab a file, load it into SQL or hit a REST API once every couple hours or maybe scrape some website and load it into SQL Server. But I have absolutely no idea what ""data engineering"" looks like when you're talking about a company like Netflix, LinkedIn, Amazon, etc.

So I'm just curious...if you work for a relatively large company...

* What are some REAL projects you have recently worked on?
* What platforms or technologies did you use? Azure? AWS? Databricks? Splunk? PowerBI?
* How much data does it handle? Millions of records per minute? Or one small file once a day?

I want to get an idea of what my life would look like if I were to actually go this route with my career. I wish I could just shadow a Sr. Data Engineer at some big FAANG company. I've been lucky enough to have the luxury of learning most of everything I know on the job. But I think I've reached a point in my career where that's not as much of an option anymore.",2024-04-01 19:08:31
1bt3jp7,"Opinions on ""Data Engineering, Big Data, and Machine Learning on GCP Specialization"" MOOC in Coursera","What do you think about this course?

[https://www.coursera.org/specializations/gcp-data-machine-learning](https://www.coursera.org/specializations/gcp-data-machine-learning)

I'm trying to learn new skills to follow the DE path (I'm a Data Analyst now).",2024-04-01 13:20:13
1btawvv,"Kimball Data Warehouse Design - Default row, Numeric data types, value","Hey all - i have a pretty standard star schema SQL server data model. 

I am working on adding the default record for when my fact doesn't have a relationship. e.g. Fact table is a loan, and some loans have a piece of collateral tied to it and some don't.  So the Fact could have a -1 in the DimCollateralID column. 

Our Collateral dimension has a -1 DimCollateralID row, so we don't get NULLs when joining the 2 tables together.  

My question is, should a numeric attribute on that default row (not the key) have a -1 value, or a 0 value? or either or? If either or depending on use case, wondering what the typical way you would lean be, 0 or -1? Any documentation would be great. 

I am using this design tip but the bottom section mentions a ""valid"" dimension row but this default row isn't quite a valid row i don't think? if the default row is valid then it sounds like numeric values are a grey area and I could do what i want for my use case. 

[https://www.kimballgroup.com/2010/10/design-tip-128-selecting-default-values-for-nulls/](https://www.kimballgroup.com/2010/10/design-tip-128-selecting-default-values-for-nulls/)

&#x200B;

&#x200B;

&#x200B;",2024-04-01 18:07:28
1btcv8o,Deep Dive into Parquet Metadata & Logical Types,"Hey Data Folks! 

I just published a blog post diving deep into Parquet metadata types, and I thought it might interest the community here. In my blog post, I explore the different metadata fields used in Parquet files, such as file metadata and logical types

Check out my blog post [here](https://rr43.net/posts/2024/3/parquet-logical-types/) and let me know your thoughts! I'm also open to any questions or discussions on the topic. Happy data engineering!",2024-04-01 19:21:17
1btbj6l,"TUTORIAL: End-to-End Basic Data Engineering Tutorial (Spark, Dremio, Superset)",N/A,2024-04-01 18:30:50
1bsxtbt,How to improve query execution time,"Hi guys,

Recently, I received a mission from my leader. He wants me to improve query execution time below 30s. In reality, these queries take at least 2 minutes maybe 15 minutes. Especially, when these things run which impact the whole system making other queries slow down too. 

I try some solutions like using CTE instead of subqueries and limiting outputs, ... but performance doesn't improve. 

Actually, I'm just fresher level. ",2024-04-01 07:57:58
1btg5ks,Any opportunity to save cost and stand out?,"I’m an aspiring DE with little to no skills. I’m currently a DA of sorts for a big school district.

Our school currently has an antiquated student information system (our data warehouse) and so we are going to to be switching to something else im a couple years- which I’m told will be tens of millions of dollars.

I’m reading fundamentals of data engineering, studying python and sql, and learning to use power bi to bring in and visualize data. I’m wondering if there is some project that would help my district’s future migration and help my learning.

Totally naive to the needs and possibilities of the situation, but I thought I’d just throw this out there. I hear everyone talking about the importance of projects for both learning and for getting the job you’re looking for- here’s my attempt. Though if it doesn’t make sense, I’ll keep thinking!

Would love to hear any ideas-

If that project is untenable, any other way I could get some real experience in the education sector? That’s where I have domain knowledge",2024-04-01 21:21:34
1bt5z8p,How to choose the right data platform?," We are medium size construction company and doing a lot of software implementation. Now we have data silos and over processes and reporting suck big time.

I have been asked to integrate all the produces for process automationg as much as we can.

And I have been asked to find a solution to reporting. My recommendation is to build a central data warehouse and create reports over that.

What do I need? I see a lot of solutions like redshift, snowflake, azure. But what do I really need?",2024-04-01 15:00:49
1btffs2,Where do you draw inspiration from? I'm looking for ideas to shape the next 3-5 years of my career.,"Looking for general advice here. I'm completing just over a year as a Data Engineer. I work in a bank, and as most banks go, the tech stack is ancient. Things move slow, and change takes time. We don't move with the market. 

That's not to say that I'm complaining, I like the work. But I'm also looking to up-skill and stay relevant. 

My question, basically, is how do I KNOW what direction I should move in? And how do I get there? For instance, I know one of the areas I've been interested in is investment firms/hedge fund firms, and i'd love to work as a D.E in one. But how do I know what kind of skill-sets they are looking for? If I had, say, some resumes to look at, I'd be able to get a better idea.",2024-04-01 20:55:33
1bt91ju,"(beginner) Troubles understanding the scheme of DLT, MV, Live Tables, Streaming Tables","Hi everybody, im a jr. DE and learning about the databricks platform.

I understand the Delta Lake, Delta tables, Spark, Structured Streaming topics but i cannot wrap my head around everything regarding DLT.  I understand what its used for, how (somewhat) and why, but i cannot pin point where one technology starts and another ends. 

I have a couple of questions, and it would be great if you could help me out:

1. DLT is proprietary Databricks ""solution"", which is basically Spark Structured Streaming with a lot of background optimisation and features, right? And if so, can i achieve the same pipeline with spark instead of DLT (without the features)?
2. Whats up with Materialized views? I understand what they are and what they are used for, but ive red multiple times that they are basically live tables. 
3. The live Keyword builds upon question 2.. if the ""live"" keyword is used to build MV, how is that i can use live as a keyword for streaming tables, like ""live streaming tables""? Reading the documentation made me think that there are only 2 DLT ""Datesets"": Streaming and Live Tables. It feels like they differentiate between the 2 like they couldnt be more different. Meanwhile every tutorial i see uses ""live streaming tables"" and i dont find any documentation on this middle ground. I do not understand what its used for and when i should use it. 

There are more questions but they might clear up if these 3 clear up. 

Any help is appreciated. I feel like i cannot move forward because of these issues. ",2024-04-01 16:57:46
1btdpk0,Data Engineering Internship,"Hi, I am a sophomore studying Computer Science + Data Science and for my first internship, I ended up accepting a data engineering role at a large F25 insurance company. From what I gathered in the interviews, python and sql are the two main languages used, but I was wondering how I can get prepared and be as ready as possible for my internship. The job description didn't really have any information on any other tools used, but from what I gathered from the manager's linkedin, there is a lot of AWS work. Thanks!",2024-04-01 19:52:33
1btbwtz,Advice needed.,"I am a data engineering student currently in my end-of-study internship. However, upon starting the internship, I discovered that I would be working on a project more focused on DevOps tasks such as building CI/CD pipelines and utilizing Terraform for Infrastructure as Code (IaC). Can you provide any career advice? I am keen on continuing in data engineering and have recently begun preparing for the Google Cloud Platform (GCP) Data Engineer Professional certification.",2024-04-01 18:45:20
1btb401,Exploring versions of the Postgres logical replication protocol,"[https://blog.peerdb.io/exploring-versions-of-the-postgres-logical-replication-protocol](https://blog.peerdb.io/exploring-versions-of-the-postgres-logical-replication-protocol)  


🚀 Did you know that the way Postgres logical replication protocol has evolved over the past few years? Did you know that Postgres logical replication has ""versions"" which make it more efficient and feature-rich?  


This blog will dive into this evolution, its impact on performance, and present some useful benchmarks. This blog is useful for anyone who uses Postgres Logical Replication in practice!  


🔍 Version 1 set the stage by allowing the streaming of committed transactions, laying the groundwork for what was to come.  


🌊 Version 2 introduced a game-changer: streaming of in-progress transactions. This dramatically improved decoding speeds and reduced peak slot size duration, addressing critical performance bottlenecks.  


📊 The blog provides a detailed benchmark of Version 2's impact compared to Version 1. TL;DR - faster decoding speed and lesser peak slot size duration.  


🔄 Versions 3 and 4 brought in support for two-phase commits and parallel apply of in-flight transactions, further enhancing the flexibility and efficiency of logical replication.  


For a detailed analysis on all the above topics on Postgres Logical Replication, checkout this blog.  
",2024-04-01 18:14:52
1btarr9,Free / Cheap Azure Pipeline & Warehouse stack,"I would like to scrape a list of website and perform around 100 API calls every 3 month, transforming the data in the process. This should net me 500 rows and 10 columns of data which I would like to store and visualize using Power BI. What's a cheap / free way to do this on Azure?",2024-04-01 18:02:11
1bt28yf,Help needed - Data product builder,"I am trying to understand whether the following product would be useful to engineering teams or not. I was told that it helps automate some data engineering tasks and is good for small teams.  
They provide templates to deploy data components and architecture. Example file below.  
What is the language of this file? Is this useful for data engineering teams?  


[https://github.com/agile-lab-dev/witboost-snowflake-output-port-template/blob/master/edit-skeleton/catalog-info.yaml](https://github.com/agile-lab-dev/witboost-snowflake-output-port-template/blob/master/edit-skeleton/catalog-info.yaml)

 ",2024-04-01 12:19:48
1bt7heq,DENODO PLATFORM 8.0 CERTIFIED DEVELOPER ASSOCIATE,"Planning to take DENODO PLATFORM 8.0 DEVELOPER ASSOCIATE exam.  
Any tips for preparing, resources ?  
How long does it take to prepare ?  
Does the results get out directly or after a few hours ?",2024-04-01 15:59:03
1bt0fon,Any valid alternative to seeq?,"Hi everyone.

So, in my company we use Seeq to analyze time series data coming from thousands IoT sensors. 

Seeq is pretty good, but also expensive. We are considering to still use it but to give it only to power users that use it for the advanced analytics features.

What could we use as a replacement for users that simply want to visualize the data or do basic analysis on it?

We have already considered and discarded classic bi/dataviz tools like tableau/powerbi, and also grafana.

The ideal tool would be something like seeq, but more basic, either paid or self hosted. Does something like this exist at all?

Thank you 🙏",2024-04-01 10:44:12
1bt0ng0,Advise on how to model the metadata database.,"Hi folks, I would like your advise on how to approach the below scenario.

I have a metadata database(Opensearch) where I store the metadata of the files stored in S3. I use the opensearch to provide search functionality on the metadata.

Let say the metadata is json document.

I have a field, sample\_field:\[\]

this field takes in an array of string.

The thing is the array of strings consist of pre defined value. and these predefined values are subjected to change.

Ex: Sample field = \['hodor','aegon','jamie'\]. If someone decides to changed 'Jamie' to 'Cersei', then this should be reflected in all the documents.

I know I can use Query DSL to find and replace this value. But is there a way to achieve this without doing so. I can think of having a pointer(UUID) of each predefined value and this uuids are stored in array instead of actual value. But this will be problematic when it comes to wildcard search. If we are to search using the predefined values then i can convert the predefined value to uuid before searching

Opensearch database. But the thing is I need to also offer search endpoint to users so that they can write their own custom queries.

How can I approach this problem. Any help would be deeply appreciated. You can consider opensearch same as no-sql having json doc or even sql database. ",2024-04-01 10:56:37
1bsvgpn,Career Development Advice for an aspiring Data Engineer,"Hey everyone,

I'm a University sophomore studying Data Engineering and I'm currently in the process of looking for a summer internship within the field. I have experience with Python, SQL, and basic data pipelines using tools like Apache Airflow and GCP. I recently earned my GCP Professional Data Engineer certification, but I haven't had any luck landing interviews yet.

To boost my portfolio, I've decided to focus on creating more data projects. So far, I've built a few simple end-to-end pipelines with basic dashboards using tools like Dash and Plotly. I'm looking for advice on how to create more impactful projects that will help me stand out to potential employers.

If you have any project ideas, tutorials, or resources that you'd recommend, I'd greatly appreciate it. Also, if you know of any companies that are open to hiring interns with my level of experience or have any leads on internship opportunities, please let me know.

Thanks in advance for your help!",2024-04-01 05:21:15
1bsthkk,How to view data fast in Glue Notebook?,"Hi, our team is migrating to AWS. When running glue notebook, it takes quite a while (10 to 15 mins) to view a spark dataframe. As I need to check dataframes to assure values are correct and also do data analysis, is there a quick way to view dataframe while doing data analysis step by step? Thanks.",2024-04-01 03:30:10
1b45r0c,How aggressive is Twitter against web scrapers?,"Just as the title says, how much web scraping would someone be able to do on twitter.com before it flags it as some bot/automated process and either ip bans or blocks the connection in the way they do?",2024-03-01 21:14:19
1b4dnze,How to automate data extraction from external VM to my computer,"My client in healthcare uses a hardware device that generates data and saves it into the VM of the hardware manufacturer. The generated data is about 10gb per file. Is there a way to automatically move these files to my own computer? The next step would be to monitor a folder in my computer to upload the file to S3 using multipart upload, but first, the files need to be there in the first place. Any advice would be appreciated. 
",2024-03-02 02:54:59
1b3vdae,Thoughts about current ELT/ETL tools,"I am investigating current ELT/ETL tools like airbyte, cloudquery, fivetran etc. and all of them seems to be very complicated to setup and maintain locally due to their architecture with different types of containers and moving parts.

Is this complexity really worth it? As a user do you think a simpler alternative like a single executable be better? What are your thoughts about the usage experience and maintainability?",2024-03-01 14:19:10
1b46sye,Beyond Reporting in a Lakehouse,"I've been wondering if anyone's using their Lakehouse for stuff beyond the usual the reporting and BI use-cases in the gold layer. It feels like everyone refers to the to the gold layer as being just for facts and dimensions, pretty much just standard Kimball warehouse stuff. But what about other uses?

Specifically, I'm looking into serving data for analytical web apps. These are not just simple reports but full-blown applications with a UI, an API, and a back-end running in Docker Containers on K8s. They would rely heavily on data in the Lakehouse, mostly from the silver layer, and wouldn't really generate much data on their own. There may be some exceptions where user input is written, but it would be a very little amount of data.

I can't imagine that they best approach for this would keeping all this data in the Lakehouse, like in Delta Format. Wouldn't it make more sense to move this data into something like a Postgres DB, or even MongoDB, especially for serving it up through an API to the front-end?

Would love to hear if anyone is currently supporting these types of use-cases on their platform.

Thanks!",2024-03-01 21:56:14
1b3xuan,"Efficiently developing a pipeline for large, messy data","Hi,

i'm currently responsible for building a database out of research data, this is my first data engineering task.  
The data in general is a bunch (magnitude 100) of 1GB CSV-Files, with groups of files (\~10) being the result of a different analysis & aggregation of 10 different base corpoa with extremly similar structure. The task is to parse these into a DB (including some more custom aggregations and restructuring) in order to build a visualization on top of it - and i don't know what my tooling should be.

So far i've been working in pandas, but it's no fun. I've been developing on a subset of files, one for each group, and a subslice of each of these files (around 10MB), but scaling it up to the full files breaks, because:  it's very messy data with unexpected missing values, different column names, encodings. One run for one group takes ""forever"" (\~1h) on the large files until i get the result ""breaking in step X"", then i'd debug this, write another edge-case-treatment and re-run (sometimes looping this for a whole day), so i'm making extreme slow process.

Any tips / literature on how to build such a system more efficiently / less painfully? Does it just take that long and i should tune down my expectations?

  
**Edit for additional information:** It's a one-off task, resources are 16GB RAM & 8 Cores, the concrete operations are as well joins between the files on dirty columns (with e.g. entities that exist only in one file, different string schemas for the same entity, ... ) as also inner-file computations and it's expected to drop faulty data but write them in a clean report for others to investigate. ",2024-03-01 16:02:52
1b439j6,[Video] Custom Python ETL connector demo - feedback welcome,"Off-the-shelf data ingestion works great about 80% of the time. The other 20% is where good data engineers make all the difference.

At Y42, we've released Python ETL connectors to cater to the ""other 20%"", next to our existing Airbyte-, Fivetran-, and proprietary ingestion capabilities. The goal of this new feature is to:

* implement custom ingestion logic,
* remove boilerplate code to load data into your data warehouse,
* get standardized metadata, lineage, and documentation out of the box.

Check out the demo video, very curious about your feedback: [https://www.youtube.com/watch?v=L252iaNylbo](https://www.youtube.com/watch?v=L252iaNylbo).  


To those who want to read more about it, check out the announcement post: [https://www.y42.com/blog/announcing-python-ingest](https://www.y42.com/blog/announcing-python-ingest).  


Thanks!",2024-03-01 19:36:53
1b3mntn,Company wants to migrate to cloud,"Hi all, asking for guidance as my company wanted to migrate to cloud. Currently our process is:
1. Collect multiple excel files from sharepoint
2. Transform it thru python code
3. Store it as a parquet in sharepoint
4. Feed the parquets to Power BI and more transformation is done in Power Query.

Now, they want to use Talend as ETL solution then use SQL to connect it to PBI but I’m not sure how well it will handle the job we do thru python. The process is done around once a week and it would be helpful if we can have those process automated. They have looked into procuring Azure but they said that Talend is more budget friendly. Are we doing the right thing? I thought what we would need the most is a data warehouse. ",2024-03-01 05:36:21
1b3zb2c,Monthly General Discussion - Mar 2024,"This thread is a place where you can share things that might not warrant their own thread. It is automatically posted each month and you can find previous threads in the collection.

Examples:

* What are you working on this month?
* What was something you accomplished?
* What was something you learned recently?
* What is something frustrating you currently?

As always, sub rules apply. Please be respectful and stay curious.

**Community Links:**

* [Monthly newsletter](https://dataengineeringcommunity.substack.com/)
* [Data Engineering Events](https://dataengineering.wiki/Community/Events)
* [Data Engineering Meetups](https://dataengineering.wiki/Community/Meetups)
* [Get involved in the community](https://dataengineering.wiki/Community/Get+Involved)",2024-03-01 17:01:00
1b44616,Training advice needed,"Hey all - I'll cut right to the chase: I am working on mentoring a mid-level data dev on skills development, and I could use some advice on where to focus. They mentioned gaining skill in the areas of using python (presumably in the context of notebooks and such) as well as sql. They are also pursuing a snowflake certification.

To give some context on me: I am a very experienced full stack developer and application/cloud architect on large-scale systems. I am conversant in data, and have built some basic ETL/ELTs, Azure Data Pipelines, relational query tuning, and some work with data lakes. That's enough to know what I don't know about data, if that makes sense.

Any tips on where to focus as it relates to the above skills? Any other thoughts welcome as well - TIA!",2024-03-01 20:11:53
1b3x4gx,A Deep Dive into the Concept and World of Apache Iceberg Catalogs,N/A,2024-03-01 15:34:11
1b3p36n,Best practices on notebook-based project structure,"Hey! I got some great help when asking about a similar topic on Reddit before so wanted to see some perspectives on this.   


I am working in a databricks environment where my team feels like it's hard to get a good setup for the projects structure and good code quality in our notebooks. For some reason, even though the notebooks are basically run top-down when running processing jobs with them, it's harder to get good code in notebooks. Does anyone have the same experience?   


Are there any given best practices when coding notebooks for spark jobs or similar processing? And how would we create a structure (like grouping folders with code by level of refinement in medallion for example, so one folder for the code used for creating bronze, one for silver and so on) to make the code make more sense? It's not a very large scale project either.

&#x200B;

Any thoughts appreciated! ",2024-03-01 08:08:46
1b41mly,Scott Hanselman Interviews Sai Srirampur from PeerDB on Postgres Replication,"The podcast touches on so many interesting topics including Postgres, Open Source, Migrations, Replication,  Data Movement, Building Fault Tolerant Enterprise-grade systems, PeerDB and so on. Loved the way Scott navigated through each of these topics and create story. Totally worth a watch!

[https://open.spotify.com/episode/3jZu78eH79aat9UozoHWIQ?si=Ow2mF2h9TB2d6EeH4UmfIQ&nd=1&dlsi=317bc349bf314f1f](https://open.spotify.com/episode/3jZu78eH79aat9UozoHWIQ?si=Ow2mF2h9TB2d6EeH4UmfIQ&nd=1&dlsi=317bc349bf314f1f)",2024-03-01 18:31:07
1b3u70d,Demo data for data modeling,"Hello.  I am looking to find demo data that I can use to practice some data modeling techniques.  Ideally, this data would be very normalized, such as how it would look when sourced from an application system.  I also hope to find data that includes snapshots of dimensional data to support constructed SCD.  Any suggestions on where I can find such a data set?",2024-03-01 13:25:13
1b3xfzr,IOMETE released the most generous free Data Lakehouse platform,"Hello Data Engineers!

We're launching, the IOMETE Community Edition on AWS, and looking for insightful testers like you. This is your golden ticket to experience our scalable data lakehouse platform, designed to transform terabytes to petabytes of data, absolutely free. You'll be amazed by what you can achieve with our platform.

We're excited to see how users experiment with the platform by Leveraging Apache Iceberg and Spark for a managed data lakehouse that grows with your data—from terabytes to petabytes—without any vendor lock-in. Enjoy complete control over your data stored in S3 in parquet format, and pay only for the AWS resources you use. Whether you're using Spot or Reserved Instances, IOMETE ensures an affordable path compared to other vendors. Ready to transform your data management strategy?

IOMETE offers several Apache Spark features including:

1. A user-friendly interface and integrated notebook service for data processing and analysis.
2. Comprehensive monitoring and debugging capabilities for Spark jobs.
3. Automatic scaling of Spark clusters based on demand.
4. Capabilities to process real-time data streams from various sources.
5. A platform for training and deploying machine learning models for tasks like predictive analytics, fraud detection, and customer segmentation.

These features are designed to help you focus on your data analytics workloads by taking care of the infrastructure and management tasks associated with running Spark - [https://2ly.link/1wFi0](https://2ly.link/1wFi0)

Intrigued? A short video awaits you to guide you through the details and the wonders that IOMETE Community Edition promises - [https://2ly.link/1wFi3](https://2ly.link/1wFi3)

If you have any questions regarding installation and usage, join our dedicated Discord community, and let's shape the future of data management together - [https://2ly.link/1wFi1](https://2ly.link/1wFi1)

As of now, the IOMETE Free Community Version can only be deployed on AWS. Please let us know where you would like to deploy the platform so we can prioritize it - [https://2ly.link/1wFi2](https://2ly.link/1wFi2) . We will let you know when your preferred deployment option becomes available",2024-03-01 15:47:07
1b3ytu1,[Video Podcast] No longer a pipe dream — Gen AI and Data pipelines,N/A,2024-03-01 16:42:22
1b3m3lh,What tools do you guys use for schema analysis,"I'm talking about something more in-depth than ERD diagrams. Possibly includes some graph theory analysis like the centrality of a table etc. Something that shows the flow of data linked by stored procedures, connections based on keys, list of triggers/keys/constraints. I've been working with SSMS/Azure Data Factory and I don't see anything on the market that currently fits my needs.   


&#x200B;",2024-03-01 05:05:28
1b3cb1t,How do I load a 70 million row table in a dataframe?,"The data is in a table in redshift. On loading this data we plan to run few ML functions on the data (random forest etc), so doing it in python/pandas is essential. Any ideas/suggestions on how this can be performed ?
Edit: Tech stack currently does not have spark. We are a small team that operates on pandas and sql",2024-02-29 21:42:59
1b3qzi7,"Data Migration best practice, book/resources?","Hi all,  
I'm currently being offered a role as the data migration lead for a healthcare organisation changing electronic patient record systems. I've been unofficially doing this for a while, but they're talking about making this more official for the next 8 months or so. My background is really in business intelligence, data analysis, etc, so this is somewhat new to me. Is there any resources I should read about best practice for this kind of thing?   


Both of the systems have a SQL Server back end in case that's relevant, although technically the process basically involves creating a bunch of CSVs and loading them into a tool provided by the software supplier.  
Thanks",2024-03-01 10:19:48
1b3jcm4,"I gather, organize and make ""insights"" out of sport data for myself as a hobby. Is this the correct career path for me?","I've been doing this for my own enjoyment for a while now, something very close to what the guy(s) at [the pop foot](https://www.google.com/search?sca_esv=59998079312419c0&q=thepopfoot&tbm=isch&source=lnms&sa=X&ved=2ahUKEwikqqyZ_9GEAxW9ANAFHd4nAioQ0pQJegQIDRAB&biw=1920&bih=953) have been doing (just to give you an example). I normally gather data of various teams/players of a certain sport, make some averages here and there and then just compare them out. Nothing too fancy but definitely not simply gathering data and organizing it. I don't think I'd be very good at making graphs/visuals so I just stick to good ol basic excel tables.

Yesterday I was having a conversation with some friends on some recent ""data comparison"" I made between teams (not sure if I should call it analysis nor ""insight"") and we all agreed that it made TOO MUCH sense in the end. Kind of made me realize I could be doing this sort of thing as a job; made some google searches and found out about data engineering.

Am I in the right place? Or is analysis the thing I should be aiming for?",2024-03-01 02:45:45
1b3ppn0,Pivoting our product from API design to Data Modelling,"Hello r/dataengineering!

&#x200B;

I'm part of the [Jargon.sh](https://Jargon.sh) team, a platform on a mission to bring the principles and tools of open-source software development to Domain Driven Design (DDD) and API design. Our scope is broad, but we've traditionally focused on providing just enough data modelling to meet our users' needs. However, we're seeing a growing demand from clients who are interested in leveraging our platform for general-purpose data modelling, marking a new and exciting direction for us.

&#x200B;

Our clients have shared how much they value the DDD approach for breaking down large models into smaller, more manageable domain models, and how our method of calculating Semantic Version (SemVer) release numbers has been a game-changer for them. These strategies have proven effective for their API design and integration architecture. Additionally, they appreciate Jargon as a platform of reusable models that can be easily searched, discovered, and imported into other domains, all based on immutable SemVer versioned releases. This capability not only enhances the efficiency of their work by promoting reusability and consistency across different projects but also significantly reduces the time and effort required in developing new domain models from scratch. This feedback underscores why our clients are encouraging us to extend our support to include more comprehensive data modelling capabilities.

&#x200B;

Our goal extends beyond offering generic tools; we aim to understand the unique challenges our users face and to develop innovative solutions. By engaging closely with the community, we believe we can customise our solutions to meet specific needs and challenges. This collaborative approach has served us well in the realms of DDD and API, and we're eager to apply it to data modelling for data engineering, hoping to add significant value.

&#x200B;

As a member of the Jargon team, I'm here to collect your feedback and insights on how an open-source software-inspired approach to data modelling might benefit you. Your input is incredibly important to us as we strive to evolve Jargon into not just a tool, but a community-driven solution that empowers practitioners to achieve what they're trying to do more efficiently.

&#x200B;

I'm quite new to Reddit, having been stalking around for a little while before finding this great community. Our research indicated that this is the most active and engaged data engineering community around, so I decided to reach out. It's clear there's a wealth of knowledge and experience here, and I'm excited to learn from you all and maybe convert some of your knowledge into features of our freely available data modelling platform in return.

&#x200B;

If you're not familiar with Jargon yet, I invite you to explore our platform. We offer a free-forever tier packed with features that could be of interest to you.

&#x200B;

Thank you for your time and insights. I'm looking forward to your feedback, and happy to answer any questions you might have!

&#x200B;",2024-03-01 08:51:33
1b3r9wi,Property-based testing in ETL/ELT flows,"Has anyone implemented such a pattern? Any resources to share?

I'd be really interested in hearing your stories.",2024-03-01 10:38:52
1b3v3ue,PostgreSQL: Protect schemas against accidental deletion,"🔥 New Article @ Tela Network

PostgreSQL: Protect schemas against accidental deletion

[https://telablog.com/postgresql-protect-schemas-against-accidental-deletion](https://telablog.com/postgresql-protect-schemas-against-accidental-deletion)

👉 There is a risk of accidentally deleting an important schema whenever we interact with a PostgreSQL server.

👉 We want to add a protective guardrail that prevents accidental deletion.

👉 We create an event trigger that fires when the DROP SCHEMA command is entered.",2024-03-01 14:07:32
1b3pi32,Performance or read from a view and table ,"Hey I am reading a hive view into a pyspark data frame and later joining it with the fact table read into another data frame. These views have equivalent hive tables partitioned by snapshot_date as well. The view only stores the latest data by taking a max(snapshot_dt). Will directly reading from the table with max(snapshot_dt) increase the performance rather than reading from the view ?

Tyia ",2024-03-01 08:37:06
1b3ip2a,What kind of validation do you perform on the data and what tests do you write for correct data handling in to a dB? ,"Let's say you are dealing with multiple csv files, each with over 10 million rows. What kind of validation tests do you perform on the data itself and what type of tests do you write for dealing with database part? Which libraries do you use for it? 

Any reference material to read up on would be very helpful! ",2024-03-01 02:14:26
1b2zhni,Please help me with the urge of resigning every day. My current job is a waste of time.,"2 YOE, 2022 CSE grad. Got placed into a data analyst job for 7.7lpa for which the tech stack is Informatica and Qliksense. My team never gave me a chance to work in Informatica projects, so I have been primarily working on just Qliksense which kills me with the shame everyday. Wanted to get SDE job, prepared for few months for it but felt like i need to achieve small goals first instead of aiming too high. Hence I changed the decision to Data engineering and preparing for it.

But currently my lead has been forcing me to get into functional domain (SAP) which is making me feel like resigning immediately. Also please note that in 2 hike cycles, I have been given only 6% and 4% hike respectively. I have considered myself a good student throughout my life and a pretty decent technical person. How do I deal with my situation now? I want to put papers and study and give interviews peacefully. But that seems not feasible since if I put papers, I still have to do functional work in my 3 months of Notice Period. How do I escape from this situation? Please help me. Please help me to get a direction. I have been thinking of putting papers everyday since last 1 year but I couldn't. This work, these people, this company just disgust me. Please advise. My current job is a waste of time and it's ruining my life.",2024-02-29 12:45:55
1b3lump,Count of different dates occurencies from one table ,"In SQL I need to count occurrences of the date when a deal was passed to a salesperson, the meeting date, and the contract signing date, along with the sum of contracts. 

All this data is in a single table with a unique ID as the primary key (PK).

The challenge is that I need to obtain the count of each of these dates by month without using any of the dates directly for grouping. This is because grouping by any of these dates directly would result in counting by that specific date. For example, if someone had a meeting in January and we group by the meeting date, we wouldn't capture their contract in February, etc.

How should I approach this?""",2024-03-01 04:52:45
1b374j2,Data Engineer jobs in healthcare or climate tech,"Do companies in healthcare or climate tech hire data or distributed systems engineers?

Are these jobs fulfilling careers? Are the people well renumerated.",2024-02-29 18:15:58
1b3fthq,Advice for small business data stack,"I own a small business and I have about two years of sales records, our data stack looks like this:  


Notion database where we keep the sales records +

Pull daily sales into Excel via Power Query through Notion API+

Generate Excel Pivot Tables and Pivot charts for analysis. The Year/Quarter/Month grouping feature has worked very well so far.

As you can see these are mostly free and basic tools (I use the single-user, free version of Notion). 

  
Now I would love to migrate this to something where I can do SQL, generate some graphs and perhaps s3 storage. Would like to keep it free as well. 

I was thinking of setting up a MySQL and MinIO server in my computer and work from there but I don't know what kind of limitations MinIO has as of today.

Does anyone have any advice?

&#x200B;",2024-03-01 00:04:26
1b3lipj,Delta Lake Staging Tables?,"Curious on people’s thoughts

Context: loading data to delta lake through spark jobs. 

Flow: 
Read incremental data from db
Merge to Staging table
Validate table
Overwrite prod with stagjng


Question is if staging tables are necessary here. The overwrite process is quite expensive since there is a 400gb table to overwrite daily. I could also merge to production after validating staging but that is also expensive and redundant.


If I just merged to a prod table and had a validation hook to time travel the table back in case of any errors, that doesn’t seem that bad given the tables aren’t being accessed during the load at all so data quality is not a huge concern

Also, no, the pattern of renaming a table doesn’t work in our enterprise

Anyways, curious what patterns people do for delta lake ",2024-03-01 04:34:40
1b3dkm3,"Many ""customers"" in single Spark job, or many Spark jobs?","Hi team,

I've got to make a report from our DW on customers and their transactions (for example). I am divided between having a single Spark job handle all the customers+transactions and give it lots of breathing room (in terms of cluster size), or whether I should have the Spark job handle a single customer+(all transactions) and rely on Airflow to get everything done.

Advice? Please and thank you.

Update: The report is like a statement. One report per customer, including all transactions for the given time period.",2024-02-29 22:32:38
1b37ca1,Polars on Databricks,"Hi there, havent seen any info on this, so perhaps its very dumb idea... would it make sense to run Polars on Databricks, using single node clusters.?

Reasoning being,... trying to get the advantages/features of databricks while at the same time using a small single node cluster with Polars instead of Spark/Pyspark to process data.   any opinions?

",2024-02-29 18:24:47
1b3afn7,"""Entry Level"" or ""Junior"" if still exist",Question for all you that had entry level positions or started off with easier projects. What was the project? What tools did you use? And what was your day to day like?,2024-02-29 20:29:44
1b2y3sd,SQL help - why shouldn't I use subqueries inside CTEs?,"Recently, I did a task for a company. One of the questions was to write a SQL query to get number of recurring customers (those that had a purchase in the 12 months before the current month) vs new customers based on an orders table. What I did was use 2 CTEs with subqueries inside them, something like this:

    -- count recurring customers
    with recurring_customers as (select extract(year from order_date) as year,
                                        extract(month from order_date) as month,
                                        count(distinct (customer_id)) as recurring_customers
                                 from orders o1
                                 where (select count(*)
                                        from orders o2
                                        where o2.customer_id = o1.customer_id
                                          and o2.order_date < o1.order_date and o2.order_date > (o1.order_date  - interval '12 months') ) > 0
                                 group by 1, 2),
    -- count all customers
        all_customers as (
            select extract(year from order_date) as year,
                                        extract(month from order_date) as month,
                                        count(distinct (customer_id)) as all_customers
                                 from orders
                                 group by 1,2
        )
    
    select a.year, a.month, recurring_customers, all_customers,
           coalesce(recurring_customers, 0)/cast(all_customers as decimal) from all_customers a
    left join recurring_customers r
    on a.year = r.year and a.month = r.month

I got the feedback that using subqueries in a CTEs wasn't incorrect per se, but they found it *strange*. I guess it's bad practice, but could someone explain why? And what would be a better way to do this? Using window functions?

Also, can you recommend a good resource to learn SQL best practices and to practice SQL? I never really worked in a position where I would get much feedback on my SQL - as long as it works and executes in a reasonable amount of time, it was alright, but I'm really looking for a way to improve my skills. 

Thank you!!",2024-02-29 11:26:15
1b3b7ky,Any good project course to learn Spark?,"It seems there are too many but I would like a simple explained, end-to-end project",2024-02-29 21:00:33
1b34lag,If DuckDb and dbt snapshot had a baby,N/A,2024-02-29 16:34:03
1b3d7m0,In Dataflow using the MongoDB to BigQuery connector. How can I pass the URI as a secret?,"When I create a Dataflow using the mongodb to BQ template, it asks for the URI. But of course, I dont want to paste the string, I need more security than that. How can I do this? Couldn't find any docs on this",2024-02-29 22:18:22
1b3cnft,Migrating images from a database to S3,We currently store images in a pg database as a base64 string. Not sure the logic behind the desicion. Now the storage is ballooning and I would like to move them to something cheaper to store like aws S3 bucket. Any recommendations on how to achieve this?,2024-02-29 21:56:38
1b3cgny,New DE Position: Need input for very first new steps,"Hi there,

Im switching in summer from a analyst position to a data engineer position (got the chance to swap internally). In preperation for the new job I am trying to soak up as much knowledge as possible on various DE-related things, but at this point I feel kind of lost. 

**My company:**

* data maturity level 1, green field on every level. first pipelines and data models are going to be a huge efficiency booster for stakeholders already
* small data sets, maybe like a few hundreds mb per day. 100% batch only
* mostly marketing data (GA4, meta, google and stuff) as well as booking data (ancient booking tool with SOAP protocoll stuff)
* based in europe/germany, so GDPR is a thing to consider

**My role:**

* i can freely choose data stack, no budget limitations. at this point im leaning towards dagster with snowflake & dbt. just love the asset centric approach from dagster from privat projects so far.
* responsible for building and maintaining pipelines as well as data modelling. on top of that im product owner for GA4 in colaboration with a tracking agency
* we've decided to go the managed route in data stack until we need to improve scaleability
* Data unit is a 2-person-team: Data engineer (me) and a data analyst. Im basically the only one in the company with DE ""know how"". 

**My problem:**

I've read so many things (like fundamentals of DE) and done so many things (private projects, data zoomcamp, datacamp dataengineer course), that I'm completely ""overwhelmed"" on what the very first steps are when I'm ""in charge"". 

Currently I'm thinking about these as the very first steps:

* Get in touch with legal and check how to use relevant data and be compliant with GDPR (hashing/salting a thing?)
* Define a priority list for data sources / stakeholders from C-Level and start from there: Workshops with data owner from stakeholders, check how often they need what kind of data, what the data models should look like, stuff like this
* Setup tooling with DevOps, in order to get data stack up and running. start with building pipelines, validating data, gathering meta data, optimize for maintainability. 
* Start with data modelling and be in regular touch with data owner / data analyst, who ever is the downstream user and needs to validate / get in feedback loop to improve data quality over time. 
* From there on build new pipelines for the next stakeholder in C-Level-based Prio list and rinse and repeat

In the meantime Im trying to deepen my knowledge in SQL (since im responsible for data modelling I figured this might be the most relevant day-to-day hard skill for me) as well as reading ""Designing Data-Intensive Applications"".

**My questions:**

* is there something really essential missing from my ""first steps""-list? Or can they be improved ""on the job"" and ""at some point while refactoring""? 
* What would you adapt / advise me to ""keep in mind""?
* Any decent resources out there I should read for the ""first x months in Data engineering: things to get done"" or so?
* Just in case someone wants to give me his/her opinion on data stack: feel free to do so

Really looking forward to community insights, everything helps <3 thank you! :) ",2024-02-29 21:49:17
1b36rlx,Python library for ELT pipelines?,"New library out of airbyte - [“PyAirbyte”](https://docs.airbyte.com/using-airbyte/pyairbyte/getting-started). Excited to use airbyte connectors without a UI or platform, I generally stick to writing scripts over no-code tools

&#x200B;

Do you think you will try this in your ELT/ETL pipelines? ",2024-02-29 18:01:20
1b2xftn,"Am I a handicap, deadweight to my team as a junior DE?","Hi folks! I am almost 9 months into my first DE job which is also my first programming job. I decided to switch to DE around 7 years after I graduated from uni and found my current job last summer.

Over the past 9 months, I have helped in building 2 end to end pipelines applying both streaming and batch processing. I have also worked on a data viz project for internal use with another junior DE who joined around the same time but is much younger and a recent college grad.

I am fearful that I am not pulling my weight at all even though my manager has never raised a performance issue and my two reviews so far have been quite good. I think this has something to do with me being closer in age to the mid level folks but being a rank amateur in competence compared to them. 

I am definitely asking for less help now and at least when I do I am able to show exactly what I did/why I did but I am keen to know what a very junior DE should be expected to do/know after 9 months. Thanks!",2024-02-29 10:44:06
1b34y1i,Microsoft 365 business central to big query,"I need to transfer the 365 data through api from 365 to big query, what would be the effective way to do this process !? Any suggestions ",2024-02-29 16:48:39
1b33ph1,Design schema use Star Schema in Data Warehouse Redshift,"I am a newbie about data warehouse. I want to design a Warehouse using Star Schema for company problems.
 Different business processes will be designed with different schemas such as sales_schema, financial_schema. However, some schemas will use common dim tables such as dim_product. So how should the dim table be designed and stored in schema to be most effective and appropriate?
- Should a table dim_product be created in a common schema (eg public_schema) and other schemas (sales_schema, financial_schema) can use data from the common table dim_product (public_schema)
- Each schema (sales_schema, financial_schema) will create a dim_product table (duplicate data)

Please help me. Thank you !",2024-02-29 15:57:42
1b319x2,I am looking for Talend Open Studio since it's not open source anymore,"Since January 31, 2024, Talend (acquired by Qlik) has been removed from the Talend Open Studio site. It's also impossible to find a version on Source forge.

Who still has a copy of Talend Open Studio?",2024-02-29 14:13:48
1b2wcm4,Redis use case?,"Can someone please help me understand a use case that fits the usage of Redis? I have never worked on it just trying to understand its use cases before diving deep into it.

My friend recently went through a system design round for a company, and faced a scenario to build a system for displaying real time user count (logged in on a website) on a dashboard.
We were thinking of a an event based setup like kafka writing onto a nosql which renders data for dashboard.
And we came to a conclusion that eventually persisting the events from queue to nosql would take all the time and defeat the solution.

Is this a right use case for Redis?

",2024-02-29 09:28:29
1b2qxvx,Is it common for data engineers to have proficiency in ML?,"I have been scrolling through the job postings on LinkedIn, and a lot of them have been asking for knowledge of ML with some MLOps experience. 

In my current company, data engineers only work on ETLs with Airflow and some services in python.

So I was wondering if I need to properly learn ML to be a good data engineer and maybe secure the future so to say.
",2024-02-29 03:59:40
1b33g3o,Storing Log data in S3,"We currently have a django web app that store all api requests and responses in our database. This table is growing significantly so I was thinking of storing such data in S3 instead.

What would be an ideal approach for this type of scenario?

Should I right the logs to a local file and push the logs on a daily basis to s3 bucket with something like lambdas? I could also do some ELT from the database and just keep a month worth of data in the database.

Sorry if the question is vague,  but there's a lot of unkowns.",2024-02-29 15:46:59
1b32ng5,Seeking Advice: Building Big Data Warehousing System for Fleet Management Company Using Hadoop on VPS,"

Hello ,



I hop you're all doing well. I'm currently in the final year of my university program, and I'm working on a project that involves building a big data warehousing system for a fleet management company. The company has graciously offered me access to their **Dedicated server** to deploy my solution.



Here's where I could really use your expertise: I've been tasked with utilizing Hadoop (specifically HDFS and Hive, for now) for this project. While I have some experience working with these technologies on my local machine using cloudera vm quick start , I'm entirely new to deploying them on a **Dedicated server**.



I'm seeking advice from those with experience in deploying and managing Hadoop on a VPS for production use. Specifically, I'd like to know:



1. Is it feasible to deploy Hadoop (HDFS and Hive) on a **Dedicated server** for a production environment?
2. How complex is the setup process for someone relatively new to deploying Hadoop?
3. Are there any particular challenges or considerations I should be aware of when deploying Hadoop on a VPS?



Any insights, tips, or guidance you can provide would be immensely helpful as I embark on this project. I'm eager to learn and ensure that I set up the infrastructure correctly to support the company's data warehousing needs.



note: The company already has a solution in place using MySQL. However, this project is more for research purposes, exploring the feasibility and potential benefits of leveraging Hadoop for their data warehousing needs. If the solution proves to be promising, the company is open to adopting it for production use.



Thank you in advance for your assistance!",2024-02-29 15:13:20
1b2xs94,Getting additional data from Debezium connector,"I am trying to stream the changes made in my Postgresql DB using Debezium and Kafka. This is my connector configuration(pic 1)

The connector is created and I am able to consume the changes in Jupyter notebook using my Kafka consumer. But I am only getting the row data that was inserted or updated(pic 2). I am not able to get the operation that was performed(whether it was INSERT or UPDATE). Is there a way to get the operation performed data as well? also is it possible to get other operations data like TRUNCATE,DROP,ALTER because WAL logs store these information(pic 3- you can see the delete operation being logged but this doesnt get consumed by kafka).",2024-02-29 11:06:12
1b31lz9,Generate keypairs on aws and get private key to dev desktop,"I know this might be more an AWS question and I'll post in r/aws if don't get any starting points here, but this is for Data Engineering problem

&#x200B;

I am looking for a way to make key pairs that will be used for authentication to Snowflake.  


How to get the public key to snowflake I'll leave for later, my final goal is to have convenient rotation managed in AWS. 

For right now I'm wondering, I think for keys that are used off of aws, e.g. on some dev's desktop, I \* **must** use  (if I want to use KMS at all) GenerateDataKeyPair.  Does that sound right so far?  Does anyone do this, and what is your mechanism for an individual dev to get a current private key when they need it?

&#x200B;

For keys that will be used only from AWS I think I want to use  GenerateDataKeyPairWithoutPlaintext and whatever connects to snowflake decrypts the private key as needed.

# ",2024-02-29 14:28:47
1b2xx29,Data virtualization / Semantic layer solutions,"We would like to have something like semantic layer Cube.js offers over our databases and APIs. What are the solution you use? Found something like Apache Apisix, Kong, Cube.js and cloud providers solutions.

Also thought of something like Trino and/or Spark but it does not really cover the need for one simple API access to other sources like databases/APIs.

 Is there any other good open source tool for this?",2024-02-29 11:14:38
1b2tcev,Unlock the Full Potential of Azure for Data Engineering and Analytics with Our Comprehensive Video Guide," 

Hey Azure enthusiasts and data wizards! 🚀

We've put together an **in-depth video series** designed to take your Azure Data Engineering and Analytics skills to the next level. Whether you're just starting out or looking to deepen your expertise, our playlist covers everything from **real-time analytics** to **data wrangling**, and more, using Azure's powerful suite of services.

**Here's a sneak peek of what you'll find:**

1. **Twitter Sentiment Analysis with Azure Synapse Analytics** \- Dive into real-time sentiment analysis and build end-to-end big data pipelines.
2. **Real-time Vehicle Telemetry Processing** \- Learn how to handle real-time vehicle data with Azure Stream Analytics and Event Hub.
3. **Fraudulent Call Detection** \- Discover how to detect fraudulent calls in real-time using Azure Stream Analytics.
4. **Weather Forecasting with Azure IoT Hub** \- Explore how to forecast weather using sensor data from Azure IoT Hub and Machine Learning Studio.
5. **Web Scraping with Azure Synapse** \- Get hands-on with web scraping using Azure Synapse, Python, and Spark Pool.
6. ... and much more across 20+ videos covering Azure Databricks, Azure Data Factory, and other Azure services.

**Why check out our playlist?**

* **Varied Topics**: From analytics to processing, explore Azure's capabilities through practical examples.
* **Skill Levels**: Content tailored for both beginners and experienced professionals.
* **Community Support**: Join our growing community, share your progress, and get support from fellow Azure learners.

Dive in now and start transforming data into actionable insights with Azure! Check out our playlist

[https://www.youtube.com/playlist?list=PLDgHYwLUl4HjJMw1-z7MNDEnM7JNchIe0](https://www.youtube.com/playlist?list=PLDgHYwLUl4HjJMw1-z7MNDEnM7JNchIe0)

**What's your biggest challenge with Azure or data engineering/analytics?** Let's discuss in the comments below!",2024-02-29 06:09:52
1b2zpju,Keen to discuss data integrity patterns.,"We discuss a lot about building data pipelines here but one thing I personally think data engineers can do better is ensuring the integrity of the data. 
1. Ensuring it lands on time
2. Ensuring it's complete
3. Maybe even alerting stakeholders when data deviates from expected distributions ",2024-02-29 12:57:34
1b2jptp,How Can I Revive My Career?,"I've spent the last ten years at a struggling consulting firm. I specialized in Pentaho. However, my firm never won any big data warehouse projects, so most of my projects were using Pentaho to make Excel spreadsheets.

I stayed because I liked my coworkers and boss. Now that they're gone, I want to leave too and realized I don't have many skills but a decent sounding background since I also shadowed some big sounding projects.

What's the best way to revive my career? I don't think Pentaho is it. Or is it? Should I focus a different low code ETL tool? Learn Python?

Maybe go a different path and focus on architecture? Data modeling? Power BI?

Has anyone else struggled with their YOE outpacing their skills?",2024-02-28 22:34:56
1b2wlrw,Datalakehouse pipeline,"Hi, everyone. I have question like this. I am trying to build datalake house with some components such as, Minio as a object storage, dreamio as SQL engine.  Requirements are like below;  


* Migrate some tables (not high volume tables) from MSSQL to Minio, so we can simply join these tables with Dreamio and query these tables. This part is not the problematic. I already migrated all tables to Minio bucket with the help of Apache Spark.   


Problem: Now, I need to bring only incremental changes to Minio. I think the way it can be done is Airbyte using enabled CDC on MSSQL to S3 (Minio bucket) connector. Lets say, one row got updated on source Sink (mssql) and CDC captured this change, and my airbyte job also migrated this incremental change as file to bucket. But then how can I implement this change to files there, so dreamio tables can also reflect these changes on tables. Source operations are insert, update, delete. Maybe someone will recommend to overwrite everything each time. So all the time there will be updated data. But I dont want to put this kind of strain, instead I want to use incremental changes. Is there anybody has idea about this ?",2024-02-29 09:46:30
1b2cr2g,Is there still a place for people who specialize in SQL?,"I suck at live Python coding interviews. I've been practicing for months and building projects on my portfolio that hiring managers won't even let me show them as a means of proving competency. I spent 8 months just to find one Junior DE role that I was able to have a conversation over and blew it because I didn't remember how to append a list within a function, and blanked out on proper syntax. That's after a few dozens of hours of practice. 

But if you ask me how to best optimize a query, do live exercises on SQL, explain and query use cases for CTEs, window functions, etc. I can do that just fine. 80% of DE tools use SQL, and you can use queries to do much of the transformation work outside of Python. Outside of SQL Developer, are there other job titles or methods of looking that I'm missing? I don't want to have to stop trying, but after a year and 7 months, I don't really have anything left.",2024-02-28 18:04:41
1b30ip6,AI-driven meme generator,We released an AI-driven meme generator for data engineering enthusiasts at Qbeast. This fun project helped us learn how to fine-tune AI models and customize datasets for humor. We're excited to share our experience with other enthusiasts who are interested in merging technology and creativity. Check out the story at https://qbeast.io/qbeasts-adventure-in-ai-driven-meme-creation/.,2024-02-29 13:38:57
1b2us18,Wanna do Microsoft Azure associate data engineer certification DP-203. How is Alan Rodrigues's course on udemy for exam preparation for extreme newbie in data engineering. ,"Wanna do Microsoft Azure associate data engineer certification DP-203. How is Alan Rodrigues's course on udemy for exam preparation for extreme newbie in data engineering. 

Edit - Also as a fresher applying for data engineering roles is this certification worth it?",2024-02-29 07:39:24
1b2rrjj,Want suggestions for making an AdTech related data engineering project,I am applying for an Advertising Technology company for data engineer role and company said they'll prefer candidates who has some AdTech related data engineering projects. Any suggestions/project ideas you guys have ( plus if you can in short write a high level design description about your project idea then it'll be helpful) ,2024-02-29 04:42:59
1b2v5fg,Data pipeline for analytics	set up help,"I'm working on a project to set up our data in a quick and fast way to generate analytics charts and tables and wondering if there is a better way to set things up as currently I'm just working with what I know.

We have Metabase acting as the frontend, and Redshift as the database.

Currently we use Stitch to replicate data sources (e.g. replicate interesting tables from our application database). We also have Segment pushing raw data from other sources direct to Redshift (e.g. clicks, views, tracks etc, plus connecting other sources like Stripe and Facebook).

For an initial version of the analytics system, I have been using AWS Redshift Query Editor to create and populate new fact and dimension tables in a 'star schema' design. Then I have some (materialized) views creating more frequently used aggregations that are accessed and displayed as charts and tables in Metabase.

The kind of thing we're looking at visualizing would be basic things like Product views, orders and the like. These would be broken down by things like date (agg by daily, weekly, monthly), brand, account manager, country, and so on. My users want to be able to compare date ranges and 'time travel' through the data (e.g. what did the orders look like this time last year?)

Issues with the current set up are maybe fine (IDK!), but wanted to check if there wasn't a better way:

1. There's a lot of data duplication: e.g Stitch incrementally creates fully replicated tables that I then extract the interesting bits from to create fact / dimension entries. Then the aggregate views further duplicate data. (I'm from an application dev background using RDMS - postgres - databases, so used to trying to keep things as nomalized and de-duplicated as possible)

2. Having a bunch of SQL scripts / snippets in Query Editor feels a bit hacky: No git style workflow, manual 'migrations' just running `ALTER TABLE...` scripts, when I set up scheduled scripts, I'm not sure how those are monitored if things go wrong (I assume AWS has this ability). 

At the same time, it also feels like an external system (like Airbyte maybe?) is overkill if we're just moving data around within a single Redshift instance.

So does any of this have a neat solution / software framework out there? I'm thinking something along the lines of a way to define and migrate the fact/dimension tables + a way to define the ETL jobs to take my source data and push it into these new tables without pulling and pushing the data off and back onto the Redshift instance, along with scheduling and so on. Preferably Python based.

And secondly, is Redshift even the right place to put this data for the speed of visualisation or should we be looking at a different solution here?

Thanks for your help!",2024-02-29 08:04:18
1b2qijg,Learning about big data techs,"Hi All. I am an engineering fresh graduate and am trying to learn about different big data technologies. I want to create a project to put on my portfolio, but I am quite confused about different tools. This workflow is what I am thinking:

Step 1: Calling financial data API / Google Trend from Python running on local laptop

Step 2: Sending above data to Kafka topics hosted on EC2

Step 3: Subscribing to the topics from pyspark and carry out stream transformation and aggregation

Step 4: Streaming the results to Snowflake for more analysis and visualization

So firstly, I feel that I am just using Kafka as a pub-sub service, and pyspark as another version of pandas. Even if I get all these to work, I am not confident that I can work with ""big"" data. What else should I learn about these tools. Are they supposed to just work when deployed to an auto-scaling compute cluster?

And secondly, is this what you would see in real-life application?

Would love suggestions on what to learn and changes to make. Thanks in advance!",2024-02-29 03:37:45
1b29psy,Feeling stagnant at first DE role,"Hey all. I've been with a large non-tech company for about 6 months now in a Jr data engineering role, where I got hired with lots of other new grads.  Before this I did a DE internship mostly using Pyspark and snowflake. I enjoyed the work, so I figured I’d go for it as my first job.




On one hand, I love my team and manager, they are all really kind and supportive. It's a great working environment, with good benefits and decent pay.




But my work is really dry compared to my internship and coursework- the relatively little work I get at least. I was placed with a maintenance team where most of the work is small SQL bug fixes, some legacy code updates, and running no/low code jobs. And as a Jr, they've been pretty slow to get me involved in any of the more in depth work despite my insistence. All of my learning is very company specific business process stuff, and I feel like I'm becoming a worse programmer every day. But pretty much everything *besides the work* is great here. I know some other new grads hired as part of the same cohort as me are working with Pyspark, designing new pipelines and features, and generally doing much more technical work , so I kinda feel like I just got unlucky with my placement in that way. I’m very concerned that I’m not learning any transferable skills if/when I’m looking for DE roles in other companies down the line.




Has anyone been in a similar position? Should I stick it out, or try to move to a different team internally? Then I run the risk of getting a much less pleasant to work with team is my worry. I'm trying to work on side projects more to regain some technical skills, it's just tough to balance with work, other hobbies etc. I wish my job stimulated those skills at least a little.",2024-02-28 16:09:30
1b2rlqj,Data stitching/update/deduplication in a data pipeline,"Hi,

We are designing a system which is going to move data from input files(in Avro format) to goldengate to kafaka topics to the database. Incoming files-->GGS--> KAFKA-->OLTP Database. This would be a heavy transactional sysetm processing \~10K txn/second. The database is supposed to show the near real time transactions to the users. The transactions which coming from kafka topics will be asynchronous in nature and also there are chances of duplicate data being ingested from kafka topics. So the data has to be stitched/ updated/deduplicated before showing it to the users a complete transaction or say before persisting it to the normalized data model which would be ready for the querying by the end users.

So where should we preform these stitching/update/deduplication stuff in this workflow? Should it happen inside the application somewhere in the kafka or should it happen in a stage area in database by persisting all the pieces as is coming from kafka topics. Adding another stage layer is going to add some more time to the data to be visible to the users and this it may not be near real time.

Or should we persist the data as is in stage area and show the data from stage itself if some users are okay with partial transaction data and showing the complete transaction data from the normalized table to other users who wants to see it as a complete transaction?

What is the appropriate design to address such use cases?",2024-02-29 04:34:27
1b27q0s,Zero to Hero: Mastering Change Data Capture For Remarkable Database Integrations,N/A,2024-02-28 14:45:57
1b2puc0,Real Time Data Lake,"So I keep hearing the Data Lake buzzword thrown around a lot and I have done some research and seems most of the real-time references are geared towards data ingestion.  Does anyone know if the elusive data lake is capable of serving data in real-time ?  I only ask because we are getting pushed towards this model and we only have encoded data that needs to be delivered end to end with ms latency and only decoded on the receiving end with no need for transformation or enrichment.  I keep saying this will not work for us and could possibly add several hundred ms of latency.  Does anyone have any insight or maybe are using a data lake  such as Snowflake for something similar.

Thanks  ",2024-02-29 03:05:16
1b27h4g,Python ETL pipeline with Airbyte and Pathway,N/A,2024-02-28 14:34:49
1b2h27l,PostgreSQL: Prevent accidental database deletion,N/A,2024-02-28 20:51:17
1b24nxy,"Airbyte + Airflow, Dagster, Prefect or Kestra?","My company is using Airbyte for ELT, and having the option to easily add a Dbt transformation with each connection was probably the main point of choosing Airbyte, but they're deprecating this feature by the end of March and focusing on the EL process. We're looking for a data orchestrator to help schedule our airbyte connections (these are created and triggered by using Airbyte's API's) followed by the Dbt transformations, and the recommended tools by Airbyte are Airflow, Dagster, Prefect and Kestra. Which of these would you recommend?

P.S. I think it's obvious that I'm a junior DE that's still very new to the field, I'd appreciate any guidance.",2024-02-28 12:14:57
1b2kluj,Storage of network traffic,"Hi All,

Today I am working on a data pipeline where my ingestion-app processes realtime network traffic from multiple sources and writes into a Kafka bus in JSON format to be consumed by multiple endpoints. Apart from realtime processing I need to store a copy of it for future  processing/analysis.

I was wondering which db would be the better option for offline consumption as the data is huge (atleast 10-20 TB per month) from more than 1000 nodes. I came across different demos where clickhouse or elastic search is used as final resting place or storage of network traffic and then grafana is used for visualization. 

I am not an elastic search expert but I feel it is a search engine capable of indexing string data. But can I retrieve the same data in future for further processing? Since Network traffic is timeseries data is elastic search good for this?

Any suggestions based on your experience would be of great help

&#x200B;",2024-02-28 23:09:20
1b2ekwa,How common in the use of Make/Makefiles in the field?,"I was reading [this article](https://www.startdataengineering.com/post/data-engineering-projects-with-free-template/) from Start Data Engineering today, and saw they suggest including Make in your projects. I generally really like the Start Data Engineering content, but I guess I don't really see the clear value of Make. In my personal projects I've developed, my IaC and things like pushing new Docker containers is governed by Github Actions. 

Is there a lot of complex CLI work in on-the-job workflows where Make would have more value? ",2024-02-28 19:15:12
1b23thj,Rental Price Prediction ML/Data system,"Hey everyone,

Just wrapped up a project where I built a system to predict rental prices using data from Rightmove. I really dived into Data Engineering, ML Engineering, and MLOps, all thanks to the free Data Talk Clubs courses I took. I am self taught in Data Engineering and ML in general (Finance graduate). I would really appreciate any constructive feedback on this project. 

**Quick features:**

* Production Web Scraping with monitoring
* RandomForest Rental Prediction model with feature engineering. Engineered the walk score algorithm (based on what I could find online) 
* MLOps with model, data quality and data drift monitoring. 

**Tech Stack:**

* Infrastructure: Terraform, Docker Compose, AWS, and GCP.
* Model serving with FastAPI and visual insights via Streamlit and Grafana.
* Experiment tracking with MLFlow.

I really tried to mesh everything I could from these courses together. I am not sure if I followed industry standards. Feel free to be as harsh and as honest as you like. All I care about is that the feedback is actionable. Thank you. 

&#x200B;

[System Diagram](https://preview.redd.it/hkikvujy9blc1.png?width=4913&format=png&auto=webp&s=8df8f6e916fd98a48ca89b7467e4b4d917b7a7bd)

Github: [https://github.com/alexandergirardet/london\_rightmove](https://github.com/alexandergirardet/london_rightmove)",2024-02-28 11:26:19
1b1ztkv,Transitioning from Analyst to Data Engineer!,"Hey, DataEngineers on Reddit! 🚀 So, I'm making this move from Data Analyst to Data Engineering and could use some advice. Right now, I'm rocking the intermediate level in Postgre SQL. My game plan is to dive into the Data Engineer track on DataCamp, focusing on Postgre and Python.

Once I've got that down, I'm eyeing AWS and Azure certifications to level up my cloud skills. Do you think this combo will be enough to score a Data Engineering role, or am I missing something crucial? Any friendly suggestions or thoughts? Cheers! 🤓 #DataEngineeringJourney",2024-02-28 07:02:00
1b2g1os,Snowflake virtualization table,"I want to create a mapping table which would contain hased value of my primary key .

Once the data gets inside the snowflake , before it stored to table a proc shall be able to hash the values for all columns which are tagged as pii.
This hashed value shall be used in all transformation and when this data gets out of snowflake  want to change it back to original value using the mapping table.
Eg. Below data to be ingested in target_table 
Id1| type|id2|type|
1234@abc.com|email|987654321|phone.


So basically id1 and id2 shall be stored in mapping table along with their hased value and type(for cluster) ,and it's hashed value shall be stored in target _table.

One way of doing it is 
Create a mapping table cluster by pii type .
Proc1 : to check into mapping table , if the I'd exists,if not then insert the I'd value along with type and it's hashed value .
Proc2 : to join the outlying data with mapping table and convert the id1 and id2 into it's hashed version.


But this solution is not scaleable. What I mean is if my mapping  table  size is huge like 100 billion records , the join operation takes too much time .
",2024-02-28 20:12:18
1b2j9ng,Kafka and gRPC,"I am wondering how to set up the environment. I have a requirement that communication between the producer and Kafka cluster has to go through gRPC and between Kafka cluster and consumer, too. 

Producer(VM)-gRPC-Kafka-gRPC-Consumer(VM).

The goal is to send images through Kafka and process them in real time. How would you do that? The image size is about 5MB.

gRPC requirement: How can it be achieved? Do you have some experience with it?",2024-02-28 22:17:17
1b2bi54,Data Catalog Tool (Open source),"We are searching for a simple and basic open-source tool to create a data catalog for our data. The objective is solely to have a shareable data dictionary for anyone who wants to access data from the database. 

Do you have any good open-source tool suggestions?",2024-02-28 17:17:52
1b2b54x,Decision log - what is your opinion,I work as a data engineer and my manager wants to keep a decision log of all decision made on the project. Is this something useful I should spent time on? Any suggestion on how to do this?,2024-02-28 17:04:05
1b27wbs,Optimizing data model for PowerBI?,"We are in the process of building out a new data warehouse, and I've been reading Kimball on dimensional modeling. Does anyone know of any good sources or have any advice on optimizing our data model specifically with PowerBI's vertipaq engine in mind?",2024-02-28 14:53:46
1b2fb9t,How would you model a price evolution in a Star schema?,"Hello, everyone! Been reading Kimball & Ross' The Data Warehouse Toolkit and trying to practice by creating a star schema for an auto insurance company, but I'm still doubting in one final detail:

* There's a price catalog that surveys the market price of a certain vehicle model (including year of manufacture) across time.
* The points when all market prices are measured are not uniformly spaced, but you can roughly approximate the update frequency as bi-monthly.
* Although a new version of all the market prices is issued, some market prices may remain constant (most do change, though).
* Business needs to know:
   * For policy grain: Effective market price at policy inception and current market price.
   * ...But they may also request a time series of market prices for a certain vehicle model

I've come with 3 approaches:

1. Treat as SCD Type 5: Create a Vehicle Price mini-dimension (SCD Type 4) of market prices, and then a Vehicle dimension with a foreign key pointing to the current market price for each vehicle model row (mini-dimension becomes an outrigger for current price). Set Effective market price foreign key in Policy fact table that maps to mini-dimension.
    * Prices are not banded...
2. Create Vehicle Price fact table, with effective since/until dates mapping to Date dimension. Both current and effective market prices are obtained by drilling across Policy fact table and Vehicle Price table using the Date dimension.
3. Embed both market


Edit: Can produce some diagrams for each approach if necessary",2024-02-28 19:43:47
1b28f8d,Guidance needed: DBA internship or DE bootcamp?,"Hi there - I have an option to either go for a DBA internship in a big tech company which might lead to job/give me a kickstart in data career OR take an expensive Data Engineering bootcamp which would teach latest tech stack like Apache Nifi, Spark, Hive, Pig, Kafka, etc. and offer job/internship placement.

Which path should I take? I am a recent grad with Engineering Major. My ultimate goal is to become Data Engineer but due to no Data related experience, I am not really able to land a relevant role.

Would appreciate any guidance/comments!",2024-02-28 15:16:22
1b2ap94,Apache Airflow in 4 minutes,N/A,2024-02-28 16:47:22
1b27lwz,Symantec Layer,"Hi fellow data plumbers,

How have you guys implemented semantic layer in your organisation? We have a Data Mesh and hybrid cloud with multiple CDPs and On Prem data platform for different regions and specific BUs. Now want to implement a Symantec layer to sit on top of them. DBT Cloud Symantec layer looked interesting and we are already implementing DBT core in one of the regions (This is on Snowflake )

Curious to know how is it being done in other large enterprises with multiple data platforms. Any insights are much appreciated 

Edit: Not sure whether it was a brainfart or autocorrect. It's supposed to be semantic layer. ",2024-02-28 14:40:51
1b25xlc,Role of Interoperability in End-to-End Data Governance: As Implemented by Data Developer Platforms,N/A,2024-02-28 13:21:52
1b29dip,Learning DBX Ecosystem- architecture qs,"Hi!  Not quite a DE question, but DE adjacent.

I come from a very traditional data warehouse background (read: I'm old) and I am trying to understand the databricks ecosystem from an architecture POV.  I've always thought of DBX as ""hosted spark"" but there is a lot of talk around the ""lakehouse"" using DeltaLake + Unity + Photon (which I assume is similar to Iceberg + Glue/ Hive + Presto/trino).  

My question is... to use DBX, are most companies using Unity or Hive catalog to organize their files?  Is the ""Lakehouse"" architecture ubiquitous or something that DBX marketing is pushing?  If your company is using DBX, what is the main value driver?

Sorry for the noob question here... just trying to understand how DBX is being used in the wild vs what I can read on the interwebs.  Is there any DBX user communities that I might get some real world information that you all recommend?",2024-02-28 15:56:03
1b26bx0,Need advice on unit testing Prefect,"Either I’m losing my ability to google, or there really is just a complete lack of comprehensive Prefect testing documentation that covers more than assert my_flow() == 42. I feel like I need to join the Freemasons to learn the ancient secrets for this. 

Basic stuff I get, but mocking dependencies? Mocking out datetime without freezegun causing everything to error out?

Has anyone successfully unit tested anything in prefect more advanced than a hello world flow?

Edit: my issues were with Python and not Prefect. Once I got over a few hurdles with patching dependencies everything became a lot clearer. Why freeze gun screws so hard with the prefect dbt plug-in I’ll never know…",2024-02-28 13:41:43
1b284mh,Best practice when ingesting data from api?,"I am fetching data in a daily basis from an api. The api doesn't have pagination as far as I understand the docs. So I'm calling the end point and fetch everything everyday, even if no new data has arrived. What is the best practice to ingest the data into a cloud storage but not ingesting same data, only new data?
",2024-02-28 15:03:53
1b1to5o,Has anyone actually employed the use of a Triple Store?,"Like turtles, `(Jim, likes, apples)` `(Jim, age, 35)` `(Jim, knows, Sam)`

Has anyone used these kinds of stores? What’d you use it for?",2024-02-28 01:44:24
1b270kj,"Orchestrating AI operations (embedding, classification) + Data Storage","Hey everyone!  


I am running a data collection. For each collected data point (text), I need to run classifications (multiple) and need to compute an embedding, which are used further down in the pipeline for different operations (filtering, serving,...).   


I am storing the data at the moment in a lakehouse (delta table), but I am wondering how to add the AI operations to the pipeline. The ingress at the moment gets the data and batch inserts it. How would you add the AI operations and where would you store the data. Specifically: would you (a) run all the additional AI operations, add them as field, and then batch insert (b) add a unique identifier to each ""row"", send the text with the id to the AI operations, which insert it into a separate storage (c) insert the AI embedding or classifications into the delta table based on the id.   


If (b), what storage would you choose (I am mostly on GCP).   


What are the tradeoffs you would look out for? ",2024-02-28 14:13:44
1b2611k,Databricks SQL Server,"Hi all, 

I am seeking guidance in terms of databricks and sql server. There is a request to ingest a table from sql server to databricks and apply cdc but with translation log of sql server (this is a request)

. I have not worked with sql server and if someone can point me to right direction would be great. Any advice, docs etc would be of most help. 

Thanks! ",2024-02-28 13:26:43
1b22r8x,OLAP vs. OLTP? Which system to use to handle business process for adress management?,"Here is my situation:

It is required to implement an address management system for an existent web-based software solution at the company That uses an OLTP Postgres database to store relevant address data. The address data covers a single state, with a complete amount of 5 million addresses. The project context is building broadband infrastructure for each of the addresses that do not have a fast-speed internet connection yet. There are several steps in the address management process involved, with each step adding more attributes to the address data. For instance, there is the source address data with basic address information such as street name, house number, or zip code. Then there is another external data provider who provides information about the amount of households at the address and a third provider who gives information about the height of the building. All this data is now the starting point for the address data process, in which addresses can be verified, changed, excluded (or deleted) and even new addresses added. These operations can be performed by different stakeholder, e.g. the municipality in which the address is located at or the project manager. However, relevant data comes from different plattforms, depending on the process step. For example

* data is delivered as Excel by mail,
* a CRM system stores status of projects and important files that need to be linked to the adress,
* A partner company handles the tracking of the construction process in their web-software
* our own web-software to handle specific attributes of an address and even add or remove addresses

Here is a (not complete) process of the data:

1. Identify addresses that are eligible to get new broadband infrastructure and flag them accordingly and add information about their currently connected technology
2. Provide the opportunity to change values of specific attributes of an address.
3. When data was verified, send the data to the public institution that handles allowance of receiving public funding for building the infrastructure.
4. Receive the data back from the instituion once the check is finished and add data check inquiries asked by that institution into the address data.
5. Provide the opportunity to change that data accordingly by project managers.
6. Provide final data to the companies that build that infrastructure grouped by incremental building areas and track the progress of the work in the address data.

I struggle with the following problem:

Is it rational to have a Data Warehouse that acts as a center piece to handle all that data? When I see the following definition of a data warehouse I found in the internet it is exactly the use case I have:

>Data warehouses are commonly used primarily for combining data from one or more sources, reducing load on operational systems, tracking historical changes in data and providing a single source of truth.

However, on the other side its main purpose (by definition) is to provide analytical capabilities, which are also to some extent necessary, but the main focus for us would be to use the data warehouse as an intermediate step, where our web software is not only the source of the data but also the target of data that was cleaned, checked and processed in the data warehouse. Or should everything happen in the existing OLTP Postgres database and a data warehouse is not necessary?",2024-02-28 10:18:53
1b22oie,Is there a renewed focus on (conceptual) data modeling?,"I'm relatively new to the world of data. So not 100% sure this is the right place to ask this question.

I've been following quite a few American and European experts on LinkedIn as part of my education. And there seems to be a renewed interest in (conceptual) data modeling. They're talking about how data teams are not often in sync with business needs and conceptual models can bridge this gap.

While I'm familiar with ER diagrams, none of the resources I've used as a relative novice put any stress on this.

Is it applicable only when it comes to enterprise use cases? Where there might be the need to have this layer that business and data teams can use?

(I'm also posting this on other related subs to get more feedback, just fyi).",2024-02-28 10:13:52
1b1z2zh,Predicate pushdown optimization in modern data processing engines,"Hello Community,

I wanted to ask if there are still relational DBMS / MPP systems around where predicate pushdown optimization is not a thing.  From my experience, the times when this optimization was not present are long gone, most optimizers are doing pretty good job at applying the filters as early as possible. Whats your experience guys?  Anyone knows any relatively modern data processing engine where it does matter in terms of performance where to put filter condition for example (WHERE clause vs.  INNER JOIN clause)?",2024-02-28 06:16:59
1b221tr,Open source data platform alternative equivalent to Palantir Foundry,Any ideas on [https://news.ycombinator.com/item?id=39535743](https://news.ycombinator.com/item?id=39535743),2024-02-28 09:31:15
1b1wec1,What are your KPIs/OKRs for this year ?,Title says all. Want to understand where is community gearing towards,2024-02-28 03:53:20
1b1gmf6,"One of the reasons why ADF is love/hate type of thing - a task that would be super easy in Python such as skipping a for loop block if a condition is not met, turns out into a creative hack that takes many hours to get right",N/A,2024-02-27 16:51:08
1b1wl4e,System Design Question,"Hi all,   
I am reading the system design book by Alex Xu and cannot understand the design. 

This is a click aggregation system. We are reading data from the log and writing it to 1(Kafka topic). Then there's an aggregation performed and it writes to 2(Kafka topic). From 2 we write it to DB. I don't understand why we need the 2nd Kafka topic, why can't we directly aggregate and write it to db. In the book it says - **To achieve end-to-end exactly once semantics (atomic commit) we need it.**  
 Can someone please explain what that means?   


https://preview.redd.it/jtncia1729lc1.png?width=650&format=png&auto=webp&s=12bac9015b3e71787f63d2cad131e5e5db2f08ae",2024-02-28 04:02:41
1b1x5o5,Best Practices for Normalizing Telemetry JSON Messages in Spark Streaming with Low Latency: Handling Complex Joins and Data Issues,"Hello, Data Engineering Community!

I'm currently working on a Spark Streaming project aimed at processing telemetry JSON messages. The primary goal is to normalize these messages for further analytics and operations. Given the nature of telemetry data, we're dealing with a high volume of messages that come in a variety of formats and structures, making the normalization process quite challenging.

One of the key requirements of this project is to maintain low latency (less 10 sec) from ingestion to processed output. This is critical as the data is time-sensitive and needs to be actionable almost in real-time. However, I've encountered several challenges, especially when it comes to handling complex joins and dealing with various issues related to data quality and structure.

Here are a few points I'm grappling with:

1. **Normalization Complexity**: The telemetry messages vary significantly in structure and content. What are some effective strategies for dynamically normalizing these messages into a unified format that's suitable for downstream processing and analysis?
2. **Spark Streaming Limitations**: I've implemented an approach that handles and pivots different parts of a JSON message using various DataFrames. These are then joined to construct a normalized message. However, some of these DataFrames can be empty, necessitating a left join that doesn't function properly without watermarks. Unfortunately, watermarks are not suitable for my solution as they could introduce performance issues. My objective is to join DataFrames within a JSON message without logically needing to use watermarks, which seems unnecessary and potentially problematic in this context.
3. **Handling Joins with Low Latency**: Some of the normalization logic requires enriching the telemetry data by joining it with reference datasets. However, these joins are proving to be a bottleneck, especially when trying to keep the processing latency low. How can I optimize these operations, considering the distributed nature of Spark?
4. **Tooling and Techniques**: Are there specific Spark Streaming features, external tools, or techniques that you've found particularly useful for these types of challenges?
5. **Examples and Case Studies**: If you've worked on similar projects, could you share your approach, especially how you managed to balance the trade-offs between normalization complexity, data quality, and latency requirements?

Any advice, insights, or references to relevant resources would be greatly appreciated. I'm looking for strategies to improve the efficiency and effectiveness of our Spark Streaming jobs, particularly around the areas of data normalization and processing latency.

Thank you in advance for your help and suggestions!",2024-02-28 04:31:53
1b1xshy,Learning plan recommendations,"I am being asked to learn pyspark. ( my manager said it will be easier to pick Databricks if you get good with pyspark- so Databricks may be the next step in this learning process) 
Please suggest some of learning paths and certification and practice methods that you experts considered to be the best from your experience. 

My background ~11 years of expert level ETL/Datawarehousing exp with Informatica Powercenter, Informatica Cloud, 

Some basic snowflake, basic AWS, training level exposure to python (I remember defining functions - could not recollect syntax of objects, methods in objects etc. no realtime project exp with python) 

I have tried installing python using anaconda distribution and have meddled with some python programming doing pandas, some web crawling spider - all following some step by step tutorial. But don’t remember much as I have not continued practising.

",2024-02-28 05:05:23
1b20b19,Need Advice about CRM Architecture,"Hello guys, I will be tasked with building an CRM Architecture in couple of months, this is gonna be my first time building an Architecture and working with CRMs, my current job is mostly maintaining and building ETL pipelines and small data migrations here and there, so I need an Advice or suggestions 

where to start with the CRM. Thank you in advance and sorry about the English",2024-02-28 07:33:34
1b1ckh8,People buy beer during hurricanes,"Walmart saw spike in sales of beer during hurricanes.

Apparently if you have to be trapped indoors the best move is to open up a few cold ones with some friends :)

What’s your favorite data insight story?",2024-02-27 14:02:31
1b1w3n5,Expensive cloud services and the counters,"What are the expensive cloud services (for de/analytics) and the counter service that you guys use?
",2024-02-28 03:38:33
1b1ug0t,Best practice for monitoring ADF pipelines,Our team currently checks the monitor tab within ADF on a daily basis to determine which pipelines failed to run overnight. Is there a better way to this than manually checking the monitor tab to determine which pipelines require re-running? Can the process of rerunning pipelines due to commonly encountered transient errors be automated?,2024-02-28 02:19:54
1b1me42,Advice on Modernizing Data Warehouse Architecture,"Hi guys,

I'm looking for advice and insights on modernizing my data warehouse architecture. Here's where I currently stand:

* Kimball methodology with a star schema.
* My ETL processes are managed by Talend Open Studio open source - has been discontinued
   * I import jobs once a day
* Tableau serves as my primary tool for reporting.
* PostgreSQL is my  DBMS.

Here's what I'm aiming for in the modernization process:

1. **Data Volume:** I process around 1-2 million rows daily.
2. **Real-Time Needs:** I'm aiming for near-real-time data processing.
3. **New ETL Tool:** I'm considering using Kafka Streams and Apache NiFi for their real-time (or near to) processing capabilities. If anyone has experience with this combination, I'd love to hear your thoughts and any advice you might have.
4. **Database Consideration:** ClickHouse has caught my attention for its potential benefits.

Any advice, experiences, or alternative suggestions are welcome!

Thank you",2024-02-27 20:40:59
1b1kx6o,Best practice approach for ingesting data from an API where many rows will be added/changed/deleted over time without any indication/warning?,"I need to request data from an API and drop it in an S3 bucket, and get the data into a data warehouse.

The API's data is updated daily around 9pm, so my first approach was to request the data each day by `created_date` and drop partitioned files in the S3 bucket.

However, I realized that the number of rows increases for previous days.  For example,

* On 2024-02-25 the initial request returned 9000 rows
* Yesterday, I got back 9500 rows for 2024-02-25
* Today, I got back 10000 rows for 2024-02-25

It seems that any number of rows can be added/deleted/updated for any day within the dataset (1+ year of data, \~4 million rows, 50 columns wide).  It is more common for rows to be added/deleted for data within the current month, however, it's still theoretically possible data could change as far back as a year.

The only ""solution"" I could think of is to request the entire dataset every day.  The API would allow me to do this, but I have to paginate the requests/results, so it would be about 80 requests per day.  Obviously, the number of requests would grow, and I'd have to rebuild all the tables in my DW everyday, so I don't feel like this is a good solution.

What would be a best practice approach for capturing all of these changes in the full data set?",2024-02-27 19:42:31
1b1j1ge,Export/Dump MYSQL Table to Parquet File,"Currently have a table on a MYSQL DB with estimated size of 170gb.

What is an optimal way to export or dump this table to a parquet file? 

Spark is not really an option at the moment, only if I have exhausted other options.

Just curious if there's something else out there I could try, maybe native.. 

It's weird that MySQL workbench doesn't support export to parquet. ",2024-02-27 18:26:51
1b1i3vh,I'm building a DBT Docs -> Notion tool for my company. Any interest in having me write a guide or sharing the source code?,"Wondering if there is anyone out there like me that wants to be able to share parts of the DBT project with other people in my org. 

I just wanted to share exposures, and the column descriptions of some of my data mart models with some savvy business users who can do pivot table stuff. 

The project is pretty simple, basically uses artifact data to push metadata changes from each run into a few Notion databases. 

So far I've got databases for:  


1. Exposures - lists which reports the project is maintaining. Lets people view the report url in Notion.
2. Models - lists last run, columns & descriptions for all models 
3. I was working on trying to bring in the compiled SQL code into a code block, but Notion's API has a hard character limit and many of my table scripts are larger than this limit, so i was unable to get this to work with the code block.",2024-02-27 17:49:39
1b1i2qw,Data Driven Culture Discussion,"Hey Everyone,

This is an insightful article discussing becoming data-driven and how it is not just about adopting new technologies but also about nurturing trust and alignment within the organization.

Article 👉🏼 [https://www.datacoves.com/post/data-driven-culture](https://www.datacoves.com/post/data-driven-culture)

Here are some focal points from the article, paired with questions I believe could spark valuable discussions:

1. **Alignment with Business Objectives**: The article emphasizes the importance of getting everyone on the same page from the beginning and ensuring that data analytics strategies are directly aligned with business goals. **Have any of you faced challenges where data projects fell short because they weren't aligned with broader business objectives? How did you navigate these challenges?**
2. **User-Centric Data Solutions**: It's pointed out that solutions should be tailored to solve actual user problems rather than coming up with an overly technical solution. **Can you share experiences where focusing on user needs led to successful data projects? Or perhaps a time when overlooking this led to failure?**
3. **Data Management and Governance**: According to the article, robust data management and governance are crucial for sustaining trust in data analytics. **What strategies, practices or tools have you found effective in maintaining data quality and governance in your work?**

Looking forward to your experiences and thoughts!",2024-02-27 17:48:25
1b1hv4r,Migrating onprem SQL Server tables to azure databricks delta lake?,"My department has this huge initiative to migrate all of our onprem processes to the cloud. We're an SSIS shop, so we've been asked to convert all of our packages and SQL stored procedures to a hybrid of ADF pipelines and databricks notebooks. Since we're using databricks, they want us to use delta lake as our replacement to SQL Server (Azure SQL Server is not an option).

I have pretty limited exposure to databricks and spark, but another team we work with suggested we - 

* convert our tables to parquet files using an ADF pipeline
* create a delta lake table based off of the parquet files in databricks.

I've already done the above two steps, but I'm wondering if there's going to be any issues with this, or if there's a preferred method that's widely used?",2024-02-27 17:39:53
1b1h5wi,Question on extracting SAP data into a data lake and how do you handle CDC?,"  

Hi team, 

I am guessing some or majority of us here in this space have been asked to extract SAP data from either SAP ECC or S/4HANA into a data lake like ADLS Gen2 then process said SAP data using Azure Synapse or Databricks. I am also going to assume the tables and data that got ingested from SAP into the data lake are large tables like ACDOCA, VBRK etc.

The question that I have if, how do you handle change data capture from SAP? SAP is a pretty complex system and for those that have experience with it will know that not all SAP tables have a timestamp field to identify change data. 

For some large tables, we ended up performing a full load everyday and ingested 6 millions rows everyday from \~10 SAP tables seems very inefficient but we have explored all avenues together with the SAP functional consultant and concluded that a full load for some tables is the only option. 

Change record is just 1 matter, some SAP tables will physically remove the records out from the table and that is another reason we need to perform a full load.

We also ended up ingesting in records from CDHDR and CDPOS (SAP change log) and these are tables with huge volumes. 

I was wondering how is everyone addressing these sorts of challenges and questions? I know Microsoft has this **SAP CDC tool** and has anyone tried it in their organization?

[https://learn.microsoft.com/en-us/azure/data-factory/sap-change-data-capture-introduction-architecture](https://learn.microsoft.com/en-us/azure/data-factory/sap-change-data-capture-introduction-architecture)

&#x200B;

Thank you for your input. ",2024-02-27 17:12:09
1b1goj4,Need advice on backfilling airflow,"Hello everyone!! 

I’m currently working with a DAG named ‘tpa’, which involves multiple calculations and a machine learning model. This model generates values that are subsequently inserted into tables, followed by additional calculations. My task now is to backfill the data for the entire year of 2023. I’m familiar with the Airflow CLI command, but I’m unsure how to execute it in segments without disrupting the pipeline. Each pipeline run for a single day takes approximately one hour. Additionally, ‘tpa’ has a dependency on another pipeline named ‘st’, which also takes about an hour to execute.

I’m seeking advice on how to proceed, preferably via a Python script for automation purposes. My initial thought was to utilize a bash operator to execute the command and pass parameters through it, but I’m uncertain if this is the best approach.

Any insights or suggestions would be greatly appreciated. Thank you!

Feel free to let me know if you need any further adjustments or have any specific preferences!",2024-02-27 16:53:28
1b1gkvl,"Data Engineering and Data Science Insights with Teradata's Developer Blog: Free Tutorials, Tools, and Resources","Hello data engineers,

I would love to invite you to follow Teradata's [Medium blog](https://medium.com/teradata) covering the latest trends and techniques in data engineering and data science. We're excited to share our expertise and resources with you. Happy learning!

* **Comprehensive Tutorials:** Dive deep into end-to-end tutorials that not only guide you through various concepts but also provide practical experience.
* **Dedicated Free Teradata Instance:** Gain hands-on experience with your Free Teradata instance. It's a fantastic opportunity to work with real-world tools in a controlled environment.
* **Free Coding Environment:** Say goodbye to the hassle of setting up your environment. Teradata offers a free coding space on ClearScape Analytics Experience, making it easier for you to focus on learning and experimentation.
* **Sample Data:** No need to worry about finding data for your projects. The blogs provides sample data that you can use to practice and enhance your skills.

[https://medium.com/teradata](https://medium.com/teradata)

**Latest topics include:**

* **Data Mesh Architecture with dbt-core and Teradata Vantage™:** In this article, dive into the process of implementing a data mesh architecture utilizing dbt-core alongside free and open-source plugins.
* **Mastering In-Database Feature Engineering with Teradata:** Dive into the nuances of feature engineering within Teradata, a crucial skill for enhancing your predictive analytics, ML, and AI projects.
* **Creating High-Quality Data for Trusted AI with In-Database Analytics:** The success of generative AI depends on data quality, especially as models and algorithms become more accessible. Teradata VantageCloud’s provides a powerful, open, and connected AI/ML capabilities, which includes a comprehensive set of [in-database analytics capabilities](https://www.teradata.com/platform/clearscape-analytics/in-database) that simplify data preparation. In this article, we’ll focus on cleaning and exploring data using ClearScape Analytics.
* **Teradata Vantage™ Destination Now Available on Airbyte Cloud:** Discover how to integrate Teradata Vantage with Airbyte Cloud, streamlining your data extraction and loading processes.
* **GenAI: Introduction to Prompt Engineering and LangChain:** Explore generative AI, including prompt engineering and leveraging LLMs for querying databases in natural language.

&#x200B;",2024-02-27 16:49:30
1b1ggok,Need Advice on Data Storage,"Hello!

My company has an on premise server for both applications and file storage.  We pay a third party to professionally manage it, including backup and recovery.  We have begun shifting applications off the server into the cloud due to issues with concurrent user issues, which leaves our server only being used for shared file storage and data retention.

No one here is any kind of IT person - a few ideas are getting tossed around, such as moving our data to a professional Dropbox, or keeping our managed server for on-premise file storage, or moving our data to Google drive (we use Gsuite for business).  Our IT vendor suggested a move to the cloud (Azure).  

I would really appreciate some advice.",2024-02-27 16:44:56
1b1g1w0,Help Needed: Apache Beam/Dataflow Pipeline Scaling Issue with BigQuery,"Hello Data Engineers,

I've been working on an Apache Beam pipeline with Dataflow to process and upload data into BigQuery. However, I'm encountering a scaling issue – after attempting to process more than 1,000 files at a time, I'm hitting memory errors.

Upon further investigation, it seems like my pipeline is processing all stages in batches rather than streaming each file or element individually. I would like to configure my pipeline to stream each file individually and limit the number of files processed at a time to avoid memory issues.

If anyone has experience with Apache Beam and Dataflow, I would greatly appreciate any insights or tips on how to make my pipeline more efficient and scalable.

Here's a simplified version of my current pipeline structure:

    import apache_beam as beam
    from apache_beam.options.pipeline_options import PipelineOptions
    from apache_beam.io.gcp.bigquery import WriteToBigQuery
    from apache_beam.runners import DirectRunner
    from google.cloud import storage
    
    class GetFiles(beam.DoFn):
    
        def __call__(self, *args, **kwargs):
            return self.process(*args, **kwargs)
    
        def process(self, element)
            # Get the id from the element
            file_path = element.get(""id"")
    
            # Get the name of the gcp bucket
            gcp_bucket_name = re.match(""[REGEX HERE]"", file_path)
            # Get the name of the gcp directory prefix
            gcp_directory_prefix = re.search(""[REGEX HERE]"", file_path)
    
            # Get the blobs associated with the file_path
            storage_client = storage.Client()
            bucket = storage_client.get_bucket(gcp_bucket_name)
            blobs = bucket.list_blobs(prefix=gcp_directory_prefix)
    
            # For each blob, pass the id on to the next PTransform
            for blob in blobs:
                yield {""id"": blob.id}
    
    class Extract(beam.DoFn):
    
        def __call__(self, *args, **kwargs):
            return self.process(*args, **kwargs)
    
        def process(self, element):
            # Get the name of the file
            file_name = element.get(""file_name"")
            bucket = element.get(""bucket"")
    
            # Get the data from GCS storage
            storage_client = storage.Client()
            bucket = storage_client.get_bucket(bucket)
            blob = bucket.get_blob(file_name)
    
            blob_data = blob.download_as_string()
    
            # Get some data for the columns
            json_data = json.loads(blob_data)
    
            # Get the id
            id = json_data.get(""id"")
            
            # Return the id with the blob data
            return {""id"": id, ""data"": blob_data}
    
    pipeline_options = PipelineOptions(
        streaming=True
    )
    
    schema = {
        ""fields"": [
            {""name"": ""id"", ""type"": ""STRING""},
            {""name"": ""data"", ""type"": ""JSON""}
        ]
    }
    
    pipeline = beam.Pipeline(DirectRunner(), options=pipeline_options)
    
    extraction = ( pipeline
        | ""Get file parameters"" >> beam.Create([""bucket_name/path/to/directory"", ""bucket_name/path/to/directory""])
        | ""Get the list of files"" >> beam.ParDo(GetFiles())
        | ""Extract the Json Data"" >> beam.ParDo(Extract())
        | ""Upload the Data to BigQuery"" >> WriteToBigQuery(
            table=""project.dataset.table"",
            schema=schema,
            method=""STREAMING_INSERTS"",
            batch_size=1
        )
    )
    
    results = pipeline.run()
    results.wait_until_finish()

*Please ignore minor bugs, this code is only an example*

The workflow works, until I reach around 10,000 or more files, then I get an malloc error. I believe this is because my pipeline is attempting to process all the files at once, instead of streaming.

Any suggestions, best practices, or sample code snippets would be immensely helpful. Thanks in advance for your expertise!

I'm using Apache Beam with Python, in case that information is relevant.",2024-02-27 16:28:35
1b1el4v,Sharding Redis 101: Using Pixel Art To Explain The Basics,N/A,2024-02-27 15:29:27
1b1e5lm,What is the best way/tool for testing SQL,"Is it possible to test a SQL database (its procedures and so on)?    


And what is the best way to do it?",2024-02-27 15:11:29
1b17q60,I want to share a docker-compose to create your own geo-replicated cluster in a cup,N/A,2024-02-27 09:20:52
1b1b98v,Lakehouse engine,"Has anyone experience  of this lakehouse engine for data quality? 
",2024-02-27 12:59:21
1b1d94s,What is best way to add cluster in a pre-existing bigquery table ?,"By what i saw is not possible to add cluster directly on a existing table, but also can't do a create or replace adding the cluster, right now It seems like i would need to put the data in a temp table, delete the current and recreate getting the data from the temp table, but i wanted a better way since i need to do this for multiple tables that have some TBs each and i also didnt want down time/risk of missing data since there are processes that put data there and aren't in my control",2024-02-27 14:32:48
1b1cgba,Most unprepared round,What’s the subject round you realized you were very unprepared for once the round was finished? ,2024-02-27 13:57:29
1b1c0iu,Data Observability: The Next Frontier of Data Engineering,N/A,2024-02-27 13:36:41
1b1byww,Why Your Data Consultancy Might Be One Password Away from Disaster And 7 Ways To Fix It,N/A,2024-02-27 13:34:33
1b1a3ni,Data ingestion suggestion in azure,"Looking for your suggestions on ingesting product usage information stored in multiple source systems. 3 Source system which are updated with 10-15 records every second.  We have SQL db for which I'm thinking to use native connectors in ADF with CDC. Then I have XML files, thinking to use just copy service. Lastly azure analytics services I want to use azure function to make API calls. Thinking to dump everything to datalake in near real time ( may be every 10min once) and process further. Suggestion required on any better azure service or open source service for ingesting and things to consider while ingesting unstructured data!? Expecting near real time processing.",2024-02-27 11:56:53
1b18xcc,AWS or Azure?,"I currently have experience in AWS services related to data engineering. I've been working in AWS for nearly 2 years now since my company is following that stack. Other than those services in aws i have experience in mssql, postgress, tableau and data manipulation using python libraries.

When I'm trying to apply for new vacancies i see most of th companies are looking for experience in azure. So I'm thinking of where it's a good idea to move my stack to azure. 

Appreciate your ideas on this",2024-02-27 10:43:17
1b18850,Seeking Advice: Migrating from Control-M to Airflow - Automated Tool or Manual Approach?," 

Hello everyone,

We're in the process of decommissioning Control-M, and we're exploring options to automate the migration process. Our current setup includes customized calendars and numerous cyclic jobs with custom schedules, along with some manual triggers.

We're wondering if there are any automated tools available that could help with this migration. If not, what would be the best approach to tackle this migration given our customized setup?

Any advice, recommendations, or experiences with similar migrations would be greatly appreciated. Thank you in advance!",2024-02-27 09:57:04
1b17ax0,How we can connect Microsoft 365 business central SQL Database!?,"We have been utilizing the Navision database ever since we adopted Microsoft Navision. I am currently in the process of connecting it with a locally hosted SQL database, for which I employ a server instance login and password. Initially, everything was functioning smoothly; I utilized this connection for integrating with Power BI and generating other external Power BI reports. However, a new challenge has arisen: our IT department is scheduled to upgrade from Navision 2017 to Microsoft 365 Business Central. The consultant overseeing this transition has informed us that connecting to the SQL database from Business Central may not be feasible. 
Consequently, please tell me how to establish connectivity with the Business Central SQL Database.",2024-02-27 08:51:07
1b16o2d,Is DBT's main strength to help engineers execute transformation and load within data warehouse?," I'm a beginner of DBT, before learning DBT I thought it as some framework of packaging some SQL scripts and allow others to run packages to perform data transformation smoothly.

Now start learning concepts of DBT ecosystem, my understand is that DBT use the SQL under /models directory to extract data from DWH, transform and load to table/view in DWH.

Which means it's main work is to perform **transform and load within DWH itself.**

Is my understanding correct?",2024-02-27 08:07:08
1b14do2,"Enterprise-wide dimensional model, does it work?","Hi, does anybody here have experience with designing huge enterprise-wide dimensional models? I have extensive experience in designing dimensional models on the data mart level for BI tools. I often see mentions of dimensional modeling being used at the core of a DWH for an enterprise-wide (whole business) model. I struggle with understanding how that is designed. 

In my experience when talking about an enterprise-wide model we talk about a layer that contains cleansed data that all reporting, ML, and analytics use cases will be building on top of. That means that this model needs to be flexible (support many use cases, some of them not yet known) and is hard to change once designed (many dependencies). Without exact reporting requirements and the ability to easily change the model (e.g. grain of a fact table) later, how can a dimensional model be successful as an enterprise-wide layer? Thanks!",2024-02-27 05:47:44
1b13uiy,"""Best"" Next DE Certification to get?","I am leaving my company at the end of the week. They leave me with \~$250 bucks in credits that can go towards virtually any certification I want and this money will stay valid for the next 3 years I think. I am looking for which cert will best set me up for a career in Data Engineering / see what hiring managers are most looking for right now.  


I have 2 YEO and my last role was Jr. Solution Architect. I am certain I want my next role to be as a Data Engineer. Currently I have the AWS Certified Cloud Practitioner & the AWS Certified Solutions Architect Associate. These are great, but not really for showing DE expertise. What would y'all recommend for me to go for next? I'm up in the air about going ""tool-specific"" like SnowPro/Databricks/Kafka or getting a broader, more wholistic, DE cert like the IBM Data Engineering one. Any opinions would be appreciated.

&#x200B;

P.S. I will be doing personal projects on the side as well. I know it's foolish to rely solely on a certification to land a new role, but the credits are use it or lose it so might as well start working towards one.",2024-02-27 05:18:13
1b12old,"Using data integration tools like fivetran, airbyte etc vs building it from the ground up.","hi team,

hope you are crushing it this week.

I have a newbie question here which i am sure have been asked many times as well and here goes.

Question is, I have been trying to understand the benefit of using tools like fivetran and what additional value it has to offers. Taking the shopify integration as an example, the benefit of using fivetran to extract from shopify's api is the data model and integration with dbt. I see the appeal that using fivetran speeds up development time but aside from just speeding up development time, cant i just build it myself as all the documentation is readily available?

We are trying to understand if using an additional service like an integration tool justify the cost in the long run.

The other example that we are challenging with is also why use fivetran to integrate with SalesForce when we have our dedicated SalesForce and SAP Concur consultants that can work with to define the table and its relationship.

I am grappling with topic at the moment and i am not sure aside from ease of integration with other cloud tools like fabebook's api, twitter's api, SalesForce and SAP. What else am i missing if we do not use them and go just in house for everything?

&#x200B;

thank you for your wisdom on this troubling thought.",2024-02-27 04:16:20
1b0xukn,Synapse reliability on private endpoints ,"I have been struggling with synapse spark pools for the past two years.  I had never thought there would be a way for Microsoft to screw up Spark ... but I was wrong.

My Spark jobs fail dozens of times a day with random, meaningless exceptions coming from everywhere.  For example, simply reading a csv file (100 MB) from local storage containers in the same region with spark.read will fail regularly with some inane message (nullpointerexception).  The pg team says it is ""expected behavior"" when using their ""intelligent cache"" technology on a private managed vnet.

That is just one example.  The managed vnet (private endpoints) seems to be the common link between the many dozens of failures I encounter each day in my production batch jobs.  In short, I think synapse on managed vnet is a house of cards.  Anyone who has a choice should avoid this platform like the plague.  I recommend databricks or even HDI instead.

Please let me know if anyone has uses spark in a synapse vnet without suffering from continual socket exceptions.
",2024-02-27 00:29:37
1b0xiyh,Data Architecture: Generate Apple Music's Top 50 multiple playlists,"Hi Data Peeps,

I was recently asked this question and failed miserably at it. Wanted to get people's opinion on how I can do better on it.   


Ingestion: The click stream data would be read into Kafka. From Kafka, the data would be written into the data lake. 

Processing: We can run a daily spark pipeline to update the playlist.   


They told me the use case was the list needed to be updated every hour and use the previously computed values to generate the playlist. What if we have to generate multiple playlists like top 50 rap, hip hop. How do we optimize this design then?   


I wasn't able to answer that. Can anyone please help me with how to design such things? ",2024-02-27 00:15:19
1b0si79,Efficient Non-Deterministic Sampling of Large BigQuery Tables,N/A,2024-02-26 20:56:19
1b0r84s,dataninjago is an incredibly useful blog for everything Spark related,"Since I've never seen it mentioned here, I wanted to give it a shout

https://dataninjago.com/

",2024-02-26 20:06:32
1b0o4bb,Looking to Dive Deeper into Data Modeling. Recommendations for In-Depth Learning?,"I am a data engineer, and while I know the basics of data modeling, I feel like I should learn more. Do you know of any platforms, books, or study materials that teach data modeling in-depth? If you can share, it would be a great help for everyone 
",2024-02-26 18:03:49
1b0m2b3,From Finance to IoT,"I’ll be transitioning positions from financial reporting where speed to market is more important than efficiency to IoT devices where efficiency is more important than speed to market. Some data sets range into the billions to tens of billions of records.

The stack consists of Azure and Snowflake, with source code handling API calls. I’ve been reading up a bit on utilizing Go/Rust to handle the extract and load through continuous Functions for real time streaming and then DBT through a Logic App with functions as the runtime per DBT node (think tests and models).

Curious to hear if other folks have advice for things to consider when working with IoT, larger data sets, and Azure (my experience is in AWS so it’s a minor pivot).",2024-02-26 16:43:34
1b0lgyz,When marketing needs your help (heard through Hightouch),N/A,2024-02-26 16:19:35
1b0kypc,"Thoughts on Certified Data Professional cert, offered by ICCP?","My manager along with one of her HR buddies have had an informal conversation about what cert I should do as industry-standard in order to get me an in-role promotion from a data analyst to a data engineer

They have collectively agreed on the ""Certified Data Professional"", which is offered by the institute for certification of computing professionals, or ICCP, as part of its general database professional program.

&#x200B;

I posted on this sub earlier regarding vendor-specific certs and looks like AWS is the better one to take, even though its the harder one! But alongside that, shall I agree to take this ICCP one also? All will be paid for by the company so money is not an issue.

&#x200B;

Are there any udemy courses out there that walk me through this ICCP one? as their website seems abit old fashioned and I dont want to be stuck with just a big book to read from, need something more interactive!",2024-02-26 15:59:14
